{
    "Segment": "No documentation found.",
    "mParticle": {
        "Guides": "This section contains in-depth information about the mParticle platform and features.For information about our SDKs, APIs, and tools, visit theDeveloperssection.For information about our latest product releases, visit themParticle Changelog.",
        "CDP": "Customer data platform. mParticle includes functionality for CDP, as well as decision making powered by machine learning with Cortex, and behavioral analytics with Indicative.",
        "Analytics": "Analytics DocsOverviewGetting StartedDevelopersExpert Training",
        "Predictive AI": "Cortex DocsOverviewGetting StartedDevelopersUsing Cortex",
        "Learn More": "BlogPodcastResourcesCommunityChange log",
        "Get in Touch": "Help CenterDemoBecome a Partner",
        "Accessing Advanced Settings": "In Analytics, click on the gear icon and selectProject Settings.This will list all of your data sources. Find the Data Warehouse that you would like to edit and selectEdit Details.Click on theAdvancedtab.",
        "Exclude Events": " We typically pull all of the events in your data source, but if there are exclusions that you would like to make, place them here. We pull data with a blacklist approach, meaning everything will be ingested except specific exclusions. In theEvent Name matchesfield, include any event names to exclude when extracting your data. You can place a pattern or if there isn\u2019t one, separate multiple event names with a;.For example, if you want to exclude all events that start with \u2018test\u2019, put \u2018test*\u2019 in this field.ForProperty Value matches, we will exclude events based on property values.For example, if you want to exclude any events where device = \u2018test\u2019, place that in this field.",
        "Exclude Properties By": " Similar to the Exclude Events section, we will pull all properties and make exclusions with a blacklist. If there are any properties to exclude during extract, list them out in this field.For example, if you want to exclude the device_screen_size property, place that in this field.",
        "Context Field Exclusions": "",
        "Include Event Property Prefix": " Note: This option is only available for Snowplow schemas.",
        "User ID Null Values": " Some organizations may use non-null values to represent an undefined User ID. Please enter any values that are used to represent null User IDs in this field. We excludeNULLby default, so you do not need to specify that here. Note: If you use an empty string for null user IDs, put two single quotes and press Enter to mimic the picture above.",
        "Before you start": "Prepare to create a journey: Define what goals you wish to accomplish with the journey. For example, do you want to convert trial users to paid users, or encourage infrequent viewers to engage with their content? The goal you wish to accomplish will affect which inputs you select, and the criteria you\u2019ll define for each audience.Verify that you can create the number of activated audiences you plan to create in your journey. In mParticle, visitAudiences > Real-timeto see how many activated audiences are available:",
        "Step 1. Create a journey": "To create a journey: Log in to your mParticle workspace and go toAudiences > Journeys, and clickNew Journey.In the Create Journey configuration dialog, choose a name for your journey and select inputs from all the workspaces you wish to include.After selecting all the inputs in all the relevant workspaces, clickCreate.The Journey canvas displays your selected journey inputs.Click the plus sign to add a milestone, and then clickMilestone. An empty milestone is created.Click the milestone to add a name, and define the criteria. You are now in the real-time audience builder, and candefine criteria.ClickSave. The canvas displays the milestone you just created and the audience for that milestone.You can connect to an output now or wait until you\u2019ve created all the milestones. SeeConfigure connectionsfor details.You can either define the next step in the existing path by clicking the plus sign, or split the path by clicking the split icon, and create one or more additional milestones:Click the plus sign under theJourney Inputsyou just created to continue the journey, and clickMilestone. The following image shows a second milestone in the same path as the first one.Click the branch symbol above the milestone you just created to split the path into multiple journeys, and clickAdd pathto create a new path and milestone, orRemaining Users splitto create a milestone for all audience members who do not meet criteria for any other milestone at this level. The following image illustrates splitting the path with a remaining user milestone.You can create an A/B test for a milestone\u2019s audience by clicking the plus sign and selectingA/B Test (Random Split)To learn more, seeAudience A/B testing from a journey.You can\u2019t modify the criteria for a remaining user milestone.",
        "Step 2. Configure connections": "For each milestone: In the audience for the milestone, clickConnect Outputto select an output for the audience you just created. For more information about output connections, seeConnections. You can add one or more outputs to each audience. You can leave the audience inactive until you are ready to activate it, which starts sending audience members to the output. To change a connection status after you have created a milestone, click the down-arrow across from the connection name, then click the status badge to open the Connection Settings dialog, and click the button to activate or deactivate a connection. After a connection is activated, all parent audiences in the path are set to \u201cCalculating.\u201d Audience size estimates are updated to the actual audience size. After you\u2019ve added all the milestones and connected and activated all the outputs, you\u2019ve fully defined the journey.",
        "Deactivate an output": "If you wish to stop sending audience updates to one of the connections in a journey, you must either deactivate or delete the connection for that audience. To deactivate an output, click on the output name in the audience box, and then click theActivebadge to display the Connections Settings dialog where you can set the output toInactive.",
        "Delete a journey": "To delete a journey: Go toAudiences > Journeysand click a journey to open it.Click a milestone at the end of a path.ClickEdit milestone.ClickDelete.Delete all the milestones in the journey, then close the journey by clicking on the Journeys label at the top of the page.In the Journeys page, find the journey you wish to delete, and click in theActionscolumn for that journey, and selectDelete.",
        "How to Configure an Embedded Widget Filter": "Create an embedded widget or public dashboardCreate a Variant ID by calling the Variant Creation APIRequest a Dashboard or Widget variant by including the query string parameter \u2018dvid\u2019 with the Variant ID in the URL.",
        "Create a Dashboard or Widget Variant ID": "Analytics offers a secure API to create dashboard variants. Before requesting a variant on any of your public dashboards via the URL, it must have first been created using the API as described below. You may create dashboard variants by passing in your specified parameters as either a JSON encoded entity in your request or as query parameters on your request\u2019s URI. There are a number of prerequisites that must be completed  before calling this API:",
        "Prerequisite: Enable Dashboard Sharing": "To use Dashboard Variants, Public Access must be enabled for the dashboard. Navigate to a dashboard, click on \u201cManage Dashboard\u201d and select \u201cDashboard Settings\u201d from the menu at the top right.  Select Enable Sharing in the Public Access section. ",
        "Prerequisite: Retrieve the Dashboard ID": "To use the Variant Creation API, you will need the Dashboard ID for a dashboard. This can be found either in your Dashboard URL, or through the Analytics application. Dashboard IDs can be retrieved by observing the URL when viewing a dashboard in Analytics. Specifically, the \u201cdid\u201d parameter contains the Dashboard ID.  For example: https://app.indicative.com/#/dashboards?did=085045b7-f7bd-4378-9b56-f3ed0e571bd3&pid=04d5cec0-a25c-4d7a-864d-f7f7deece4fd The Dashboard ID and the full variant creation API URL can be retrieved through the Analytics Web Application. Navigate to a dashboard, click on Manage Dashboard and select Dashboard Settings from the menu at the top right.  Look for the section titled Dashboard Variant API Endpoint.  Click on the URL to copy to your clipboard. ",
        "Prerequisite: API Authentication": "Analytics\u2019 Variant Creation API requires HTTPS/SSL and uses Basic Authentication header for all requests. Basic Authentication is a simple authentication scheme built into the HTTP protocol. The client sends HTTP requests with the Authorization header that contains the word Basic followed by a space and a Base64-encoded string username:password. Use your Project API Key as the username and your Private Access Token as the password. This information can be found in the Analytics web application within Project Settings on the General tab. Please treat your Private Access Token as you would a password - it is meant to only be known to you.  An example curl : curl\u2019s \u2018-u\u2019 parameter automatically encodes the username and password and inserts them into the Authorization header. Depending on your client implementation you may need to do this manually.",
        "Creation API Endpoint": "https://web.indicative-prod.mparticle.com/service/dashboard/variant/{dashboardId}",
        "Creation API Request": "Requesting Analytics\u2019 Variant Creation API requires you to pass in your dashboard ID as part of the request URI as well as additional parameters to define the variant.",
        "Creation API Response": "The response to the Analytics\u2019 Variant Creation API is a JSON object including information about the dashboard variant.",
        "Date Ranges": "There are two methods of filtering by a date range, absolute and relative date ranges. If no date range is specified, the queries default date range is used. Parameter Name: dateRangeParameter Value: startRange,endRangeDate Format: YYYY-MM-DD (ISO 8601)Required: False To change the start and end date for all widgets, simply add&dateRange=to your URL string, followed by the start date and end date, comma delimited, in ISO 8601 date format. Use the word \u2018Today\u2019 to indicate that the endRange should always be inclusive of today. For example: &dateRange=2020-11-15,2020-11-30: This will filter the date range to be from November 15th, 2020 to November 30th, 2020 inclusive.&dateRange=2020-11-15,Today: This will filter the date range to be from November 15th, 2020 to Today inclusive. Parameter Name: dateRangeParameter Value: rangeNumber,rangeTypeDate Format: YYYY-MM-DD (ISO 8601)Required: False To change your date range to a relative date range for all widgets, simply add&dateRange=to your URL string, followed by therangeNumberandrangeTypefrom the options below, comma-delimited. HourDayWeekMonthYear Examples: &dateRange=7,Days: This will change the date range for all widgets in your dashboard to include the Last 7 days.&dateRange=4,Months: This will change the date range for all widgets in your dashboard to include the last 4 months. Special Cases:If a user changes the date range via the URL for funnel widgets with individual step based date ranges, we will remove the individual step date ranges, and apply the new date range to the entire funnel.",
        "Time Zone": "The time zone setting determines the display time zone for all events within the project. All new projects default to (GMT+00:00) UTC (00:00). Note, only one time zone may be selected per project. You are able to set up queries to be in a time zone that is not in your project time zone: The time zone selector is located in the top right corner of the query builder within each tool.",
        "Query Interval": "Parameter Name: intervalParameter Value:A string corresponding to the interval constant with which to run the query. The interval determines event timestamp grouping. For example, an interval of Day produces x-axis ticks for each day in the specified date rangeRequired: False In order to change the interval for all widgets, simply add &interval= to your URL string followed by the appropriate interval constant. If interval is not included, the query interval will not be changed. HourDayWeekMonthYear",
        "Query Filters": "Parameter Name: filter (multiple parameters can be included in one URL)Parameter Value: [match filter],[replace filter]Filter: [propertyName,propertyOperator,propertyValue],[propertyOperator,propertyValue]Required: False Details of individual queries within a dashboard or widget can be filtered based on matching specific parts of the query and specifying the values the corresponding match should be changed to.  The components of a query that can be modified include the property operator and the property value.  For reference, in the screenshot above, \u2018Browser\u2019 is the property, \u2018is equal to\u2019 is the property operator, and \u2018Chrome\u2019 is the property value. The syntax of a filter is a tuple corresponding to the matchingFilter, and the replaceFilter.",
        "Matching Filter": "The matching filter contains three comma-delimited parts, propertyName, propertyOperator, and propertyValue which will be used to isolate individual queries in a Dashboard or Embedded Widget. propertyName: The case sensitive property name to be matched. (This field corresponds to the raw property name, not the display value. See the Events and Properties section in the Analytics web application for more information. Must not contain commas \u2018,\u2019.)propertyOperator: The operator type to be matched. See the table below for valid values. Must not contain commas \u2018,\u2019.propertyValue: The case sensitive property value to be matched. Must not contain commas \u2018,\u2019. NOTE: In a Match Filter, the * symbol can be used as a wildcard match for any component.",
        "Replace Filter": "When a Matching Filter matches a query row, the replacement filter is applied to it. propertyOperator: The operator type to be replaced. See the table below for valid values.propertyValue: The case sensitive property value to be replaced. NOTE: In a Replace Filter, the * symbol can be used to preserve the Matching Filter value of a component.",
        "Multiple Filters": "Multiple filters can be applied to a single URL by including multiple filter query parameters.",
        "Property Operators": "The available property operators, and the corresponding constant. Change all instances of device_type is equal to \u201cweb\u201d to device_type is not equal to \u201cweb\u201d using the Query String:&filter=[device_type,eq,web],[neq,*] Alternatively, with JSON: Change all instances of device_type to device_type \u201cmobile\u201d using the Query String:&filter=[device_type,*,*],[*,mobile] Alternatively, with JSON:",
        "Viewing a Dashboard or Widget Variant": "Using the Variant ID generated using the Variant Creation API, add a \u201cdvid\u201d parameter (from the Creation API Response) to any Dashboard, Public Dashboard, or Widget URL Request.",
        "Example": "Standard Dashboard URL:https://app.indicative.com/#/dashboards?did=085045b7-f7bd-4378-9b56-f3ed0e571bd3&pid=04d5cec0-a25c-4d7a-864d-f7f7deece4fd Variant Dashboard URL:https://app.indicative.com/#/dashboards?did=085045b7-f7bd-4378-9b56-f3ed0e571bd3&pid=04d5cec0-a25c-4d7a-864d-f7f7deece4fd&dvid=12345-6789-abcde",
        "Updating a Dashboard": "Dashboard variants are read-only. They can only be modified by updating the original dashboard within your project. Changes made to the original dashboard will propagate to their variants the next time the variant is refreshed server-side. The timing for the server-side refresh may vary. When refreshing dashboards, Analytics uses an algorithm to balance data \u2018freshness\u2019 and efficient resource utilization. When first applying a new filter to a dashboard or widget, please allow up to 15 minutes for changes to be reflected in the dashboard\u2019s variants. Subsequently, each request to render a dashboard or widget will display the most recently cached result before initiating an update.",
        "Overview": "The Users tool returns a list of users who have either performed an event or are in a user segment. Each line in the visualization area represents a single user. Each user will be identified by their User ID (anonymous or known, see theAliasingarticle for more information), along with each user\u2019sUser Properties. For more information on how to create a Users query, see theUsers: Getting Startedarticle.  Click on any user in the Users chart to see their individual event stream. Clicking on the user, or the row containing the user\u2019s User Properties will direct you to a User Activity Stream.  The User Activity Stream contains the following components: A user profile displaying User PropertiesA visualized Timeline of the number of events a user has completed on a given dayAn activity stream displaying the events performed by the user, grouped by session Click on a session to view the events performed in a chronological order.  Click into a specific event to view the event properties associated with the specific event. ",
        "Metrics": "The following metrics are available: Invocations- how many times the rule was invokedThrottles- how many times a 429 throttling response was returned when calling the ruleErrors- how many errors have occurred when calling the rule These metrics are for the last 24 hours and apply to all connections. Summaries for each rule can be seen on the main rules page. Detailed graph of the previous 24 hours is available on theMonitoringtab of the individual rule page. ",
        "Path insights": " Path Insights displays a variety of important funnel metrics. To access Path Insights, select an event in your funnel. Pathdisplays which step users completed before completing the selected step.Total Dropdisplays the percentage of users lost in the progression from the previous step to the selected step.Total Conversiondisplays the percentage of users that made it from the previous step to the selected step.Average Step Timedisplays the average amount of time it took users to get from the previous step to the selected step.",
        "Conversion precision": "You may adjust the precision of your Funnel query using the Conversion Precision setting. In a sequential funnel, events in the must occur in sequential order. In an approximate funnel, events may occur simultaneously. All new queries default to Sequential. For a full explanation, see our article onConversion Precision. ",
        "Zoom": "You can zoom your view of the Overview Map in and out by clicking the + or - magnifying glass icons in the bottom left corner of the map. ",
        "Funnel flexibility": "Funnels typically follow the critical steps to complete a user journey. For example, a commerce app would have browse, cart and buy as steps. But Analytics Funnels support more complex business logic including optional steps. The only steps in Funnels that are required are the first step and the last step. All others can be flagged as optional. When users flow through the optional steps, there is path exclusivity. That means that a user can appear in only one path or if the user appears multiple times, it is once for each path that they completed. In the commerce example, this could be the product compare feature or a size chart. For more information, see theMultipath Funnelarticle. Funnels allow you to test the importance of funnel steps by setting steps to inactive without completely deleting them. This is useful for understanding the impact of a step in overall conversion. And once your key funnel reports are in use, the trending option helps you monitor funnel performance over time. When there are key user characteristics that significantly contribute to conversion patterns, funnels can be built to group together users by an additional attribute. User by age group or user by sign up channel are interesting ways to dig into how users interact with a digital property. Depending on the attribute selected, users may land in the funnel more than once. Consider a user who visits first from search and then shortly after from a social ad. The grouped funnel can handle the complexity of treating those visits separately to understand which inbound channel has a more positive impact on conversion.",
        "Conclusion": "Journey reporting lets you explore common flows through your digital properties. These flows can help you identify points of friction. They can also uncover different flows through the site that you can then formalize and monitor with Funnel reports.",
        "How to Use It": "To apply this feature, follow these steps: Select the user attribute you want to filter by.Choose either\u201cAt Event Time\u201dor\u201cMost Recently\u201das the filtering method.Analyze user behavior with precise, context-specific insights. ",
        "Filtering options": "When building queries, you have two options for filtering events by user attributes: At Event Time:Uses the attribute value as it was when the event occurred, preserving historical accuracy.Most Recently:Uses the latest attribute value in the user\u2019s profile, offering real-time insights.",
        "Implications of both filtering options": "Consider a user who made a purchase while living in Seattle and has since moved to New York: Selecting theat event timefiltering option would show Seattle as the location.Selecting themost recentlyfiltering option would show New York as the user\u2019s current location. This helps ensure that historical reports, audience segmentation, and campaign targeting remain aligned with either the historical or current context of user attributes.",
        "Example use cases": "This feature benefits customers who use mParticle\u2019s CDP for analytics, segmentation, and personalized experiences. It\u2019s particularly useful for: Historical audience segmentationCohort analysis over timeAccurate event filtering for reporting",
        "Supported Tools": "You can apply this filtering option in: SegmentationFunnelsCohorts",
        "Example Use Cases": "Retaining users is crucial for the sustained success of a mobile application. By identifying and re-engaging inactive users, you can boost overall engagement and reduce churn. Goal:Improve user retention by targeting users who have not opened the app in the last 7 days.Segmentation Strategy:Identify users who have been inactive for a week but used the app actively within the previous month.Engagement Strategy:Send personalized push notifications offering a discount or exclusive content to re-engage users. Leveraging predictive analytics allows businesses to anticipate user behavior and proactively engage those most likely to convert, thereby optimizing marketing efforts. Goal:Increase conversion rates by targeting users predicted to make a purchase in the next 7 days.Segmentation Strategy:UtilizePredictive Audiencesto identify users with a high likelihood of purchasing based on machine learning models analyzing past behaviors and interactions.Engagement Strategy:Deliver personalized email campaigns featuring product recommendations or special offers to these high-likelihood purchasers. Identifying subscribers at risk of churning enables proactive engagement strategies to maintain a stable subscriber base. Goal:Minimize churn by identifying at-risk subscribers.Segmentation Strategy:Identify users whose subscription renewal is within the next 30 days and who have decreased engagement (e.g., fewer logins or interactions).Engagement Strategy:Offer these users tailored incentives such as a loyalty bonus or a discounted renewal rate to encourage continued subscription.",
        "Create a New Audience": "Once you have clearly defined your use case, it\u2019s time to begin building your audience.",
        "Navigate to the Audience Creation Page": "From the mParticleOverview Map, selectSegmentation.On theAudienceslanding page, clickCreate New.",
        "Configure Your Audience Group": "Individual audiences are contained within folders calledAudience Groups. The first step in creating a new audience is to set the configurations for this folder:  Enter a name for your folder.Select your Inputs (the platforms and feeds that will supply data to define the audiences within your folder).ClickCreate.",
        "Define Your Audience": "After saving your audience group, you\u2019ll enter theEditor. Here is where you can add, view, edit, and connect audiences. Follow the steps below to create your first audience within this folder: Click theAdd criteriabox to open theAudience Builder Modal.In the audience builder modal, choose the environment (Production,Development, or both) from which the audience will receive data. (Note theaudience environment considerationsbelow.)ClickAdd Criteriato begin adding criteria for your audience.Select the type of data you would like to use for your first criteria (more oncriteria typesbelow).Referring to thedata types and matching rulesbelow, use the criteria editor to further target the users who fit your use case..Once you have added your first criteria, clickDone. After you have added your first criteria, a number displays that represents the estimated audience size:.\nThis estimate is based on a sample of data. As you continue to add criteria, you will see an estimated size for both individual criteria as well as for the whole audience. Continue adding more criteria usingAnd,Or, orExcludelogic to further refine your audience.",
        "Audience Preview": "After defining your audience criteria, you can use thePreviewtab to inspect a sample of users that match your current segmentation. Audience Preview helps validate audience composition before activation, reducing errors and improving targeting accuracy.",
        "How to Use Audience Preview": "Navigate to thePreviewtab in the audience editor.View a list of sample users that meet your audience criteria:Each row in the preview includes:mParticle ID\u2013 A unique identifier for each sampled user.Last Seen\u2013 The most recent interaction timestamp.February 27, 2025",
        "Explore users from a funnel step": "To explore a list of users from a funnel, choose any data point or table cell within your results. Within the dropdown, select Explore Users. If your query contains a series, you may choose to view users from a single data point or from the entire series. From this Point creates a list of users from only one interval, whereas From Entire Series creates a list of users from all of the intervals within the date range.  To explore a list of users from a single point, choose any data point or table cell within your results. Within the dropdown, select Explore Users. ",
        "Explore all users who entered the funnel": "To access a list of all users who entered the funnel during the date range, click on or in the circle representing Step A in the results field. Within the dropdown, select Download Users in this Point to CSV.  Alternatively, you may select Column A in the table at the bottom of the results field. Be sure to find the row labeled Overall to view all users who entered the funnel.  If your funnel contains a breakout, you may choose to view users from a single breakout or from the entire step. From Breakout creates a list of users from only one breakout whereas From Entire Step creates a list of users from all of the breakouts. ",
        "Explore users from a single path in a multipath funnel": "To access a list of all users who completed a single path within a multipath funnel, select the circle representing the last step in the results field. Then, select the relevant path from the table within the Path Insights dropdown. Within the dropdown, choose Explore Users. The relevant path will be highlighted in the results field upon hover.  It is not possible to select a single path within a multipath funnel from within the table at the bottom of the results field.",
        "Explore users in Cohort": "To analyze a funnel path in Cohort, select the last funnel step in the visualization area. Then, find the path you would like to analyze, and select Analyze as Cohort.  Indicative will then map your user journey using our Cohort tool. Here, you can get more granular in examining retention on an interval basis. Please note that Indicative will automatically set the breakout to be a time generation. For more information about our Cohort tool, seeCohort: Getting Started.",
        "Create a Scheduled Report": "To create a scheduled report, you must first create a dashboard to send it from. For more information on how to build a dashboard, seeGetting Started: Dashboards. If you already have a dashboard that you\u2019d like to use, navigate to that dashboard via Saved Analyses & Dashboards in the left navigation menu. For best results, it\u2019s recommended that your dashboard be inPrint Modebefore scheduling a report. If you desire to keep your report in Screen Mode, then Analytics recommends duplicating the dashboard and creating a Scheduled Report from the duplicate. Once you have selected your desired dashboard and changed your dashboard layout mode to Print Mode, open the Reports dropdown in Dashboards settings at the top right of the dashboard. Here, you may create a new Scheduled Report or view your existing Scheduled Reports. We recommend that you check and see if someone else within your organization has already created a Scheduled Report from this dashboard. If there is an existing report, you may open the report to edit the recipients. Once you have selected \u201cNew Scheduled Report\u201d, a sidebar will appear on the right side of your screen. The default title of a Scheduled Report will be the title of the underlying dashboard. You may edit the title, select the frequency of the report, and select the recipients. You may schedule your report to be sent daily, weekly, monthly by day (i.e. first Monday of the month), or monthly by date. Next, choose the time at which the dashboard will refresh and the report will be sent. Scheduled reports are sent on the hour. Please note your project time zone when saving scheduled reports, taking into acco\u2026 In the recipients field, enter the email addresses to which you would like to send the report. Recipients do not need an Analytics account to receive a scheduled report: Analytics users will receive a PDF of the dashboard and a link to the dashboard itselfNon-Analytics users will receive a PDF of the dashboard and a public URL Remember to review the reports\u2019 editing privileges. You may choose to enable editing privileges to your entire team.",
        "Send a Test": "Analytics recommends sending a test report to your email to confirm that the scheduled report is configured correctly. Remember to review the attached PDF to ensure that results are correct and up to date. The test report will be sent to the email associated with your Analytics account. Click \u201cSave Report\u201d to save your scheduled report settings. Once you click \u201cSave Report,\u201d your scheduled report is confirmed, and it will be sent at the interval selected in the previous steps.",
        "Manage your Reports": "View your existing Reports by selecting the Settings icon on the left navigation menu and navigating to Scheduled Reports. Here, you can view all of your organization\u2019s Scheduled Reports in a table format, including the following information: Report titleThe time the report is scheduled to be sentThe creator of the reportThe time that the report was created Additional report actions are available by choosing the three-dot menu on the right of each report, including: View DashboardEdit SettingsUnsubscribeArchiveDelete",
        "Archiving and Deleting": "Archiving and deleting are the two methods of disabling an existing scheduled report. Archiving a report will disable the report indefinitely. Archived reports will be greyed out in the Scheduled Reports list, which can be filtered using the settings on the top right. To unarchive a report, select the three-dot menu to the right of the report and choose \u201cActivate\u201d. Deleting a scheduled report is permanent and irreversible. Only do so if you are sure that the report is no longer relevant. The underlying dashboard will remain, but the report will be permanently deleted.",
        "Table View": "Selecting Table View allows ytable-viewou to view the underlying data of your analysis. Data is broken out along the X-axis by intervals of the selected time interval. Along the Y-axis, data is broken out byquery rowspresent in the query builder, ordered by row number.",
        "Aggregate": "In Table View, you can choose the \u201cAGG\u201d button to view aggregate numbers. This will add an extra column at the far right end of the chart showing aggregate numbers for each row. Count shows you underlying data that is visualized the amount of users for each time period. These are the same values that you see when hovering over data points on the graph.",
        "Average": "In Table View, you can choose the \u201cAVG\u201d button to view averages. This will add an extra column at the far right end of the chart showing the average of the numbers in each row.",
        "Count, Statistics, and Breakout Modes": "There are three modes that can be explored in table view. Count\u00a0mode displays counts of users, broken out by the selected time interval. These are the same values that you see when hovering over data points on the graph.Statistics\u00a0mode shows statistical summaries that are automatically calculated from the raw data. This gives you access to information that can help identify important trends in your data and build out analysis.Breakout\u00a0mode allows you to use the event and user property breakouts as column headers. To use Breakout mode, each query row must have the same event or user property breakout applied. X- and Y-axes are switchable via the double arrow button on the upper left corner of the table.",
        "Line Chart": "Selecting Line Chart will display analysis results as a graphical display of information that changes continuously over time.",
        "Bar Chart": "Selecting Bar Chart will display analysis results as separate sets of vertical bars above each period in the selected time interval.",
        "Stacked Bar Chart": "The Stacked Bar Chart is a visualization option available when an event has beenbroken outby its component event properties or user properties. The option to use a Stacked Bar Chart appears after aGroup By clausehas been added to one or more query rows.",
        "Pie Chart": "The Pie Chart is a visualization option available whenbreaking outa single event by an event property or user property with Full Range as thetime interval.",
        "Combination Line/Bar Chart": "The Pie Chart is the default visualization when performing acalculation. After performing a calculation, selecting either the Line Chart or Bar Chart options will present the same visualization. The bars represent the component events of the calculation, broken out by time interval, and the line represents the results of the calculation across time intervals.",
        "United States Map": "This chart type is used to break out a single event using location-based data; for example, if you wanted to see email opens broken out by state. Currently, this chart type only supports locations within the United States.",
        "Define a conversion window": "To define a conversion window, click on Conversion Settings, located within the menu bar beneath the query builder, then select Limited. Next, you may select whether to apply the conversion window to the entire funnel, or to each step within the funnel. ",
        "Entire funnel": "The options for funnel conversion windows are: 5 Minutes, 1 Hour, 1 Day, 1 Week, 1 Month, Custom, or Entry Date Range. Custom allows you to define a new conversion window, for example two minutes or six months. Entry Date Range will include only users who complete the Funnel within the date range defined in the query\u2019s date range. This appears after Entered Funnel in the date range selector located in the menu bar beneath the query builder.  In the following example, each user in the funnel parameters (A) Site Visit, (B) Blog View, (C) Create Profile, and (D) Subscribe must complete their conversion within one week.",
        "Each step": "Use Each Step Tracked By mode when you would like to track a funnel through an event property, but the event property has different naming conventions for each event. For example, PetBox may want to track the user journey Site Visit - Open App - Purchase Product tracked by Session ID, but the property that captures Session ID is named Cookie ID for the event Purchase Product. You may select 1 tracking property per step in Each Step mode. To switch to Each Step Tracked By mode, select settings from the menu bar, just below your query builder. From there, toggle the tracked by mode line from Simple to Each Step.  Once you have selected Each Step, you must then input an event property to track by in each row. A tracking property must be selected for each step in the user journey in order to run the query. Note that if you already had an event property selected in Simple tracked by mode, this property will populate the tracked by line in each query row in Each Step mode.  By using Simple tracking properties, PetBox is able to ensure that each user journey in this Funnel occurs whilst holding User IDandSession ID constant. However, what if the Purchase Product event uses Cookie ID instead of Session ID, but they represent the same values? In order to account for this, PetBox should use Each Step mode. Then, in the tracked by inline, PetBox should select Session ID for the events Site Visit and Open App, and select Cookie ID for the event Purchase Product. Now, PetBox has now ensured that all events in the user journey are completed by the same user, and in the same session. Thus, this Funnel analysis is counting user-property pairs, and not just users.",
        "Questions": "How does interaction with the orientation screen affect onboarding flow conversion?What is the impact of the A/B test on onboarding flow conversion?",
        "Create Funnel Visualization": "To create our funnel, let\u2019s drag in the events in our new onboarding flow. Connect PetCam: Within the app, users click this button to begin the flow.Open PetCam: Once connected, users open the PetCam and start streaming.Subscribe: Users who appreciate the service will subscribe for future access. ",
        "Make a Step Optional": "The eventStart Support Chatis an optional action by the user. A user may choose to skip the orientation screen or go through the orientation, before progressing through the flow. However, our current funnel requires users to have completed the eventStart Support Chat. To also see what our numbers look like for users who chose not to Start Support Chat, we can utilize Analytics\u2019optional stepfunctionality.  Let\u2019s click the pushpin to the left ofStart Support Chatin the query builder.Next, let\u2019s execute the query to update it. Above, we can see a Funnel visualization with multiple paths. It is currently showing the onboarding flow with theStart Support Chatevent included. As we can see, the numbers are the same as in our initial funnel.  Let\u2019s click on the path aboveStart Support Chatto look at the flow that excludes this event.",
        "Observation": "We see \u201cnot enough data for visualization\u201d which means there is no data available\nfor this query. It looks like Cat Food Purchasers did not purchase cat toys\nat all since April. At this point, you can analyze further by changing the\ndate range to see if these users purchased any cat toys in the past or start\na marketing campaign to target these users for cross-selling.",
        "Create a Shared Breakout": "TheShared Breakoutfeature in Funnel allows us to segment a specific step using a property and follow those users throughout the flow.  UnderneathConnect PetCamin the query builder, select +group by. ChooseAB Groupunder Event Properties.By default, the query results will be grouped by the first events\u2019 AB test. This is because we want to measure what happens to users after seeing the different instructions for connecting the PetCam. This segments the users who didConnect PetCamby their group and follows them through the rest of the funnel. We can then measure and compare conversion rates and drop off.",
        "Compare AB Groups": "By clicking on a wedge representing either Group A or Group B, we can highlight this group. After a group has been highlighted, the percentages in the webbings will only reflect step-to-step conversion for that group.  Above we can see that 94.63% ofGroup Ausers progressed fromConnect PetCamtoOpen PetCam. ForGroup B, this number was 94.91%.",
        "Audience sharing permissions": "Permissions allow the account which owns an audience to define what data is shared with any receiving accounts. Permissions can be set per-audience for each account in your organization.",
        "Create default audience-level permissions": "From your Settings page, navigate to theAudience Settingstab. From here you can view and edit current default audience-level sharing settings or add new defaults.  Note that any changes to your defaults willonlybe applied to any new audience created. Changing your defaults won\u2019t update existing sharing settings for current audiences.",
        "See which audiences are shared": "You can access all audiences that are shared with you via theShared with metab of theAudiencespage. TheAccesscolumn shows the level of access your account as been granted. ",
        "Create or update a shared audience": "To share an audience or to update the access settings for a shared audience, start by navigating to the mainAudiencesview.Select the audience you would like to change.Click the details icon to view theAudience Details.Click the gear cog next toAccess Levelsto view the access sharing settings.Select an account within your organization to share the audience with by clicking the+button. You can change thepermission levelfor each account via the dropdown menu underShared Level.ClickSaveto save any changes.",
        "Respond to an access request": "Users of an account with \u201cview only\u201d access are able to submit a request for additional access. A notification email is sent to the creator of the audience. If you choose to grant the request you can update the audience permissionsas above. You can also seek clarification from the requester by replying to the email.",
        "Request \u2018usable\u2019 access for a \u2018view only\u2019 audience": "If you have \u201cview only\u201d access for an audience, you can request a different level of access.  To make a request you must provide: The sharing level you are requesting.The date range of the campaign you are proposing.Details of the proposed campaign.Activation details, including the proposed campaign platforms.  The owner of the audience will be automatically notified of your request by email and may request further details by email reply.",
        "Identity-level permissions": "In addition to setting access permission for each audience, you can choose whether to make each identity type available to each account when you share audiences. For example, you can choose to make Google Advertising ID and Apple IDFA available to a particular account, but email unavailable. These settings are at the account level and apply to all audiences shared from the account. These identity filters applyonlywhen the user hasn\u2019t already been seen in the receiving account. Only users with Admin access can manage identity-level permissions. Navigate to yourSettingspage.Navigate to theIdentity Settingstab and scroll to theIdentity Sharingheading. From here you can see how many accounts have been granted permission to receive each identity type, and add new permissions.For each identity type, you can view a list of current permissions and add or update permissions by account.",
        "Saving to a Dashboard": "When you select Save into Dashboard, a pop-up window will appear. Use the dropdown menu to select the dashboard in which to save your Segmentation query. If you want to save your query to a new dashboard, you can create a dashboard from the save window.",
        "Saving Your Analysis": "If you select Save Analysis, a pop-up window will appear. Here, use the dropdown menu to select the folder in which to save your Cohorts analysis. ",
        "Saving Modified Queries": "If you access and modify a query or an analysis from a dashboard, choose the Save button in the top right corner above the query builder to replace the existing file. If you want to retain the original analysis, then save the modified query as a new dashboard analysis or saved analysis. Don\u2019t forget to change the query name in order to differentiate between the two! ",
        "Journey workflow": "At the start of the journey, you have access to all the users available from all the inputs from all the workspaces in your account. You choose the workspaces and inputs you wish to select audience members from, and then build the journey: In a series of steps called milestones, you break down all the paths taken by your users within a customer lifecycle stage. For example, let\u2019s say that a user signs up for a trial account, interacts with free content, and then saves some content for later. Milestones define the steps that users take or that you want users to take to achieve a set of goals such as sign up, makes a purchase, or become a repeat customer.Each milestone generates an audience that you can forward to an integration to convert users from one step to the next in their journey. Following on the previous example, if you see a drop off between \u201csign up to trial account\u201d and \u201cwatches content\u201d milestones, you can activate the trial account audiences and send it to a CRM partner to send promo emails that help attract and bring content to non-engaged users.Keep defining milestones until you\u2019ve reached the final goal for the customer lifecycle stage. Using the previous example, the \u201cconverted to paid user\u201d milestone is the last milestone in the journey.Verify that the data flow is behaving as you expect using the same tools and techniques you use for an integration.When a journey needs to be changed, you can modify or delete milestones. The following diagram shows a simple journey with two milestones: In this journey, all customers who engage with the app are sent an email, and those who open the email are sent a mobile push notification. Notice the following: The audience is displayed in a box underneath its milestone. You can connect to an output here, or view the status of the output with the down-arrow.If it can be estimated, the audience size displays in the audience box.The path symbol appears above each milestone, and a plus sign appears at the bottom of each milestone. Click the path symbol to split off additional paths and milestones, or click the plus sign to create a new milestone on the same path.You can tell whether an audience has been activated by the green on/off icon in the audience. If grayed out, the audience isn\u2019t active, if green, it is.",
        "Journey path splits": "Each step in the customer journey can be split into additional paths. For example, you could define a set of milestones for customers who buy handbags, shoes, or winter coats. Each milestone becomes the start of a new path. You can also create a milestone for all audience members that haven\u2019t fit any previous milestone criteria. This split is called a remaining user split.",
        "Split behaviors": "The first milestone and audience definition is the left-most milestone, and milestones are added left-to-right.Splits are mutually exclusive and evaluated left to right. Audience members are placed in the first audience where they meet the milestone criteria.If you create a remaining user split, it displays as the last (right-most) milestone.You can create a remaining user split after the first milestone for a path is created.",
        "Examples": "The tutorials in this guide follow the process of setting up mParticle in the mPTravel app: a mobile and web app that sells luxury travel packages to its users. Later on in this guide, you\u2019ll learn about sending data from mParticle to some of our many integration partners. As examples, the tutorials use services which are simple to set up and verify, and which offer a free account tier, so that you will be able follow the examples exactly if you wish. However, mParticle is agnostic about which integrations you choose and you can follow the same basic steps from this guide to implement any of our integrations.",
        "Use predictions to reduce churn and vary customer communications": "Powered by Cortex, customer-centric teams can predict a user\u2019s likelihood to churn. Using that prediction, teams can deliver a unique set of experiences for users who have a high likelihood to churn.  In this example, a brand engages high-churn risk users on multiple channels to ensure they get the message. In addition, the brand sets up a fail safe for the users who received an email but didn\u2019t open it, engaging them over SMS.",
        "Engage customers in the right channel based on customer consent": "One key piece of user preference is their consent status, what they\u2019ve told the brand about how they want their data to be used for marketing purposes.  In this example, an eCommerce retailer delivers two different kinds of experiences based on a user\u2019s consent to GDPR. If users have consented, the retailer sends a mobile notification as a reminder to convert. For the remaining users, those who have not consented, the retailer triggers an in-app coupon if the user returns after several days.",
        "Trigger post-purchase sequences based on a customer\u2019s lifetime value": "For many retailers, a user\u2019s purchase of a product is just the beginning of the relationship between the customer and the brand. After a purchase, the retailer can suggest to the customer many possible next steps to prolong and deepen that relationship: purchasing more products, leaving a review, referring a friend, posting on social media, and more.  In this example, retailers trigger the next best step based on a user\u2019s lifetime value.  For high LTV customers, brands can assume that they\u2019re fans of the brand, and would be more willing to refer a friend. If the retailer knows a loyal customer\u2019s preferred engagement channel, they can communicates with them there. For the remaining users, those who are not high LTV customers, a retailer can recommend products that pair well with the one that a customer just purchased.",
        "Audience estimator": "Each audience that you create in a journey provides an estimated audience size immediately, so that you don\u2019t have to wait for the audience calculation to complete. Once an audience has at least one active connection, audiences and all parent audiences in same path begin calculating the real size. When an audience begins calculating it no longer shows the estimated size.  To estimate the audience size quickly, mParticle samples the total number of users. You see the estimated size of the audience with all criteria applied (as shown in the previous image). Estimated audience size per criteria is also displayed on the milestone. Use the audience estimator\u2019s immediate feedback to adjust criteria definitions and parameters if needed: The audience size is much bigger or much smaller than expected.Your team wants to explore different ways to target your customers.You see a big drop-off of users between milestones and want to target an intermediary moment in the journey, to nudge more users toward your conversion goal. In some cases, you may see different symbols instead of an estimated size: The symbol~indicates that the population is too small relative to the overall user base, which prevents a meaningful calculation. For example, imagine a company that has 100 million users. If you create an audience that will have 13,000 members when fully calculated, it\u2019s likely that the random sample won\u2019t encounter enough members to be represented in the estimate. This symbol doesn\u2019t mean your audience will have no members, just that it will have so few members relative to the total number of users that estimation isn\u2019t possible.If an audience can\u2019t be estimated for a technical reason, you\u2019ll see a red triangle with an exclamation point instead of an estimated size. For example, if the input is not configured correctly, you\u2019ll see this warning sign.",
        "Journeys and billing": "When an audience is actively connected, that audience is activated and consumes a real-time audience credit. Unlike the real-time audience experience, there is no explicit audience status of Draft or Active. The status is now derived from the connection status. Activated audiences count toward your account limit.Parent audiences are calculated, but do not consume additional real-time audience credits. To view the number of audiences available to you, in mParticle go toAudiences > Journeysto display the list of journeys. The number of activated and available audiences is displayed under the New Journey button:",
        "Cumulative and Non Cumulative": "When you run a Segmentation query, the default will be set to the Non-Cumulative setting. The Cumulative setting essentially sums up the data points as you move from left to right and shows an increasing series. The Cumulative setting is not available when calculating the total count of the number of users who performed a particular event.",
        "Aggregate and Average": "When your Segmentation query is set to the\u00a0Table\u00a0or\u00a0Metrics\u00a0visualization type, you may choose to display either the aggregate or the average.",
        "Table": "Aggregate:\u00a0Select\u00a0the AGG button to view aggregate numbers. This will add an extra column at the far right end of the chart showing aggregate numbers for each row.Average:\u00a0Select the AVG button to view average numbers. This will add an extra column at the far right end of the chart showing average numbers for each row.",
        "Count vs. Statistics vs. Breakout": "If you choose to visualize using the\u00a0Table\u00a0visualization, you may also choose to show your analysis in Count, Statistics or Breakout modes. Count\u00a0mode displays counts of users, broken out by the selected time interval. These are the same values that you see when hovering over data points on the graph.Statistics\u00a0mode shows statistical summaries such as range, standard deviation and confidence intervals. This is calculated from the raw data, and will give you access to information that can help identify important trends in your data and build out analysis.Breakout\u00a0mode allows you to use the event and user property breakouts as column headers. To use Breakout mode, each query row must have the same event or user property breakout applied.",
        "Decimals": "You may also customize the decimal places for any visualization type in the Segmentation tool. For example, if you\u2019re analyzing revenue, you\u2019ll likely want to set the decimal places to .00. To do so, select the decimal type in the menu bar above the visualization area, and select your desired decimal format.",
        "How to access the new UI": "You can toggle the new UI on or off any time, at your discretion. To turn the new UI on: Log into your mParticle account atapp.mparticle.com.Click theSettingsbutton at the bottom of the left hand nav bar.ClickGo to New Experience.",
        "How to return to the old UI": "To switch back to the original UI: Log into your mParticle account atapp.mparticle.com.Click theSettingsbutton at the bottom of the left hand nav bar.ClickGo to Classic Experience.",
        "The evolution of mParticle\u2019s UI": "At mParticle, we understand the importance of staying ahead of the curve. That\u2019s why we\u2019re proud to introduce a user interface that doesn\u2019t just update the aesthetic of the platform, but expands its functionality. Inspired by a combination of customer insights and requests, and industry trends, our team has meticulously crafted a user-centric interface that simplifies complex tasks, exposes previously hard-to-find features, and presents a comprehensive, navigable map of your data infrastructure. There are three cornerstones to the new UI: A comprehensive overview map of your entire mParticle implementationAn interactive \u201cmini-map\u201d that allows you to jump between mParticle suitesA contextual left-hand nav bar Continue reading for a detailed overview of what\u2019s changed, and what\u2019s new, in the mParticle UI.",
        "Changes to existing features": "After switching to the new UI, the first thing you\u2019ll notice is a detailed schematic of your entire mParticle implementation. Your data inputs are listed on the left, your connected outputs on the right, and all of the mParticle features and products that you use to manage your data are shown in the center. This is the new mParticle Overview Map, and its job is to give you a bird\u2019s eye view of how data flows through your particular implementation of mParticle. Think of it like an interactive transit map for your data. Every route and station is clearly labeled, and you can click on each input or feature to configure its settings. Every workspace\u2019s overview map will look a little different, depending on the exact inputs, outputs, and features that are configured. You\u2019ll also notice that the left-hand nav bar disappears when viewing the Overview Map. When viewing the Overview Map, navigate to the feature or suite you\u2019re interested in by clicking directly on the map. Once you\u2019ve navigated to a suite, you\u2019ll see the new contextual left-hand nav bar appear.",
        "The Overview Map: your guide to mParticle": "The Overview Map illustrates the direction your data travels in, from your inputs to your outputs, including the various features and tools it passes through along the way. For a detailed guide on how you can interact with your Overview Map, refer to the Overview Map user guide. For a quick summary of what the Overview Map can do, keep reading below.  Your data inputs includePlatform inputs(such as iOS, Web, or Android) andFeed inputs(such as third-party marketing tools or data warehouses).  The mParticle Data Platform includes the different tools and features you use to manage, manipulate, and leverage your data before sending it to your outputs. These tools can be separated into several product suites:  The Overview Map displays all of yourEvent outputs, third-party marketing and data warehouse tools where you can forward your event data. ",
        "The minimap": "We have also added an interactive mini-map that you can use to quickly jump between suites, no matter where you are in the platform. To access the mini-map, hover your cursor over the mParticle button in the bottom of the left-hand nav bar. Click any suite to navigate directly to that area of the platform. ",
        "The contextual navigation bar": "In addition to the mParticle Overview Map, the new UI includes a contextual left-hand navigation bar that makes it easier to access (and navigate between) specific features within each mParticle suite. While the new overview map shows you what features are related, and where they sit in your data\u2019s journey through mParticle, the updated left-hand nav bar shows you the most relevant tools and options for each specific mParticle suite. The new contextual nav bar includes a Jump To menu that shows links to other areas of the product that are the most relevant features for the particular suite you are viewing. Simply hover your cursor over Jump To in the left nav bar to view these options. When viewing the Oversight suite, the left-hand nav bar displays links to: System AlertsDSRsPrivacy The Jump To menu gives you quick access to the following relevant related tools: TrendsSetupUser ProfilesJourneys  When viewing the Data Platform suite, the left-hand nav bar displays links to: TrendsSetupLive StreamData CatalogTransformationsEvent Forwarding The Jump To menu gives you quick access to the following relevant related tools: System AlertsUser ProfilesCalculated AttributesJourneys  When viewing the Customer 360 suite, the left-hand nav bar displays links to: User ProfilesEnrichmentCalculated attributes The Jump To menu gives you quick access to the following relevant related tools: Data CatalogJourneysSetup  When viewing the Segmentation suite, the left-hand nav bar displays links to: AudiencesJourneys The Jump To menu gives you quick access to the following relevant related tools: Data CatalogSetup  When viewing the Predictions suite, the left-hand nav bar displays links to: PipelinesProjectsDataAPIsInsights  When viewing the Analytics suite, the left-hand nav bar displays links to: My Hub: hover your cursor over My Hub to view links to each Analytics toolSavedData ",
        "FAQ": "We know it takes time to get comfortable with a new interface, but here are some common questions and answers:",
        "Where is the left-hand nav bar?": "The left-hand navigation bar has been replaced with an updated nav-bar that provides contextual links: it displays the most relevant features for each particular mParticle tool suite you\u2019re currently viewing. However, when viewing the Overview Map, the nav bar is removed completely because the Overview Map provides links to each mParticle suite.",
        "Why can\u2019t I access everything?": "The overview map showcases how the entire platform works together. There might be some features that you can see in the overview map that you cannot see when you click into the feature. If you do not have access to a particular feature in the platform, you can request access from your admin.",
        "Why are some of my inputs and outputs missing?": "The overview map was created to show event flow. However, we have heard from customers that they want to see their entire data flow. We are working on an update to show additional Output categories, including Audiences.",
        "Define your Cohort": "To begin a cohort query, determine an initiating event (called the Cohort event). The first event of a cohort is required; a user must complete the initiating event or Cohort event and then return to perform a second event which is explained in the section \u201cTarget Behavior\u201d. Custom Events and Merged Events can be used. As with other tools, you may apply aFilter Where.  You may select a different time zone from your project time zone on a per query basis by locating the globe icon on the top right of the query screen. You can chain multiple events in a sequence using an \u201cand then performed\u201d clause to define your cohort of users. In the following example, PetBox wants to measure users who download their app, and then start the app.",
        "Generations and Breakouts": "In addition to the first event, cohorts can be defined by a shared generation or a shared property. A generation is a unit of time, such as a month. A monthly cohort would include all users who entered the cohort during that month. A property is a characteristic or attribute, such as device type. Cohorts defined by device type would include all users with an iPhone, all users with an Android, etc. A user will only appearoncein the results of a cohort analysis. For generation cohorts, users will be put into the property breakout in which they first appear during the time interval. ",
        "Target Behavior": "After selecting an initiating event, you must select a Target Behavior event. This second event of a cohort is also required; Custom Events and Merged Events can be used. As with the initiating event, you are also able to apply a Filter Where. Event:Often this is an event that is repeated multiple times, such as a purchase. This is the event that represents the subsequent user behavior that you wish to analyze.Revenue:Target behavior can also be represented asrevenue. Using revenue as the target behavior will analyze the revenue generated over time by each cohort.",
        "Date and Time Range Settings": "Every query requires you to select a date range. In Cohort analysis, the date range refers to the time period during which a user completes all steps of the cohort query, defined in Row A. All new queries default to Last 30 Days. To open the date range selector dropdown, click on Last 30 Days. The start date is the first day to be included in the search. The end date is the last day. As mentioned in Cohort Basics, you canset the time zonefor your Cohort query. ",
        "Generation": "Every cohort is defined by a first event, a breakout, and a second event. To appear in the results, a user must complete the first event and also have a defined value in the breakout. Breakouts may be an Event Property, a User Property, or a User Segment; or they may be a Generation; or they may be a combination. A generation is a time-based grouping that describes the cohort. It describes when the first event occurred. You may select Hour, Day, Week, or Month. For example, to create monthly cohorts of users using the date they signed for a newsletter, the first event would be Newsletter Signup and the generation would be Monthly. This will produce a list of all users who signed up for the newsletter, broken out by month: January signups, February signups, March signups, etc. ",
        "Date Range": "To select fixed start and end dates, use the date range selector on the left side of the dropdown and select a start date and end date from the calendar. You can also enter a specific date by selecting the date at the top of the selector and entering a value. Use the left and right arrows to navigate the calendar. Select the Today checkbox to create a dynamic end date. The right side of the dropdown lists all of the dynamic date ranges that are available. You may choose a dynamic date range, for example Last 7 Days or Last Full Month. This will automatically update the date range of your query, counting backwards from today. If you select Last Full Week, then Analytics will analyze the most recent complete week, defined as Monday to Sunday. If you choose Last Full Month, then Analytics will analyze the most recent complete month. You can quickly navigate the calendar to select full months using the links in the lower left corner of the dropdown. To save a new custom date range, for example \u201cLast 45 Days,\u201d simply choose Add Custom Date Range in the lower right corner of the dropdown. Your previously used custom date ranges will be saved for future use and are viewable alongside the default dynamic date ranges. Note: Custom relative date ranges are saved within each individual user\u2019s account, rather than for all users across a particular project. In Segmentation, you may also exclude any recent partial intervals. For example, if you\u2019re measuring a query with a monthly frequency, but you happen to be querying in the middle of the month, you can exclude the most recent, partial month from your visualization area.",
        "Interval": "In order to visualize your data, the date range will be organized into units of time called intervals. You may choose Hourly, Daily, Weekly, or Monthly intervals. Hourly is our smallest interval that you can run analysis on. For weekly intervals, Analytics considers the week to start on a Monday. Interval options are dependent on the date range selected. For example, if the selected date range is Last 7 Days, then the available time intervals will be Hourly, Daily, and Full Range. Weekly will not be available because there is only one week in a seven day date range. If the date range is changed to Last 90 Days, then the intervals dropdown will update with the additional options of Weekly and Monthly, but will no longer allow Hourly analysis due to visualization constraints. To calculate the total event or user count across an entire date range, select Full Range.",
        "Recurring vs First-Time": "You may adjust your cohort query to observe the frequency of recurring target behavior or to measure the first time that the target behavior occurred after completing the first event. If a query is set to Recurring, then a user will appear multiple times within a row if the user repeated the target Behavior multiple times. If a query is set to First-Time, then a user will appear only once in a row, describing when they completed the target behavior.  To exemplify the difference between recurring and first-time cohort queries, consider a box subscription company that wants to measure customer behavior among their newsletter subscribers. The first event could be \u201cNewsletter Sign Up\u201d and the target behavior could be \u201cPurchase\u201d. User A signed up for the newsletter in January, and then completed purchases in March, April, and May. In a recurring cohort query, the user will appear in the January cohort row, and they will appear in the Month 3, Month 4, and Month 5 columns. In a first-time cohort query, the user will appear only in the Month 3 column. Thus, exclusivity of cohorts as they appear in the interval counts only occurs in a first-time cohort. Recurring cohorts can have a user appear in multiple intervals, though the total number of users will be exclusive.",
        "Cumulative vs Non-Cumulative": "All new queries default to the Non-Cumulative setting. Non-cumulative cohort queries show the count or percentage of users in the cohort who performed the target behavior within the interval. Cumulative cohort queries show the count or percentage of users in the cohort who performed the target behavior as a running total over time. Cumulative counts are only available for queries measuring first-time behavior. For a full explanation of cumulative and non-cumulative, seeCumulative vs. Non-Cumulative Analysis in Cohort.  Non-Cumulative Percent:Displays the percentage of users who completed the target behavior for each selectedtime intervalorbreakout.Non-Cumulative Count:Displays the count of users who completed the target behavior at each point in time defined by the selected time interval.Cumulative Percent:Displays the percentage of users who completed the target behavior for thefirst timeas a running total over time.Cumulative Count:Displays the count of users who completed the target behavior for thefirst timeas a running total over time.",
        "Visualization Options": "Cohort analyses have four different visualization options: Circle Heatmap, Heatmap, Line Chart and Area Chart. You can toggle between these options in the visualization dropdowns. You can also download a cohort analysis as a CSV file. ",
        "Annotations": "Cohort annotations act as general notes about the cohort analysis over the entire designateddate range. To add an annotation to Cohort, click on the Annotation flag icon in the query builder window and click Add an Annotation. To access existing annotations, click the Annotation icon in the Data Panel. For more information about annotations, visitthis article.",
        "Adjust date range": "To adjust the date range, select \u201cLast 7 Days\u201d under the left side of the query builder to open the date range settings. The start date is the first day to be included in the search. The end date is the last day.  To select fixed start and end dates, use the calendar on the left side of the dropdown. You can also enter a specific date by selecting the date at the top of the calendar and entering a value. Tick the \u201cToday\u201d checkbox to create a dynamic end date. The right side of the date range menu lists the available dynamic date ranges. Choosing a dynamic date range, for example \u201cLast 7 Days\u201d or \u201cLast Full Month\u201d, will automatically update the date range of your query, counting backwards from today. The terminology \u2018Last Full\u2019 refers to a full period of time as defined in calendar terms. For example, \u2018Last Full Year\u2019 will show data from the last year as defined January through December. For the last year of data, use \u2018Last 365 Days\u2019. If you select \u201cLast Full Week\u201d, then Analytics will analyze the most recent complete week, defined as Monday to Sunday. If you choose \u201cLast Full Month\u201d, then Analytics will analyze the most recent complete month. You can quickly navigate the calendar to select full months using the links in the lower left corner of the date range menu.",
        "Custom date ranges": "To save a new custom date range, for example \u201cLast 45 Days,\u201d simply choose \u201cAdd Custom Date Range\u201d in the lower right corner of the menu. Your previously-used custom date ranges will be saved for future queries and are viewable alongside the default dynamic date ranges. ",
        "Adjust time intervals": "In order to visualize your data, the date range will be organized into units of time called intervals. Adjust interval settings by selecting the dropdown menu to the right of the date range settings. Interval options are dependent on the date range selected. For example, if the selected date range is \u201cLast 7 Days\u201d, then the available time intervals will be Hourly, Daily, and Full Range. A Weekly interval setting will not be available because there is only one week in a seven-day date range. If the date range is changed to \u201cLast 90 Days\u201d, then the interval dropdown will include Weekly and Monthly intervals, but will no longer allow Hourly analysis due to visualization constraints. To calculate the total event or user count across an entire date range, select \u201cFull Range\u201d. For information on how to tailor date and time settings for use with a specific tool, see our documentation regarding: Date Range and Time Settings in SegmentationDate Range and Time Settings in FunnelDate Range and Time Settings in CohortJourneys: Date RangeTime Settings in UsersTime and Interval Settings in Dashboards",
        "Period of Day": "ThePeriod of Dayfeature allows you to see your events broken out into five timestamp intervals: Overnight:12:00am - 4:59amMorning:5:00am - 9:59amDaytime:10:00am - 5:59pmEvening:6:00pm - 8:59pmNight:9:00pm - 11:59pm To break out an event byPeriod of Day: Select thegroup byclause under the event.Setgroup bytoPeriod of Day. ",
        "Identity scope": "mParticle data is organized in three tiers: organization \u2192 account \u2192 workspace. Your identity scope determines how user data is shared between multiple workspaces and accounts under your organization. In other words, an identity scope is a set of user data in which each user profile and \u2018known user\u2019 identity is required to be unique. Multiple accounts or workspaces under a single mParticle organization can share the same scope, but a single workspace cannot be connected to more than one scope. For some use cases, it might be beneficial for an organization to maintain more than one scope. For example: Food delivery apps have both customers and couriers as users of their app ecosystem, but analytics requirements for each group are very different. Additionally, a courier may also use the app as a customer. Storing the data from both roles against the same profile could create confusion. By creating a separate Identity Scope for each set of users, data is kept clean and relevant.Large enterprise organization may not yet have a consistent way of identifying users across branches and subsidiaries. Creating separate Identity Scopes allow pools of differently identified users to be kept separate.Businesses that operate internationally may need to separate their customers geographically to comply with local laws.Multi-sided organizations, such as social media organizations, may conduct separate B2C and B2B business. For example, a user of a social media app may use the same login to post personal status updates and also to purchase advertising. Multiple Identity Scopes allow these activities to be considered separately.",
        "Identity strategy": "The default identity strategy is set toprofile conversion. The profile conversion strategy is designed to help you build a complete record of a user\u2019s journey through the entire signup funnel, from an initial page view or app load to the creation of an account. This identity strategy also supports the use ofaliasing. On Anonymous BrowsingIf match on device ID or cookie, use existing user.  Else, create new user. On New Known UserConvert anonymous user to known, persist previous event history. On LoginResolve existing known user.  Business decision to explicitly copy event history from anonymous session to logged in user or not. On LogoutCreate new user.",
        "Identity hierarchy": "The default identity hierarchy is:",
        "Considerations": "Replaying event attributes requires replaying of eventsReplaying event attributes is not possible without replaying their associated events, which can lead to event duplication.Avoid additional MTU chargesIf the backfilled MPID and the original MPID do not match, the user will be counted twice and the number of unique MPIDs that determines your mParticle bill will be impacted.Sooner is better than laterWe advise replaying data no longer than 2 weeks from the date it was quarantined. Many downstream tools will not accept data over a certain age. The sooner you replay data, the better.Batch and event timestampsTo send data to mParticle via our Events API, events are stored in a batch (see our Events APIdocs pagesfor additional detail). Both mParticle batches and events have a timestamp attached to them. To ensure that events are backfilled with the original timestamp, it\u2019s essential to preserve the value stored in thetimestamp_unixtime_msfield that each event object contains (the timestamp attached at the batch level can be ignored).Avoid batch deduplicationTo avoid batches from being deduplicated in mParticle\u2019s internal data pipeline, make sure to remove thebatch_idfrom the blocked batch before backfilling it to mParticle.Backfilling data requires some coding skillsTo fix and replay data, you need to know how to code.",
        "View all trace activity": "The Trace Activity page displays a list of all recent traces for your development data, and any traces you have configured for your production data. All traces are available for up to 14 days. To view the details for a specific trace, click the purple ID under the Trace ID column.",
        "Trace status": "You can access a trace from Observability as soon as mParticle begins receiving and processing data, but it\u2019s important to note that a trace can\u2019t provide complete information about a data flow until all data in the trace has been fully processed. This typically occurs within 30 minutes. Traces that are ready to be used for troubleshooting will display a \u201cComplete\u201dTrace Statuson the Trace Details Page. Traces for data flows that are still being processed have a Trace Status of \u201cIn Progress\u201d.",
        "Sort and filter trace activity": "You can filter your results by time frame by clicking the button labeled \u201cLast hour\u201d and selecting one of the predefined date ranges or entering a custom range.  To further sort and filter your results, clickSort and Filtersto view the following options: Use theOrderdropdown menu to sort your traces from most recent to oldest, or oldest to most recent. UnderFilters, select any of the following criteria to limit the traces displayed: Trace Type:Event: displays only traces for the Events APIIdentity: displays only traces for the IDSync APIResult:Success: displays only traces where all data was processed without any issues.Insight: includes traces that experienced an interruption in data flow resulting from a configuration setting (such as a Rule or Filter).Needs Attention: displays only traces that include an error message.Warning: includes traces where an issue was encountered during data processing that could be resolved with a retry.Environment:Production: displays only traces for data in your Production environmentDevelopment: displays only traces for data in your Development environmentmPID: filters results based on the MPID associated with a call to the IDSync APITrace Configuration ID: displays only traces created for the given trace configuration IDInputs: filters results based on one of your configured data inputsOutputs: filters results based on one of your configured data outputs After selecting your desired sorting and filter options, clickApply. This refreshes the Trace Activity page to display only filters matching your selected criteria.",
        "Customize trace activity page": "You can configure the columns that are displayed on the trace activity page by clicking theView Columnsbutton.  To remove a column from the trace activity page, click the toggle switch. Some columns, like \u201cTrace ID\u201d, cannot be removed. To change the order of the columns, click and drag the handle next to the column name. ",
        "Trace details": "After opening the details page for a specific trace, you will see the following information:",
        "Trace summary": " Trace ID: the unique ID for the trace.Trace Configuration ID: the ID of the trace configuration that the trace belongs to.mParticle ID: any MPID that is associated with the trace.Trace Status: either \u201cIn Progress\u201d or \u201cComplete\u201d. The trace status indicates whether a trace contains enough information for it to be useful when troubleshooting your data flow. Traces that are \u201cIn Progress\u201d indicate that the data being traced is still being processed. For more information, seeTrace status.Trace Start Time: the date and time (in UTC) when the data was initially received by the mParticle platform.Duration: the total time elapsed during the trace.",
        "Setup details": " Inputs: the name of the input configuration where the data originated.Output Configurations: the configured outputs for the data.Data plans: any active data plans that were used during processing.Environment: indicates whether the data flowed through your development or production environment.",
        "Trace result": "For each trace, you will see eitherSuccess,Insight,Needs Attention, orWarningdisplayed under \u201cResult\u201d along with any applicable messages under \u201cAdditional information\u201d. You can use this information to determine whether an issue encountered during the trace was intentional or accidental, and what steps you may need to take to resolve any issues.  In some cases, you might see additional information with instructions to contact mParticle Support or your mParticle Account Representative, who can help you determine the root cause of an error or issue.",
        "Related traces": "It\u2019s possible for one process to trigger other related processes in the mParticle platform. Any related processes that are traced will be listed here. For example, when mParticle ingests a batch of event data or a request to make a bulk update to your data, each subsequent data flow will have its own unique trace, which you can find and access here.",
        "Timeline view": " The Timeline View provides a visual picture of how your data flows through the mParticle platform, broken into different spans, with each span representing a different stage of data processing. Hover your cursor over the span in the timeline to see its exact start and end times. To view details for a specific span, click on the span within the timeline and review the information panel at the bottom of the UI. Not all spans will be presented sequentially, and some will appear to occur at the same time. This is because mParticle executes different processes in parallel to reduce the amount of time it takes to process your data. Most gaps between spans on your timeline are likely due to networking delays or internal processes that are not represented on the trace timeline.",
        "Span details": " The Span Details view provides more granular information about a particular span. The details shown will vary depending on the span category. To learn more about each span category, view theSpan Glossary.",
        "About trace IDs": "Each individual trace is uniquely identified by a 36 character Trace ID resembling66e0c0cd9bb8998a579595e42bae7077. Trace IDs are essential for pinpointing specific data processing traces. You can find them in two primary ways:",
        "1) Find a trace ID in an API response": "Trace IDs are included with all responses to calls to the Events and IDSync APIs. To find a trace ID in an API response: Search your API response for the header titledX-MP-Trace-Id. The value of this header is the trace ID for the corresponding API call. ",
        "2) Find a trace ID in Live Stream": "Log into your mParticle account and navigate toActivity > Live Stream.Select any event row.In the right hand sidebar, you will find the trace ID associated with that event.",
        "Search for a trace using a trace ID": "To search for a specific trace on the Trace Activity page: Find and copy the trace ID from an API response or Live Stream as described inAbout trace IDs.Log into your mParticle account and navigate toOversight > Observability.From the Trace Activity page, enter your trace ID in the search bar and clickSearch. ",
        "Open a trace from Live Stream": "You can also view trace details for a data flow directly from the mParticle Live Stream. Log into your mParticle account and navigate toActivity > Live Stream.Select the event row you want to view the trace for.In the right hand side bar, you will find the trace ID associated with that event.Click on the Trace ID to open up the Trace Details page in the Observability suite. ",
        "Trace configurations": "To view your trace configurations, navigate toObservability > Trace Configurationsin the left hand navigation.  You define what data you want to trace using a trace configuration. A trace configuration initiates an individual trace with a unique trace ID for each request to the Events or Identity API according to the trace configuration settings you specify. The trace configurations page displays a list of all configurations, sorted from newest to oldest. You can see the start and end date and times for each configuration, the trace configuration ID, the percentage of data that will be traced, and one of the following trace configuration statuses: Active: Active trace configurations will generate a trace for any data ingested from the configured input(s).Pending: Pending trace configurations will become active once the selected start date and time is reached.Completed: Trace configurations are marked Completed when the scheduled duration for the trace has elapsed.Canceled: If you cancel the trace configuration, its status is changed to Canceled. ",
        "Create a trace configuration": "To create a new trace configuration: Navigate toObservability > Trace Configurations, and clickCreate Trace Configuration.This opens the Add Tracing Configuration window:UnderInputs, select the connected data input you want to trace.You can select one of your configured Warehouse Sync pipelines as the input. Warehouse Sync pipelines will be listed under the Feeds input category.Use the date and time picker to select theStart Date & TimeandDurationfor your trace. Traces will only be generated after the start time and for the duration you specify.If you select a Warehouse Sync pipeline as your input to trace, you will see a unique trace generated for each event batch within the timeframe you specify.Select the percentage of your data you would like to be traced using the Sample Size drop down menu. You can select either 1%, 3%, or 5% for small sample sizes, or you can select 10% through 100% in increments of 10 for larger sample sizes.You should uselargesample size for short-term tracing. For example, when first launching a new mParticle configuration, creating a short-term trace configuration with a large sample size can help you detect problems early on.You should use asmallsample size for long-term tracing. For example, once you have a stable configuration but still want to monitor your data, you can create a long-term trace configuration with a small sample size to keep your tracing cost low.Use the date and time picker to select theStart Time (UTC)andDurationfor your trace. Traces will only be generated after the start time and for the duration you specify.ClickSubmit. After clicking Submit, you will see your new tracing configuration listed on the Trace Configurations page.",
        "Introduction": "The purpose of this guide is to walk you through the basic steps of setting up mParticle in your app, unlocking core functionality, and troubleshooting common issues. Along the way, you\u2019ll cover some important concepts you need to understand to be successful with mParticle. This is not a complete guide to all of mParticle\u2019s features and capabilities. If you already know your way around mParticle and you\u2019re looking for in-depth docs, head to ourDevelopersorGuidessections.",
        "Authentication": "Analytics\u2019 Export API requires HTTPS/SSL and uses Basic Authentication header for all requests. Basic Authentication is a simple authentication scheme built into the HTTP protocol. The client sends HTTP requests with the Authorization header that contains the word Basic followed by a space and a Base64-encoded string username:password. Use your API Key as the username and your Access Token as the password. This information can be found on the Project Page in the Analytics web application. Please treat your Access Token as you would a password, as it is meant to only be known to you. An example curl:curl -v -u \"apiKey:accessToken\" https://query.indicative.com curl\u2019s -u parameter automatically encodes the username and password and inserts them into the Authorization header. Depending on your client implementation you may need to do this manually. The resulting HTTP Response should have a 200 OK status.",
        "Response Format": "All API responses are in JSON format.",
        "List User Segments": "This endpoint lists all User Segments in a project. Endpoint: GEThttps://query.indicative.com/service/v1/segments/listRequest Parameters: NoneRequest Headers: Basic AuthenticationResponse: A list of User Segments objects for the specified project. See User Segment Object Response below. User Segment Object Response Example: List User Segments $ curl -s -u \"5a6646c4-ae5a-4de4-9d47-eb6f228405b2:nux06hkn870v0ni4fhtd81kf\" https://query.indicative.com/service/v1/segments/list",
        "Request a User Segment Export": "This endpoint invokes a new export request for the specified User Segments. Endpoint:POST https://query.indicative.com/service/v1/segments/export/Request Parameters: POST body, containing the following fields:segmentId: the ID of the segment to exportoutputFormat: the output format of the export file (currently only CSV is supported)Request Headers:Basic AuthenticationContent-Type: application/jsonResponse: An Export Status object representing the status of the newly created export. See Export Status Object Response Example: Request User Segments Export",
        "Check Export Status": "This endpoint checks the status of a previously submitted export request. Once the request has finished processing, it will include a URL to download the results. Once the request has been completed, the file will remain in Amazon for no more than 48 hours. Otherwise, a new request must be submitted. Endpoint:GET https://query.indicative.com/service/v1/segments/export/{exportId}Request Parameters:exportId: the unique identifier of the export for which to check status retrieved from either the Status or List APIRequest Headers: Basic AuthenticationResponse: An Export Status object representing the status of the specified export. See Export Status Object Response. Example: Check User Segment Export Status $ curl -s -u \"5a6646c4-ae5a-4de4-9d47-eb6f228405b2:nux06hkn870v0ni4fhtd81kf\" https://query.indicative.com/service/v1/segments/export/8f9fdd71-204c-41e1-a2ab-de7f85002f65",
        "Filter Data": "You can filter the data shown in the Live Stream in several ways: Inputs: Select an individual Platform or Feed to show only data from that input.Outputs: Select an individual output event configuration in your workspace. If you set this filter, you must also set Message Direction to either Outbound or Both In and Out.Message Direction: Select Inbound, Outbound, or Both In and Out. Inbound messages are data arriving in mParticle from instrumented apps or from partner feeds. Outbound messages are data sent by mParticle to an output service.Device: Often, during testing, you will want to monitor a specific test device. The Device drop-down allows you to choose a device to monitor from all devices that are currently sending messages to all workspaces in the account, as well as all devices that have been saved. Live Stream shows only development data, but if you filter for a specific device, the Live Stream will also show events from the Production environment. When attempting to match a device to a device ID, mParticle will look for the following per platform: iOS: IDFA (ios_advertising_idin the Events API)Android: GAID (android_advertising_id)Weband other platforms: Device Application Stamp (mp_deviceid) To save a specific device: ClickAdd/Edit Deviceto display the Device list.Click+next to the device you want to add, or clickAdd New Deviceto display the Add Device form.Enter/Modify the Device ID, Device Name, Device Type and clickAdd Device.ClickSave.",
        "Examining a Specific Event": "To view the details of a specific event, select the event from the Live Stream list. The Live Stream pauses, the selected event expands to display additional message details, and the Event Details panel is shown. The Event Details panel contains additional event information arranged by category. If you select aBatchmessage, the Event Details panel will display general batch details, user attributes, user identities and location information. If you select aneventmessage, the Event details panel will display general event details, app version, event attributes, device information, platform information and location information ClickView Eventin the event details panel for a JSON representation of the data. ",
        "Limits": "Similar to our event limit for workspaces, data plans support up to 1,000 data points.Managing plans with more than 400 data points in the UI can become unwieldy. Manage larger plans outside the UI, either via aData Plan Builderor theData Planning API.You can block data only for unplanned violations: events or attributes with names that diverge from the schema defined in a data plan.",
        "Prerequisites": "Before you start this activity, you should have already: Created an inputStarted to collect some basic data points",
        "Getting Started": "In order to use the BigQuery Export Integration for Analytics, the customer must provide programmatic BigQuery access to the Analytics Platform. The customer is required to grant dataViewer BigQuery access to Analytics. In order to perform the following steps you must have administrative access to the BigQuery Console as well as your BigQuery database.",
        "More About Data Plan Builder and Templates": "Data Plan Builder is a Google Sheet add-on and template that helps you create a data plan: A template with full instructions to specify your events and attributes. You simply clone the spreadsheet and add your data.A one-button process for turning the specifications into a JSON that you load into the Data Plan UI (or use theData Planning API). Just selectmParticle > Generate Data Planfrom the Google Sheet menu, and copy the output onto your clipboard.  Choose from one of the industry-specific templates or the generic template. GenericTravelQSRGamingFinTechRetailMediaHere\u2019s an example of the first part of the JSON file that you create from the generic template with no changes of your own: Once you have the JSON from Data Plan Builder, paste it into the Data Plan import window (as explained in Step 1.3 below), or store the file and upload it using theData Planning API.",
        "Step 1: Create Your Plan": "To create a plan: In the mParticle UI, selectData Master>Plans>Create Plan.Choose how you will import data points, using the instructions in the dialog to pickthe most suitable method.",
        "Step 2: Activate Your Plan": "To start verifying incoming data against your plan, you first need to activate it. To do this, click theActivatebutton on your data plan\u2019s home screen. Then in the Activate modal, use the Status dropdown to select the environment in which you want to activate your data plan (devorprod). (You also have the option to save the plan as a draft to return to later.)  Now that your plan is active, you need to ensure that incoming data is tagged with your plan\u2019s id. Continue to the next step to learn how.",
        "Step 3: Validate Incoming Data with Your Plan": "Before mParticle validates incoming data against the plan, the data must be tagged with a plan ID, an environment, and optionally a plan version. This is the step that requires a small code change, as mentioned inPrerequisites. Plan ID: This is the \u201cslugified\u201d form of your data plan name. You can find it during plan creation, and on the plan listing page.Plan Version (optional): The plan version that the data should conform to. If omitted, mParticle uses the latest active version for your environment.Environment(developmentorproduction): The environment of your data. mParticle uses this value to look for plans that are activated in the given environment. To find your plan ID, navigate to theplan listing page. In the following image,fintech_templateis the plan ID and should be used in the code snippets below:  Include the plan ID and environment in all batches sent to mParticle. For client-side SDKs, you must provide this metadata on initialization of the SDK.For theEvents API, you must include it in every request body.\nIn addition to plan ID, you can optionally add a plan version, which pins your validation to a specific version. If the plan version is omitted, mParticle will choose the latest version active in a given environment. Example Code in Four Languages You can cut and paste the following example code in either JSON, Swift, Kotlin, or JavaScript for your developer to implement: Now that you have tagged incoming data, use Live Stream to debug violations as they occur. ",
        "Step 4: Monitor Your Plan": "Once your plan is validating data, violations reports help monitor your data quality. To view violations, clickUnique Violationsin the header row of your data plan\u2019s home screen. This will display a violation report like the one below: ",
        "Step 5: Update Your Plan": "Your data needs change over time. Data plans can be easily updated to reflect these changes. Smaller changes can be made directly to an existing plan version. Updates to active data plans are live immediately: simply update the plan in the UI and save your changes. For larger changes, we recommend creating a new plan version. Creating a new plan version allows you to track changes over time and to revert back to an older version if necessary. If you\u2019re using aData Plan Builder, make the update in the builder and follow instructions to export a new data plan version into mParticle.  To view the version history of a data plan: Log in to mParticle and navigate toData Master > Plansin the left nav bar.Select a data plan from the list.Hover your cursor over the details icon (\u2026), and clickView Plan History.",
        "Step 6: Block Unplanned Data from Being Forwarded to Downstream Systems": "Once you are confident that your plan reflects the data you want to collect, you can block unplanned data from being forwarded to downstream systems. Learn more about blocking data inthe next section.",
        "Blocking Bad Data": "Using Data Plans, you can block unplanned data from being forwarded to downstream systems. You can think of this feature as an allowlist (sometimes called awhitelist) for the data you want to capture with mParticle: any event, event attribute, or user attribute that is not included in the allowlist can be blocked from further processing. ",
        "Limitations": "1 group definition per workspace in your mParticle account.Each group definition can include up to 10 group attributes.Each group can contain up to 10 users.Group ID limits:Minimum 1 characterMaximum 32 charactersIncludes only the alphanumeric charactersa-zand0-9",
        "Quarantine Connections": "To prevent blocked data from being lost, you can opt for blocked data to be forwarded to an Output with a Quarantine Connection. To illustrate a typical workflow, assume you choose to configure an Amazon S3 bucket as your Quarantine Output:  Anytime a data point is blocked, the Quarantine Connection will forward the original batch and metadata about what was blocked to the configured Amazon S3 bucket. You will then be able to: Examine the blocked data in greater detail.Backfill data that was mistakenly blocked by following ourbackfill guide.",
        "Blocking Data Sent to mParticle Kits": "In most cases, data collected by the mParticle SDK is sent to mParticle and then forwarded on to an integration partner server-to-server. However, in cases where a server-to-server integration cannot support all required functionality for an integration, an embedded kit can be used instead. You can learn which integrations are kits for a given SDK here: WebiOSAndroid By default, the current Block feature supports blocking for server-side integrations. If you would like to enable blocking for mParticle kits, you need to follow additional steps outlined below for each of our most popular SDKs: Web, Android and iOS. Before you can enable the blocking feature, you need to create a data plan and initialize the respective SDK with a data plan ID in your code. Read our\u201cGetting Started\u201d sectionfor detailed guidance. Our SDKs are served by a CDN that caches SDK configuration, including your data plan, for some period of time (the \u201cTTL\u201d). As a result, updates to a data plan can take time before they are reflected in your client code. To avoid caching a plan version while you are iterating on it: Explicitly mention the plan version in your code.Create a new plan version when you make changes.Update the plan version in your code to point to the latest version. The resulting changes in the URL will sidestep previously cached versions. You can now turn on Block settings for the type of data you would like to block by completing the following steps: Open your data plan version in the UI and navigate to the Block tab.Enable \u201cBlock unplanned events\u201d or any other block setting. Events are a good place to start blocking. For Web, you can use the developer console to verify when a kit\u2019s underlying SDK uploads an event to the partner\u2019s API. For iOS and Android, you can typically use verbose console logs or a proxy such as Charles Proxy. Depending on your block settings, you should see unplanned data removed from payloads. For example, if you have not planned \u201cBad Event A\u201d, \u201cBad Event A\u201d will not be forwarded to a specific partner integration. Follow your usual software development process to deploy your code changes to production. Remember to also promote your data plan version to prod through the mParticle UI to start blocking production data that does not match your plan. Plan versions active on production are locked in the UI to prevent accidental updates. The recommended flow for updating a production plan is to clone the latest version and to deploy a new version after testing.",
        "How does the memory quota work?": "To protect shared resources, every mParticle account includes a memory quota for active data plan versions. The byte size of a plan version\u2019s JSON representation is a good estimate of its memory footprint. The typical data plan version size is approximately 50 KB. You can verify your current usage, check the size of a data plan, and if needed, take action to reduce your memory quota usage: To find your quota limit and current usage, navigate toData Master > Data Plans.To download the JSON file for a data plan to check its size, navigate toData Master > Data Plans, click a plan name to open it, and then click theDownload as JSONicon above theUnique Violationsarea.To stay within an account\u2019s memory quota, deactivate plan versions that you aren\u2019t using. Draft plan versions don\u2019t count against your quota, only active plan versions do. Contact your mParticle representative if you need more memory provisioned for your account.",
        "How do I enable validation?": "To enable validation, you need to point your code to adata plan idwithat least one active version. For a version to be considered active, its status has to be set todevordev & prod. You can either pin your code to a specific data plan version or omit the version, in which case mParticle will match the data you send with the latest plan version that is active in a given environment (devorprod). Learn more about how to implement a data plan withGetting Started. ",
        "Which events are supported?": "You can plan for and validate the following events: Custom Events (including events emitted by the Media SDK)Screen EventsCommerce Events The following events are not yet included: Application State Transition EventsSession EventsAttribution EventsUser Attribute Change Events",
        "Which user identifiers are supported?": "You can plan for and validate the following user identifier types: Amp Client IDGoogle Advertising IDAndroid Device IDCustomer IDmParticle Device Application StampEmail AddressFacebook IDFacebook Audience IDFire Advertising IDGoogle IDApple IDFAApple IDFVMicrosoft IDMicrosoft Advertising IDMicrosoft Publisher IDMobile Telephone NumberPhone Number 2Phone Number 3Push TokenRoku Advertising IDRoku Publisher IDTwitter HandleYahoo IDOtherOther 2Other 3Other 4Other 5Other 6Other 7Other 8Other 9Other 10 TheOtheridentifiers allow you to enter up to ten different custom strings against which to validate data.",
        "How do I validate the shape of event schemas?": "Here\u2019s an example schema configuration for a screen event called \u201cSandbox Page View\u201d:  This configuration states the following: Thecustom_attributesobject is required and any additional attributes that are not listed below should be flagged \u2013 the behavior for additional attributes is implied by the validation dropdown for thecustom_attributesobject.An attribute calledanchoris a string, and it\u2019s required.An attribute callednameis a string, and it\u2019s optional. Let\u2019s look at a couple examples to see this schema validation in action. This eventpassesvalidation. This eventfailsvalidation since the requiredanchorattribute is excluded. This eventpassesvalidation: Thenameattribute is excluded but optional. This eventfailsvalidation: Thelabelattribute is unplanned andcustom_attributeshas been configured to disallow additional attributes. You can change this behavior by changing the validation of thecustom_attributesobject toAllow add'l attributes(see below). ",
        "What do valid events look like on the developer side?": "If you\u2019re looking for an example of how to implement events that conform to your data plan, download your data plan and check out ourSnippets Tool. This tool will show you how to implement every data point in your plan for any of our SDKs.",
        "How are attribute types validated?": "Since various mParticle features (Audiences, Calculated Attributes, Forwarding Rules, some integrations) will automatically convert string representations of numbers and booleans to their respective types, data planning does not distinguish between raw numeric or boolean values (e.g.42ortrue) and their string representation (e.g.\"42\"or\"true\").  As long as the value can be converted to a type, it is considered valid.",
        "How can I validate specific event, user and identity attributes?": "You can validate specific attributes differently depending ondetectedtype. Learn more abouthow type validation works here. Number can be validated in two ways: An inclusive numeric range as implemented by the JSON Schema\u2019sminimumandmaximumkeywords. Learn morehere.A fixed list of integers as implemented by the JSON Schema\u2019senumkeyword. Learn morehere.  String can be validated in three ways: A fixed list of allowed strings as implemented by JSON Schema\u2019senumkeyword. Learn morehere. Within an enum value, commas are not allowed.A regex pattern.A list of pre-defined formats defined by theJSON Schema standard, including email, URI, date, time, datetime and others. ",
        "Where in mParticle\u2019s data pipeline are plans enforced?": "Ingestion, plan validation (and blocking), and event forwarding occur in the following sequence:  Use any API client or SDK to send data to the Events API, and tag the data with your plan ID and, optionally, a plan version. For instructions, see Step 1 inGetting Started. If you are using an mParticle kit to forward data to a destination, and you have enabled blocking of bad data, you can configure popular client SDKs to block bad data before it is forwarded to a kit. Learn more about blocking bad data before it is sent to kitshere. Your data then passes through themParticle Rules engine. You can use your Rules to further enrich or fix your data. Data is then validated and, optionally, blocked. You can see dev data being validated in real-time withLive Stream. Data is then sent to the mParticle profile storage system. When you block bad data, it is dropped before being stored on a profile. Learn more about what happens when data is blockedhere. Your data then passes through the rest of the mParticle platform and is sent outbound, including: Outbound Rulesthe mParticle Audience systemall Event and Audience-based integrations",
        "What do the different violations mean?": "During plan enforcement, mParticle will generate violations when actual data does not match expectations. mParticle tracks the following types of violations: The event type and name combination is not expected. The attribute is not expected on a specific event. The user attribute or identity is not expected. This means the attribute is expected, but it has one or more data quality violations such as: Invalid Data Type: The data type of an attribute\u2019s value does not match expectations. Learn more about type validationhere.Invalid Expected Value: The value associated with an attribute does not match expectations. Learn more about attribute validationhere.",
        "What do I need to know before enabling block settings?": "You can\u2019t replay blocked data through the UI. If you have set up aQuarantine Connection, we offer instructions and sample scripts for replaying blocked data in ourbackfill guide.You can\u2019t block the following items:Unplanned identifiersInvalid dataThe eventuser_attribute_changecan\u2019t be blocked as unplanned data Blocked data is dropped from your data stream before it is consumed by other mParticle features, such as: User Profiles, as viewed inUser Activity Viewand accessed through theProfile APIFiltersCatalogAudiences For debugging and reporting purposes, blocked data is shown inLive Streamand theData Plan Report. Unless you create aQuarantine Connection, you won\u2019t be able to recover blocked data.",
        "Does blocking data impact how mParticle counts MTU or events?": "Blocking data does not impact MTU or (ingested) event counts. To prevent blocked data from being lost, you can opt for blocked data to be forwarded to an Output with a Quarantine Connection. To illustrate a typical workflow, assume you choose to configure an Amazon S3 bucket as your Quarantine Output.  Anytime a data point is blocked, the Quarantine Connection will forward the original batch and metadata about what was blocked to the configured Amazon S3 bucket. You will then be able to: Examine the blocked data in greater detail.Backfill data that was mistakenly blocked. Learn more about how to use quarantined data in theBlocked Data Backfill Guide.",
        "Linting": "We\u2019ve developed tools for you to be able to lint your Swift, Kotlin/Java, and JavaScript/TypeScript code. For more information, seeLinting Tools.",
        "mParticle Snippets Tool": "The mParticle Snippets tool helps you to generate example code blocks that log events using the mParticle SDKs in a way that conforms to a specified data plan. For example, if a data plan includes a data point for a custom event with 10 different attributes, you can create the exact code that will log that event with all of its attributes by running the data plan through the Snippets tool. This is helpful when integrating the mParticle SDK into your app if you are unsure which method to call to log a specific event or how to ensure that all of an event\u2019s attributes are captured correctly.  To use the Snippets tool: Copy the raw JSON of your data plan. For an example, you can test the Snippets tool using thedata plancreated for the mParticle sample web app, The Higgs Shop.Navigate to the Snippets tool atmparticle.github.io/data-planning-snippetsPaste the data plan JSON in the left column.Use the language dropdown menu to select the appropriate language for the SDK you are using. For the sample Higgs Shop web app data plan, select Web SDK.The right column will automatically populate with example code blocks for each data point in the data plan. The Higgs Shop web sample app plan includes 16 different events, and the generated code block for each event includes a comment describing the action that will trigger the event based on the data point\u2019s description plan. For example: For this data point, you must first create the product being added to the cart using themParticle.eCommerce.createProduct()method, passing in the attributesproductName,productId, and19.199for the product\u2019s name, ID, price, and amount. To log the event, your app must call themParticle.eCommerce.logProductAction()method passing in the product object just created and the product action type (AddToCart). Visit themParticle developer documentationto learn more about integrating the SDKs into your application. For more information about the Snippets Tool, visit theGitHub repo.",
        "Next Best Action Details Page": "Select the name of your Next Best Action inEnrichment > Predictive Attributesto view its details:  This section displays the total number of users that are included in the target segment you have specified. Predictions are generated for all users in your target segment, though you will often want to target the highest likelihood users. Using theUser likelihood rangedropdowns in this section, you can see how many users fall into each of your offer categories when targeting different percentile ranges to understand how targeting different likelihood percentiles will impact your campaign. Note: Experimenting with percentile values does not actually do anything with them.You will need to remember which percentile you want to target when youactivate your NBAs in a campaign. This section displays how many users within the selected likelihood range were selected for each of your three offers. (For the least likely converters, it displays the total number of users in this category.)",
        "Javascript Method Calls": "Indicative.sendAlias() The Analytics client automatically generates a default unique ID (a UUID) to use on all events untilIndicative.setUniqueID(id)is called. At that point,Indicative.sendAlias()can be called to alias the UUID to the \u2018id\u2019 parameter set withinIndicative.setUniqueID(id) Indicative.setUniqueID(id, true) This will automatically callIndicative.sendAlias()after setting the new unique ID. If you just callIndicative.setUniqueID(id)`\u00a0(without \u2018true\u2019) it will not send the alias call.",
        "Roles": "Roles enable you to manage the different permissions within your organization and its projects. You may assign different roles to your teammates, depending on your desired level of access. For example, trusted data engineers may have Admin permissions, while end users may have Member permissions. Similarly, you may limit some users to have view-only permissions. To access Roles settings by open the Account Settings dropdown- usually indicated by your initials in the top right corner of Analytics- then select \u201cRoles\u201d. The default roles enabled for Analytics are: Owner*: full edit accessAdmin**: full edit access to all projectsMember: may not edit Organization, Projects, or TeammatesRead Only: may not edit Organization, Projects, Teammates, or Dashboards, and View-only access for all other areas *There may be only one Owner per project. **Admins have access to all Projects within the Organization. You may also create a custom role by clicking \u201c+ New Role\u201d at the top right of the table.",
        "Rights of Data Subjects": "The GDPR defines some rights of Data Subjects, including: The right to have data concerning themerased. Also known as the \u2018right to be forgotten\u2019.The right toaccessdata concerning them.The right toportabilityof data concerning them, for transfer to another controller. The CCPA defines that consumers have rights of: The right to request the data saved concerning them.The right to request any data collected from the consumer be deleted.",
        "OpenDSR Request Framework": "mParticle is a collaborator on theOpenDSR framework, which provides a simple format for Data Controllers and Data Processors to collaborate towards compliance with requests from their Data Subjects to honor the above rights. This framework was formerly known as OpenGDPR; it was renamed in early 2020 to include CCPA support. To find out more about OpenDSR, read the full spec on theGithub page. mParticle\u2019s OpenDSR implementation handles three types of DSRs: \u201cErasure\u201d, \u201cAccess\u201d and \u201cPortability\u201d.",
        "General Request Workflow": "Each DSR follows the same basic workflow: The data subject submits a DSR to the data controller.The data controller must log, authenticate and verify the request. If they choose to accept the request, the data controller forwards a request to mParticle in its role as a data processor. The request provides:One or more identities for the data subjectThe type of request: \u201cErasure\u201d, \u201cAccess\u201d or \u201cPortability\u201dThe time the data subject submitted the requestAn optional list of status callback URLsOn receipt of the request, mParticle sets the status of the request to \u201cPending\u201d and sends a status callback request to all URLs listed in the original request. This callback includes an expected completion time for the request, which is calculated as: the time it will be scheduled for processing plus 48 hours to ensure the job completes in time.The Data Controller can check the status of the request at any time.When the request is complete, mParticle sends a status callback request to all URLs listed in the original request. For Erasure requests, this callback will simply confirm that the request has been fulfilled. For Access and Portability requests, a download link will be provided.For Access and Portability requests, the download link remains valid for 7 days. Attempting to access the download link after that time will result in a410 GoneHTTP response. This workflow can be managed in mParticle UI or programmatically via theOpenDSR API.",
        "Identifying affected user data": "mParticle stores data against user profiles, each identified by an mParticle ID (MPID). To respond to DSRs, mParticle first matches identities in the DSR against observed user profiles. This is handled the same way as mParticle\u2019s regular IDSync process: provided identities are resolved to MPIDs to identify affected user data. Data subject requests submitted without alogin IDwill not be fulfilled for known profiles that have an associated login ID. For example, if you submit a data subject request that only includes the device ID for a user, mParticle will not be able to find the correct profile to fulfill the request. When finding the correct profile for a DSR, mParticle follows the same identity resolution process used for general identification requests made to IDSync (the mParticle identity management system). The exact profiles returned for a data subject request depend on the specificuser identifierssupplied with the DSR and theidentity strategyconfigured for your account. All DSR requests are scoped to a single workspace by API authentication. If you need to apply a DSR to multiple workspaces, please submit it within each workspace.",
        "Data Subject Request Settings": "To get started, enable GDPR and/or CCPA compliance features on your workspace fromWorkspace Settings>Workspace>Regulation. This will allow you to see the DSR UI. mParticle will honor all requests received via API even with these features disabled. You have the option to include a copy of the live user profile in access/portability requests. Navigate toPrivacy>Privacy Settingsto include a copy of the users profile with GDPR and/or CCPA DSRs. This is for clients whose privacy teams determine that this is required for compliance. The profiles will include: devices, identities, audience memberships, user attributes and calculated attributes. By default, profiles are not included.  The following video explains how to use consent to control data forwarding with mParticle: ",
        "Develop a strategy for accepting Data Subject Requests": "As a Data Processor, mParticle will match user profiles for a Data Subject Request based on any identities we are given. As a Data Controller, it is your responsibility to determine how to accept and forward Data Subject Requests in order to best meet your GDPR responsibilities and manage risk. This decision should be managed in conjunction with your Identity Strategy. You also have the option of using theIdentity APIto identify for yourself the MPIDs you wish to include in the request and submitting them directly, rather than letting mParticle match IDs for you. Be sure to consult your internal privacy and compliance experts when determining your strategy for accepting and forwarding Data Subject Requests.",
        "Erasure": "After mParticle receives an erasure request, a 7 day waiting period starts. This waiting period gives you the opportunity to cancel a pending erasure request before it is initiated. After the 7 day waiting period, any pending erasures are initiated. Once begun, it may take up to 14 days before the erasure is complete. For each completed erasure request, mParticle sends a callback to any specified URLs indicating that the request has been fulfilled. By default, erasure requests are completed between 7 and 21 days after being received by mParticle. The initial 7 day waiting period provides an opportunity to cancel a pending erasure request before it is carried out. To skip the initial 7 day waiting period when submitting a data subject erasure request to mParticle, check the option labeledSkip waiting periodin theNew Data Subject Requestmodal. Skipping the waiting period shortens the request cancellation window. This reduces the total time required to complete an erasure request to between 1 and 14 days after it is received by mParticle. If you wish to remove users from audiences or from event forwarding during the waiting period, set a user attribute and apply audience criteria and/or forwarding rules to exclude them. In response to a data subject erasure request, mParticle deletes the data it stored, such as historical event batches, audience data, and profiles. A delete request will also not prevent additional data concerning the subject from being received and processed by mParticle. If the data subject wishes to prevent all future data processing, they will likely need to take additional steps, for example, ceasing to use your service/app.",
        "Access / Portability": "Access and Portability requests are treated exactly the same way, as follows: mParticle identifies the MPIDs that match the request.Just after midnight each Monday and Thursday, mParticle searches for data related to each MPID, including the user profile and historical event batches.mParticle compiles the data into a single text file. This data includes device identities, user identities, user attributes (including calculated attributes), as well as current audience memberships.mParticle sends a callback to any specified Callback URLs indicating that the request has been completed. The callback will contain a secure download link to the text file containing the Subject\u2019s data. If you submit an access and portability request for more than one profile using multiple MPIDs, the data for every profile returned will be included in a single file. Since the resolution process for DSRs is the same as the process for IDSync, an access and portability request that includes only a device ID will not return any profiles that are protected by a login ID. For example, imagine that a user opens your app and is tracked with an anonymous profile, but they do not create an account with a login ID. Later, a different user on the same device opens your app and logs in with a login ID. If you submit an access and portability request but only supply the device ID, then only the data for the anonymous user will be returned. The data gathered in response to an access or portability request will be delivered in a.zipfolder containing many.jsonlfiles (JSON Linesformat).  The zip may contain: profile.jsonl: A file that contains the live profile at the time of the request. This includes: device identities, user identities, current audience memberships and user attributes (including calculated attributes).one or more additional.jsonlfiles: These results are split into many files to avoid a single, large file to make them easier to transmit and process. Controllers are encouraged to re-process the files as they see fit. These files contain the event batches sent to mParticle. Each line of the data files represents a complete mParticle event batch. See ourJSON Referencefor a guide to the event batch format.empty.txt:  A file which indicates that mParticle found one or more MPIDs associated with the identities in the request, but that there is no data available for them. Note that if no records can be found matching the identities in the request, the request for the zip file returns a404error. A sample portability response can be downloadedhere.",
        "Managing Data Subject Requests in the mParticle Dashboard": "In addition to the OpenDSR API, users with theCompliance or Admin and Compliance rolecan create, delete and monitor DSRs directly in the mParticle Dashboard.  To view details about a request, click the Request ID number.",
        "Forwarding Data Subject Requests for Erasure": "You can configure mParticle to forward Data Subject Requests (DSRs) for erasure with one or more integrations.  This detail UI for a data subject request for erasure shows the forwarding status for a request that is being forwarded to three different outputs. The forwarding status field contains different values, depending on the situation: Pendingmeans that a request has been queued for forwarding, but hasn\u2019t been forwarded yet.Skippedmeans that a request for forwarding has been skipped because mParticle could not find suitable identities to forward, either from the original request or the user profile.Sentmeans that a request was forwarded and an acknowledgement of the request to delete the user from the integration was received by mParticle.Failedmeans that an attempt to forward the request was made, but an error occurred.Not Sentmeans that the request was not forwarded, because the request was made using an older version of the DSR API. You must upgrade to the DSR API v3 in order to forward DSR erasure requests. In addition to the forwarding status, the identities that were forwarded are also shown. mParticle determines which identities to forward based on the identities supplied in the original request, the identity resolution strategy, and what identities each output supports: When a single generic identity type (such as email address) is submitted in the erasure request ORWhen multiple generic identities of different types (such as email address and device ID) are submitted in the erasure request and:mParticle resolves it to a single user profile: mParticle enriches the request with all IDs found on the corresponding user profile. mParticle will include all identities supported by the output in the forwarded request.mParticle resolves it to multiple user profiles: mParticle will try to resolve it to a single user profile following your Identity resolution strategy. mParticle then enriches the request with all IDs found on the corresponding user profile. mParticle will include all identities supported by the output in the forwarded request.mParticle cannot resolve it to any user profile: The request may still be forwarded if the vendor supports the ID type provided in the original DSR request.When a single MPID is submitted in the erasure request and:mParticle resolves it to a single user profile: mParticle enriches the request with all IDs found on the corresponding user profile. mParticle will include identities supported by the output in the forwarded request.mParticle cannot resolve it to any user profile and nothing will be forwarded. In the case where the data in a user profile does not match what was provided in the original erasure request, mParticle will use the information from the original erasure request as the source of truth to process and forward the request. Once a request is forwarded, mParticle can\u2019t guarantee that data is ultimately deleted by the integration partner, so confirm that each vendor fulfills the request. If an integration supports forwarding erasure requests, the integration documentation contains a section \u201cData Subject Request Forwarding for Erasure\u201d and that section contains specific instructions and information about which identities are forwarded. To find all the integrations that support forwarding erasure requests, visitIntegrations.",
        "Retention of Data Subject Requests Records": "mParticle retains Data Subject Request records for up to 1 year.",
        "Use cases": "The Profile isolation strategy is designed to maximize the integrity of each user profile and to prevent anonymous data from being kept together with the data of logged-in users. Under the profile isolation strategy, any time a user creates an account, a new identity record and a new user profile are created. Any anonymous data collected prior to the user signing up is not carried over to the new profile. One of the main reasons to choose the profile isolation strategy is to ensure compliance with consumer protection and privacy laws. For example, if your user agreement includes permission to collect user data, it may be important not to combine user data from before the user signed up, accepting the user agreement, and after. The profile isolation strategy is based on building highly reliably profiles around login IDs, so any time an identity request includes an email address or customer ID, mParticle will return a unique Identity Profile for the user, regardless of which device they are using.",
        "Identity flow": "Example identity priority: Customer IDEmail AddressIDFVRoku ID",
        "Identity API": "The identity API is used by all of mParticle\u2019s SDKs to log users in and out of your app, to search for, and to modify a current user\u2019s identities. It is also available as an HTTP API. The identity API provides four endpoints for identifying users: Identify - called when a session begins with whatever identifying information is availableSearch - called to find current user identities or determine if a specific user existsLogin - called when a known user signs into the app.Logout - called when a known user signs out These four endpoints are called in response to different user actions, but they all perform the same function - resolving a request containing all known identifying information for the current user into a single, unique mParticle User Profile. That profile might be: An existing profile that matches all identifying information in the requestAn existing profile that matched some identifying information in the request, updated to include new information.A new user profile, created when no existing profiles matched the request.",
        "Identity strategies": "Identity strategies determine which user profile to add data to when the current user can be identified, and what to do when the current user cannot be identified. There are 5 identity strategies that have been designed to handle different business and privacy requirements.",
        "Profile conversion": "Theprofile conversion strategyis designed to help build a comprehensive picture of a user\u2019s entire journey through a traditional sales funnel. The main distinguishing feature of this strategy is that when a new login ID is received, IDSync will not create a new profile. Instead, it will simply add the new login ID to the previous profile used to store data collected when the user was anonymous.",
        "Default identity strategy": "Thedefault identitystrategy is a simplified version of the profile conversion strategy. The main difference is that the unique ID and login ID for the default identity strategy are set tocustomer_id, and they cannot be changed.",
        "Profile link": "Theprofile linkstrategy is optimized to track the events that drive users to create an account or make purchases.",
        "Profile isolation": "Theprofile isolationstrategy is built to prevent any anonymous data from being attributed to known users. This strategy is helpful when strict compliance with consumer protection and privacy regulation is required.",
        "Best match": "Thebest matchstrategy is not optimized to help uniquely identify users. It is best suited for businesses that do not have a login flow or that provide their primary services without requiring users to log in.",
        "Identity priorities": "Each identity strategy must define the order of precedence for matching user profiles. When an identity request is received, mParticle looks up matching profiles for each identifier in the order defined by the identity priority until a single profile can be returned. Keep in mind that some identity strategies impose minimum requirements that a request must fulfill in order to return a user even if they match (seeLogin Identities). For now, let\u2019s just look at how the identity priority can affect which profile is returned by a request.",
        "Feed specific user identifiers": "There are some input feeds that always include a specific user identifier that you may or may not want to include in your identity priority list. To ensure that IDSync requests from one-off feeds like this can still be resolved even if you decided to omit their specific identifiers from your identity priority, you can configure an additional identifier for that specific feed without having to modify your overall identity priority. If you ingest data from a feed with an extra identifier configured, mParticle still attempts to resolve any IDSync requests using your normal identity priority list first. Then, if no matching profiles are found, mParticle tries any additional identifiers configured for the feed. If a matching profile is still not found, then mParticle handles the anonymous user according to your identity strategy. To configure additional user identifiers for a feed, contact your mParticle account representative.",
        "Use a custom manifest": "A custom manifest allows you to use files created by other systems without transformation. In the mParticle UI, when you\u2019re configuring the Custom CSV input, you can provide a JSON manifest to describe how you want to map your CSV data to mParticle\u2019s fields in one of two ways: With a header row: Map your column names to mParticle\u2019s column names.Without a header row:  Map your columns, which must appear in a specific order, to mParticle\u2019s column names. In order to guarantee that the new/changed manifest applies to your CSV files, please ensure a gap of about 5 minutes between the manifest change and uploading CSV files.",
        "With a header row": "If your CSV has a header row, you can map the column names in your header directly to mParticle fields. The manifest must set\"hasHeaderRow\": trueand contain an array of columns objects, each giving a column name, an action (\"keep\"or\"ignore\") and a target mParticle field. The order of entries in the column array doesn\u2019t need to match the order of columns in the CSV file. Example with a header row Assume that you drop the following manifest on the SFTP server: Then assume that you drop a CSV file with the following header row and one row of data: Event Name,Custom Type,Email,Facebook,Home City,Category,Destination,Time,EnvironmentViewed Video,other,h.jekyll.md@example.com,h.jekyll.md,London,Destination Intro,Paris,1466456299032,development The resulting batch will be:",
        "Without a header row": "If your CSV does not have a header row, you can map columns to mParticle fields by the order they appear in the CSV. The manifest must include\"hasHeaderRow\": falseand contain an array of columns objects, each giving an action (\"keep\"or\"ignore\") and a target mParticle field. For this method to work, you must be able to guarantee the same column order in each CSV file you upload. The following is an example of the same custom manifest as the previous example, but without a header row:",
        "Encrypted files": "The Custom CSV Feed can accept files encrypted with PGP, using mParticle\u2019s Public Key. You can use software like GPG Tools for OS X or Windows. Never use web-based tools to encrypt, hash, or encode data. To enable encryption, set the configuration settingExpect Encrypted Filestotrue. It\u2019s most efficient to send multiple files in a zipped, or gzipped format. Encrypt the final ZIP file instead of each individual file. Use the public encryption key corresponding to your pod: US1EU1AU1US2",
        "Sensitive data": "Use the following public encryption key for sending sensitive data.",
        "Processing speed": "For files within recommended size limits, processing speed is consistent with the Events API: about 270 rows per second. To increase the processing speed for your CSV feed, contactmParticle Support.",
        "Processing Behavior": "Files aren\u2019t guaranteed to be processed in sequence; files are not linked to one another. Each file is independent and there\u2019s no way to indicate if two files were split from a master file.You can observe how much data has been processed using Data Master and your outbound connections. There is no notification.Once dropped, files start processing at any time. Deleting a file from the dropped folder is not a guarantee that it won\u2019t be processed. Overwriting files can lead to partial or incomplete imports, or other errors may occur.Rows may be batched together for processing. Thus, there may be fewer processed batches than rows.\nThe only fields not considered unique identifiers for batching are event-specific, such as event name, custom event attributes, and the batch-level timestamp. If two rows have the exact same set of attributes and identifiers otherwise, then they may be batched together for processing.Each file is processed beginning to end. A file is never split or read asynchronously.\nHeader mappings are on a per-configuration basis and are applied to all potential files (if their headers are not valid already). There is no way to associate a mapping with either a filename or filename pattern.Processed files are deleted within 30 days.",
        "Time Zones": "Each dashboard in your project will be in the same timezone as designated in your project settings. To change a project timezone, see our Projects, Roles, and Teammates article.",
        "Refresh Interval": "The analyses on your dashboard will be refreshed as designated by your dashboard refresh interval. Each query is then independently run and your charts will refresh upon completion. Your dashboard refresh interval can be set to: Every 15 MinutesEvery 30 MinutesEvery 1 HourEvery 2 HoursEvery 24 HoursNo Refresh The default dashboard refresh interval is Every 24 Hours. To adjust your dashboard refresh interval, navigate to the menu bar above your dashboard, and click on the \u201cEvery 24 Hours\u201d dropdown. Then, select your desired dashboard refresh interval.",
        "No Refresh Option": "Sometimes, you will want to create a one-off dashboard for a quick analysis that you don\u2019t necessary want to refresh on a regular interval. For these cases, select the \u201cNo Refresh\u201d option. You can select a refresh interval at any time after setting a dashboard to \u201cNo Refresh.\u201d ",
        "Explore Users from Cohort Cell": "To explore users within a certain cohort cell, simply select the desired cohort cell, and select Explore Users.  In the above example, PetBox is exploring users that have done Site Visit on 6/6/2020, and returned to do Purchase Product on Day 3.",
        "Heat Map Visualizations": "In order to explore users from an entire time generation or breakout, simply select the time generation or breakout on the left side of the visualization area, and choose Explore Users.  In this example, PetBox is exploring users that did Site Visit on 6/6/2020, and returned to do Purchase Product in any interval within the selected Date Range.",
        "Graph Visualizations": "If your cohort visualization type is set to Area or Line, then you may only explore users from a certain time generation or breakout. In order to explore users from an entire time generation or breakout, select into any data point within the visualization area, and choose Explore Users.  In this example, PetBox is exploring users that did Site Visit on 6/6/2020, and returned to do Purchase Product in any interval within the selected date range.",
        "UTM Tracking Within Analytics": "Analytics automatically parses out the UTM values from page URLs that contain UTM parameters. You\u2019ll be able to see the following properties if you are sending Analytics data with UTM parameters within the URL. These parameters will then appear withinUser Propertiesor withinEvent Properties. For example, when using these parameters within User Properties, Analytics would display users with these specific UTM parameters. However, when using UTM parameters within Event Properties, Analytics would display events that were completed with the UTM parameters. When you want to see the users who completed an event or the total number of times an event was completed with a specific UTM parameter, it would be best to use the UTM parameters within Event Properties. Let\u2019s say you have a group of users who completed [pageload] of the url: www.indicative.com/ and you wanted to narrow down your results to users who performed [pageload] with utm_source=Facebook or in other words with a url of www.indicative.com/?utm_source=Facebook. In this case, you would filter utm_source=Facebook as an event property. However, if you wanted to see the total count of users with a specific UTM parameter who completed an event, it would be best to use UTM parameters within User Properties. Let\u2019s say you have a group of users who completed [pageload] of the url: www.indicative.com/ and you just wanted to see if these users have an utm_source=Facebook, not if they performed a [pageload] of the url www.indicative.com/?utm_source=Facebook. In this case, you would use a user property. Note:Analytics uses the \u201d&\u201d character to delineate separate UTM parameters in a URL. For instance, an event with a property named pageUrl URL with a value ofhttps://www.website.com/?utm_source=SRC&utm_id=252325would have the following UTM properties: \u201cpageUrl.utm_source\u201d: \u201cSRC\u201d\u201cpageUrl.utm_id\u201d: \u201c252325\u201d If including an \u201d&\u201d character in a UTM parameter is important, you need to use \u201camp\u201d in the URL to escape it. For example, the URL below (assigned to a property pageUrl) would be parsed into the following UTM properties: URL:https://www.website.com/?utm_source=SRC&amp;&someOtherField=someValue&utm_id=ID \u201cpageUrl.utm_source\u201d: \u201cSRC&\u201c,\u201cpageUrl.utm_id\u201d: \u201cID\u201d",
        "User Properties": "User Properties are the properties associated with the user performing an event, such as demographic factors, an email address, or the marketing channel through which the user was originally acquired. While event properties can differ from event to event, user properties are associated with every event performed by a given user. In order to enable user properties for use in queries, the Attribution Mode under Manage Data must be set to either first or last. Consider the example table of events below: \u2018First\u2019 will store the first value seen for a property for each user across any event that they fire. From the image above, user 1\u2019s first value for this property will beNULL, user 2\u2019s will beSocial, and user 3\u2019s will beSearch.\u2018Last\u2019 will store the last value seen for a property for each user across any event that they fire. From the image above, user 1\u2019s last value for this property will beSearch, user 2\u2019s will beSocial, and user 3\u2019s will beSearch.Because user properties are stored at the user level, the scope of the analysis does not impact what value is stored at the user level. For example, if an analysis was looking at events that occurred between 8/18-8/20 from the example above, user 1\u2019\u2019s value for the property will still beNULLif this user property was configured for First attribution mode. Thus, user properties are selected to view results from users who performed a particular action, even if their first or last actions are beyond the scope of the analysis that has been created.",
        "Event Properties": "When an event property is used in an analysis, Analytics will look at each individual event\u2019s payload and reference the property associated with each event. The event property has the query ask\u201cwhat was the value of the property at the time of the event?\u201d, which is the most common scenario for most analyses. For rows set to display the \u201ctotal count of\u201d events, the results will include all of the events seen as long as they meet the query parameters, even if they were performed by the same user multiple times.For \u201cusers who performed\u201d rows, event properties will still filter or group based on each time that the event was seen, but the results will represent user counts instead of raw event counts.",
        "Identity": " Displays summary metrics for identity data collected about your users: % of Users: The percentage of users that have a given identity type.Overlaps: The percentage of users and count that have a combination of the two given identity types. This section is filterable by the date range, environment and input options at the top of the page.",
        "Connections": " Displays summary information representing the current workspace\u2019s usage, showing the following metrics: Inputs Active Platforms: Count of platforms configured with credentials.Active Feeds: Count of configured inbound feeds.Custom Events: Count of custom events received.User Attributes: Count of user attributes received. Outputs Active Event Integrations: Count of active event configurations.Active Audience Integrations: Count of active audience configurations.Active Real-time Audiences: Count of active real-time audiences.Standard Audiences: Count of \u2018ready\u2019 Standard Audiences.",
        "User Activity": "The User Activity view allows you to see a detailed summary of data associated with a single user. Note that only users with the Admin or Support roles can access the User Activity view. To find a user, begin entering any known ID for the user, an email address, customer ID, device ID, etc. The ID must match exactly to return a profile. If multiple profiles are returned, select the user you want from the list.",
        "User Search": "You can search for any element in your Identity Set.\nTo perform a search do the following: Navigate to theActivity>User Activityscreen. The User Search dialog displays on the screen.  Enter the search terms in the search field.You can narrow your search by clicking the Any Profile field drop-down and selecting any field that is part of your Identity Set.You can search for users that belong to agroup identityby searching for the group identifier or group attribute for the group.Click the Search button. The screen refreshes and search results are displayed in the Information screen. View individual results by clicking a row. From the individual results you can do the following: Submit a New GDPR Request. For more information about GDPR requests, seeSubject RequestsView User Events by clicking theEventstab or by scrolling to the bottom of the screen and clicking theView User EventsbuttonExpand any of the sections on the Information screen for more information about your results The information shown on this screen is explained in the following sections.",
        "Information": "The following user information will be displayed if available: mParticle IDCustomer IDEmail AddressFirst and last seen dates (across all workspaces) First and last seen dates (for the individual workspace)Data Inputs and Partner Feeds the user has appeared in.Device ID - only one Device ID will be shown, for the most recently seen device. The Advertising ID (IDFA or GAID) will be shown for preference, with the Vendor ID (IDFV or Android ID) as a fallback. List of all devices, including the date the device was last seen, device platform (iOS, Android, etc) and whetherLimit Ad Trackingis enabled for the device. List of all available user attributes. Display priority is given to reserved user attributes: AgeFirst NameLast NameGenderMobile NumberAddressCityStateZip CodeCountry All other user attributes will be displayed in alphabetical order. User attribute values may be updated by different inbound data streams. For example, a profile may be created, attributes may be added, updated, or deleted. Each change triggers a timestamp change for the altered attribute. When resolving differences in attribute values for the same profile, the value with the latest timestamp is used. A list of campaigns, showing Partner and Campaign name. A list of allAudiencesthe user is a member of.",
        "Events": "The Events tab will show a timeline of historical event data from the user,up until the previous day. Events can be filtered by Date, Input, Event Type and Device. In addition to filtering, you can also choose to highlight selected event names in the timeline. Events are grouped in batches, with the input source in bold. Click a batch heading to view common attributes for the batch, or an event name to view attributes for the event. eCommerce events show details for the transaction and also for each product, including quantity and total price per product.  Click the Link button in the details view to copy a sharable direct link to the event to your clipboard. As with theLive StreamYou can expand any event or batch to view the raw JSON data. ",
        "Edit a user profile": "The User Activity View allows you to manually edit certain properties of individual user profiles. This can be useful if you need to correct profile data and you want the updated profile data to be immediately forwarded to your downstream connections. Whenever you modify user profile data, you must select one of your configured inputs. This allows mParticle to route your changes through that input, updating the user profile, before finally forwarding the changes to any active, connected outputs. To edit a profile\u2019s user attributes: Navigate toActivity > User Activityusing the left-hand nav bar.Find the profile you want to edit by searching for one of the profile\u2019s identifiers under User Search.On the Information tab for the profile, scroll to the User Attributes section and click theEditpencil icon next to the mParticle workspace where you want to make your changes.Remember that user attributes are scoped at the workspace level, so a profile that exists in multiple workspaces may contain different attributes in each workspace.  In the Edit modal, select one of your configured and active inputs. The changes you make will be sent to every output that is connected to the input you select. If you don\u2019t want your changes sent to your downstream outputs, create or select an input that has no connected outputs.  The Edit modal displays a list of the profile\u2019s attributes and attribute values. To change an attribute value, modify or replace it using the text box to the right of the attribute\u2019s name.  To permanently delete an attribute from a profile, click thedeletetrash icon next to the attribute.  After you\u2019ve made your changes, clickUpdate. Navigate toActivity > User Activityusing the left-hand nav bar.Find the profile you want to edit by searching for one of the profile\u2019s identifiers under User Search.On the Information tab for the profile, scroll to the Consent and Compliance section and click theEditpencil icon next to the mParticle workspace where you want to make your changes.Remember that user consent and compliance values are scoped at the workspace level, so a profile that exists in multiple workspaces may contain different values in each workspace.  In the Edit modal, select one of your configured and active inputs. The changes you make will be sent to every output that is connected to the input you select. If you don\u2019t want your changes sent to your downstream outputs, create or select an input that has no connected outputs.  The Edit modal lists the profile\u2019s consent purposes (which are not editable) and the corresponding consent values. You can edit a consent value by entering eithertrueorfalsein the text box.  After you\u2019ve made your changes, clickUpdate. Navigate toActivity > User Activityusing the left-hand nav bar.Find the profile you want to edit by searching for one of the profile\u2019s identifiers under User Search.On the Information tab for the profile, look for the User Details section and click theEditpencil icon.Remember that user identities values are scoped at the organization level: a user\u2019s identities are consistent between all workspaces in your mParticle organization.  In the modal that appears, select one of your configured and active inputs. The changes you make will be sent to every output that is connected to the input you select. If you don\u2019t want your changes sent to your downstream outputs, create or select an input that has no connected outputs.  To edit one of the profile\u2019s identities, enter the new value in the text box to the right of the user identity.  To delete an identity from a profile, click thedeletetrash icon.  After you\u2019ve made your changes, clickUpdate. To add an identity to a user profile: Repeat steps 1-4 ofEdit a profile\u2019s user identities.ClickAdd Identity.  Use the left dropdown menu to select the type of identity to add, and enter the value of the new identity in the text box on the right.  ClickUpdate.",
        "System Alerts": "The System Alerts dashboard reports all errors returned when forwarding data to your connected outputs. This dashboard helps you to find any connections that are failing to forward data, and it can help you to begin debugging a connection you already know is experiencing problems by highlighting the specific errors reported.",
        "System Alerts dashboard": "To view system alerts, log into your mParticle account and navigate toActivity > System Alertsin the left nav bar.  The System Alerts dashboard lists all of your connections sorted by their alert volume, from high to low according to alerts reported during the last 12 hours. You can view alerts reported during different date ranges by using theDatedropdown menu. To view alerts reported for only the production or development environment, select the environment using theEnvironmentdropdown menu. You can also look for a specific connection with the search bar.",
        "Alert details": "To view details on the specific alerts reported for a connection, select it from the list labeledConnections.  The alerts displayed are sorted by volume and organized according to their type. To view the volume of alerts for each input, click the+icon next to the alert type. You can search for a specific alert using the search bar on the System Alerts dashboard.  The following alert types are supported:",
        "Event Forwarding": " The Event Forwarding report provides information on your app\u2019s incoming event data and the data that is forwarded to enabled output event services. This is where you can verify how much data mParticle captured per event, and how much data was forwarded.  If multiple configurations are enabled for an integration, the data sent to each configuration will be shown separately. We recommend checking this report if you notice any discrepancies in your vendor dashboards. The report displays a daily summary of events, plus counts for each message type and event name, along with the inbound and outbound counts for each enabled service.  By default a summary of data for a full day is shown, but you can also select a specific hour to display hourly data. You may notice differences between an inbound data count and an outbound data count. There are several reasons these differences may occur. Here are some good questions to start with: Did you enable/disable the service on the date in question?Are you using data filters?Is the message type not supported by the integration?Have you chosen configuration or connection settings that exclude certain message types?Are you sendingcommerce eventsthat are expanded before being forwarded?",
        "Commerce Event Types and Event Forwarding": "Four events, called commerce events, capture information about financial transactions: product commerce events, purchase or refund commerce events, promotion commerce events, and impression commerce events. Commerce events behave differently from other events: If the event is forwarded using an mParticle SDK with an embedded kit that doesn\u2019t implementlogCommerceEvent, then the event is expanded to ensure that no data is lost.If the event is forwarded server-to-server or using an mParticle SDK with an embedded kit that does supportlogCommerceEvent, then no expansion is needed, and no data is lost. In addition, the expansion behavior is different depending on the commerce event type: Product commerce events and impression commerce events expand to one event per product.Purchase or refund commerce events add an additional event with the total value.Promotion commerce events expand to one event per promotion action type such as \u201cclick\u201d or \u201cview.\u201d For more details, see theiOS SDKorAndroid SDKdocumentation.",
        "Create a Predictive Audience": "The first way to leverage Predictive Attributes is to them them as inclusion criteria for an audience, therefore creating aPredictive Audience. To maximize the value of your Predictive Attribute, review the information in theprediction details, and determine the likelihood range that meets your campaign goals and budget considerations. Higher likelihood rangeswill contain fewer total users with a greater likelihood of conversion. If your primary aim is to maximize the efficiency and ROI of a campaign intending to drive a specific action like driving purchases or membership upgrades, it likely makes sense to target higher likelihood percentiles.Lower likelihood rangeswill contain more users with a lower likelihood of conversion. If you have a broader objective like increasing brand awareness or driving general user acquisition, targeting users with a slightly lower conversion likelihood that still exceeds the average may be best for accomplishing your goals.",
        "Specify your likelihood range within a Predictive Audience": "Once you have determined which user likelihood range to use, you need to manually set it as a Predictive Attribute within aPredictive Audience, or update it on an existing Predictive Attribute within a Predictive Audience. To do this: Create or navigate to the Predictive Audience in which you want to apply your Predictive Attribute.Add a new Audience criterion.SelectUsersfrom the dropdown.Open theChoose User Featuredropdown and selectUser Attributes.Select your Predictive Attribute from the left-hand dropdown.Use the comparison operators to target your desired likelihood range.SelectActivateto re-calculate your Audience with your Predictive Attribute. For example, to target customers in your prediction\u2019s Most Likely Users range, you would do the following: ",
        "Forward Predictive Attributes to a partner tool": "You can forward Predictive Attributes directly to third-party tools in the same way you would with any other User Attribute. To do this, create aconnectionbetween yourCortex feed inputand the partner system of choice as an event integration.",
        "Query Predictive Attributes with the Profile API": "You can also use theProfile APIto query user profiles for Predictive Attributes, then use these attribute values to drive personalized recommendations and content suggestions throughout the entire customer journey.",
        "Question": "What is the most valuable marketing channel by conversion percentage?",
        "Build the Query": "The three-step funnel will include the eventsSite Visit,Blog View, andSubscribe.  Next, let\u2019s collapse the query builder so we can focus on the funnel.  This is a three-step funnel showing the conversion fromSite VisittoBlog ViewtoSubscribeover the last 7 days. The percentages in the webbing between steps indicate the percentage of users that have moved from one step to the next \u2014 keep in mind that your data may vary from this example. The percentages in the webbings should be interpreted as: 22.78% of users who visited the site went on to view the blog.45.05% of users who viewed the blog went on to subscribe The total conversion rate of 10.26% is displayed above the funnel. Percentages will change if thedate rangeis adjusted. For example, conversion percentages across 30 days will be different than those across 7 days.",
        "Modify the Cohort": "Once the basic cohort analysis has been created, we can go back into the query builder and modify it to fine-tune our results. Currently, our cohort includes all users who downloaded the PetBox app. However, to answer our initial question\u201cAt what rate do non-subscribers who downloaded the app convert to box subscribers over time?\u201d, we only need to see non-subscribers who took this action. We can achieve this by adding a\u201cWhere\u201d clauseto the first row in the query builder, which defines our cohort of users. Hover underneath the Define a Cohort step and select filter where, then choose the User Properties tab and select the propertySubscription Plan.Finish out our parameters by setting the \u201cWhere\u201d clause to \u201cis equal to None\u201d. Our new group only includes users who completed Download App and who were not subscribers: ",
        "Select Between First Time and Recurring": "For the purpose of our analysis, we only need to know about the first time a user subscribed after downloading the app. Selecting\u201cFirst Time\u201dwill limit our results to only show the first time this event was performed. ",
        "View Results": " Our cohort analysis now shows us the rate at which non-subscribers who downloaded the PetBox app are converting to subscribers over time.",
        "Export Users from a Point": "To download a list of users from a single point, click on any data point or table cell within your results. Within the dropdown, select Download Users in this Point to CSV. A list containing all users associated with this data point will be emailed to the address connected to your Analytics account. There is no limit on the number of users that can be exported.",
        "Export Users from a Series": "To download a list of users from an entire series, click on any data point or table cell within your results. Within the dropdown, select Explore Users. If your query contains a series, you may choose to view users from a single data point or from the entire series. From this Point creates a list of users from only one interval whereas From Entire Series creates a list of users from all of the intervals within the date range. Once your Users query results have loaded, simply click on the export icon located in the menu bar beneath the query builder, then select Download CSV. Users analysis is not available for metric visualizations.",
        "Prerequisites:": "To integrate with Snowflake, you will need to access your Snowflake console. For this self-service integration, we also have some data requirements: All of your events must be unified into one singular table as opposed to having separate tables for each event type.By definition, an event must have a user_id, event_name, and timestamp field. The fields do not have to be named as such, and any additional fields will be treated as event properties.There can only be a maximum of one authenticatedID and one unauthenticatedID for aliasing.The event timestamp must be in UTCJSON fields must be pre-parsed and flattened into their own fields.All joins must be done beforehand. For any advanced enrichments or modeling such as creating custom user aliasing logic, pleasecontact us",
        "Instructions:": "In Analytics, click on the gear icon and selectProject Settings.Select theData Sourcestab.SelectNew Data Source.SelectConnect via Data Warehouse or Lake:SelectSnowflakeas your data connectionSnowplowas the connection schema and clickConnect.You should see thisSnowflake + Snowplow Overviewscreen. ClickNext.",
        "Connection Information": " Open theBigQuery consoleon Google Cloud Platform andSelect a project.Enter theGCP Project IDcontaining your Snowplow data.Enter theDataset Name.Enter theTable Nameand clickNextin Analytics.",
        "Grant Permissions": "You will need to copy and paste these code snippets into your Snowflake worksheet.Navigate to theWorksheetstab and paste the snippets into theSQL runner, and hitRun All.The last snippet needs to be applied in Admin -> Security -> + Network Policy.ClickNextto test your connection.",
        "Data Loading": "Load Timestamp FieldSelect the field used to identify new data. We recommend using a timestamp that denotes when the event was published, not the actual event timestamp to allow for late data to be collected. This will not impact your analyses since we reference the event timestamp for our queries. If you select to load data every 3, 6, or 12 hours, make sure to select a load timestamp field with at least hour precision (not a date only field).For example, if an event with an event timestamp of 12/1 was published to the table on 12/3, this will not be collected unless we use the publishing timestamp since every daily extract would look for events that occurred on 12/3. Using the publishing timestamp will allow us to extract all new data that was published to the table on a nightly basis.Start DateSelect the date from where Analytics should load your data from.::: success\nIf your data history exceeds 1 billion events, a Solutions Engineer will contact you to assist with the integration.\n:::Schedule IntervalSelect the frequency to make new data available in Analytics.Processing DelaySelect when we should start extracting your data in UTC. This time should be when all of your previous day\u2019s data is fully available in your table for extraction.",
        "Event Modeling": "In theEvents Fieldsection, input the field that should be used to derive Analytics event names.ForTimestamp, input the field that represents the time that the event was performed.ClickNext. After this step, we will perform a few checks on your data with the model that you provided. The checks are:Valid event field (Do at least 80% of your records have a value for the event field?)Valid timestamp field (Do at least 80% of your records have a value for the timestamp field?)Total number of unique events. We recommend 20-300 unique events and limit it to 2000.",
        "User Identification (Aliasing)": " For more information on User Identification (Aliasing), please refer tothis article. Note:If aliasing is not preferred, please set the Authenticated ID Type to None and pressNext. Select theTypefor the Unauthenticated ID:Atomic- This will allow you to choose between thedomain_useridandnetwork_useridfields that are part of the standard Snowplow event structure. We typically recommenddomain_useridsince this uses a 1st party cookie. Clickherefor more information.Context- If the unauthenticated ID is part of aSnowplow context, choose this option. Enter the values forVendor, Name, Version,andField.Other- If the unauthenticated field is not either of the options, please specify where we can find the unauthenticated ID in the data.Select theTypefor Authenticated ID:Atomic- Enter the field name that should be used for known users. Typically, it is theuser_idfield in the raw enriched event archive data.Context- If the authenticated ID is part of aSnowplow context, choose this option. Enter the values forVendor, Name, Version,andField.Other- If the authenticated field is not either of the options, please specify where we can find the authenticated ID in the data.None- choose this option to skip aliasing.",
        "Assisted Modeling": "You should see a summary of your data based on the last 7 days in two main blocks. You should only be concerned if the margin of error is significant. If so, please reach out to a product specialist: Events SummaryYou should see a daily breakdown of your Total Event Count, and the number of Unique Event Names. If there are certain events to exclude, please click on the Exclude checkbox for those events.If you would like to exclude any events by regex or property value, please contact a product specialist.Properties SummaryHere you will see the number of Unique Property Names. If there are certain properties to exclude, please click on the Exclude checkbox for those events.If you require more advanced configurations such as parsing out JSON fields, creating derived properties, or excluding properties based on regex, please contact a product specialist.Users SummaryThis section lists the number of Unique users seen. If the numbers do not look correct",
        "Waiting for Data": "",
        "Advanced Settings": "For additional advanced settings such as excluding certain events and properties, please refer tothis page.",
        "Manage your event tiers": "There are two ways to modify your event tiers: Individually through the mParticle UIIn bulk by modifying a CSV formatted Event Volume Report",
        "Modify an event tier in the UI": "To change the event tier using the UI: Navigate toData Master > Catalogand select the event type such asCustom EventsorScreen View.Next to the event, click theEvent Tierdrop-down and select a new tier.ClickApply. ",
        "Modify event tiers in bulk": "You can make bulk modifications to your event tiers by downloading a copy of your Event Volume Report, entering your new tiers in the New Event Tier column in the downloaded file, and then re-uploading your new report to commit your changes in mParticle. To make bulk changes to your event tiers: Navigate toData Master > Catalogand hover your cursor over theEvent Tiertooltip.ClickManage Event Tiers.  SelectDownload Event Volume Reportand clickNext.  Under Select Event Volume Data Range, use the date picker to select one of the date range presets or enter a specific date range to include in your Event Volume Report.Regardless of the date range you select for your report, it will represent every event type that mParticle has ingested for your account. The date range selection simply allows you to view the relative volume of events ingested for each type within a specific timeframe.   After entering the date range of the event data you want included in your report, clickDownload Report.Open the Event Volume Report downloaded to your computer. Find the row for each event type you want to change the tier for, and enter the tier in theNew Event Tiercolumn. Make sure to save your changes.Remember, the three event tiers areConnect,PreserveandPersonalize.  To upload your modified report, repeat steps 1-2, selectBulk Update Event Tiers, and clickNext.  In the Bulk Update Event Tiers modal, clickUploadand select your modified Event Volume Report.ClickUpdate Event Tiersto upload your modified Event Volume Report. When the upload is complete, a new Event Volume Report is automatically generated and downloaded to your computer so you can confirm that your changes were made correctly.",
        "Change the default event tier": "By default, mParticle assigns all event types to the Personalize tier. However, you can change the default assignment to Preserve or Connect if your account usesvalue-based pricing. Changing the default tier is helpful. For example, if you create a data plan with many new event types and most of them won\u2019t be needed for real-time evaluations, you could assign them to the Preserve or Connect tier by default. To change the default event tier for a workspace: In the left-hand navigation, click theSettingsgear cog to open the settings menu.  ClickWorkspaces.From the list of workspaces, click a workspace name to open the workspace settings dialog.ExpandDefaultsand use the drop-down in the Event Tiers section to choose a new default tier.ClickSaveto save your changes. After you set a default tier, all new events of all types seen in mParticle are assigned the new default tier. You can override the default tier assignment of an event type inData Master > Catalog. For example, you could change the default tier to Connect in the workspace setting and also set the tier for event type MyEvent to Personalize. Then, all new and existing events of type MyEvent are assigned the Personalize tier, while all other new events are given the new default, Connect.",
        "Event Volume Report": "The event volume report, accessible fromData Master > Catalog, is a CSV file that lists all the events used in calculated attributes and audiences. It also shows data volumes for the time range you specify.  The Event Volume Report displays an approximate count of each event type ingested from your inputs within a certain date range. You can use these metrics to identify event types to change to a different event tier, or event types to exclude from calculated attributes and audience definitions.",
        "Event Volume Report and value-based pricing": "There are differences between the Event Volume Report and your Monthly Credit Usage Report, which is a record of thebillable itemsyou are charged for. To learn how billable items are calculated, seeUsage metering for value-based pricing (VBP). The two reports present different views of your data ingestion: TheMonthly Credit Usage Reportprovides a retrospective view of the services your account is charged for according to the billing logic documented inUsage metering for value-based pricing.TheEvent Volume Reportprovides a view of how data is being ingested at the time the report is generated, and should be used to help you select your event tier assignments.",
        "Differences between the Event Volume Report and Monthly Credit Usage Report": "Following is a list of differences between the Event Volume Report and Monthly Credit Usage Report: The Event Volume Report doesn\u2019t include events added or dropped as a result of Rules, but those events are included in the Credit Usage Report.The Event Volume Report displays event tier assignments at the time the report is generated, but the Credit Usage Report displays event tier assignments for events at the time they were ingested and processed. For example, imagine the following scenario:Day 1 of your billing period: you ingest one million events with theScreen Viewevent type in theConnectevent tier.Day 5 of your billing period: you change the event tier forScreen ViewfromConnecttoPreserveand you ingest another one million events.Day 6 of your billing period: you download your Event Volume Report and see two millionScreen Viewevents in thePreserveevent tier. However, your Credit Usage Report will show one million events in theConnecttier and one million events in thePreservetier.The Event Volume Report includes automatically generated events like thebatch.SystemNotifications, which could include events likeConsentGrantedandConsentDenied. However, automatically generated events are not included in the Monthly Credit Report, because they are not billable items.If you download an Event Volume Report before a batch of events is processed, they will not be reflected in the Event Volume Report, but they will be reflected in the Credit Usage Report.Events timestamped 72 hours prior to when they are ingested are not be reflected in the Event Volume Report.For example: Imagine that on February 24, 2022 at 11:00 UTC, you ingest an event that is timestamped atFebruary 21, 2022 at 10:00 UTC. An Event Volume Report report created on February 21st will not reflect that event. However, it will be reflected in your Monthly Credit Report.",
        "Calculated attributes basics": "A calculated attribute contains the following elements that you define: Name: a descriptive label for the calculated attribute. Once activated, the name can\u2019t be changed.Description: an optional text field to describe the attribute.Calculation definition: The logic for computing the attribute: the calculation type, criteria, and the date range. For example, you might specify a sum calculation type.Status: Calculated attributes are created as drafts, indicating that they are not being calculated. When you change a calculated attribute status to active, mParticle starts calculating the attributes and you can use them across the mParticle platform and downstream.",
        "Scope": "Calculated attributes are defined and calculated per workspace; calculations use data available within the same workspace where they are defined. You can create calculated attributes with the same name and functionality in multiple workspaces.",
        "Initialization": "After you activate a calculated attribute, it initializes using existing data in the mParticle CDP along with any seeded value that was sent usingSeeding API. Depending on the date range selected in the calculated attribute definition, this can take several hours. After initialization, calculated attributes continue to recalculate with new data.",
        "Calculation speed": " Calculations are eithersynchronous(sent with the batch of data being processed) orasynchronous(sent with the next batch): Synchronous calculations are evaluated immediately and updated values are included in the same outgoing event batch.Asynchronous calculations are evaluated with a small delay (usually a few minutes) and updated values are included in the next outgoing event batch or when the feed input named Calculated Attributes is connected (more about this feed). If your event output doesn\u2019t support the Calculated Attributes feed, user attribute values may grow stale until mParticle receives the next event or batch for the relevant user.",
        "Calculation categories": "We currently support 13 calculations organized into four categories: Count: calculate the number of times an event has occurred, such as the number of logins or cart views.Aggregation: calculate a standard statistic about an event attribute such as sum, minimum, or maximum.Occurrence: calculate the timestamp for an event, or the first or last value or timestamp.List: calculate the unique list of values for an event attribute. For example, all the unique game titles played or unique product categories viewed. For a list of calculations and details about each, seeCalculated Attributes Reference. For an overview of how to use calculation categories, view the following video: ",
        "Calculation date range": "Calculated attributes can apply to a date range that you choose: Within the Last: limit calculations to a rolling window of time (days or weeks). For example, \u201cmost frequent product categories viewed over the last 30 days.\u201d This will maintain calculations over that specified period of time.Since: limit calculations to the period of a specified start date to now.Note that above the date range selection drop-downs, the UI displays the date that data was first ingested into mParticle.\nYou can choose a date earlier than the first date that data was ingested, however, mParticle only calculates as of the earliest ingestion date.All time: calculate based on all the data available per yourlong-term data retention policy. This option is only available if you have the Unlimited Lookback feature.",
        "Conditional logic": "After selecting an event, you can add conditions to the attribute in order to more precisely define results. For example, a retailer creating a calculated attribute might use the Contains operator with a Count attribute to count only the purchases that contain \u201cSock\u201d in the product name. For a complete list of operators for the four categories of attributes (Count, Aggregation, Occurrence, and List), seeConditions.",
        "Calculation attribute seeding": "Seeding allows you to pass in historic values for calculated attributes that mParticle builds on as new data arrives, without passing in the raw events. Seeding allows you to transition from your own calculations to mParticle calculation. For example, you may have data from outside mParticle about the last four months\u2019 bookings. You can create a calculated attribute, then send the seed data to mParticle usingthe Calculated Attributes Seeding API. mParticle then combines your seed data and live data so there\u2019s no interruption. You can seed calculated attributes in both draft (recommended) and active states; the calculated attribute must exist before you can seed it. Seeding requires two pieces of information: The seed values: the values required to calculate the attribute.The seed cut-off date: any data prior to this date is processed by your team into seed values, and mParticle only uses live data on or after this date in the calculation, combining the result with seed values. Using the correct cut-off date ensures an accurate transition into mParticle and avoids duplications of data in calculating a calculated attribute. Note the following calculated attribute behavior: After a calculated attribute is already in an active status, receiving data via historical API doesn\u2019t automatically trigger a recalculation. You must either create a new or update the definition of the calculated attribute to trigger a recalculation.Seeding of a rolling time window is not supported. Thus, seeded values don\u2019t decrement when they pass out of the calculated attribute window, for example a calculated attribute date range of \u201cwithin the last 4 weeks.\u201d",
        "Forwarding calculated attributes in an audience integration": "If a partner supports user attribute forwarding, you can forward calculated attributes in an audience integration alongside user attributes. Different partners have implemented user attribute forwarding in different ways. For example,Salesforceuses a separate data extension whileGoogle BigQueryuses the configuration setting Send User Attributes.",
        "Use Case Guide": "To walk through several different scenarios for using calculated attributes, download theCalculated Attributes Use Case Guide.",
        "Date range": "In the date range dropdown, Enterprise customers may select from Analytics\u2019 3 default date ranges: Last 1 Day, Last 7 Days, and Last 30 Days. Pro customers are limited to Last 7 Day Journeys queries. You may also customize your date range. Select the +Add Custom Date Range button, and input the amount of days you want your Journeys query to look back. Please note that Journeys queries are restricted to a 30 day look back.",
        "Widget Layout Flow": "In your Dashboard Settings, you may also toggle your Layout Flow between Free Flow and Auto Flow. If your dashboard is inFree Flowmode, then each analysis can be placed anywhere within the dashboard.If your dashboard is inAuto Flowmode, then each analysis can only be placed where there is space within the dashboard.",
        "Full-Screen Mode": "Often, you\u2019ll want to keep a dashboard open on a shared screen so that everyone is aware of KPIs in real time. To display a dashboard in full screen, navigate to the menu bar above your dashboard and click the \u201cFull Screen\u201d button. Your Dashboard analyses will update at regular intervals, as indicated by yourRefresh Interval settings.",
        "Print Mode": "Print Mode formats the dashboard for viewing on a printed page, which is especially useful for dashboards that are printed on a regular basis, or for those that are to be rendered in aReport. To toggle your dashboard to print mode, click on the \u201cManage\u201d dropdown in the top right, and select \u201cDashboard Settings\u201d. From the sidebar, toggle the Dashboard Layout Mode from Screen to Print. Keep in mind that analyses formatted for print mode may automatically adjust to fit. Once your dashboard is in print mode, you may print your dashboard by clicking \u201cPrint\u201d in the menu bar above your dashboard.",
        "Reports": "For more information on how to send scheduled reports, see ourReportsarticle.",
        "Instructions": "In Analytics, click onSettingsand selectData Sources.Click onNew Data Source.Select theGoogle Tag Managericon.You will need to use thisAPI Keyin step 9.Navigate to yourGoogle Tag Managerconsole and selectNew Tagin the top left corner.Click onTag Configuration.Click onDiscover more tag types in the Community Template Gallery.Select theIndicative Analyticstemplate, and add it to your workspace.Once you have added the Indicative Analytics tag to your workspace, you\u2019ll need to input your Analytics project\u2019s API key from step 4 into theAPI Keyfield.Once you have inputted your Analytics API key, you must select yourTag Type. You have the option to select Initialization, Custom Event, Clear Stateful Properties, or Clear Unique ID.Please note:all selections initialize the SDK, and send the Page View event to Analytics.Tag TypeMeaningInitializationInitializes the SDK and sends thePage Vieweventonly.Custom EventSends custom GTM events.Clear Stateful PropertiesClear properties stored as a persistent cookie (e.g. at user logout).Clear Unique IDClear the user ID stored as a persistent cookie (e.g. at user logout).If Custom Events is selected as your \u201cTag Type\u201d, an additional field called \u201cEvent Name\u201d will become available. This allows you to choose the Analytics event name from the available variables in GTM. Press the + on the right of the field to select a variable. \u201cEvent\u201d is the recommended variable so Analytics event names match GTM custom event names.If Custom Events is selected as your \u201cTag Type\u201d, you also have the option of adding Custom Properties. This allows you to choose the event\u2019s property names and values from the available variables in GTM. Press the + on the right of the property field to select a variable. Variables can be either normal event properties, which add context to an event, or they can be stateful properties, which remain the same for a user until the property is cleared (see above).Property TypeMeaningEvent PropertyAdd context to an event, like device type or marketing channel.Stateful PropertyEvent properties that remain the same for a user until the property is cleared.You may then optionally choose to set a custom unique ID. This variable will be used as each user\u2019s User ID within Analytics. You may then choose to set this ID as your alias. For more information on aliasing, see documentationhere.Next, click onMain Configurationto configure your session and cookie settings. Here, you can choose to track sessions and set the timeout threshold for each session. The default defines sessions as being separated by 30 minutes of inactivity.Then you must configure your cookie settings. Analytics recommends that you write cookies on yourMain Domainso that each subdomain on your website is tracked using the same cookie.More advanced Google Tag Manager users may choose to customize their Analytics-GTM integration further. In your Advanced Settings, you may set tag firing priorities, enable custom tag firing schedules, add tag metadata and more.Once you have configured your tag, select specifically which events you wish to send to Analytics in theTriggeringsection.Next, save the Indicative Analytics tag to your workspace by clickingSavein the upper right corner.Finally, in order to finish your Google Tag Manager integration, you must Submit your changes to your Workspace. Click on theSubmitbutton in the upper right corner of your \u201cOverview\u201d page.",
        "Start In Analytics": "In Analytics, click on the gear icon and selectProject Settings.Select theData Sourcestab.SelectNew Data Source.SelectAmazon Redshiftand clickConnect.",
        "Create Security Group": "The next step is to grant access to your Redshift cluster when accessed from Indicative\u2019s IP addresses. This involves creating a new Security Group in your VPC. Go toIAM Managementin the Console and chooseRolesfrom the sidebar.ClickSecurity Groupson the left side.ClickCreate Security Group.Fill outName tagand copy theGroup nameandDescriptionfrom Analytics. Make sure you choose the VPC that also includes your Redshift cluster. Click \u201cYes, Create\u201d to continue. We recommend using the values below.Add Analytics\u2019 IP addresses to the newly created Security Group.a. Select your newly created Security Group.b. Click the Inbound Rules tab.c. Click the Edit button.d. Add the following three rules to allow access to your Redshift cluster to Analytics\u2019 IPs:- Type: Redshift; Source: 54.227.242.108/32\n  - Type: Redshift; Source: 104.196.66.86/32\n  - Type: Redshift; Source: 35.227.102.123/32\n  - Type: Redshift; Source: 35.227.125.106/32",
        "Prepare Redshift Cluster": " Go toRedshift Console.Click the name of your Redshift cluster.Go to Cluster > Modify.  Select the Indicative security group, in addition to your existing security groups.Set \u201cPublicly accessible\u201d to \u201cYes\u201d.  Select an Elastic IP from the list.When complete, your cluster status should look like this:",
        "User Modeling": " After some basic checks, we can define your users within your data. For more information on User Identification (Aliasing), please refer tothis article. If you choose to enable Aliasing, click onEnabled:Type- Select the Snowplow field type Atomic- If the anonymous ID field is an atomic field, select this option.Field Name- Select the field that should be used to identify anonymous users.Context- If your anonymous ID is contained within the Contexts field, choose this option.Field Name- Select the context field that contains your anonymousID. If you choose to disable Aliasing, pressDisabled: Type- Select the Snowplow field typeAtomic- If the anonymous ID field is an atomic field, select this option.Field Name- Select the field that should be used to identify anonymous users.Context- If your anonymous ID is contained within the Contexts field, choose this option.Field Name- Select the context field that contains your anonymousID. If you have a non-null value that represents null UserID values, please click on theShow Advancedbutton. In this field, please enter these non-null values.",
        "Scheduling": " Select theSchedule Intervalto adjust the frequency at which new data is available in Analytics.Set theSchedule Timefor when the data should be extracted from your S3 bucket. It is critical that 100% of the data is available by this time to avoid loading partial data.SelectNext.",
        "Outputs": "Outputs are the destinations of your customer data. These are third-party marketing, analytics, or data warehouse tools that can accept either event data or user data. Your outputs are shown on the right side of the Overview Map. To add an output, click the+ Addbutton next to Events under OUTPUTS to create an Events output. This opens theSetup > Outputspage where you can select one of the many available output integrations to configure. To see which inputs are connected to an output, hover your cursor over an output\u2019s card and a highlighted purple line will appear between the output and any connected inputs.  A single output can have multiple configurations. To see an expanded list of configurations for an output, click the output\u2019s card. By hovering your cursor over each card in the expanded list, you can see which inputs send data to that particular configuration.",
        "Example - Connect an Output to Analytics": "mParticle has over a hundred event outputs, and the connection process for each is similar. This tutorial creates a connection to Analytics as an example. You can follow the same steps with a different output, or create afree Analytics accountto follow along exactly.",
        "Find Analytics in the Directory": "ClickDirectoryin the sidebar, and search for Analytics.Click the Analytics tile to display Output: Event Configuration.Enter the configuration information:Enter a configuration name.Leave the box checked to use the same settings for Development & Production.Select a field as the user identity field. Leave the default if you\u2019re not sure what to choose.Enter the Analytics API key which you can find in the Analytics project settings. Remember to save your new output configuration.",
        "Create the Connection": "Now that you have both an input and an output set up, it\u2019s time to connect them: ClickConnections > Connect, and select the input you\u2019ve already set up.ClickConnect Output.Select your Analytics configuration.Complete the Connection Settings, different for each output. For Analytics you can leave the default selections.Set the status toActiveand clickSave.",
        "Verify: Check that data is arriving in Analytics": "Once you have enabled the connection, open up the development build of your app again and create a few more events. Each output service displays the data it receives differently. For Analytics, you can view real-time data in theDebug Console.",
        "Troubleshoot": "For simple audiences, it\u2019s a good idea to check your Live Stream to see if you can find an event that should match your criteria. Here, you can see a user who has triggered the correct event.  Some things to check: Make sure you selected the right platforms. If the matching events are all from iOS, and you only selected the Android platform when creating the audience, you won\u2019t match any users.Examine each of your conditions against your test data from the Live Stream. Matches in the Audience Builder are not case sensitive. If you\u2019ve set attribute conditions, do the attribute values in your test data exactly match the value you\u2019ve provided in your condition?If you have multiple criteria, make sure your chaining statements are correct. Did you selectAndwhen you meantOr? Remember that recalculating an audience will take some time, so check your criteria thoroughly before you save your changes.",
        "Next steps": "Congratulations on making your first audience in mParticle! You will have noticed that mParticle populates your options in the Audience Builder based on the data you have captured. This means that as you add new sources, and send more data, you will unlock new options for building audiences. Check in periodically to make sure that you\u2019re getting the most out of your data. Some mParticle clients create hundreds of audiences, each with dozens of chained criteria to target hyper-specific user segments. You\u2019re only limited by the data you can capture and your imagination. A few things to read or think about: TheAudience docs in the Platform Guideprovide more detail about building criteria and advanced features like A/B Testing, and Audience Sharing.Audiences are a part of mParticle where the quality and consistency of your data plan become apparent. If your developers name an attributefavorite_colorin your Web implementation, andfavoriteColorin your Android implementation, it\u2019s going to be much harder to build a cross-platform audience to capture your users who lovegreen. Check out some docs about the importance of nameshere. Next up, you will learn how toconnect an audienceto one of mParticle\u2019s Audience partners.",
        "Open the Funnel Tool": "Select Create New Analysis, and then find Conversion and Drop Off in the dropdown menu. Selecting Conversion and Drop Off will set up a pre-configured query to identify points of friction between events. ",
        "Break out by Marketing Channel": "Tobreak outour results, let\u2019s add a User or Event Property. In this case, we\u2019ll use the event propertyMarketing Channel. Breaking out our funnel by this property will group users based on the marketing channel they were acquired through. To add these breakouts, re-expand the query builder and select Group By underneath the first funnel step, then chooseMarketing Channelunder the event properties tab. ",
        "Select from First/Last/All": "We\u2019re interested in understanding the marketing channels these users were acquired from.  Select First next to the event propertyMarketing Channelin the query builder. First will only display the channel that first led users to the homepage within the specified date range and will exclude marketing channels users were re-acquired through later on.",
        "Run the Query": "  The table below the chart is broken out by events along the X axis, and by marketing channel along the Y axis. For example, the first row tells us that of users who were acquired through Paid search: 43,484 completedSite Visit.10,309 completedBlog View, a 23.71% conversion fromSite Visit.5,551 completedSubscribe, a 53.85% conversion fromBlog View.The total conversion rate across all steps was 12.77%",
        "Compare Conversion Rates": "Let\u2019s click the row corresponding to the largest count in user signup, which is Email.  Clicking a row in the table expands the selection. All conversion rates within the webbings and top line update to reflect the selected campaign source. The total conversion rate is 10.26%, while the conversion rate for users whose acquisition marketing channel was Email is 8.34%. Now, let\u2019s compare that to the row with the highest (non-direct) conversion rate, which is Paid (12.77% conversion).",
        "Toggle User Properties On/Off": " Analytics automatically turns on certain common user properties. Some of these include first marketing channel, last device, last location, and last user profile information such as email. Any traits and attributes sent through anIdentify Callwill be turned on automatically. Under the Attribution Mode column, you may toggle the Off/First/Last selector to indicate which event properties you want to appear in the platform.",
        "First vs Last": "Each user property can take the first observed value or the last observed value. In order to toggle a user property on, you must select the attribution mode.",
        "Rename, Define, and Categorize User Properties": " Under the Property Key, you see the raw property name that is sent to Analytics. This can be renamed in the Display Name Column.Under the Display Name column, you may rename a user property. User properties without a display name will default to their original name.Under the Description column, you may add a description to a user property. Descriptions are useful for new users, and/or when your project contains similarly named events.Under the Category column, you may create categories and assign user properties to them. Creating categories helps keep your Data Panel organized.Under the Data Type column, you may designate the format of your user property. You may select one of the following: Auto, String, Numeric, Location (U.S. State), Date/Time.",
        "Saving to a dashboard": "Once you choose \u201cSave into Dashboard\u201d, a pop-up window will appear. Use the dropdown menu to select the dashboard in which to save your Journeys query. If you want to save your query to a new dashboard, you can create a dashboard from the save window. ",
        "Saving your analysis": "If you select Save Analysis, a pop-up window will appear. Here, use the dropdown menu to select the folder in which to save your Funnel bookmark. ",
        "Saving modified queries": "If you access and modify a previously-saved query, click the \u201cSave\u201d button in the top right corner above the query builder. This will replace the existing file. If you want to retain the original query, then save the modified query as a new analysis. Don\u2019t forget to change the query title in order to differentiate between the two! ",
        "Set a project-level sample size": "By default, Query-time Sampling is set toDisabledat the project level. This means that 100% of users will be queried unless a user toggles on sampling at the query level. Project admins can set queries to sample 10% of users as the default behavior. To do this: Navigate to theGeneraltab underProject Settings.Set Query-time Sampling (underFormats and Defaults) toEnabled. ",
        "Toggle sampling at the user level": "Individual users can toggle Query-time Sampling on and off when executing queries by clicking the lightning icon at the top-right of the query. A filled-in icon means sampling is enabled:  When sampling is enabled or disabled at the user level, this setting will persist within a session, but will be reset when changing projects.",
        "Applicable analysis types": "Query-time sampling is available inSegmentation,Funnel, andCohortanalyses. Samples can also be applied in the User tool in Event Lookup mode, but but not in User ID or User Segment lookup modes.",
        "Real-time audience page": "VisitAudiences > Real-timeto see a list of your audiences, and a count of how many active audiences you are using. Audiences are separated intoSingle Workspace,Multi Workspace, andShared With Metabs. The ** tab shows metrics for each real-time audience, including: Size: count of MPIDs in this audienceAdds (last 24 hours): number of additions to the audienceDrops (last 24 hours): number of drops to the audienceVolatility (last 24 hours): change in the audience calculated as: (adds + drops) / sizeConnected Outputs: count of connected outputsTagsLast UpdatedCreated ByAccess: whether the audience is private, view only, or usableStatus: whether or not the workspace is activeActions you can take on the workspace",
        "Workspaces, inputs, and audience criteria": "Every audience is populated from data sources that you specify when you create the audience: The workspaces that you select become the data sources for user-based criteria types.The inputs you select become the data sources for event-based criteria types. Because user and device identities are scoped at the workspace level, not the input level, they are available to all audiences in a workspace, for all inputs defined in the workspace, regardless of which input is selected for an audience. For example, if a profile is seen in a workspace in the audience scope, mParticle extracts all identities from the profile and uses them to evaluate against the audience criteria. Thus, if an audience is scoped to input A in a workspace, and a profile is seen from input B in the same workspace, user and device identities will be available for evaluation in an audience with input A, as long as A and B are both inputs defined in the same workspace.",
        "Create a real-time audience": "The audience workflow is simple: Define your use case.Create an audience.Specify the audience criteria.Select an output and configure it.",
        "Define use case": "Before you create audiences, define your segmentation and engagement strategies: What user audiences are important and why?How will you engage and/or monetize each user audience?How will you evaluate the effectiveness of your strategy? These decisions drive your implementation. Example: Audience Suppression The following video shows how to create an audience that excludes users from a particular campaign or from all campaigns: ",
        "Create an audience": "Before you create your first audience, the following video may help you understand the overall process:  To create an audience: SelectAudiencesfrom the main navigation, and then selectSingle Workspace, orMulti Workspaceif your input sources are in multiple workspaces, and clickNew Audience.Enter theAudience Name. You also have the option to provide anExternal Name. If provided, the external name is forwarded to Audience connections.If noExternal Nameis entered, theExternal Namewill be the same as theAudience Name.If you have enabledUnlimited Lookback, the date range is not displayed.UnderInputs, check all the Platforms and Feeds whose data you want to use to define the audience.Click theCreatebutton. The screen refreshes with the new Audience added to the list of audiences and theAudience Detailsscreen shown. If you are ready to define the audience, continue in the next section. Otherwise, clickSave as Draft. This screen shows a single Workspace Audience. Clicking theMultiple Workspace Audienceselection from the main navigation shows a dialog asking if you would like to switch to theMultiple Audience Workspacescreen.",
        "Define audience criteria": "After you create an audience, you can specify criteria to further define it. The scope of data that is evaluated by your audience criteria is dependent upon: The configurations you have selectedThe amount of data the mParticle platform has available for the configurationsThe data storage limit of your current subscription planThe tier setting for events. If set to any tier exceptPersonalize, an event can\u2019t be used as criteria in a real-time audience and won\u2019t be evaluated. After being set to any other tier, the event is grayed out in real-time audience selection drop-downs. The full audience definition is available once it is created. This means, for example, that you can create an audience, and before it is finished calculating, create a second audience that excludes members of the first audience. The first audience definition is available in full to the second audience definition, even if calculations are not complete. To add criteria to the audience definition:  You can add one or more criteria and logic either between two different criteria (and, or) or you can exclude users from an audience with criteria (exclude).After you define a criteria with the real-time audience builder a number displays that represents the estimated audience size:This audience estimator only supports real-time audiences.The estimate is based on a sample of user data within your organization.Audience criteria are scoped per workspace.When the calculation is complete, you can see the estimated size for an individual criteria next to theAppicon, and the estimated size of the whole audience in the Audience Details section. If there aren\u2019t enough users in the sample data to estimate audience size, you\u2019ll see a~without a number as illustrated in the example above. To add criteria to your audience definition: If you don\u2019t already have your audience displayed, navigate toAudiences > Real-timeand click on your audience to open it. Make sure you are in theBuildtab.ClickAdd Criteria.Select the type of data to be used in the audience definition. If your audience is built from both platform and feed data you will need to specify where the data should be drawn from.Specify additional qualifying parameters (i.e. attribute value, recency, frequency, platform version, build version, etc.) and clickDone.Optionally add additional criteria to the audience by addingand/or/exclusivecriteria to the definition.Click one of the following buttons once you have completed the definition of your audience:Save as Draft- to keep the draft of your Audience to work on laterActivateto begin populating the audience with users and make it available for connecting to outputs The audience builder allows you to build criteria based on two sources of data: Events: Criteria that checks for specific events and their properties. These criteria are subject to the audience event retention policy of your account. Within thenew criteriaoption in the audience builder, the following options create event-based  criteria: Events: access custom eventsEcommerce: access ecommerce eventsCrashes: access app crashesInstalls: access install eventsUninstalls: access uninstall eventsSessions: access session eventsUpgrades: access upgrade eventsScreen views: access screen view events As you define your audience criteria, a list of suggested matching values will appear based on what you\u2019ve entered. This feature works both when building new audiences and fine-tuning existing ones, helping you save time, reduce manual effort, and improves accuracy. To use this feature, you must have one of the following standard Roles:User,Admin,Audiences-only,Support, orAdmin+Compliance. Alternatively, you can create a Custom Role with any of the following tasks:audiences:draft,audiences:edit,catalog, oraudiences. User profiles: Criteria that checks your active user profiles. These criteria are subject to the user profile retention policies of your account. Within thenew criteriaoption in the audience builder, the following options create profile-based criteria: Users: access user profile information such as user attributes, calculated attributes, current audience memberships, consent state, location, etc.Attribution: access user install and uninstall information to build criteria based on the attributedcampaignandpublisher. As mentioned above, you can build audience criteria based on user attributes from the user profiles. These attributes can be of any data type including: numbers, strings, dates, lists, booleans, etc. All user profile data is scoped and maintained within a single workspace; In multiworkspace audiences, you can select which workspace to use by pressing the number in the top right of the criteria in the audience builder. To build an audience criteria based on a user\u2019s profile information, press theadd criteriabutton and selectUsersto view options for user based criteria: Audience membership: Checking a user\u2019s membership in other audiences. Only audiences which do not contain nested definitions can be selected. When using a standard audience membership criteria, the population starts with the real-time audience and refines from there. This criteria is not affected by standard audience expiration.Calculated attributes: Check a users calculated attribute valueConsent: Check a users CCPA or GDPR consent stateDevice, OS, Carrier: Check a users device type, carrier and operating systemFirst seen: Check the date the user was first seenLocation: Check user locationUser and device identities: Check the format and presence of user and device identitiesUser attributes: Check user attributesUser attribute lists: Check user attribute lists  When building audiences based on string attributes, several matching rules can be applied. All matches are case insensitive. Contains / Does Not Contain- Will match substrings. For example, \u201cblue\u201d, will match \u201cblue\u201d or \u201cblue shirt\u201d.Exact Match / Does Not Match- Entire string must match, no substrings. For example, \u201cblue\u201d, will match \u201cblue\u201d, but not \u201cblue shirt\u201d.Pattern- Wildcard style matching.*represents any number of characters,?represents any single character. For example, \u201cbl?e\u201d or \u201cb*e\u201d would both match \u201cblue\u201d.Includes / Does Not Include- The specified list includes an attribute you will specify. If you select this operator, an additional field appears for the attribute value. The full attribute value must be specified. For example, if the list is movies and you specify \u201cChicago,\u201d the movies \u201cChicago\u201d (2002, 1927) are returned, but not \u201cChicago Cubs.\u201dPartial Match- The inverse of Includes/Does Not Include, the specified list includes an attribute you will specify. If you select this operator, an additional field appears for the attribute value. For example, if the list is movies and you specify \u201cChicago,\u201d all movies with Chicago in the title are returned. There are two ways to build time-based criteria for audiences: by recency, and by date. Recency criteria define a period in time in relation to \u2018now\u2019, when the audience is actually being calculated, for examplewithin the last 7 days. Date criteria are based on fixed calendar dates which do not move in relation to when the audience is calculated. For example,after 09/12/18. Keep in mind that audiences defined using fixed calendar dates will have a shorter useful lifespan, as the audience builder only uses data from within a set range (last 30 days for most customers). Recency-based criteria select events occurring between two moments in time, relative to \u2018now\u2019. A \u2018day\u2019 represents 24 hours, and not a calendar day. For example, consider the following criteria:  If this audience is calculated at 1:00pm on September 9th 2018, then the earliest qualifying event would occur at 1:00pm on September 3rd, and the latest qualifying event would occur at 1:00pm on September 5th. Date-based criteria are concerned with calendar dates in UTC time and are not defined in relation to when the audience is calculated. Before date criteria is NOT INCLUSIVE of the given date. For example,Before September 9th 2018means that the latest qualifying event would occur at 11:59pm on September 8th 2018 UTC.After date criteria is INCLUSIVE of the given date.  For example,After September 9th 2018means that the earliest qualifying event would occur at 12:00am on September 9th 2018 UTC.On Date criteria cover from 12:00am to 11:59pm UTC on a given calendar day.Between Dates criteria are inclusive of the given dates. For example,Between September 7th 2018 and September 9th 2018means that the earliest qualifying event occurs at 12:00am UTC on September 7th, and the latest qualifying event occurs at 11:59pm UTC on September 9th. Attribution criteria can be used to segment users who have installed your app from a specific campaign and publisher or users who have purchased or re-engaged based on an engagement campaign. There are two ways to add attribution criteria: Profile criteria: SelectAttributionand then eitherInstallorUninstallto build criteria based on thepublisherandcampaignfields from an installattribution eventor uninstall attribution events. Like other profile based criteria, this is subject to profile retention limits.Event criteria: SelectEvents,Attributionand the event name to build criteria based on attribution events such as install, engagement or re-engagement events. Use the event name ofattributionto target install attribution events. This allows you to select any information included with the event ascustom_attributes.  Like other event criteria, this is subject to audience event retention limits. Identity criteria allow you to segment users based on their stored identities to test existence of a given identity or write logic against the identity as a string. This criteria will still scope the audience based on the workspaces included; It will not automatically include all users in theIdentity Scope. For example, if the identity scope is set at the account level and the account has 3 workspaces, an audience created in one workspace will only include users with activity in that workspace (and not the other two). Segment users by their location these two options available underUsers,Location: Equals: Segment users that are in a specific city, state, zip or DMA, using geolocation of the usersIP address.Within: Segment users that are within a set distance to any global city, usinglatitude & longitude coordinates. When using our Ecommerce events, you can easily target users that have added products to their cart, but not completed a purchase by usingcart abandonmentcriteria: Cart abandonment:New criteria->Ecommerce->Shopping - Cart Level->Cart Abandonment From here you can define how long to wait without seeing aproduct actionto include them in this audience.  UseExists / Not Existsto check for the presence of an attribute. For example, User Attribute Gender EXISTS evaluates as true for Gender = \u201cFemale\u201d and also evaluates true for Gender ={undefined}.",
        "Set up an audience output": "The next step is to connect the audience to an output service that can use the data.  See ourIntegrationsdirectory for a full list of output options. To add an audience output: Find the integration you want in theDirectory. You can filter the Directory to show only partners with an Audience Configuration.Click the card for your chosen partner.Click+ Add {partner} to Setupand, from the popup dialog, selectOutput Audience.Complete theConfiguration Settingsdialog. Each partner will require slightly different information. Some require an API Key/Secret/Token, others require you to log in from mParticle using Oauth. See theIntegrations Centerfor details for your integration. Give the configuration a name and clickSave.You can update your configurations at any time by navigating toSetup > Outputs, and selectingAudience Configurations.",
        "Connect an audience": "Once you have set up your output configuration, you can connect the Audience you have defined in mParticle. From theAudiencespage, select theConnecttab and clickConnect Output.  Select an output and complete theConnections Settingsdialog. This will be different for every integration. See theIntegrations Centerfor details for your integration.Make sure theStatusswitch is set toSendingand clickAdd connection. Any users that fit your audience criteria will begin to be available in the output platform. Some integrations take longer than others for this to happen. See the documentation for your specific integration for details. When mParticle forwards an audience to an output, we are only sending identities. mParticle is capable of collecting many types of identities for both devices and users, but most Audience partners will only accept the limited set of identity types that they actually use. For example, a partner that handles email marketing may only accept email addresses, a push messaging partner may only accept push tokens, and a mobile advertising platform may only accept device advertising identifiers (IDFA for iOS and GAID for Android). When building your audiences in mParticle, you don\u2019t have to worry too much about this. You can simply define your matching criteria, and mParticle will forward to each output as many available identities for each matching user as that partner accepts. You define a set of audience criteria, and mParticle finds 100 matching profiles. All profiles include one Apple Advertising ID (IDFA), but only 65 include one email address. You create connections to two Outputs: Partner A accepts IDFA and GAID identity types. Partner B accepts only the email identity type. It\u2019s not necessary for you to know which profiles have which identity types. mParticle simply forwards the 100 available IDFAs to Partner A, and the 65 available email addresses to Partner B.",
        "User profiles and identities": "mParticle creates audiences by comparing your matching criteria with each user profile. If a profile fits the criteria, each accepted identity included in the profile is forwarded to any connected Outputs. User profiles can contain data \u2014 including identities \u2014 collected from multiple workspaces. Even if your matching criteria only concerns data from a single workspace, once a matching user profile is found,allaccepted identities are forwarded to the output, even if the identities were collected in a different workspace. You have created 2 workspaces in your account to track activity for two related apps, App A and App B. User John Smith signs up for both apps, using the email addressjohn.smith@example.com. However, he uses his iPad for App A and his iPhone for App B. This means that there are two different IDFA identities associated with John Smith\u2019s profile. (note: read ourIDSyncdocumentation to understand more about how profiles with multiple identities are managed). You create an audience in the App A workspace, and your criteria match John Smith\u2019s user profile. When you connect that audience to an output that accepts IDFAs, mParticle will forward both of John Smith\u2019s IDFAs.",
        "Common settings": "The following video explains some of the common settings you use when creating an audience. ",
        "Audience A/B testing": "Audience A/B Testing allows you to split an audience into two or more variations and create connections for each variation independently, to help you to compare the performance of different messaging platforms. For example, if you have an audience of low engagement users that you want to reengage with your app, you might devise a test like this: Send 40% of the audience to Messaging Platform ASend 40% of the audience to Messaging Platform BKeep a control group of 20% who are not targeted with any messaging You can then compare the engagement outcomes for each group and apply the most successful strategy to the entire audience.",
        "Create a test": "From the Audience details page, select theA/B Testtab. If no test is set up for this audience, you will see only one \u2018Control\u2019 variation containing 100% of the users in the audience. Begin setting up the test by clickingAdd A/B Test Variation.  Enter the percentage of users you want your variation to contain. You can also provide a custom name for your variation.Create as many variations as you need for your test, up to a maximum of 5. The total of all your variations must always add up to 100%. You will notice that the \u2018Control\u2019 variation adjusts itself to100 - [sum of all created variations]. If you try to assign a percentage to a variation that would cause the total to exceed 100, you will see an error message.  When you are satisfied with your variations, clickSave.",
        "Create a connection": "After defining your variations, you can connect each one, including the control group, to any output by clicking+ Connect Output. There is also an option to connect the full audience to an output by clicking+ Connect Outputon the original audience card prior to the A/B split. In theAudiencessummary screen, audiences with an active A/B test will be marked with a%symbol.  Note that whenever the Audience Name is used in forwarding the audience to downstream partners, variant audiences will be named using the format[Audience External Name] - [Variant Name].",
        "Change an audience definition": "You can edit an audience definition without affecting the audience split, even after connecting to an output. When the audience is updated, the variants will still be balanced as defined when you created the test. You cannot modify the percentages of an A/B test after creating the test.",
        "End a test": "When you are ready to end a test, navigate to theA/B Testtab and clickDelete Test  Deleting a test will delete all variations and any connections you have set up for each variation.",
        "Download an audience": "You can download a calculated audience as a CSV file. This is useful if you want to troubleshoot your audience criteria, or if you want to share your audience data with a partner without an official mParticle Audience integration. Audience downloads take some time to prepare depending on the volume of users in the audience, ranging from a few minutes up to ~6 hours for extremely large audiences. Audience downloads are available on Real-time audiences only. To download a Standard Audience, connect and send it to an infrastructure output, like Amazon S3 via a Kinesis connection, and download it from there.",
        "Initiate a download": "You can initiate an Audience download, either from the mainAudiencespage:  or from theAudience Detailstab of an individual audience page:  If the Audience includes A/B Testing Variants, you can select which variants you want to download.  You can also choose to download an audience sample. Downloading a sample will likely take less time than downloading a full audience (depending on the audience size), and is useful for testing or troubleshooting.  You also need to select the identity types you want. ",
        "Download the file": "The download takes some time to prepare. When your download is ready, you will receive an email with download link. ",
        "Download format": "The download will be a ZIP file which, when extracted, will contain a CSV file for each audience or variant, plus amanifest.jsonfile, with metadata about the csv files. Audience CSV files have a row for each identity in the audience. Remember that a single user profile can have multiple identities and, therefore, multiple rows. The four columns show a unix timestamp for when the audience membership was retrieved, the mParticle ID of the profile, the identity type, and the value: The Manifest file will be in JSON format. See the following example for included fields:",
        "Delete an audience": "An audience can be deleted in the UI in a few ways, described as follows. In the Audience Overview:  In the Audience itself:  An audience can also be deleted with thePlatform APIusing the/audiencesendpoint. Note that if an audience is nested in another audience for exclusion or inclusion criteria, it can\u2019t be deleted.  It must be removed as nesting criteria for all audiences before being deleted.  If attempting to delete an audience nested in other audiences, the following message displays:  Integrations behave differently downstream after an audience is deleted:",
        "Bulk audience connections": "If you have defined a large number of audiences that you want to send to an output, you can establish the connections for many audiences at once, rather than doing them one at a time. Navigate toSetup > Outputsand selectAudience Configurations.SelectConnect Audiencesto the right of the Audience Configuration you want to connect audiences to.  Select the audiences you want to connect and clickNext.  Choose your settings. The same settings will apply to all audiences. ClickConnect.  You will see a status message showing all successful audience connections. If any audiences cannot be connected, error details will be shown.",
        "Audience tags": "As you continue to add audiences, you can use tags to help keep them organized. A tag is simply a label you can use to sort and search for audiences. For example, if you give all of your retargeting audiences a tag named \u2018Retargeting\u2019, you can easily find them all by filtering for the tag. You can add/remove tags for an audience directly from the Audience page, or in the audience settings. If you select more than one tag, the Audience page shows only audiences with both tags. There is no limit to the number of tags you can create, but each tag name is limited to 18 characters or less. If you clone an audience, it\u2019s tags will be cloned, also. ",
        "Audience faults": "If mParticle encounters errors forwarding an audience to an output, it will mark the connection as faulted. Audience Faults are visible from the Audience page, the Audience Connection screen, and the Audience tab onSetup > Outputs. While an Audience is faulted, mParticle will stop trying to forward audiences until the fault is resolved. Click the fault icon to view a detailed error message. If you can\u2019t determine the cause of the fault, the most common causes of faults include: AuthenticationSome integrations use OAuth tokens and require you to sign into your account in the Configuration Settings. These tokens eventually expire. You may need to go toSettings > Outputsand login again.Your username, password, API Key, API Secret, etc, may be incorrect. Check any API credentials.Permissions- Some integrations require the creation of an API User, whose credentials are used to access the partner\u2019s API. If the user whose credentials you provided in the Configuration Settings does not have permission to update audiences, you will see a fault.Rate Limiting- Many mParticle partners have limits on how often their API can be invoked within a given time frame. If this is exceeded, they return a \u2018Too Many Requests\u2019 error. If mParticle receives this error, we perform a number of retries in an \u2018exponential backoff\u2019 pattern - leaving more time before each successive retry. If the retries are exhausted before a \u2018success\u2019 response is received, the audience will be marked as faulted and mParticle will cease forwarding data. If your error message contains the text \u2018Too Many Requests\u2019 or the code \u2018429\u2019, the fault was caused by rate limiting. Contact the partner to clarify your account\u2019s API limits. It may be possible to increase your usage limit. When you believe you have resolved the issue, open the fault notification and clickResumeto resume sending data.",
        "User attribute sharing": "mParticle\u2019s Audience User Attribute Sharing feature allows you to include user attributes along with identities when you connect a supported audience connection. This allows you to use richer data in your activation platform, such as LTV, lead score or propensity to convert.  This feature does not forward or share your user data to any company beyond what you are explicitly configuring as an audience connection.",
        "Set up user attribute sharing": "Set up user attribute sharing in three steps. Create the Audience Connection in the usual way. For affected partners, you will see the following notification:  If you want to forward User Attributes to this partner, make sure you set the Status to Inactive as you create the connection. This will make sure you do not begin forwarding data until you have selected the user attributes to forward. From the connection screen, select the User Attributes you want to include. By default, all attributes are disabled. It may take up to 15 minutes before attributes begin to be forwarded.  Once you have selected the User Attributes you want to forward,Save and Activatethe Audience, open theSettingsand set theStatustoActiveto begin forwarding identities.",
        "Profiles and user attribute forwarding": "mParticle\u2019s user profile stores user attributes across platforms, workspaces and accounts. This means that, if your audience output uses device IDs, and if you are tracking a user across multiple platforms (mobile and web, for example) you may be able to forward user attributes that were not collected on the targeted mobile devices.",
        "Find Your Data Points": " The first time you visit the Data Filter, you will see an empty grid. To start filtering you need to: Choose your input source.You can filter data from \u2018Platforms\u2019 (iOS, Android, Web, etc.) or from incoming Feeds. Note that data for all your platforms will be included on one page in the Data Filter, so if you have used different event names or data types for different platforms, make sure you account for all of them.Add outputs.You can add an output to each column by clicking the+, or select and sort your outputs in the Choose Integrations dialog by clicking the button above, and to the right of, the grid.Choose a Data Type.Depending on the data available in your workspace, you can select from up to four data tabs: Events, Users, Screens and E-Commerce.",
        "Disable Data Points": "You can filter event data at 3 levels: Event Type, Event Name and Event Attributes. Disabling an event type will disable forwarding for all events and attributes of that type.Disabling an event name will disable forwarding for all that event\u2019s attributes. ",
        "New Data Points": "The Filter allows you to disable any current data point from being forwarded to any current output service. However, as you continue to use mParticle, update your app instrumentation, and add inputs, you will continue to generate new data points. One of the most important decisions to make in the Filter is how to handle new data points for each output service. By default, mParticle will automatically forward new data points to each output service. If you uncheck theSend new data points by defaultbox, no new data points will be forwarded to that output until you explicitly enable it in the Filter. New data points will be added to the Data Filter the first time mParticle receives them. It should only take a few minutes from mParticle receiving a data point for it to be visible in the Data Filter. ",
        "Shortcuts": "A mature mParticle project may have hundreds of events. That\u2019s a lot of filter switches. To save you time, we provide some convenience methods to help you set your filters quickly. To access the shortcuts for an output service, click the elipsis near the top of the column.  From this menu you can: Turn all filters on or off.Copy all settings for an output and apply them to another output.",
        "Step 1. Create a user segment in Analytics": "Click on an individual step within any funnel you have created in Analytics (i.e. a single bar in a bar graph, an individual data point in a line graph, etc.).Select eitherCreate User SegmentorSync User Segmentfrom the context menu:  Enter information to define your new user segment:To create the audience in mParticle, setProfile SynctoOn:  Select the mParticle workspace(s) in which you would like this segment to be available for activation. (Note: The checkbox below the workspace selector is automatically selected. If you unselect it, you will need to create the audience as described in themParticle documentation.)ClickSync & Go to Connect Audience. (If you have pop-ups blocked, allow them for this page.)",
        "Step 2. Activate your segment in mParticle": "Completing the previous step will bring you toAudiences > Real-timein mParticle, where theConnecttab will be displayed:  Click a listed output, or clickConnect Outputto select an output and configuration.",
        "Step 3. Verify your data flow": "Use the method described inVerify Your Audience Connectionto ensure that data is flowing as expected.",
        "First step vs. individual steps": "In Funnel, you may choose a date range for only the first step or you may choose a unique date range for each step. All new queries default to \u201cFirst Step.\u201d Once a user enters the funnel, they may complete the subsequent steps at any time after the first step.  If you select \u201cIndividual Steps,\u201d you may select a different date range for each step of the funnel. If a user does not complete the event during the selected date range, then the user will not appear in count of users for that step and they will be considered \u201cnot converted.\u201d Date ranges for individual steps are selected within each row of the query builder. ",
        "Conversion limit": "A Conversion Limit is a type of time-based filter. Users must complete the steps in the funnel within a defined time limit. The time limit is applied to either the entire funnel or on a per-step basis. All new queries default to Unlimited. For a full explanation, see our article onConversion Limits. ",
        "Average conversion time": "The Average Conversion Time is displayed in the top left corner of the results field. It describes the amount of time that it took converted users to complete all of the steps of a funnel. Only users who complete all of the funnel steps are included in the average conversion time calculation. Median conversion time is not available. To increase the accuracy of your average conversion time, consider using a conversion limit to exclude outliers. ",
        "Track Events": "We offer two options for submitting events to our REST API: submitting one event at a time, or submitting batches of events.",
        "Track a Single Event": "Single events can be tracked either through the POST method or the GET method Our REST API\u2019s single event endpoint is https://api.indicative.com/service/eventorhttps://api.indicative.com/service/event/{Your API KEY} To send us an event, you\u2019ll need to make a POST request to that URL with a Content-Type of \u2018application/json\u2019. In the POST body, include a JSON object with the following fields: As an example, your JSON Object would resemble the following: Our servers will respond with one of a few status codes indicating the outcome of your request: Analytics supports GET requests via its HTTP API. To utilize, send the same arguments included in the JSON of a normal POST request encoded as query parameters, with the exception of the properties dictionary. To send properties, include multiple query parameters named \u201cpropertyNames\u201d and \u201cpropertyValues\u201d corresponding to the keys and values in the properties dictionary. Note that the number of propertyNames and propertyValues must be the same. Our REST API\u2019s single event endpoint is: https://api.indicative.com/service/event To send us an event, you\u2019ll need to make a GET request to that URL with a Content-Type of \u2018application/json\u2019. In the GET body, include a JSON object with the following fields: As an example, your GET request would resemble the following: Our servers will respond with one of a few status codes indicating the outcome of your request:",
        "Track a Batch of Events": "To send multiple events at once, you\u2019ll want to use our batch endpoint at: https://api.indicative.com/service/event/batch It works similarly to the single event endpoint \u2014 it requires a POST with Content-Type: \u2018application/json\u2019 and a POST body containing the following: We recommend that your implementation include up to 100 events per batch. All events sent in a batch default to the current time. To override this, you can set the \u201ceventTime\u201d key for each event to a specific Unix timestamp value. As an example, a batch of two events that would be assigned with the current time would resemble the following: The status codes returned by the batch endpoint are the same as the single event endpoint specified above. When sending\u00a0either single or batched\u00a0events, there is a rate\u00a0limit of 250 events per second. If you need a higher rate limit, please contactsupport@mparticle.com.",
        "Identify Users": "Analytics\u2019 Identify endpoint allows customers to update user properties for a specified unique key. User properties that are specified through this endpoint persist until the property value has been overwritten by another call to the Identify or Track Events APIs. Our REST API\u2019s Identify endpoint is: https://api.indicative.com/service/identifyorhttps://api.indicative.com/service/identify/{Your API KEY} To call this method, make a POST request to the above URL with a Content-Type of \u2018application/json\u2019. In the POST body, include a JSON object with the following fields:",
        "Alias Users": "Our REST API\u2019s alias endpoint is: https://api.indicative.com/service/aliasorhttps://api.indicative.com/service/alias/{Your API KEY} For more details on how aliasing works, see more detailed documentationhere. To send us an alias, you\u2019ll need to make a POST request to that URL with a Content-Type of \u2018application/json\u2019. In the POST body, include a JSON object with the following fields:",
        "Suppress User Data": "Analytics\u2019 Suppress endpoint allows customers to suppress future data from being processed for a specified unique key. The REST API\u2019s Suppress endpoint is: https://api.indicative.com/service/suppressorhttps://api.indicative.com/service/suppress/{Your API KEY} To call this method, make a POST request to the above URL with a Content-Type of \u2018application/json\u2019. In the POST body, include a JSON object with the following fields:",
        "Delete User Data": "Analytics\u2019 Delete endpoint allows customers to submit a request to irrevocably delete data for a specified unique key from Analytics. In addition, a call to this endpoint suppresses future data from being processed for a specified unique key. The REST API\u2019s Suppress endpoint is: https://api.indicative.com/service/deleteorhttps://api.indicative.com/service/delete/{Your API KEY} To call this method, make a POST request to the above URL with a Content-Type of \u2018application/json\u2019. In the POST body, include a JSON object with the following fields:",
        "1. Create CSV files": "Prepare the CSV files for import. Files must follow the guidelines in this section.",
        "File guidelines": "Files must adhere to theRFC4180 standardsfor CSV formatting.Files must be sent in one of the following formats:A plain CSV (.csv)A ZIP file containing one or more CSV files (.zip)A gzipped CSV (.csv.gz).A PGP/GPG-encrypted file with the additional extension.gpgappended, for example,.csv.gpgor.csv.gz.gpg). Only encrypted OR unencrypted files can be accepted, but not both.  You must use PGP encryption with mParticle\u2019s public key. SeeEncrypted filesfor additional instructions.File sizes should be between 5 MB and 2 GB. If you upload files outside these limits, processing speed is slower. If possible, split the data across multiple small files, because their processing can be parallelized.Each file can only contain events of the same event type. You can\u2019t mix events of different types in the same file.Don\u2019t use subfolders.Each row size should be under 80 KB. Larger rows may impact performance.All column names must be unique.Each CSV file must contain fewer than 50 columns.File name requirements:Do not use any dashes ( - ) or dots ( . ) in your file name, other than what is described below.End the file name based on the event content in your file:-custom_event.csv-commerce_event.csv-screen_view.csv-eventless.csvfor eventless uploads of user identities and attributesColumn names: specify fields according to ourJSON Schema, using dot notation.Column names described are case sensitive.",
        "Data guidelines": "Environment: include a column nameenvironmentset todevelopmentorproduction. If anenvironmentcolumn is not included, data is ingested into the production environment.User and Device IDs: as with any data sent to mParticle, you must include a column with at least one user ID or device ID.Device IDsdevice_info.android_advertising_iddevice_info.android_uuiddevice_info.ios_advertising_iddevice_info.att_authorization_statusdevice_info.ios_idfvdevice_info.roku_advertising_iddevice_info.roku_publisher_iddevice_info.fire_advertising_iddevice_info.microsoft_advertising_idUser IDsmpiduser_identities.customeriduser_identities.emailuser_identities.facebookuser_identities.microsoftuser_identities.twitteruser_identities.yahoouser_identities.otheruser_identities.other2user_identities.other3user_identities.other4Important: CSV files must have all the required identity columns, and the rows must have valid values in those columns to prevent processing errors.User attributes:If you include user attributes, for each, include a column named asuser_attributes.key, wherekeyis a user attribute key. For example:user_attributes.$FirstNameuser_attributes.communication_preferenceuser_attributes.Member TierAttribute names with spaces are allowed and do not require quotes. All the keys listed in theJSON Referenceare supported.Events:Use a column namedevents.data.timestamp_unixtime_msto set the event time.Use a column namedevents.data.custom_attributes.key, wherekeyis an event attribute key, to set custom event attributes.Attribute names with spaces are allowed and do not require quotes. All the keys listed in theJSON Referenceare supported.Screen view events: use a column namedevents.data.screen_nameif you want to include the screen name.Custom events: use columns namedevents.data.event_nameandevents.data.custom_event_typeto include custom events.Commerce events: use columns with the following names for commerce events.events.data.product_action.actionevents.data.product_action.products.idevents.data.product_action.products.nameevents.data.product_action.products.categoryevents.data.product_action.products.brandevents.data.product_action.products.variantevents.data.product_action.products.positionevents.data.product_action.products.priceevents.data.product_action.products.quantityevents.data.product_action.products.coupon_codeevents.data.product_action.products.added_to_cart_time_msevents.data.product_action.products.total_product_amountevents.data.product_action.products.custom_attributesOnly one product per event can be included for commerce events uploaded via CSV.Data types:All data in the CSV is converted to a string. The only exceptions to this are values that require a particular data type, such as MPID or IDFA.Only standard custom events and screen views, and eventless batches (eventless drops of user identity and attributes), have been tested.Attributes sent as arrays are not fully supported. When the entire array is present in a single cell of the CSV file, it is supported and is converted to string. Because there is no way of specifying anything but the first item in an array, repeated header columns, each subsequent column overwrites the previous one. Multiple columns don\u2019t append to the array. This is why you can only include one product for ecommerce events. Commerce events in the Events API support arrays in multiple places, but with CSV files, you can only populate a single item in each of these arrays.Custom manifest: You can use a custom manifest to drop files created in another system without transforming them. For details, seeUse a custom manifest.",
        "2. Get credentials for the mParticle SFTP server": "mParticle maintains an SFTP server where you will drop your CSV files.\nUse the following instructions to securely retrieve your credentials and find the hostname and path to use when you drop your files on the SFTP server. To get your SFTP username and password: Sign up for a Keybase account with your work email athttps://keybase.io/. Keybase is a secure tool which includes end-to-end encrypted chat.Provide your Keybase account name to your Customer Success Manager or your mParticle Solutions Consultant so that they can pass it on to our Ops team.Expect to receive your SFTP access credentials in a Keybase chat from mParticle. Note that if you need to use credentials that you already have, you\u2019ll share those credentials in the Keybase chat.",
        "3. Configure the Custom CSV Feed": "Configure the Custom CSV Feed as input. This step provides the hostname and folder path on the SFTP server where your CSV files must be dropped. To configure the Custom CSV Feed: VisitSETUP > Inputs > Feedsin the mParticle UI and click theAdd Feed Inputbutton, then select Custom CSV Feed from the list.If you\u2019ve already added the Custom CSV Feed, it won\u2019t show up in the list. Scroll through the list of feeds until you seeCustom CSV Feed, and then click the large plus sign in the gray bar to create a new feed. You need one feed for each different event type.Enter the following values:Configuration Name: enter a name that makes this feed easy to recognize in your list of feeds.Custom Event Name: if you are importing a custom event, enter the name that will be used for the custom event.Custom Event Type: if you entered a custom event name, select the event type.Custom Manifest: if you are using acustom manifest, paste it in the text box provided.Expect Encrypted Files: if you will import a PGP/GPG-encrypted file, select this option.After you complete the connection configuration, clickIssue SFTP Details. mParticle displays your hostname and path for mParticle\u2019s SFTP server.",
        "4. Drop CSV files on the SFTP server": "Connect to the mParticle SFTP server using the credentials provided. Once you have connected, the mParticle creates thedropfolder. If you don\u2019t see one, create a folder nameddrop.Create a new folder inside thedropfolder, and name it using the pathname provided in the mParticle UI as shown in the previous section. For example, based on the previous example, the folder path and name issftp.mparticle.com:4422:drop/us1-123456789123456789/.Hint:Verify that there are no trailing spaces in the name.Use your credentials to upload your CSV files to mParticle\u2019s SFTP server, using the correct path and folder name from the previous step. Files on the SFTP location are added to the processing queue nearly immediately.  Depending on file count and file size, a backlog may develop. You can observe how much data has been processed using Data Master and your outbound connections. There is no notification of processing progress or completion.",
        "How to access the Overview Map": "The Overview Map is the first page you see after logging into your account atapp.mparticle.com. You can navigate back to the Overview Map at any time by clicking the Overview button at the bottom of the left hand nav bar.  The Overview Map is unique to each workspace in your account, because each workspace may contain a different configuration of inputs, outputs, and mParticle features. If you can\u2019t find an input, output, or feature that you are expecting to see on the Overview Map, make sure you have selected the correct workspace.",
        "Overview Map settings": "The Overview Map has a few basic settings allowing you to control its appearance, and the type of information it displays.",
        "Switching views": "The Overview Map provides three different views. Each view is designed to highlight a different collection of information describing your mParticle account. You can switch between different views of the Overview Map using the View button to the left of the zoom controls.  The Connections view only displays your inputs, outputs, and the connections between them. To see which inputs and outputs are connected, hover your cursor over any input or output, and a highlighted purple line will appear showing the connections. Every output connected to a single input will be highlighted. For example, in the map below you can see that the iOS input is connected to the Amplitude and Amazon Redshift outputs, in addition to a selection of other outputs. If your workspace contains a large number of inputs or outputs, you can view them in a complete list by clicking the+ more Eventsbutton.  The Overview view provides a simplified diagram of the feature sets that you have configured in addition to your inputs and outputs.  The Data Flow view provides a complete diagram of each specific feature your data flows through between your inputs and outputs, including Rules, Data Plans, and Filters. ",
        "Navigating with the Overview Map": "The Overview Map is interactive: you can use it to navigate to different products, features, and settings within mParticle. If you see a product or feature shown on the map that you can\u2019t click on, your role doesn\u2019t have adequate permissions to view it. Contact your mParticle account administrator to request access.",
        "Inputs": "Your inputs are the sources of your customer data. You can create Platform inputs, which use the mParticle SDKs to collect and ingest data directly from different device platforms (like iOS or Web), or Feed inputs, which ingest data from third-party marketing or data warehouse tools. All of your inputs are listed on the left side of the Overview Map, with the Platform and Feed inputs separated into two groups. To add an input, click the+ Addbutton next to Platforms or Feeds under INPUTS to create an input of either type. This opens theSetup > Inputspage, where you can create a new Platform or Feed input. To view connections between an input and your outputs, hover your cursor over an input\u2019s card and a highlighted purple line will appear between the input and any connected outputs. To add a connection from an input, click the+icon next to an input card.  This opens theConnections > Connectpage for that particular platform input, where you can select one of your configured outputs to connect the input to.",
        "mParticle Suites": "The mParticle platform includes several suites, or collections, of tools that you use to monitor, manage, and leverage your customer data. There are six different suites, and they are are displayed in the center of the Overview Map:  This group of features allows you to monitor the health of your data infrastructure and manage your data privacy settings. Features here include: System AlertsObservability (currently in beta)Privacy ControlsData Subject Requests  The Data Platform features provide an overview of what data is flowing into mParticle, both in real time and historically. Features include: TrendsLivestreamData CatalogEvent Forwarding  Customer 360 is the collection of tools and features related to your customer data, including: User ProfilesGroup IdentitiesCalculated Attributes  From the Customer 360 Profile, you can gain a deeper understanding of your users\u2019 behavior, and make predictions based on their behavior to power a highly personalized, omnichannel customer experience: Predictions: discover how best to engage with your users with mParticle\u2019s machine learning and AI powered predictive analytics suiteAnalytics: gain actionable insights into how your users behave and interact with your brandSegmentation: create groups of related users and perform user journey analytics, testing, and cross-channel orchestration ",
        "Data platform management": "mParticle provides several data management tools that help you control exactly what data you ingest from your inputs, what data you send to your outputs, and to improve the quality of your data. These tools areData Plans,Rules, andFilters, and they\u2019re shown on the Overview Map at the intersections between the mParticle Suites and your inputs and outputs. ",
        "Data Plans": "Data Plans allow you to improve the quality of the data you ingest by validating inbound data against a schema. Data that doesn\u2019t fit the schema you define isn\u2019t ingested by mParticle, ensuring that you have high quality when creating Audiences, Predictions, or when forwarding data to your downstream marketing and engagement tools. Data Plans only operate on data as it is ingested from inputs into the mParticle Suites. To view your Data Plans, click thePlansbutton on the Overview Map. Learn more about Data Plans.",
        "Rules": "Rules are scripts that manipulate data that is either being ingested from an input, or forwarded to an output. They allow you to cleanse, enrich, or transform your data by changing event names, modifying event or user attributes, or adding or removing events from a batch. Rules can be used on both inbound and outbound data. To view or add Rules, click theRulesbutton on the Overview Map. Learn more about Rules.",
        "Filters": "Filter a query to include or exclude users who meet a certain condition. You can filter on event properties, user properties, or user segments. There is no limit to the number of filters you can apply to a single query row. In the following example, PetBox wants to exclude any users who downloaded their app on Android. Therefore, the user selects the filter where function, and creates a filter where device type is not equal to Android. ",
        "Identity Resolution": "IDSync, mParticle\u2019s identity resolution framework, allows you to manage how your users are identified wherever, and whenever, they engage with your brand. IDSync works in tandem with Customer360 to give you control over exactly what data is attributed to which user profiles. The Identity Resolution process is noted on the Overview Map between Data Platform and Customer360, but it is not currently navigable via the Overview Map. To manage your IDSync settings click theSettingsicon in the left nav bar, clickPlatform, and select the Identity Settings tab. Or, click on the identity resolution circle. Learn more about IDSync.",
        "How Profiles Are Used": "Use profiles in mParticle to understand users and create personalized experiences: Inspect the profileof any user in the mParticle UI.Enrich a profilewith complete and up-to-date information about the user it represents.Deliver profiles in JSON format with the Profile APIto create personalized experiences based on user attributes or audience memberships.Drive real-time calculations and Customer segmentations.",
        "User Activity View": "Inspect the profile for any user in the mParticle UI via the User Activity View. To find user profiles, you can search by any user identity type you capture, or by the mParticle ID. You can also navigate to the User Activity View for active users in your development environment by inspecting a batch in the Live Stream. For more information, seeUser Activity.",
        "Profile Enrichment": "Profile enrichment allows mParticle to make sure that each of your downstream systems has the most complete and up-to-date information about each of your users. For example, mParticle receives data as batches from native SDKs, our HTTP API and third-party data feeds. Each batch is a JSON object containing an array of events and contextual information about the user, such as identities, user attributes, and device information. mParticle processes the information in each batch and forwards it to downstream systems through event integrations. Before a batch from a particular source is forwarded, mParticle compares it to the matching user profile and adds additional information.  If you collect the same user attribute from multiple sources, for example an iOS app and a web app, mParticle accepts the most recent version. For example, if Bob sets his favorite drink to \u201cBeer\u201d in the web app, and then changes it to \u201cCoke\u201d in the iOS app the next day, Bob\u2019s user profile will use the most recent value for enrichment. Enrichment ensures that all of your downstream tools can receive complete and accurate information about your users. Remember that you can still usedata filtersto prevent downstream systems from receiving user attributes that they don\u2019t need.",
        "Profile API": "The following limit applies to resources in theProfile API.",
        "Audiences and Calculated Attributes": "Audiences allow you to define segments of users based on rule based criteria of their event behavior and profile data. mParticle builds and maintains these segments of users over time which can then be connected to hundreds of outputs for activation. Audiences and Calculated attributes are built from all your profile data, including calculated attributes, and events that have been collected across all your data sources. For real-time audiences, these are within the audience look back window. Standard audiences utilize your extended Data Retention policy. For more information about data retention, seeData Retention. For more information, seeAudiencesandCalculated Attributes.",
        "Understanding Profile Data": "Two main classes of data provide context about your users and the events they trigger. User dataUser data describes the attributes of individual user profiles. It includes information such as what identities they have, device types and IDs, and a number of custom attributes such as membership status and demographic information. An attribute can reflect either current or previous values, depending on its nature and how often it is updated. User data is stored in profiles.Event dataEvent data describes actions that your users take. They contain information current for the moment at which the event was triggered. For example, the event \u201cSign up\u201d could have an event attribute of \u201cmembership tier\u201d, which denotes the membership status at time of signing up. Event data is stored in events.",
        "Profile Schema": "Each profile describes useful details about the user associated with the profile.  A user profile contains the following elements: mParticle ID - a unique identifier for the user within mParticleTheIDSync APIresolves identities like email, customer ID and device IDs to a single mParticle ID.Each mParticle ID maps to a single user profile.First-seen timestampUser responses to Limit Ad Tracking (iOS) and global opt-outAll known identities for the userAll Audience memberships for the userThe most recently seen IP address associated with the userThe IP address is always updated with the most recent value based on theX-Forwarded-Forheader for client SDK batches or theipfield for batches ingested from server-to-server connections. The IP address on a user profile is only forwarded to connected outputs if theEnrich IP Addresssetting is available and enabled for the connection.For each workspace the user has been seen in:All user attributes captured in the workspace, including Calculated AttributesAny consent information captured for the userFor each device the user has been seen on:Device Application Stamp - a unique identifier for a device within a workspaceDevice information, including device identitiesFirst-seen timestampApple App Transparency Tracking statusInstall attribution information More about the mParticle schema can be foundhere.",
        "How a Profile Is Updated": "Most of the time, mParticle automatically keeps user profiles updated as you capture new data with any of the following methods: ThesetUserAttributeSDK methodSending a \u201cbatch\u201d of user and event data to the HTTP APISending user data from a third-party feed However, sometimes it is necessary to make direct updates to your user profiles in bulk. This happens most often when you\u2019re loading large amounts of data from legacy systems. To directly update a profile, you can make a standard request to our HTTP API, and leave out theeventsnode. For example: User attributes updated in this way will not be immediately updated in all downstream event integrations. Most event integrations in mParticle will not process a batch with no events. However, as long as you are sending some event data to each integration, the enrichment process will make sure that user attributes are updated the next time an event is sent for each user. If it is important for profile updates to be reflected across all your systems immediately, add an event to the batch. For example:",
        "User attributes and input source priority": "For a given user, attributes are stord at the workspace level, not the device level. User attributes are ingested according to the following priority: Calculated attributesAccount-level attributes (a premium feature)Custom feed inputSDK inputCustom CSVPartner data feed (PDF), for examplethe Punchh integrationis a partner data feed. Once ingested from a particular source type, subsequent source types for that same data won\u2019t be ingested. For example, once you set a user attribute key value using the Web SDK, you won\u2019t be able to set that same value from a partner data feed.",
        "Input protections": "You can override the default behavior for input feeds, and specify whether data ingested from a feed creates new profiles or updates existing profiles. Choose from one of three input protection levels: Create & Update - Allows the input to both create new profiles and update existing profiles. All inputs default to this setting.Update Only - Prevents input from creating new profiles, but allows the input to update existing profiles.Read Only - Prevents the input from creating or updating profiles. The default profile protection level for all input feeds isCreate & Update. Profile protection levels are specific to each input configuration, providing more granular control over multiple configurations even if they are contained in a single workspace. When ingesting user data from an input, mParticle searches for an existing profile that corresponds with any of the user identifiers included with the data. The identity strategy configured for your account determines if a new profile is created when an existing profile can\u2019t be found. You can prevent unwanted changes by defining one or more input protections.",
        "Configure the profile protection level for an input": "To set the protection level for an existing input: Log into your mParticle account and navigate toSetup > Inputsin the left nav bar.Click the Feeds tab and expand the list of configurations for your input.Select the configuration you want to change the protection level for.Use the Input Protection Level dropdown menu to change the protection level for the input. When creating a new input feed, you are given the input protection dropdown menu, allowing you to set the protection level for that input from the start.",
        "Create & Update Profiles": "This setting removes all restrictions from an input\u2019s access to a user profile, meaning that new user profiles can be created and modified as data is ingested. For example, if mParticle receives data from the input that it can\u2019t associate with an existing profile and your account uses thedefault identity strategy, then it creates a new profile for the user. If a matching profile was found, the data from the input enriches the found profile.",
        "Only Update Profiles": "This setting ensures that only data from known, existing users in the workspace is ingested. If IDSync can\u2019t find a matching profile (in other words, if the user is unknown) then the data is discarded. Only existing users within the workspace can be updated, and all other unknown users are ignored. For example, if mParticle receives data from the input that it can\u2019t associate with an existing profile and your account uses thedefault identity strategy, mParticle will not create a new profile for the user and the data will not be associated with an MPID. However, if a matching profile was found, then the user data from the input enriches the found profile. If an event batch includes user data with an MPID or user identifiers that don\u2019t resolve to an existing MPID in your mParticle workspace, mParticle will drop the data and it will not be forwarded to any outputs. However, this is still a billable event due to the processing needed when searching for a matching MPID in your workspace. Imagine you have a business with a large customer base, but multiple storefronts or customer touch-points. These various touch-points are housed under separate brands and any one customer engages with many of them. Furthermore, you have chosen to use a centralized data warehouse to help keep your customer data consolidated. However, you want to ingest customer data into mParticle from your warehouse while maintaining a single user profile for each customer. To do this, you must prevent new profiles from being created. This is where theOnly Updateprotection level is useful. By setting your data warehouse input feed toOnly Update, mParticle ensures that as new data for a known customer is ingested, that data is attributed to a single user profile. Without this protection, it would be possible for a separate user profile to be created for each of your brands a customer engages with. This would prevent you from maintaining a holistic perspective of your customers\u2019 history.",
        "Only Read Profiles": "Prevents all write access to profiles and IDSync. Event batches are ingested normally, but existing profiles can\u2019t be modified and new profiles can\u2019t be added to protect user profiles and IDsync records from undesired modifications from the input. For example, if mParticle receives data from the input that it can\u2019t associate with an existing profile and your account uses thedefault identity strategy, then it will not create a new profile for the user and the data will not be associated with an MPID. If a matching profile was found, mParticle will not associate the data with the found profile. If an event batch includes user data with an MPID or user identifiers that don\u2019t resolve to an existing MPID in your mParticle workspace, mParticle ingests and forwards the event batch to other outputs, but it will not create a new profile for the MPID. Keeping your customer data clean and uncontaminated is critical. However, it\u2019s also essential to be able to test new connections from your data warehouses to mParticle. To configure and test a new input feed from a warehouse without allowing any ingested data to modify your existing customer profiles, you can set the initial protection level for the new feed toOnly Read. This allows you to verify that data is ingested properly, without any of that data leading to the modification or creating of new user profiles. After you\u2019re confident in your new input configuration, you can change the protection level to allow data from that input to begin enriching profiles.",
        "Data retention and profiles": "Profiles are available for different periods of time, depending on thedata retention policyfor your account.",
        "Enhance User Profiles with UID2": "The Professional Services team at mParticle provides the following guide for enriching your user profiles with UID2 for more effecive activation through The Trade Desk and others. If you have further questions while implementing the guide, please reach out to your Professional Services contact or account manager. This guide references resources that can be found onGitHub. There you will find source code and supporting files for a serverless application that you can deploy with AWS\u2019s serverless Application Model Command Line Interface (SAM CLI). It includes the following files and folders: Trade Desk Enrichment - Code for the application\u2019s Lambda function.Events - Invocation events that you can use to invoke the function.Trade Desk Enrichment/tests - Unit tests for the application code.template.yaml - A template that defines the application\u2019s AWS resources.",
        "Architecture Diagram": "",
        "Setting Up the Solution in mParticle": "In mParticle, create an audience with the criteria for all users who haven\u2019t had their user profile enriched in the last n days.  Connect the audience to the enrichment module. mParticle will then send emails that need a raw UID value. The enrichment service talks to either a private or public UID instance to retrieve the token.Once the UID value is obtained, it will send a custom event to mParticle via custom feed input, adding the UID value as a partner ID.",
        "Create a predictive audience": "To create and activate a predictive audience: In mParticle, navigate toAudiences > Real-timeand click New Audience.Enter your information in the New Audience form and clickCreate.In the Build tab, clickAdd Criteriato display a list of items:If you haven\u2019t already created a user prediction, click User Prediction from the list and enter the User Prediction information:If you\u2019ve already created a user prediction, select it from the list inUsers > Choose User Feature > User Attributes.ClickCreate prediction.mParticle displays the audience definition, where you can see that two user predictions have been created, one for score and one for percentile. Choose one.Specify the threshold. For example, if you wanted a likelihood of greater than 90 percent, you\u2019d set the following values:ClickDone.Choose eitherSave as DraftorActivate.Once you chooseActivate, mparticle prompts you to optionally create an A/B test, and thenconnect to an output. After you create a prediction, a pipeline is created in Cortex, and the pipeline begins calculating. Calculation may take up to 24 hours, though typically the delay is about an hour. If there\u2019s not enough data to learn from, the calculation may fail and the prediction won\u2019t exist, resulting in an empty audience. To correct this, choose different criteria or troubleshoot your data for issues. To check on the status of your pipeline, view it in Cortex.",
        "Include and exclude users from predictions": "When setting criteria for a new user prediction, you can specify whether Cortex should generate that prediction for all users or a specific subset. Narrowing predictions to a subset of users can help improve the accuracy of your predictions, and avoid generating predictive attributes for users who are not relevant to a specific use case. To focus on a subset of users, select the optionA subset of usersin theMake predictions forfield.  Once you\u2019ve selected this option, you can build queries with user attributes and behavioral events to select the users for whom Cortex will generate this prediction. ",
        "Why include or exclude users?": "While it may be counterintuitive, using more data to generate a prediction does not mean the prediction will be more accurate. Not all users are relevant to every prediction you want to create, and irrelevant users can make it more difficult for the ML model to identify meaningful patterns and trends. This is why it\u2019s best to consider the business outcome you want to achieve when defining the user segment that will generate a prediction.",
        "Use case examples": "Objective:Non-subscriber to subscriber conversionInclude:Non-subscribersExclude:SubscribersReasoning:Subscribers have already converted, so there is no benefit to generating a subscription likelihood for these users.Objective:Subscription upgrade (tier 1 to tier 2)Include:Tier 1 subscribersExclude:Tier 2 subscribersReasoning:Since Tier 2 subscribers have already upgraded, there is no benefit in predicting their likelihood of upgrading.Objective:Churn preventionInclude:Active usersExclude:Lapsed usersReasoning:Since lapsed users have already churned, they should not be part of the effort to prevent churn. As such, there is no value in generating an attribute predicting their likelihood of churning.Objective:Cross sell: Get purchasers of item A to buy a variant at a higher price pointInclude:Customers who purchased item AExclude:Customers who did not buy item AReasoning:Since customers who have already purchased item A are more likely to purchase its more expensive counterpart, predicting the likelihood of cross sell for these customers only would be more efficient.Objective:Predicting which viewers who have not yet watched a show will watch it in the futureInclude:Viewers who have not watched this showExclude:Viewers who have watched this showReasoning:Since the objective of this campaign is to acquirenewviewers of a show, this predictive attribute should exclude viewers who have already watched the show.",
        "Apply All Filter Where Clauses": "Apply All allows you to quickly update multiple query rows with Filter Where clauses at once. This function is available in theSegmentation,Funnel, andCohorttools.",
        "Notes:": "Notice the query rows below the overlay. The query rows and operators/values that will be affected by these changes will flash based on your selections.The Apply button will also indicate the number of rows that will be affected by this change.This will not affect any hidden query rows. Our example reads: We are making changes to multiple rows containing theCountryproperty and having the \u201cis equal to\u201d operator and any value. We will update these three rows with the \u201ccontains\u201d operator and the valueUS. Step 4: Finally, select the Apply button to complete the changes.",
        "Build a multi-path funnel": "To create a multi-path funnel, you must indicate which step or steps is an optional step. You may haveup to four optional stepsin one multi-path funnel. Neither the first step nor the last step may be optional. There are two ways to indicate that a step is optional: Select the green pushpin icon that appears to the left of the row in the query builderSelect the gray pushpin icon that appears to the left of the event name within the results in the chart area To highlight a particular path, select the webbing between the steps of the path that you wish to view. Depending on the number of optional steps, you may need to select multiple webbings to highlight the path.  You may also view a list of all path combinations by clicking on the dropdown in the top right corner of the chart area. The conversion rate for each path is displayed next to the description of the path. Select a path to highlight it within your results.  The path list is also viewable in the Path Insights dropdown that appears when you choose a funnel step circle. In addition to the conversion rate, the table also displays: Total Dropoff and Avg Step Time. ",
        "Path exclusivity": "The Path Exclusivity setting is enabled if a funnel contains an optional step. It is located in the Settings dropdown within the menu bar beneath the query builder.  There are two options for path exclusivity. Exclusive: Users completed the steps in the selected path, and they did not complete steps outside of the selected path. In an exclusive multi-path funnel,users will appear in only one path. Consider the following exclusive funnel: (A) Site Visit, (B) Blog View (optional), (C) Create Profile, and (D) Subscribe. When the A-C-D path is selected, the results exclude users who did Blog View because Blog View is an off-path step. When the A-B-C-D path is selected, only users completed all the steps are included in the results.Inclusive: Users completed the steps in the selected path, and may or may not have completed steps outside of the selected path as well. In an inclusive multi-path funnel,users may appear multiple times, once for each path that they completed. Consider the following inclusive Funnel: (A) Site Visit, (B) Blog View (optional), (C) Create Profile, and (D) Subscribe. When the A-C-D path is selected, the results include all users who did Site Visit, Create Profile and Subscribe. Some of these users may or may not have also done Blog View, and they are included in the result regardless.",
        "Create a Predictive Attribute within Journeys": "Predictive Attributes can be created in the process of defining a new customer segment using Journeys. Here are the steps to accomplish this: Navigate to the Journeys homepage atSegmentation > Journeys.SelectNew Journey.Give your Journey a name, and select the workspace(s) and data inputs you would like to have access to within this Journey.In the Journey canvas, create a new Milestone.SelectUser Predictionfrom the criteria dropdown options.Give your Predictive Attribute a name that describes the user behavior it is predicting.Use the query builder to specify the user action you would like to predict.Specify whether you want to generate predictions for all or a subset of users. If choosing the latter, use the query builder to define this subset.  Once you save your Predictive Attribute, Cortex will immediately begin calculating your prediction. Note that new predictions need up to 24 hours to calculate.",
        "Create a standalone Predictive Attribute": "Creating a new Predictive Attribute does not require using any of mParticle\u2019s customer segmentation tools. To create a standalone Predictive Attribute: Navigate to thePredictionshomepage.SelectNew Predictive Attribute.Give your Predictive Attribute a name that describes the user behavior it is predicting.Use the query builder to specify the user action you would like to predict.Specify whether you want to generate predictions for all or a subset of users. If choosing the latter, use the query builder to define this subset. ",
        "Use Cases": "Identity Reconciliation Users interact with digital environments in many different ways. When a new user accesses your website or app, they are unknown, so a unique Anonymous ID is assigned to track their online behavior. Users may remain anonymous for the duration of their visit or they might create an account, log in, or otherwise share identifiable information. Once the user becomes known, it\u2019s important to reconcile the known identity with any previously anonymous user identities, and to combine the activity timelines of each ID. This creates a unified user activity timeline and enables a complete analysis of the full customer journey. Without user reconciliation, an individual user may be assigned multiple user IDs, which can cause duplication of user counts and fragmentation of user activity in your data. The process of reconciling known and unknown user identities is called aliasing. Aliasing is also known as identity reconciliation, stitching, or merging. Cross-Device Identification Users can access your website or app from a variety of devices and locations. Maybe a user creates a profile on your website using their laptop, then downloads your app onto their smartphone and also onto their tablet. Maybe they log in on a friend\u2019s phone to place an order. In addition to reconciling known and unknown user identities, aliasing combines the activity timelines of each device, platform, and geographic location. This again creates a unified activity timeline to provide a complete analysis of the full customer journey. Cross-Domain Tracking Aliasing can also help with cross-domain tracking. To achieve this, pass the Anonymous ID from the second domain as an aliasing parameter. Then contact your Customer Success Manager for more information on how to enable cross-domain tracking with aliasing.",
        "How Does Aliasing Work?": "Depending on your integration type, aliasing is achieved in different ways. For example, if you send events to Analytics in real-time by using Segment, Amazon Kinesis, or via the Analytics API, the process is different than if you send events to Analytics daily through a data warehouse integration. The following documentation describes our process for aliasing user identities in real-time integrations. An aliasing API call consists of three components: Anonymous ID:The anonymous ID is an ID used to identify a user before they register, log in, or otherwise identify themselves.User ID:The user ID is the ID used to uniquely identify a user in your database. This should be an immutable field that is recorded in the user table. As mentioned, the user ID is created and assigned whenever a user makes themselves known.API Key:The project API Key is used to direct the alias call toward which Analytics project. After receiving an aliasing call, Analytics reconciles the anonymous ID and the user ID, and handles all incoming events from both IDs as the same user. In short, they\u2019re \u201cstitched\u201d together. Multiple anonymous IDs may be aliased to a single user ID, however you may not alias multiple user IDs to a single anonymous ID, nor may you alias a user ID to another user ID. Alias calls are processed once per day, so please allow up to 24 hours for best results. JavaScript (SDK) Alias Calls Indicative.sendAlias() The Analytics client automatically generates a universally unique identifier (UUID) to apply to all events until \u201cIndicative.setUniqueID(id)\u201d is called. \u201cIndicative.sendAlias()\u201d can be called to alias the UUID to the ID parameter defined within \u201cIndicative.setUniqueID(id)\u201d Indicative.setUniqueID(id, true) After setting the new unique ID, then this will automatically call \u201cIndicative.sendAlias()\u201d. If you perform \u201ccallIndicative.setUniqueID(id)\u201d without \u2018true\u2019, then the alias call will not be sent.",
        "Best Practices": "Annotation names should be brief, descriptive, and consistentUse categories to organize your similar annotationsAnnotations with a date range are displayed as a period of time, annotations with the same start time date and end date are displayed as a point in time",
        "Event Aliasing from mParticle": "When user profiles and events are sent from the mParticle CDP to Analytics, Analytics will inherit all aliasing functionality that was performed usingIDSync. This means that Analytics automatically has access to the same unified view of user profiles and events established within mParticle.",
        "Retroactive aliasing": "For customers on the Enterprise plan, alias calls can be performed retroactively to combine two activity timelines that have already been processed. Retroactive aliasing is useful when events performed anonymously before an aliasing API call is received need to be reconciled with a known user profile. Retroactive aliasing takes place once every 24 hours. For example, consider the events below, all performed by the same customer: Event 1:Customer views a registration page, establishinguser_id AEvent 2:user_id Asubmits registrationEvent 3:Customer successfully registers while using a different device, establishinguser_id BAliasing API call is made, establishing thatuser_id A is user_id BEvent 4:user_id Bviews an adEvent 5:user_id Aviews a login screenEvent 6:user_id Blogs in successfully In this sequence, only event 3 would need to be retroactively reconciled with the known user ID, while events 1, 2, 4, 5, and 6 would be immediately associated with the unified profile following the aliasing API call. Once daily retroactive aliasing is complete, the event in step 3 becomes associated with the same user profile as the rest of the events. This diagram further illustrates retroactive aliasing: ",
        "Custom Aliasing": "Custom aliasing rules may be supported for Enterprise customers only. This can be requested by contacting Support or your Customer Success Manager.",
        "Third Party Aliasing": "Aliasing calls from a third party are not supported. Analytics rejects these calls because they are considered \u201cchained\u201d alias calls (i.e. Anonymous ID > Third Party User ID > Indicative User ID). If you are concerned that your method of integration is causing aliasing issues, please contact Support or your Customer Success Manager.",
        "Aliasing with GDPR": "Under GDPR privacy regulations, users have the option to opt into or out of session tracking. If a user does not consent to tracking, then all personally identifiable information (PII) must not be collected and existing PII must be deleted. The only data that is tracked by the Analytics client is the Anonymous ID, a random string ID used to unify a single user session. This ID does not persist across multiple user sessions. It is not possible to alias the IDs of users who opt out of session tracking.",
        "Check the mParticle Status Page": "Any known issues affecting mParticle are tracked on ourstatus page(mParticle login required). If you are encountering problems in mParticle, first check this page to see if any service interruptions have been reported. You can also subscribe to receive service updates by email, SMS or RSS feed.",
        "Ongoing Incidents": "The status page displays information about any ongoing incidents above the first table, after the \u201cAbout This Site\u201d section.",
        "Table of Components": "A table displays past and current availability for the following mParticle components: mParticle Dashboard:app.mparticle.comDocumentation Site:docs.mparticle.comData Ingestion: data collection for mobile, JavaScript, pixels, partner feeds, the SFTP ingestion endpoint, and CookieSyncAPIs: mParticle\u2019s ability to receive data at HTTP endpoints:Events APIIdentity APIUser Profile APIPlatform APISDK Configuration API: a private API used to pass settings to client-side SDKsGDPR API: a private API for GDPR (not data subject requests)DSR API: data subject requestsJavaScript Tags CDN: a private content delivery network for JavaScript tagsData forwardingAudienceProfileUser Activity ViewRules You can also viewuptimesfor the last ten years (mParticle login required).",
        "System Metrics": "The status page\u2019s second section displays average latency for mParticle\u2019s key API endpoints, updated every five minutes. On this page, Latency means the average time, in milliseconds, between mParticle receiving a request at an API endpoint and sending a response. You can view the metrics by day, week, or month.",
        "Past Incidents": "The status page\u2019s third section displays any known incidents that caused recent service disruptions. You can also viewreported incidents for the last ten years(mParticle login required).",
        "Troubleshoot Events": "Many configuration settings or other circumstances may cause event data to not be forwarded: Time zones\nIf you compare mParticle event forwarding to numbers in a system using a different timezone, the numbers won\u2019t match. mParticle reporting uses UTC.Missing data points\nYour integration may require application, device, or user data available in order to forward events. Check that all required data points have been configured for forwarding.Data point mapping\nSome integrations require data points to be mapped. Check that the relevant data points are mapped.Rules\nYour connection may have sampling, a minimum app version, data filters, or conditional (event/user attribute, consent, identity) based forwarding rules that are reducing the amount of data being forwarded. You may also have a rule that limits the amount of events being forwarded.Server-to-server\nIf server-to-server data is being sent in with a duplicate, batch, or source request ID, that data won\u2019t be forwarded. Use the following techniques to find the cause of the data discrepancy.",
        "Events Fail to Arrive in mParticle (Input)": "For web apps, is the app able to see a network request to the mParticle Events endpoint? With verbose logging enabled, is the app able to see the events logged and uploaded in the browser\u2019s inspector or developer tools?For iOS or Android apps, with verbose logging enabled, is the app able to see the events logged and uploaded in the Xcode or Android Studio logs? If the answer is yes, then continue diagnosing the problem. If not, review the knowledge base or log a ticket withmParticle Support.",
        "Validate Connection Output Settings and Data Filters": "Confirm that the connection has been configured and is active for the correct workspace, environment (Dev or Prod), and input. SeeTroubleshooting Connectionsfor details.Confirm whether \u201cMinimum app version\u201d setting has been filtered - data from versions older than the filter will not flow.Confirm that data filters for the data points are set to on.Some integrations require that data points be mapped in order to be sent to a downstream service. No mapping results in no data forwarding. The following integrations have this requirement\u2014for other integrations, check thedocumentation for that integration:AdjustGoogle Marketing PlatformKruxSFMC EmailConfirm whether event attribute or attribution-based forwarding rules could be causing data to not be forwarded.",
        "Verify Event Forwarding and System Alerts": "Check theevent forwarding reportand note whether the numbers for the input in question line up to the output in question.Checksystem alertsand look for issues.For kit-based integrations, you may not have included the kit:Look for errors such as \u201cno route available.\u201dSome services require the presence of a kit, even to send data that is sent to mParticle server-side. For example, AppsFlyer, Adjust, Adobe MCID, and Airship all require a kit. Check thedocumentation for the integrationto verify whether a kit is needed.These integrations use the kit to obtain an identifier (an integration attribute). For data that is sent to mParticleserver-side, the user needs to have been in an app version that contains the kit in order to send the data. This means that for server-side data, not having seen the user in a version of the app with the kit would trigger errors in system alerts and cause data not to be sent. Data not being forwarded is expected in this case.Check for invalid credentials in output or connection settings which prevent data from flowing.Check for missing required data. Many integrations require device or user information in order to forward data. System alerts expose the missing data points.For data being sent in via SDK, a certain percentage of users (on average ~15%) won\u2019t have IDFAs or GAIDs. If a user limits ad tracking on their mobile devices, these identifiers are not available. For data being sent server-side, the customer is not sending the missing parameters to mParticle. You must send mParticle the missing parameters to start event forwarding. If you still can\u2019t identify the issue, review the knowledge base or log a ticket withmParticle Support.",
        "Troubleshoot SDKs": "For additional SDK troubleshooting: AndroidiOSWeb0",
        "Creating a reverse funnel": "There are two ways to create a reverse funnel. The first, and the most common, is to create a forward funnel in the query builder, and then reverse it in the Settings dropdown within the menu bar. Once you change the funnel direction to reverse, notice that the change is reflected in the query builder. Each step after step A now begins with \u201cand preceded by.\u201d While in reverse funnel mode, date ranges will still apply to Row A, or the left-most donut in the Funnel visualization. In the following example, PetBox is trying to measure the influence of their email marketing campaign on who eventually purchased their product sales.  A forward funnel measures how many people who clicked their email and then went on to purchase the product. However, the more relevant journey here is a reverse funnel. Once PetBox changes the funnel direction to reverse, they can now calculate the amount of users that came through their email campaign as a percentage of the total users who purchased their product. ",
        "An event is blocked by a forwarding rule": "Forwarding rulesallow you to prevent events from being forwarded to an output according to criteria like the event\u2019s attributes, user consent preferences, or whether the user who triggered the event was logged in. Sometimes, a forwarding rule might block an event that you actually want to send to your output. Using Observability, you can more easily identify and troubleshoot these instances. For example, consider a scenario where we send the following event to mParticle when a user subscribes to a premium service, but we don\u2019t see the event appear downstream in our output. Example event batch First, let\u2019s check to see if Observability flagged any recent traces by logging into mParticle and navigating toObservability > Trace Activity.We clickSort & Filtersand select our input feed underInputs. UnderResultswe select \u201cWarning\u201d, \u201cInsight\u201d, and \u201cNeeds Attention\u201d and clickApply.  We see a trace matching our input and output with \u201cInsight\u201d as the result, which indicates some detected behavior (either intended or not) that we should investigate. We click on theTrace IDto open the details page to learn more.  In the Trace Details page, we scroll through the Timeline View until we see a highlighted span under the section calledTransformations. This includes the error message:\u201cEvent Batch Filtering: The batch was dropped because all events contained within it were filtered out by event attribute forwarding rules. If this is not expected, check the event attribute rules for this connection.\u201dNow we know our event was dropped due to a forwarding rule.  We navigate to the connection between our custom feed input and our webhook output to view our forwarding rules.We go to the Forwarding tab and see that we have a Forwarding Rule set to \u201cOnly Forward\u201d events if they contain the event attributeplan_typeset tobasic.  After re-examining our original event batch, our event\u2019splan_typewas set topremium, which explains why it wasn\u2019t forwarded to our output.We can now either remove our feed filter or adjust it to forward subscription events with either basic or premium plan types.",
        "An event is blocked by a feed filter": "Filtersprovide granular control over exactly which data points are forwarded to a specific output. You can switch OFF certain data points to exclude them from the data you forward, or you can switch ON data points to forward them. Sometimes, you might mistakenly toggle a filter for a data point that you actually want to send to an output. For example, consider a scenario where we send the following event to mParticle when a user subscribes to a premium service, but we don\u2019t see the plan_type event attribute appear downstream in our output. Example event batch First, let\u2019s check to see if Observability flagged any recent traces by logging into mParticle and navigating toObservability > Trace Activity.We clickSort & Filtersand select our input feed underInputs. UnderResultswe select \u201cWarning\u201d, \u201cInsight\u201d, and \u201cNeeds Attention\u201d and clickApply.  We see a trace matching our input and output with \u201cInsight\u201d as the result, which indicates some detected behavior (either intended or not) that we should investigate. We click on theTrace IDto open the details page to learn more.  In the Trace Details page, we scroll through the trace timeline until we see a highlighted span underTransformations. If we click on this, we see the error message:\u201cEvent Batch Filtering: The batch was dropped because all events contained within it were filtered out by event attribute forwarding rules. If this is not expected, check the event attribute rules for this connection.\u201dNow we know our event was dropped due to a feed filter. Let\u2019s check our filter settings to see if we\u2019re unintentionally removing any event attributes.  To find our feed filter settings, we navigate toConnections > Feed Filtersusing the left hand navigation, and then we select theEventstab. The column for Webhook (our output that didn\u2019t receive the event) lists each event type that will be forwarded (or filtered).If we expand theOtherevent type section, we see our subscription event. If we expand this, we see that theplan_typeevent attribute is switched OFF. As long as this filter is enabled, mParticle won\u2019t forward theplan_typeattribute with subscription events.  Now we know we need to remove the feed filter so theplan_typeattribute is always forwarded.",
        "An event is blocked by a data plan": "Data plansallow you to describe the extent and the shape of the data you collect using mParticle. Each data plan contains a list of data points which describe the event names, event attributes, user attributes, and user identities that you want mParticle to ingest. If your input sends data that doesn\u2019t match your data plan, it can be either marked as invalid or blocked entirely, depending on your data plan settings. Sometimes, data plans will block data that you actually want to ingest. We can use Observability to help identify and troubleshoot these instances. For example, consider a scenario where we send the following event to mParticle when a user subscribes to a service, but we don\u2019t see thelanguage_preferenceuser attribute appear downstream in our output. Example event batch First, let\u2019s check to see if Observability flagged any recent traces by logging into mParticle and navigating toObservability > Trace Activity.We clickSort & Filtersand select our input feed underInputs. UnderResultswe select \u201cWarning\u201d, \u201cInsight\u201d, and \u201cNeeds Attention\u201d and clickApply.  We see a trace matching our input and output with \u201cInsight\u201d as the result, which indicates some detected behavior (either intended or not) that we should investigate. We click on theTrace IDto open the details page to learn more.  In the Trace Details page, we scroll through the trace timeline until we see the highlighted span called\u201cEvaluating Data Plan \u2018higgs_data_plan\u2019\u201dunderTransformations. If we click on this, we see the error message:\u201cData Planning: All user attributes in the batch were dropped by the data plan. Not all integrations and use cases support batches with no user attributes.\u201dNow we know our user attribute was blocked by the Higgs Data Plan.  TheSetup Detailssection on the left side of the trace details page gives us a direct link we can click to take us to our Higgs Data Plan.Once on the Higgs Data Plan settings page, we clickBlock Volumeto view the data plan report.  From the Report tab, make sure to select \u201cBlocked\u201d. Under the list of blocked data points, we see our language preference user attribute.  At this point, we can modify our data plan\u2019s block settings to allow unplanned user attributes, or we can add the language preference attribute to our list of expected data points. Note: if you have a quarantine connection configured for your data plan, you can backfill any blocked data.",
        "Configure Your Project Settings": "Owners and Admins have access to Project Settings, however Members and View Only teammates do not. Select the cog wheel icon on the left menu bar to review Project Settings.  Within Project Settings you\u2019ll be able to access General Settings, Teammates, Data Sources, Scheduled Reports, and Data Filters. ",
        "Date Format": "The date format setting determines the display style for dates in Analytics. Dates are commonly found along the x-axis of query results. There are three options for date format: US MM/DD/YYYY, European DD/MM/YYYY, or ISO YYYY-MM-DD. All new projects default to US MM/DD/YYYY. ",
        "Currency Format": "The currency format setting allows you to change the currency and symbol displayed for all Revenue results in Analytics. To use the Revenue event, you must complete a Revenue mapping in the Events and Properties settings. All new projects default to US Dollars $. This setting is applied to Revenue results for reports within your entire Analytics project.",
        "List of supported currencies": "If your currency is not currently supported, please contact support.",
        "IP Address Collection": "If your project is integrated using the Analytics SDK, then the IP Address Collection setting appears in your project settings. You may enable or disable the collection of IP address information for your project. All new projects default to enabled. Note: IP geolocation tracking is restricted under the Child Protection Regulation (COPPA). Please ensure that you have disabled this within any project that contains data from online services directed toward children 13 years of age or younger. Analytics reserves the right to disable IP address collection if we suspect any violation, intentional or otherwise, of this regulation.",
        "Public Sharing": "Permit the sharing of embedded widgets, dashboard links, and scheduled reports externally.",
        "AI Features": "Enable or disable AI-assisted features including Assisted Analysis.",
        "Teammates": "View and modify all of the teammates in your organization. Users are listed with their name, role, status, and last login date.  Project Owners and Admins may change project member roles by clicking on the downward arrow within the role tab. ",
        "Data Sources": " The Data Sources section displays a summary of the active and inactive data sources for your project, including the name, type, and date that event data was last seen (or received). A green dot indicates that the project received data from this data source recently. A red dot indicates that the project has not received data from this data source recently. A gray dot indicates that there is no information available about the status of this integration. You are able to add a new data source to the project by clicking on New Data Source on the top right. This brings you to the Connect Your Data screen.  You may also view all data sources within an organization by accessing Data Sources within Organization settings. This will display data sources across all projects.  Data warehouse users may also view the specifics of their data warehouse integration by selecting Edit Details. ",
        "Scheduled Reports": "Project Settings also contains a tool to manage Scheduled Reports. The table shows when reports were created and by which user, and the three-dot menu on the right allows editing of the underlying dashboard, as well as the features of the report itself. See ourReportsdocumentation for more details. ",
        "Data Filters": " If your project contains any data filters, then the data filters section displays the defining criteria for the filter(s). Data filters are applied to all queries within a project; they do not modify or otherwise change the underlying data in Analytics.",
        "Create a journey": "Following the instructions inCreate a journey, begin creating a journey.Once you\u2019ve added a milestone with criteria, click the plus sign beneath the milestone\u2019s audience and selectA/B Test (Random Split).  Click+ Add A/B Test Variationto add a new variation to your A/B test. Enter the percentage of users you want each variation to contain. If the sum of your variations\u2019 percentages is less than 100, the remainder is automatically assigned to the Control group. For example, if you create Variation A and Variation B with 40% in each, you will have a Control group with 20%. You can also provide a custom name for each variation.Create as many variations as you need for your test, up to a maximum of 5. The total of all your variations must always add up to 100%. If you try to assign a percentage to a variation that would cause the total to exceed 100, you will see an error message.  When you are satisfied with your variations, clickSave. After saving your variations, you will see each variation in a new branch within the Journey Builder. ",
        "In Analytics": "In Analytics, click on the gear icon and selectProject Settings.Select theData Sourcestab.SelectNew Data Source.SelectAmazon S3andDefine your own schema. ClickConnect.You should see this screen.ClickNext.",
        "Score and percentile values": "When you create a predictive audience, you can choose between two types of results: score, the likelihood of an audience member performing a future eventpercentile, the likelihood of an audience member performing a future event as compared to other users scored by Cortex, mParticle\u2019s machine-learning engine: typically the number of active users in the last 90 days For example, if you wanted to know how likely it is that a user will purchase shoes in the next 7 days, you could see that likelihood displayed as a score or percentile: With a score of 80%, the user has an 80% chance of purchasing shoes.With a percentile value of .9, the user is more likely to purchase than 90% of users.",
        "How it works": "Creating a predictive audience is simple: Create an audience using the same criteria builder that you use for any audience or journey creation.In the criteria builder, select User Prediction and fill in the requested information.When you save the audience, mParticle creates two user attributes for the user prediction, one expressed as a score and the other as a percentile. Choose one.mParticle populates your audience. After a delay of up to 24 hours, your audience is available for use.Connect the output to the forwarding destinations as you do for real-time audiences. For step-by-step instructions, seeUsing Predictive Audiences. Predictions are rerun weekly to regenerate fresh predictions.",
        "More about Cortex": "Cortex is the machine-learning engine available with mParticle\u2019s CDP. To learn more, you can visitthe Cortex documentation.",
        "Okta": "To configure single sign-on using SAML2.0 and Okta you must create a new Application within the Okta administrative portal, populate the appropriate configuration information from Analytics in Okta, copy and paste the Identity Provider XML file into Analytics and assign users. Follow the instructions below to get started: Log into Okta and then select ApplicationsChoose on Create App Integration  Within Create App Integration, highlight SAML 2.0 and then choose Next  In the General Settings tab, insert \u201cAnalytics\u201d in the App name field. If you would like to add the Analytics logo, you can retrieve it from the Extras section of Analytics\u2019SSO Settings. Otherwise, select Next.Select SAML Settings and enter the following fields for which the values are located in Analytics\u2019SSO Settingssection:Single sign on URLApplication URI (SP Entity ID)Default RelayState Choose Next at the bottom  On the Feedback tab, select \u201cI\u2019m an Okta customer adding an internal app\u201d and choose Next.On the Sign On tab, select Identity Provider Metadata to download the XML configuration file, open it using a text editor, and copy and paste into Identity Provider Metadata XML within Analytics\u2019SSO Settings.  Your application is now set up for SAML2 authentication. You may test your application using the dedicated sign-on URL which can be found on theSSO Settingssection within Analytics. To enable Home Realm Discovery, mapping your business domain to your SSO provider, configure the Identifier-First Authentication section and make sure to disable username and password authentication underEmail & Password Authentication.",
        "OneLogin": "To configure single sign-on using OneLogin, first add the SAML Test Connector to your Admin Portal atonelogin.com, then enter your custom organization settings. When your configuration is complete, copy and paste the metadata into your Analytics Organization Settings to complete your SSO setup. Log into your OneLogin Admin Portal, then click Applications.In the top right corner of the screen, select Add App.  Under Find Applications, enter SAML into the search bar, then select select SAML Test Connector.  For the Display Name, enter Analytics.Upload the Analytics logos, available at the bottom ofthis page.Use the custom field values fromhttps://app.indicative.com/#/organization/ssoto complete the required fields in Configuration:  Add the following Parameters:  Click Save.Click to open the More Actions dropdown, then click download your unique SAML MetaData.  Copy and paste XML file intohttps://app.indicative.com/#/organization/sso, then click Save.On the Default Project Access tab of Organization Settings, enable default project access for at least one project, then click Save. To test your SSO configuration, use the dedicated sign-on URL listedhere. Optional: Once your SSO configuration is tested and confirmed, disable Email & Password Authentication on the Email & Password Authentication tab of Organization Settings,here.",
        "Understanding mParticle Credits": "mParticle Credits are a universal currency that mParticle customers can use to buy any product on the mParticle platform.",
        "How mParticle Credits are purchased": "Your company makes an upfront contractual credit commitment to unlock access to a discounted rate for mParticle Credits based on your usage.\nThe higher the minimum commitment, the higher the discount tier that is available.\nYou can also choose to purchase additional mParticle Credits at any time.",
        "How mParticle Credit usage is calculated": "At the end of each billing cycle, mParticle measures your usage, and then calculates the number of mParticle Credits consumed, using the pricing in your contract. Remember that events are counted in units of one million (1M).",
        "Example mParticle Credit usage calculation": "Assume your contract specified the following unit prices in mParticle Credits (mPCs) for three billable items: 60 mPCs per million Preserve tier events.74 mPCs per million Personalize tier events.5 mPCs per additional six months of long term data storage. In one billing period you use the following quantities: Ingested five million Preserve tier events (300 mPCs)Ingested two million Personalize tier events (148 mPCs)Two additional six-month units of long term data storage (70 mPCs)\nTo calculate the additional storage cost, multiply the additional storage unit price (5 mPCs) times the number of units (2 additional units) times the number of Preserve and Personalize tier event units (7). The total mPCs consumed for the billing period is 518: 300 + 148 + 70 This guide explains how mParticle calculates the quantities for each billable item. Each quantity is then multiplied by the unit price in your contract to determine the number of mPCs used for the billing period. To ensure you receive the fairest usage calculation, mParticle evaluates all billable items as described in the following sections. Use the following quick reference table to understand at a glance how quantities are calculated.\nEach quantity calculation is explained in detail after the table.",
        "Where can I find my credit usage?": "Customers who have value based pricing contracts with mParticle receive a Credit Usage Report at the end of each billing period directly from their mParticle account manager. Your credit usage report breaks down your total credit usage per event tier and anybillable itemsfrom the past billing period by each organization, account, and workspace on your contract.",
        "Billable items quick reference": "The following quick-reference table lists the billable items for credit usage calculations.",
        "Billable items": "The usage for each billable item listed in the quick-reference table is calculated and the total value is drawn down as mPCs each billing period.",
        "Tiered Events": "Each event is a data point that records an action taken by a user in your app or system. Events may be of a predefined event type ready for use in mParticle, or custom events that require mapping.Events are collected automatically after initializing an mParticle SDK or including the JavaScript snippet for Web, or events are ingested via feeds or an API. Some events matter more to you than others. mParticle provides three tiers to help you map event types to how they are used, and billed, in the platform: To calculate the quantity of events, count the number of event units in each tier. Use the larger of:The number of events originally ingestedThe number of events after All Output rules have been appliedTo calculate the credit usage for events, multiply the number of event units in each tier by their unit price in your contract. That total is the number of mPCs used. A few additional factors affect event usage calculations: To maximize data delivery in all network conditions, an SDK sometimes sends the same event more than once. Even if we send an event more than once, we only count it once.mParticle charges for events blocked byData Planning.mParticle respects the final event type encountered in the pipeline. If the event type of an event changes using rules, we use the most recent one for metering per tier.Sometimes, mParticle can\u2019t determine the tier of an event dropped using \u2018All Output\u2019 rules. For events of unknown tier, mParticle counts them at the lowest tier available in your contract. For example, assume that 100 events are ingested into mParticle (33 Connect tier, 33 Preserve tier, 34 Personalize tier). After the \u2018All Output\u2019 Rules stage we have 70 Personalize tier events. In this scenario, the 70 Personalize tier events and the remaining 30 events are counted at the lowest tier, which is Connect tier for most customers.",
        "Additional long-term retention": "Additionallong-term retention for eventsis a billable item. For example, if you purchased two six-month units for an additional twelve months of long-term storage for Preserve and Personalize tier events, those extra storage units are calculated and included in your credit draw down. To calculate the quantity of events, count the number of event units in the Preserve and Personalize tiers.To calculate the credit usage, multiply the number of additional units by the unit price in your contract for each additional unit, and then multiply that value times the number calculated in step 1. That total is the number of mPCs used.",
        "Additional Real-Time Audience Storage Lookback": "Additional Real-Time Audience Storage Lookback (specified in your contract) is a billable item. For example, if you purchased two 30-day units for an additional 60 days of storage for Preserve and Personalize tier events, those extra units are calculated and included in your credit draw down. To calculate the quantity of events, count the number of event units in the Personalize tier.To calculate the  credit usage, multiply the number of additional units by the unit price in your contract for each additional unit, and then multiply that value times the number calculated in step 1. That total is the number of mPCs used.",
        "Eventless batches": "Batches that are sent server-to-server with only user attributes or user identities but no events are called eventless batches. Each eventless batch is counted as one Personalize tier event. If the event tier is not specified in your contract, we count each Eventless Batch as one event of the lowest tier available in your contract. To calculate the quantity of eventless batches, count the number of eventless batches in each tier. Eventless batches are counted the same as one event, and are calculated using the same one million per unit.To calculate the credit usage, multiply the number of eventless batch units in each tier by their unit price in your contract. That total is the number of mPCs used.",
        "Real-time products": "Active real-time products include Real Time Audiences and Calculated Attributes. Real-time products are metered per workspace on two dimensions: Number of active products in a given billing periodThe largest number of active real-time audiences and calculated attributes at any point in the given billing period. Five real-time audiences are included in the model, so mParticle subtracts five before using this number to calculate credit usage.Number of Personalize tier events in the same billing periodTotal count of Personalize tier event units ingested in mParticle based on the Event metering logic above. To calculate the quantity of real-time units for each workspace, multiply the number of Personalize tier event units by the count of real-time products (Maximum active Realtime Audiences + calculated attributes in the billing period minus five). Then sum the values for all workspaces.To calculate the credit usage: multiply the number from step 1 by the unit price in your contract for Real-time Products to determine the mPCs usage for real-time products.",
        "Real-time invocations": "Some early VBP customers are on a version of the pricing model where real-time products are priced based on the number of real-time invocations. To calculate the quantity of real-time invocations, count the number of invocation of events for each active real-time audience and calculated attribute in a billing period.To calculate the credit usage: multiply the quantity by the unit price in your contract to determine the credit usage for real-time invocations.",
        "mParticle-hosted rule invocations": "mParticle-hosted rules are lambda functions that mParticle maintains and administers on behalf of customers in mParticle\u2019s AWS instance. To calculate the quantity, count the number of times the lambda is invoked in a billing period.To calculate the credit usage: multiply the quantity by the unit price in your contract to determine the credit usage for hosted rule invocations. Note the following: Retries aren\u2019t reported or counted as additional invocations.Customers who host the lambda functions on their own AWS instance aren\u2019t charged under this category.",
        "Backfill and data replays": "Customers sometime have to retrieve events from long-term storage and evaluate them for standard audiences or real-time audiences with Unlimited Lookback enabled. Customers may also request that Support perform a data replay. For example, you may create a new audience, a new calculated attribute or request a historical data replay from mParticle Support. To calculate the quantity of backfill and data replay, count the total number of event units retrieved from storage.To calculate the mParticle Credit usage, multiply the number of event units in each tier by the unit price in your contract. That total is the number of mPCs used.",
        "Cortex Intelligent Attributes": "Customers can use mParticle\u2019s Cortex machine learning engine to create Intelligent Attributes (IAs). IAs are attributes generated by predictions, look-alikes, and classification models. Cortex usage is metered on two dimensions: Number of active intelligent attributes (IA)An active IA is defined as an IA which has been trained or had predictions run on it at least once in the previous billing period.Events forwarded to CortexNumber of event units forwarded to Cortex from mParticle in a given billing period. To calculate the quantity of Cortex units, count the number of events from mParticle used by Cortex and multiply that with the number of Active Intelligent Attributes (IAs) in the billing period.To calculate the mParticle Credit usage, multiply the result in step 1 by the unit price in your contract for IAs.",
        "Indicative": "Behavioral analytics in the mParticle platform. mParticle also includes functionality for customer data platforms (CDP), as well as decision making powered by machine learning with Cortex.",
        "Pausing and Resuming": "To pause the event view, clickPauseor click on a particular event. When the event view is paused, the Pause button changes to a Resume button.  The stream remains paused as you select additional events.  It will resume by clickingResume.",
        "Clear Entries": "When initiating a new session on the test device, you may want to clear the Live Stream from the last test session. To do this, simply clickClear Entries. The Live Stream and event details clear immediately, but the filters retain their values.",
        "Incoming event batches": "The following limits apply to event batches being ingested.",
        "SDK event data": "The following limit applies only to data being ingested with an SDK.",
        "Events per workspace and user": "The following limits apply to events per workspace or user. mParticle reserves the right to restrict average events per user to ensure platform quality of service.",
        "Events API": "The following limits apply to resources in theEvents API.",
        "Platform API": "The following limits apply to resources in thePlatform API.",
        "Data storage": "The following limits are explained in detail inData Retention.",
        "Data Subject Request API": "The following limits apply to resources in theData Subject Request API.",
        "Data plans": "The following limits apply to resources used indata planning. Similar to our event limit for workspaces, data plans support up to 1,000 data points.You can upload data plan JSON files smaller than 10 MB.Managing plans with more than 400 data points in the UI becomes unwieldy. Manage plans outside of the UI for larger plans. For more information, see theData Planning API guide.You can block data only for unplanned violations: events and attributes with names that diverge from the schema defined in a data plan.",
        "Data Planning API": "The following limits apply to resources in theData Planning API.",
        "Warehouse Sync API": "All functionality of Warehouse Sync is also available from an API. To learn how to create and manage syncs between mParticle and your data warehouse using the API, visit the mParticledeveloper documentation.",
        "Workspaces, users, and name length": "The following limits apply to workspaces, users, and the name length limits for audiences and tags.",
        "API throttling": "mParticle APIs have two types of rate limits in place to protect mParticle\u2019s servers from high demand: Speed: limits the rate of traffic.Acceleration: limits the rate of increase of traffic. You can configure your mParticle integration to respond programmatically to prevent data loss according to the 429 response header you receive. Throttled resources include a percentage representation of your current total usage in 2xx response headers. These percentage-used headers allow you to modify your request speed or acceleration to prevent exceeding a rate limit.",
        "429 rate limit exceeded": "An API request that exceeds the rate (speed) limit of a resource receives a 429 response and a header with the format: X-mp-rate-limit-exceeded: \"LIMIT\" where\"LIMIT\"is the ID of the source of the limit as described in the table below: An API request that exceeds any applicable acceleration limit receives a 429 response and a header with the format: X-mp-rate-limit-exceeded: \u201cacceleration\u201d",
        "2xx percentage used": "Rate limited endpoints return anX-mp-rate-limit-percentage-usedheader with 2xx responses including the percentage of the limit used. If a resource has multiple limits, the response header includes the greatest consumption percentage of all applicable speed and acceleration limits, omitting any user-based limits. For example, if a request is not throttled and is subject to both a speed and acceleration limit, and the current consumption is 92% of the speed limit and 50% of the acceleration limit, then the header lists the consumption of the speed limit because it is higher than the current acceleration limit usage: X-mp-rate-limit-percentage-used: 92",
        "Recommended actions": "If you receive a 429 response for exceeding a speed limit, reduce the frequency of your requests. Use exponential back-off with jitter and respect theRetryAftervalue which is a non-negative decimal integer indicating the number of seconds to delay your request. If you receive a 429 response for exceeding an acceleration limit, you can still submit requests but you should slow the increase of your request speed. Use exponential back-off with jitter when determining the new frequency of your requests.",
        "Define your backfill strategy": "Backfilling blocked data is non-trivial because you typically are interested in backfilling data to several downstream event integrations. Based on your unique set of target event integrations, you should devise a strategy for your data backfill. The following questions will guide your backfill strategy: Which integrations do I need to backfill?Different integrations have different limitations when it comes to receiving historical data. Establish the limitation of a target integration by reading their developer docs or by sending a small amount of test data through an mParticle connection.Do I need to backfill unplanned event attributes?Event attributes cannot be replayed without their associated events. You\u2019ll need a strategy (e.g. deleting previously sent yet incomplete events) to avoid event duplication if you want to replay blocked event attributes.Which mParticle Input should I use to backfill my data?The cleanest solution is typically to create a newCustom Feedfor the purpose of your backfill. You can connect only the integrations that you want to backfill to that feed and then tear it down again once the backfill is complete.However, some integrations are not available through theCustom FeedInput. In those cases, you will need to either (i) use the keys and secret of the original Input (e.g. Web) in our backfill script or (ii) send data directly to the integration\u2019s API (after transforming it to match their data model).",
        "How to backfill blocked data": "Once you have a strategy for your backfill, here are the steps to backfill your data: Go to your Quarantine output and find the data you want to transform.In your data, find thecontextnode. Within the context node, you will see a node labeledblock_metadata. This node contains the data you have blocked. Reference our sample data below to understand the complete data structure.Pull out the events you want to fix and replay.Apply fixes to your data, then re-upload it to mParticle in its correct format. Use one of our sample scripts provided, or follow steps 5 - 8 for guidance on writing your own replay script.Using the JSON data file containing blocked events, get theblock_metadatanode.Create a new events array and user attribute object containing only the data you would like to backfill.Perform any data fixes.Using mParticle\u2019s Events API, create a new batch containing the events to be replayed along with any corrected event or user attributes. Send the fixed batch to mParticle\u2019s Events API.",
        "Resources": "Quarantine Replay Scripts Example Quarantine Batch",
        "Calculation credits": "Standard audiences are purchased by buying annual calculation credits. Each calculation credit lets you run a calculation across 365 days of your historical data, regardless of how many audiences are included. You can calculate many standard audiences at once. There are prompts in the product to select the audiences to calculate and confirm how many credits you are spending. Some example calculations and costs: 1 standard audience spanning from 1/1/2018 to 1/1/2020: this costs 2 credits as it scans 2 years of data.3 standard audiences spanning from 1/1/2019 to 12/31/2019: this costs 1 credit as it scans 1 year of data (with many audiences).",
        "Standard audience lifecycle": "Standard audiences have a 4 stage lifecycle: Draft: The audience is being drafted and has not yet been calculated. To calculate it, press \u2018calculate\u2019 and confirm that credits will be spent.Calculating: The audience is being calculated. Progress indications are shown in the UI and the time this takes depends on the date range selected (and thus the data volume scanned).Ready: The audience has been calculated and is ready for use by connecting and sending it downstream.Expired: 30 days after a standard audience is calculated, it is expired and appears in the Expired tab inAudiences > Standard.An expired audience can no longer be connected, but it can be cloned for recalculation.Any real-time audience criteria that checks user membership in a standard audience is not affected by standard audience expiration, as the users membership is saved in the users profile.Audience membership is not removed from downstream partners.",
        "1 - Create a new standard audience": "Standard Audiences are managed separately from Real-time audiences. ChooseAudiences > Standardfrom the main navigation menu, and clickNew Standard Audience. ",
        "2 - Define date range and inputs": "Just as withreal-time audiences, you can define which inputs you want use to calculate the audience. For Standard Audiences you also need to define a date range. You can chooseAll available dataor define any period within the available range. When you\u2019re ready, clickCreate. The start and end dates are inclusive and it uses the UTC timezone. ",
        "3 - Define audience criteria": "Define your audience by clicking the plus sign to create and define one or more criteria. This step is the same as inreal-time audiences. When your definition is ready, clickSave as Draft. Notice that after a moment, aCalculatebutton displays next to the grayed-outSave as Draftbutton. ",
        "4 - Calculate one or more audiences": "At the top of the Standard Audiences page from step 3, clickCalculate. Select any additional DRAFT audiences from the list to add them to the calculation. This modal shows you how many calculation credits will be deducted from your account. When you\u2019re ready clickStart Calculation. ",
        "5 - Set up one or more connections": "At first your audience shows asCalculatingin the list viewStatuscolumn underAudience Details. While you wait for the calculation to complete, you can set up one or more audience connections by clicking the green plus sign in theConencted Outputscolumn. Calculation can take many hours for large amounts of data. You can track progress via a popup in theSizecolumn. The Connections screen functions the same as forreal-time audiences. Add and configure one or more connections. The only difference is that when you save the connection, no data is forwarded until you explicitly send the audience.",
        "6 - Send your calculated audience": "Once your audience is completely calculated, you can see it in theReadytab. Click Send next to an Audience to go back to theConnectionsscreen.  From theConnectionsscreen clickSend. You can also adjust your output connections here as needed.  Select one or more audience Outputs and clickSend.  All members of the audience are forwarded to the output. Calculated audience will remain in theReadytab for30 days, after which they will need to be recalculated and can be found in theArchivetab. Once an audience is in the archive tab, you can clone and recalculate it.  Remember that the audience will not be updated in real time. If you want to update the audience, you must run the calculation again.",
        "Hide / show filters": "Dashboard filters are displayed in the top row of a dashboard above the analyses, and they are shown by default. To hide the dashboard filters row, selectHide Filtersin the top settings bar. To show them again, selectShow Filters.",
        "Date range filter": "The date filter at the top of your dashboard lets you set a date range across all analyses within that dashboard. By default, there is no date range applied to a dashboard when it is created. The data displayed for each analysis reflects the date range set at the analysis level. Set a date range for your dashboard Click \u201cSelect Date\u201d to expose the date selector. Here, you can choose a custom or preset date range. ",
        "Property filter": "You can apply event property filters across all analyses on your dashboard. To do this, first selectAdd Filterto expose the event property query builder:  The query builder contains three components: Event Property (category and name):Select the event property you want to apply as a filter. The query builder organizes events into categories, so select the category first, followed by the event itself.Operator:Choose an operator for evaluating the logic in your filter. The default operator isis equal to, but you can change this to any other operator that is available to that the event property you have selected.Value:Provide a valid value for your chosen event property. You can continue stringing filters together to hone in on a precise subset of your users. For example, applying the filter settings below to a dashboard will result in all analyses displaying data from the last full month whereCityis equal toNew York,Subscription Planis equal toMonthly, andBrowser Nameis equal toChrome. ",
        "Remove a filter": "To remove a filter from your query, click the \u201cX\u201d icon to the right of the filter description. This will remove that single filter from your query.",
        "Apply filters": "Each time you add or remove a new filter your query, you must selectApply Filtersfor your changes to be reflected in the dashboard. After applying filters, there will be a delay before the filtered data is loading. The loading state is indicated by spinning to the right of the title of each analysis widget. ",
        "Impact of dashboard filters at analysis level": "Each dashboard filter will be applied to the individual analyses within the dashboard, provided the filter and filter value are available within the analysis. Once you have applied filters to a dashboard, each analysis will display a filter icon in the bottom right indicating how many of the filters are applied at the level of that analysis. The names of the filters applied to this analysis will appear on hover:  In cases where filters you\u2019ve set at the dashboard level do not apply to an analysis, both the applied and unapplied filters will appear on hover:  When do dashboard filters apply to individual analyses? Dashboard filters override analysis-level filters, provided the event property is available at the analysis level. For example, if a dashboard includes the filterState=CAas a filter, and an analysis within this dashboard has access to this property but is not using it as a filter, the analysis will have the filterState=CAapplied once the dashboard-level filters are applied. When do dashboard filters override analysis filters? If a filter applied at the dashboard has been previously applied at the analysis level, the dashboard filter will override the analysis filter at analysis level. For example, if an analysis in a dashboard has the filterState=CA, and you apply a filter at the dashboard level sayingState=MN, the dashboard filter will override the analysis filter, and theStatewill be updated toMNat the analysis level. When are dashboard filters not applied at the dashboard level? When a dashboard filter value does not exist in any of the analyses in that dashboard, this filter will not be applied at the dashboard level. For example, if you set a date range of 90 days on a dashboard that includes only Journeys, this filter will not be applied, since the lookback period for Journeys is 30 days. Modifying dashboard filters at the analysis levelWhen you modify a filter on an analysis that was set at the dashboard level, the change will persist in the underlying query, but will not impact the original dashboard filter. For example, say you have a dashboard with the geo filterState=New York, and you open an analysis on this dashboard on which that filter is applied. If you change that filter toState=Californiaat the analysis level, the analysis will haveState=Californiawhile the filter at the dashboard level will continue to beState=New York. Open Query This option opens the query with the most recent dashboard filters applied. Open Query without Filters Applied This option disregards any dashboard filters. It will display the results based on the filters applied previously on the analysis.",
        "Managing filter updates across teammates": "Dashboard filters include checks to ensure smooth collaboration between members of the same organization. Here are some of the scenarios these checks account for:",
        "Another user has recently made changes to the dashboard": "When viewing a dashboard for the first time after someone else in your organization has updated the filters, you will see a banner at the top of the dashboard indicating this:  In these cases, you will need to refresh the page to reflect the most recent filters before you are able to apply additional changes you have made to the dashboard filters. ",
        "Dashboard filters user access": "Dashboard filters mirror dashboard-level and project-level permissions: At the dashboard level: Edit privileges:Any user who can edit a dashboard can also edit that dashboard\u2019s filters.View privileges:Any user who can only view a restricted dashboard has read-only privileges at the dashboard level. These users can view but not edit that dashboard\u2019s filters. At the project level: Within project settings, users can be assigned one of three dashboard permission levels:Full, View,orNone. Full:Any user withFullaccess at the project level can view and edit all dashboards, including their filters.View:Users withViewproject-level permissions have read-only access to dashboards, and can view but not edit dashboard filters.None:Users whose project-level permission is set toNonemay not view or edit dashboards within the project.",
        "Optimize dashboard performance": "You can optimize the performance of your dashboards by adhering to best practices when creating and filtering them, as well as when building the analyses they contain. Learn morehere.",
        "Goal": "Understand whether users have tried out the PetCam.",
        "Create a Target Segment": "Create auser segmentfor all users who opened the PetBox app but have not opened PetCam since\nit was launched in Segmentation. ChangeTotal count oftoUsers who performed. Select theOpen Appevent from the data dropdown\nand selectOpen PetCamfrom theDid [not] Performdropdown.Selecting an event fromDid [not] Performinvokes the\u201cFor\u201d clause.\nWe use this clause to help us understand whether a user performed an action\nbefore/after the target event. We are also able to examine this action based\non a specific date range. In this case, we selecteddid not do,betweenand used03/01/xxxx to Todayfor our \u201cFor\u201d clause date range as March\n1st is when the PetCam was launched.Since we are saving a user segment out of this query, we also want to open\nup the date range to03/01/xxxx to Todayso we can capture\nall of the users who have opened the app since the feature was released.\nThis is your final query:Note: The date ranges used have a static start date\nof03/01/xxxxand a relative end date ofToday. Using a dynamic date range enables us to\nexamine only those who have not used the filters feature even at\na later date but remember to save it as auser segment with a daily update cadence.Below is theCreate a User Segmentmodal. Name the segment\nand provide a description for future ease of use.Select a category or create a new one by typing the name in so you\ncan easily find your user segment in the future.Toggle toDaily.",
        "Analyze or Activate with this User Segment": "Congratulations on creating the user segment. Let\u2019s put it to good use.",
        "Saving to the Dashboard to Track Trends": "Analyze the ratio of users who have opened PetCam against those who have\nnot opened PetCam using thecalculator tool.\nFirst, create two queries: one filters for events performed by users within\nthis segment and one filters for events performed by users who are not in\nthis segment.Now, we can use the calculator tool to calculate our ratio.After we run the query, we can save the line chart to adashboardas one widget and the average as a separate metric widget. Now you can track\nthis ratio on your dashboard.",
        "Export your users to send targeted notifications": "You can export your users to engage with them outside of Analytics in one of\ntwo ways: Download aCSVand upload it to your marketing tools.Connect to ourSegments API(available for Pro and Enterprise users) to make API calls to your automation\ntools. This tutorial shows just a few of the ways you can use Analytics to analyze your\ndata and achieve actionable insights from it. If you have any questions or comments,\nplease reach out tosupport@mparticle.com.",
        "Create a BigQuery Dataset (Optional)": "Open the BigQuery web UI in theGCP Console.In the navigation panel, in theResourcessection, select your project.On the right side of the window, in the details panel, clickCreate dataset.On theCreate datasetpage:ForDataset ID, enter a unique datasetname.(Optional) ForData location, choose a geographiclocationfor the dataset. If you leave the value set toDefault, the location is set toUS. After a dataset is created, the location can\u2019t be changed.ForDefault table expiration, choose one of the following options:Never:(Default) Tables created in the dataset are never automatically deleted. You must delete them manually.Number of days after table creation:This value determines when a newly created table in the dataset is deleted. This value is applied if you do not set a table expiration when the table is created.ClickCreate dataset.",
        "Share Your Dataset with Analytics": "In the menu panel along the left side of your BigQuery instance, underResources, click on the triangle to the left of your project name to expand the view. This will display the available datasets.Select a dataset fromResources, then clickShare Datasetnear the right side of the window.  In theShare datasetpanel, in theDataset permissionstab, clickAdd members.In theAdd memberspanel, enter \u201devent-stream@indicative-production.iam.gserviceaccount.com\u201d into theNew memberstext box.ForSelect a role, selectBigQueryand choosebigquery.dataEditor.ClickDone.Contact your account manager with the following information once you have granted Analytics \u2018bigquery.dataViewer\u2019 access:ProjectId: the Analytics project ID to exportBQProject: the GCP project name to write data intoBQDataset: the BQ dataset name to write data into",
        "Building Your Query": "In the Analytics app, events can be filtered by event or user properties, grouped by those properties, or both: In the finished analysis, the query row will indicate which type of property was used.",
        "When to Use Event vs. User Properties": "To better illustrate the differences between event and user properties in Analytics, consider the example of a query set to view the eventBannerImpressionwith the propertyChannel, where the date range is equal to 8/17 - 8/20. Here, filtering by \u201cevents\u2019ChannelequalsSearch\u201d, for example, would essentially pose this question: \u201cGive me the total count of events performed between 8/17 and 8/20 where the propertyChannelis equal toSearch. \u201d The results, filtered by event property, would show one event. Let\u2019s say that the user property forChannelis set to \u2018Last Seen\u2019. If the filter is set to \u201cusers\u2019ChannelequalsSearch, the query instead asks: \u201cGive me the total count of any events performed between 8/17 and 8/20 where the user who performed them had a last-seen value ofSearch. \u201d In this case, the results, when filtered by user property, would show five events: four performed by userID = 1, and one performed by userID = 5. userID = 1 shows up four times because they performed theBannerImpressionevent within the time window, while having a last-seen value ofSearch. Because user properties focus more on the individual users than on the events, they can be useful for cases focusing on the user journey. Questions that might benefit from filtering or grouping by user properties might be: What does a funnel look like for users who became a subscriber, vs. users who did not?What types of banner impressions were seen by users who started by viewing the blog?What is the most effective first or last channel (When grouping by users\u2019 first or last seen channel)?",
        "A/B Testing Best Practices": "When setting up A/B tests in Analytics, the best practice is to set up the data to use event properties, for several reasons: User properties will show whether the users are in an experiment or not, but will not take into account the time that they performed the event, as discussed in the above sections. Event properties will take into account the time of the event, as well as when the user entered or exited the experiment.Typically, users are placed into experiments based on their user ID, which means the variant will remain constant through the experiment. However, in experiments that use cookie ID, there is a risk that aliasing will combine users with different cookie IDs under the same user ID, making event properties a more accurate method for tracking these variants.",
        "Forwarding data from feeds": "When planning a feed implementation, it is important to consider if and how you want the data to be forwarded to any event outputs. Event integrations connect an \u2018input\u2019, which is a single platform or feed, to a single \u2018output\u2019. Most outputs can only accept connections from a limited set of platforms. When setting up connections from a feed, you need to know that some feeds can \u201cact-as\u201d iOS, Android, Web, or other platforms.",
        "\u201cAct-as\u201d feeds": "Act-as feeds are feeds which mParticle can treat as if they belonged to a single platform. For example, if a feed can \u201cact as\u201d iOS, you can connect the feed to any output that accepts data from the iOS platform. When you configure an act-as feed, you will need to select the platform you want the feed to act as:  You can connect an act-as feed to any output that can support the platform it is configured to act as. Note that for act-as feeds, if you wish to capture data for multiple platforms, you will need to configure multiple instances of the feed \u2014 one for each platform.",
        "\u201cUnbound\u201d feeds": "If you do not see an option to select an \u201cact-as\u201d platform when you configure a feed, the data from that feed is treated as \u201cunbound\u201d: not tied to a particular platform. For example, a feed that forwards data about user actions in response to an email campaign will be an unbound feed. These events don\u2019t specifically belong to a platform, like iOS or Android, and won\u2019t have the necessary identifiers to be processed as iOS or Android events. Outputs that only support iOS / Android / Web will not accept data from unbound feeds. However: Unbound feeds can still be forwarded to Data Warehouse outputs and webhook outputs, like Amazon S3.Events from unbound feeds can be used to power audience selection criteria.Some unbound feeds primarily forward user attributes. User attributes updated by a feed can still be forwarded to event output partners through the enrichment process.",
        "Sequential order": "Typically, each user in the funnel must complete each event in a chronological order, one after the other. In the example below, this means thatSubscribemust happen beforeCreate Profile, with a distinctly different timestamp. The same logic applies for each event that follows. ",
        "Flexible order": "If you select Flexible Conversion Order, then each user in the funnel may complete each event in a chronological order,or up to one day prior. The options for Conversion Order Limit start at one second, and are configurable to minutes, hours, and to one day. This would mean that users can perform subsequent events in the funnel with a timestamp prior to the first event in the funnel, and still be counted in the results. In the example below,Subscribecan happen with an earlier timestamp thanCreate Profile, and this user will count as having converted between the two steps. The same logic applies for each event that follows.  In the case of events with the same timestamps, timestamps are defined as the same if they occur within the same second. This means that if a user performsSubscribeandCreate Profileat 00:00:00.99 and 00:00:01.00, then this user willnotcount as having converted. If a user performsSubscribeandCreate Profileat 00:00:00.01 and 00:00:00.99, then this userwillcount as having converted. Custom, joined and repeat events are supported in Flexible Conversion Order, but please note that there must be distinctly separate instances of each event for a user to count as converted.",
        "Conversion logic": "Which event is prioritized in the conversion rate \u2013 for example in the case of the Average Conversion Time metric? When the conversion rate is calculated, Analytics\u2019 logic prioritizes different events based on their timestamps, prioritizing the shortest amount of time, orabsolute value, of their occurrence to event A. Take a funnel with two events:Download App(Event A) andStart App(Event B). Conversion Order is set to Flexible, with a time limit of one hour.  In this example, the timestamps for events performed by a single user are as follows: Start Appat 1:15 PMDownload Appat 2:00 PMStart Appat 2:01 PM Here, the absolute distance fromDownload App, or event A in the Funnel, is counted. Therefore, the instance ofStart Appthat occurred one minute afterDownload Appwill be counted instead of the instance ofStart Appthat occurred 45 minutes prior. Some additional logic affects how these events are treated in the results: In the case of identical values (say, 30 minutes before and 30 minutes after) for such events, the event which occurred after event A will take precedence.In terms of Conversion Time results, any events occurring before event A will be counted as occurring with zero difference, as if they occurred at the same time as event A.",
        "Enable AI assistance": "On the Calculated Attributes landing page atEnrichment > Calculated Attributes, select theCalculated Attributesbutton in the top right. In theCreate Calculated Attributemodal, check theCreate with AI Assistanceoption. In theDescriptionfield, enter natural language that will be translated into a set of criteria for generating your Calculated Attribute.",
        "Write a description for your Calculated Attribute": "When writing your description, you should be as specific as possible about what you want your CA to do or represent. Use as many of the elements below that pertain to your use case: Calculation type:The type of calculation to perform, like count, total, average, etc. We support 13 calculations organized into four categories. View themhere.Event name / event attribute:The behavioral events and event attributes that your CA will track. (e.g. purchases, page views, video views, etc.)Conditions:Parameters that narrow down the event (e.g. \u201cwhere category is shoes\u201d).Time Period:The window of time in which the event must have taken place. (If no time period is specified, the CA will default to all time.) For most calculations, you\u2019ll need to specify an event attribute in your description. The only exceptions are the following: CountFirst_occurrence_timestampLast_occurrence_timestamp For all other calculation types, an event attribute is required.",
        "Description Examples": "Below are examples ofhigh-qualityandlow-qualitydescriptions for Calculated Attributes. High-quality descriptions include all necessary elements, while low-quality descriptions lack important details or contain errors that make it difficult for AI assistance to generate accurate criteria. Countofpurchaseswherecategory is shoesin thelast 30 daysIncludes all key elements: calculation type (Count), event name (Purchases), condition (Category is shoes), and time period (Last 30 days). Specific enough for the AI to generate accurate criteria.Total money spentonflight bookingsin thelast 30 daysClearly defines the calculation type (Total), event name (Flight bookings), event attribute (Money spent), and time period (Last 30 days). Specific and avoids unnecessary complexity.Totalofflight bookingsin thelast 30 daysProvides the essential elements: calculation type (Total), event name (Flight bookings), and time period (Last 30 days). Effective despite the simplicity. Purchaseswherecategory is shoesin thelast 30 daysMissing a calculation type, making it unclear whether to count or sum purchases.Totalin thelast 30 daysMissing an event name, so the AI cannot determine what data to analyze.Countofpurchaseswhere thecategory is shoes, andtotalofpurchaseswhere thecategory is electronicsCombines different calculation types, which is invalid since a single Calculated Attribute must represent one value.",
        "Objective-C": "After downloading the client, drop the Analytics project files into a Group in your app. To start using it, specify your Analytics API key by calling Indicative\u2019s\u00a0launch:\u00a0method in the\u00a0didFinishLaunchingWithOptions:\u00a0method of your app delegate: To record an event, simply call the\u00a0record:\u00a0method, passing in the name of your event. For example: To record an event with properties call the\u00a0record:withProperties:\u00a0method, like so: The\u00a0record:\u00a0and\u00a0record:withProperties:\u00a0methods add an event object to a queue. Every 60 seconds, the events on this queue will be asynchronously sent to our servers via HTTP POST requests. If debug mode is enabled via the debug BOOL in Indicative.m, the status code and body of our response will be outputted to your device\u2019s logs. To specify the user who performs an event, call the\u00a0identifyUser\u00a0method, passing in the user\u2019s unique identifier. If you choose to not identify the user, Analytics will automatically generate an identifier for the user based on their device\u2019s MAC address.",
        "Swift": "Analytics\u2019 iOS SDK is written in Objective-C and is compatible with Swift files. To learn more about importing Objective-C into Swift, please see Apple\u2019s documentationhere. After downloading the client, drop the Indicative project files into a Group in your app. To start using it, specify your Analytics API key by calling Indicative\u2019s\u00a0launch\u00a0method in the\u00a0didFinishLaunchingWithOptions\u00a0method of your app delegate: To record an event, simply call the\u00a0record\u00a0method, passing in the name of your event. For example: To record an event with properties call the\u00a0record withProperties\u00a0method, like so: The\u00a0record\u00a0and\u00a0record withProperties\u00a0methods add an event object to a queue. Every 60 seconds, the events on this queue will be asynchronously sent to our servers via HTTP POST requests. If debug mode is enabled via the debug BOOL in Indicative.m, the status code and body of our response will be outputted to your device\u2019s logs. To specify the user who performs an event, call the\u00a0identifyUser\u00a0method, passing in the user\u2019s unique identifier. If you choose to not identify the user, Analytics will automatically generate an identifier for the user based on their device\u2019s MAC address.",
        "Adding a Data Source In Analytics": "In Analytics, click on the gear icon and selectProject Settings.Select theData Sourcestab.SelectNew Data Source.SelectConnect via Data Warehouse or Lake.SelectSnowflakeas your data connection andDefine your own schemaas the connection schema and clickConnect.You should see thisSnowflake Overviewscreen. Click Next.",
        "Waiting For Data": "If you see this screen, you\u2019re all done! You should see your data in Analytics within 48-72 hours and will be notified by email.",
        "First Time vs. Recurring Behavior": "First Time:Only takes into account the first time a user performed the target behavior during the selecteddate range. Use the first time mode to model the distribution of time it took for users to first perform a target behavior. For example, if the selected time interval is daily, and a user performed the event on three different days, they will only be counted in the data for the first day on which they performed the event.Recurring:Shows the number of users who perform the target behavior over time. Users who performed the target behavior multiple times over time are considered in this analysis. For example, if the selected time interval is daily, and a user performed the event on three different days, they will be counted in the data for each of those three days. To toggle between the First Time and Recurring cohort analysis modes, click on Recurring in the Query Builder: ",
        "Complete vs. Incomplete": "In queries that use a generational breakout, such as hour, day, week, or month of the event, the setting for Only Complete/Include Incomplete controls how the cohort visualization accounts for periods with incomplete data.  If the query is set to Only Complete, only cells where all users have had a chance to complete the target behavior in the analysis will be shown. For example, consider a query showing users who first performed the event Blog View and returned to perform the event Subscribed, grouped by Week of Blog View.  With the visualization set to show Only Complete, results from this week will not be displayed. This is because users who entered the analysis by Blog View in the week have not yet had a full week to be able to complete Subscribed and show up in the completed analysis.  If the visualization is set to Include Incomplete, an extra series of data will be displayed in the cohort. These numbers will typically be lower than the completed figures, because there has not yet been enough time to capture the full extent of users who are completing the parameters of the query. By utilizing the Only Complete/Include Incomplete setting, you can customize your results so that you\u2019re viewing data as it comes in, or so that you filter incomplete data from your analysis.",
        "Cumulative vs. Non-cumulative": "The Non-Cumulative and Cumulative cohort analysis modes determine how your cohort data is analyzed over time. To toggle between the non-cumulative and cumulative cohort analysis modes, click on Non-Cumulative in the menu bar, just below the Query Builder.  Non-Cumulative Percent:Displays the percentage of users who completed the target behavior for each selectedtime intervalorbreakout.Non-Cumulative Count:Displays the count of users who completed the target behavior at each point in time defined by the selected time interval.Cumulative Percent:Displays the percentage of users who completed the target behavior for thefirst timeas a running total over time.Cumulative Count:Displays the count of users who completed the target behavior for thefirst timeas a running total over time.",
        "Inputs and Outputs": "One of the key functions of mParticle is to receive your data from wherever it originates, and send it wherever it needs to go. The sources of your data are inputs and the service or app where it is forwarded are outputs. A connection is a combination of an input and output. Inputs include:Apps or services built on any platform we support, such as iOS, Android, or Web. You can view the full list inSETUP > Inputsin the PLATFORMS tab.Data feeds of any other data you want to send into mParticle. This could be data you have collected yourself or from a feed partner. Once configured, feed inputs are listed inSETUP > Inputson the FEEDS tab.Outputs may be configured for events, audiences, cookie syncs, or data subject requests depending on what the output supports. You can see the list of configured outputs inSETUP > OutputsorSETUP > Data Warehouses. Outputs include:Analytics partners such as IndicativeAdvertising partners such as FacebookIn-app messaging partners such as BrazeData Warehouse partners, such as Amazon Redshift, Google BigQuery, or Snowflake To get started with mParticle, you need some data, which means you need to create at least one input.",
        "Create Access Credentials": "The first thing you need to do is to to create a set of access credentials that will allow a client-side SDK or a server-side application to forward data to this workspace. Login to your mParticle account. If you\u2019re just getting started, your first workspace is created for you. The first screen you see is an overview of activity in the workspace. Since you haven\u2019t yet sent any data, there\u2019s nothing to report, so far.Navigate toSetup > Inputsin the left column. Here you can see each platform type accepted by mParticle. Different platforms are associated with different sets of metadata, such as device identifiers, and most outputs only accept data from a limited set of platforms, so it is important to select the right platform. To capture data from your native Android app, chooseAndroid. Just click the+next to your chosen platform.ClickIssue Keys.Copy and save the generated Key and Secret.",
        "About Access Credentials": "mParticle labels the credentials you create for an integration the key and secret, but they are not exactly like an API key and secret, since you embed these credentials in the app. However, this is not the security risk that exposing API credentials would be: The client-side key and secret can\u2019t read data from the system.You canblock bad datato stop any traffic that doesn\u2019t match the data you expect as defined in your schema. Most anonymous client-server architectures, including Adobe, Braze, Firebase, Google Analytics, and Segment don\u2019t have per-session or per-instance credentials, nor does mParticle.",
        "Install and Initialize an mParticle SDK": "You need a developer to help you install and initialize an SDK. See the Getting Started guides for theiOS,AndroidorJavascriptSDKs to get set up before continuing.",
        "Verify: Look for Incoming Data in the Live Stream": "Navigate toActivity > Live Streamin the left column. The Live Stream lets you inspect all incoming data from your development environments. It\u2019s an easy way to check that you have correctly initialized mParticle in your app. When you first open up the Live Stream, it will be empty, as we haven\u2019t yet started sending data.Start up a development build of your app (get a developer to help you if necessary). The mParticle SDKs automatically collect and forward data about installs and user sessions, so just by opening a development build of your app, you should start to see data in the Live Stream.",
        "Advanced Platform Configuration Settings": "For the iOS, Android, tvOS, and Web platforms, some advanced configuration settings are available. To change these settings, navigate toSetup > Inputsin the left column and select either iOS, Android, tvOS, or Web from the list of platforms. Expand theAdvanced Settingsby clicking the + icon.",
        "Restrict Device ID by Limit Ad Tracking": "iOS, Android, and tvOS (Apple TV) devices allow users to limit the collection of advertising IDs. Advertising IDs are unique identifiers you may use to associate event and user data with a specific device. For both iOS and Android devices, if a user has not provided explicit consent to share their device\u2019s advertising ID, then the value of that ID is set to an all-zero value. By checkingRestrict Device ID by Limit Ad Tracking, mParticle will not collect advertising IDs from users who have enabled the Limit Ad Tracking setting on their device. Remember, mParticle will collect advertising IDs for both iOS and Android devices, regardless of whether or not a user has enabled the Limit Ad Tracking setting on their device. However, the IDs collected from users who have opted out will be all-zero values. Following are descriptions of Apple and Google\u2019s policies for device advertising IDs: After the release of iOS 14.5, Apple introduced the App Tracking Transparency (ATT) framework, which requires app developers to request users\u2019 explicit consent to share their advertising IDs. If a user of your app has not provided this consent, Apple\u2019s advertising ID (IDFA) will be set to all an all-zero value:00000000-0000-0000-0000-000000000000. Read more about Apple advertising identifiersin their documentation. For more information about the ATT framework, visit theiOS 14 Guide. Google allows Android users to opt out from sharing their devices\u2019 advertising IDs. Similar to Apple\u2019s policy, Google will set a user\u2019s advertising ID (GAID or AAID) to an all-zero value if that user has opted out from sharing their ID. Read more about Google\u2019s advertising identifiers intheir documentation.",
        "Collect Integration-Specific Identifiers": "The Web SDK can collect integration-specific identifiers to enrich the user data forwarded to your connected outputs. WhenCollect Integration-Specific Identifiersis checked, these integration-specific identifiers are collected and used to enrich your user data to help optimize the match rate of your audiences in downstream tools. Currently, these identifiers include Facebook\u2019sfbcandfbpfields.",
        "Next Steps": "Congratulations, you have created a working data input. Now it\u2019s time tostart capturing some data.",
        "Open the NBA workflow": "To create a new NBA, navigate toEnrichment > Predictive Attributes, selectNew Predictive Attribute, then chooseBest Offer for Each Customer. This takes you to the NBA canvas, where you will carry out the three main steps that will generate a Next Best Action for your users.",
        "Define the business outcome you want to achieve.": "Give your predictive attribute a name in thePredictive Attribute Namefield. The name you enter will generate the name for this attribute that is added to your user profiles.Click the dropdown in theBusiness Goalsection. For this use case, we will select the optionDrive subscription upgrades.",
        "Identify the users you want to target.": "ClickAdd criteriain theTarget Userssection.In the query builder, select among Event and User Attributes, define their values, and string them together with logical operators to define a targeted user segment.",
        "Define potential offers for your users": "In thePotential Offerssection, give your first potential offer a name.Note: The name of each offer will appear as the predictive attribute value added to the user profile.Using the query builder next to the offer name, define the event that indicates a user has successfully engaged with this offer.SelectAdd offerto continue adding additional offers.You can define up to 10 offers. Here are what the three offers for our subscription upgrade campaign would look like:",
        "Create your Next Best Action": "Once you have completed all three steps, clickCreateto generate your prediction. Predictions can take up to 24 hours to calculate. Once calculated, each user\u2019s optimal offer will be added to their user profile. Predictions are updated weekly. Once NBAs are generated, they function asPredictive Attributesthat are added to theUser Profile.",
        "Manage Saved Analyses": "If changes are made to a query that is already saved, Analytics will prompt you to update the saved analysis. If you would like to continue editing the query before updating the saved analysis, click hide. A banner at the top of the page will persist as a reminder that there are unsaved changes in your query. If you\u2019d like to create a new saved analysis instead, change the title of your query, or select a different folder to place it in.",
        "Enabling BigQuery Export": " Sign into theFirebase Console.Click on theSettingsicon, then clickProject Settings.On the Project Settings page, click theIntegrationstab.On theBigQuerycard, clickLink.",
        "Predefined Events": "Snowplow has a set ofpredefined event typesthat can be instrumented: Page viewsPage pingsEcommerce transactionsErrors If instrumented, the integration will generate these events, where the \u2018event\u2019 field in the Unified Log is used as the event name in Analytics.",
        "Common and Platform-Specific Properties": "For all Snowplow events, a range ofdatetime, user and device fieldsare recorded along with the event. If instrumented, all of these fields are generated as Analytics properties. Additionally, anyplatform-specific fieldswill be recorded as well, such as page referer and URL information for a web-specific instrumentation.",
        "Structured Events": "Snowplowcustom structured eventsare generated using the \u2018se_action\u2019 field as the event name for Analytics (or the \u2018event_name\u2019 if \u2018se_action\u2019 is not populated). The \u2018se_category\u2019, \u2018se_label\u2019, \u2018se_property\u2019 and \u2018se_value\u2019 fields are added as Analytics properties in addition to all common and platform-specific properties.",
        "Unstructured Events": "Snowplow allows customers to model flexible customunstructured eventsas needed. The \u2018event_name\u2019 field is used for the event name in Analytics (or it defaults to \u2018unstruct\u2019 if \u2018event_name\u2019 is not populated).",
        "Custom Contexts": "Snowplow allows customers to define their owncontextaround events, such as extra user properties for a customer (membership information, age, etc.) or extra properties about a product for a purchase event (SKU, tags, product name, etc.).",
        "Aliasing": "Analytics supports aliasing between anonymous IDs and user IDs to allow customers to unify event streams submitted with separate unique keys.Click herefor a full walkthrough of Analytics\u2019 aliasing protocol.",
        "Driving Freemium-to-Paid Upsells": "Let\u2019s explore how a gym would use NBA to determine the membership tier offer that is most likely to turn trial customers into paying members. Focus on trial users or existing members on lower tiers who regularly attend the gym (e.g., moderate to high engagement with classes, personal training sessions, or equipment). Avoid targeting users who are already on the Elite membership plan. Use NBA (Next Best Action) to assess different gym membership tiers\u2014Basic, Premium, and Elite\u2014as potential upgrades based on the user\u2019s activity level and interests. Let NBA analyze factors like the user\u2019s workout habits, class attendance, and upcoming promotions (e.g., a new fitness class launch or personal training package) to recommend the best tier, maximizing the likelihood of an upsell.",
        "Driving New Product Sign-ups": "Let\u2019s explore how a retail or financial institution would use NBA to determine the best co-branded financial product offer for frequent shoppers. Focus on customers who frequently shop in-store or online and demonstrate financial engagement, such as frequent use of store loyalty rewards or payment methods. Avoid targeting customers who already have one of the available co-branded products. Use NBA (Next Best Action) to assess different financial products\u2014Savings Account,Credit Card, andLine of Credit\u2014as potential offers based on the user\u2019s purchasing patterns, credit needs, and spending habits. Let NBA analyze factors like transaction history, average spending, and upcoming promotional events (e.g., a seasonal sale with cashback bonuses) to recommend the most suitable financial product. This approach increases the likelihood of new product sign-ups.",
        "Driving User Engagement and Retention": "Let\u2019s explore how a media company would use NBA to determine the best type of content to promote for increased user engagement and retention. Focus on users who have shown moderate to high engagement but may need additional encouragement to stay active on the platform. Avoid targeting users who have already reached high engagement with frequent content consumption across multiple genres. Use NBA (Next Best Action) to assess different content types\u2014Comedy,Horror, andAction\u2014as potential promotions based on the user\u2019s viewing history, preferences, and genre exploration. Let NBA analyze factors like the user\u2019s past viewing patterns, session duration, and upcoming releases (e.g., a new season of a popular show in their preferred genre) to recommend the most engaging content. This approach boosts user activity and long-term retention.",
        "Driving Upsell and Cross-Sell": "Let\u2019s explore how a quick-service restaurant (QSR) would use NBA to determine the best product category to recommend for increasing user purchases. Focus on customers who have made recent purchases but have not yet added multiple categories to their orders. Avoid targeting customers who consistently order a full meal that already includes appetizers, mains, and desserts. Use NBA (Next Best Action) to assess different product categories\u2014Appetizer,Main, andDessert\u2014as potential recommendations based on the user\u2019s ordering patterns and preferences. Let NBA analyze factors like the user\u2019s past order history, favorite items, and upcoming promotions (e.g., a limited-time combo deal or seasonal menu item) to recommend the most appealing product category. This strategy encourages larger orders and higher purchase value.",
        "Use NBA Attribute and Percentile to Create an Audience": "In the Audience editor, give your audience a name that reflects the offer being promoted (e.g., \u201cShow Standard Plan\u201d). Then Define your audience criteria:  Add a user attribute(s) selecting users in the segment you want to upsell (e.g., Freemium users).Add another user attribute to select yourNBA attribute. (This will appear with_attributeappended to your NBA name.)Set a value for yourNBA attribute, in this case the product offer (e.g., \u201cPremium Plan\u201d).Narrow the audience further by adding aNBA percentile. (This will appear with_percentileappended to your NBA name.)Set the percentile to your desired range (e.g., >= 0.7, or top 30%). ClickDone, then selectSaveto finalize the audience.",
        "Create Audiences for Additional Product Offers": "Add a new path in your journey by clicking thefork iconabove the first audience\u2019s name.Open the audience editor for the new split.Follow the same steps as above to define the audience with the following exceptions:Use the NBA attribute to identify users for the next potential offer (e.g., \u201cPremium Plan\u201d).Adjust the conversion likelihood range as needed.Repeat this process to create separate audiences for all additional offers.",
        "Activate Your Audiences": "Under each audience, selectConnect Output.Follow the steps to forward the audience to your chosen engagement platform (e.g., an email marketing tool, ad platform).In the engagement platform, use these audiences to power personalized recommendations based on their assigned NBA. By segmenting users based on NBAs and conversion likelihood, you can efficiently target the right users with the most relevant offers, improving personalization and conversion rates.",
        "Understanding Data Points and Data Plans": "For every mParticle workspace, you may have many data plans.Every data plan contains one or more data plan versions.Each data plan version contains data points that you have defined.  A data point is a unit of data collected by mParticle. Data points represent Events such as Custom Events or Screen Views, or User attributes such as Customer Id or Email. Each data point includes criteria used to select the data point from an incoming data stream, and a definition called a schema that validates the contents of the data point. You can view data points in all three Data Master tools: Catalog, Live Stream, and Data Plan. For example, here is a view of the Play Video data point in Catalog:  Here is a view of the a Commerce Event data point in Live Stream:  This view of a data point is from Data Plan: ",
        "Tracing": "The foundation of Observability istracing. As data flows through mParticle, it is ingested, processed, and routed to various services or forwarders in discrete steps. Atracein mParticle\u2019s Observability is a detailed record that connects all of these steps on through a single timeline. Think of a trace as a trip report for your data: it shows where it came from, where it went, and any stops it made along the way. Tracing is provided by default for all data flowing in your development environment. For production data tracing, Observability lets you configure what data is traced through customizableTrace Configurations. When Observability is used in conjunction with tools like System Alerts and Live Stream, it can help you diagnose and troubleshoot issues with your data pipelines.",
        "How does tracing work?": "All data sent through your mParticle development environment is traced by default, and can be viewed on the Trace Activity page. If you want to trace production data, you can do so by creating a customtrace configurationto gain specific insights. If a data flow has an active trace configuration, a call made to one of thesupported mParticle servicesinitiates a unique trace. Each trace is identified by aTrace ID, which you can use to find and view specific details about the data\u2019s journey in the Observability tool. Traces record information such as which input the data originated from, which outputs the data will be sent to after processing, any rules or filters applied to the data, any user MPID\u2019s related to the trace, the types of events included in the data, error codes, and any data plans that will be used to process the data. In addition to these details, the trace details page presents a graphic display of the following stages, known as \u201cspans\u201d, that your data happen to flow through: Data ingest: the initial step of receiving data from one of the mParticle SDKs, a feed, or a warehouse sync pipeline.Processing: the stage in which event batches are received and routed to the next stage.Transformations: the process of applying rules to your data. This stage is executed both when data is ingested into mParticle and when it is forwarded to an output.Identity Resolution: the process of identifying or creating user profiles to attribute user data with.Payload Delivery: the stage in which outbound data is collected and forwarded to one of your connected outputs.",
        "What data can be traced?": "Observability provides end-to-end tracing for all event data, starting from it\u2019s first ingested through via an mParticle platform SDK or server-to-server inputs, to when data is forwarded to any of the real-time event destinations. Specifically, tracing is available for data processed by the following APIs: Events APIIDSync API(Identity Resolution)You can also use Observability to trace data ingested fromWarehouse Syncpipelines. By default, data in your development environment is traced automatically. You don\u2019t need to configure anything, and you can immediately begin reviewing trace details for your development data from Live Stream or the Trace Activity page in Observability. To trace your production data, you must firstcreate a trace configurationthat specifies which data you want traced.",
        "Tracing limitations": "Observability does not currently does not currently support tracing for data sent to mParticle\u2019s bulk forwarding event outputs or audience pipelines.",
        "Observability fair use limitations": "You can trace up to 10% of your annual batch volume sent through your production environment at no extra charge. If you exceed this limit: Your mParticle account representative may contact you to discuss your needs and options.You will not be able to create new trace configurations.New traces will not be generated for any data sent through your existing trace configurations, but any existing traces will remain available until 14 days after they were created.Your development data will still be traced automatically.",
        "How long are traces available?": "Traces for development and production data are accessible for up to 14 days.",
        "How do I configure traces for my production data?": "To learn more about the information provided in a trace and how to create a trace configuration for your production data, continue reading theObservability User Guide.",
        "Tracing best practices": "Observability provides detailed information about how data is ingested, processed, and forwarded through mParticle\u2019s CDP. To make the best use of Observability while keeping your cost low, we recommend the following strategies: 100% of your development data is traced automatically, at no extra cost to you. This makes Observability a powerful testing and troubleshooting tool that you should use when setting up a new mParticle configuration. By sending test data through your development environment, you can gain insights into how your platform or feed inputs are functioning; whether your data plans, rules, and filters are behaving as expected; and whether the data you expect to see in your output is successfully being forwarded. Even after deploying a well tested mParticle configuration to process your production data, it\u2019s still possible to encounter issues. mParticle recommends creating a tracing configuration to monitor new configurations with a high sample rate(up to 100%)but for a short period of time(up to one week). This allows you to detect issues quickly. Remember that you can trace up to 10% of your annual batch volume at no extra charge, but if you reach this limit you will be unable to create new trace configurations. If you do reach this limit, you can contact your mParticle account representative to discuss the unique needs of your business and what additional tracing options may be available. Once a new mParticle configuration has been successfully processing production data for a period of about a week, you can transition to a long-term monitoring approach. mParticle recommends creating long-term tracing configurations(more than one week)with a low sample rate(under 10%). This allows you to still detect and troubleshoot anomalies that may occur in your existing pipelines without causing you to reach yourtracing usage limit. If you do begin to approach your limit, your mParticle account representative will contact you.",
        "Organize Widgets": "Free Flowlets you place your dashboard analyses wherever on your dashboard.Auto Flowautomatically compacts analyses to the top of your dashboard. For more information on organizing your dashboard, check out theDashboard Visualizationarticle.",
        "Dashboard Refresh Intervals": "Your dashboard analyses will refresh at the interval designated in the dashboard menu bar. For more information on dashboard refresh intervals, check out theTime and Interval Settings in Dashboardsarticle.",
        "Open Query": "To open the underlying query behind an analysis, select the analysis\u2019 title. Then, you may edit your query and save. You may also open the underlying query behind an analysis by selecting on the three dots in the top right of your analysis, and selecting \u201cOpen Query\u201d.",
        "Settings": "To edit your analysis\u2019 settings, select the three dots in the top right of your analysis, and select \u201cSettings\u201d.",
        "Title": "Type your desired analysis name in the \u201cTitle\u201d field. You may also rename your analysis by clicking into the analysis itself and renaming the query from the query tool.",
        "Display Options": "In your analysis settings, check the corresponding checkboxes to display the following within your dashboard analysis: Show TitleShow DateShow Legend",
        "Embed Widget": "This feature is available only for users on our Enterprise pricing plan. Add a live widget to your website, shared workspace, or other HTML environment to promote data access and establish a single source of truth. To get started, open your favorite Analytics dashboard, then locate the analysis you\u2019d like to embed. Next, open analysis settings (click the three dots that appear upon hover in the top right corner of the analysis), select your preferred display size, and click on the iframe code to copy it to your clipboard. You can now paste your widget anywhere iframe elements are enabled. Analytics supports the ability to filter embedded analyses programmatically via an API. You may create variants of your embedded analyses by adjusting Date Ranges, Time Zones, Query Intervals, and Property Filters. To configure this, please see ourDashboard Filter API documentation. Please note that this feature requires advanced instrumentation from a technical resource. When refreshing dashboards, Analytics uses an algorithm to balance data \u2018freshness\u2019 and efficient resource utilization. When first applying a new filter to a dashboard or analysis, please allow up to 15 minutes for changes to be reflected in the dashboard\u2019s variants. Subsequently, each request to render a dashboard or analysis will display the most recently cached result before initiating an update.",
        "Move": "To move an analysis, select the three-dot menu in the top right corner. Then, select \u201cMove\u201d. Select a dashboard in which to move your analysis, and confirm by choosing \u201cMove\u201d, or click \u201cCancel\u201d if you changed your mind.",
        "Duplicate": "To duplicate an analysis, select the three-dot menu in the top right corner. Then, select \u201cDuplicate\u201d. Create a name for your new analysis, and select a dashboard in which to save it. Select \u201cDuplicate\u201d to finalize your dashboard duplication, or choose \u201cCancel\u201d if you\u2019ve changed your mind.",
        "Delete": "To delete a dashboard, select the three-dot menu in the top right corner of your analysis. Then, choose \u201cDelete\u201d. You will be asked to confirm if you want to delete your analysis. Please note that your analysis will not be recoverable.",
        "Optimize Dashboard Performance": "Here are a few ways to optimize your dashboard for improved query performance: Minimize date ranges:Unless the date range of an analysis is updated, it will continue displaying more data as time goes on. The more historical data the system has to refresh, the greater the impact on performance. To avoid this, restrict the date ranges for analyses to only what is required for data accuracy and gathering insights.Minimize widgets in a dashboard:The fewer widgets in a dashboard, the faster the dashboard will be refreshed when filters are applied.Select User Attributes Efficiently in your Analyses:Minimize querying/updating user attributes in an analysis.Streamline Segmentation Queries:Simplify segmentation queries, especially those with multiple \u2018for\u2019 clauses.Optimize Text Field Queries:Avoid \u2018Contains\u2019 clauses with a large number of values in text fields. Note:With the exception of \u201cMinimize widgets in a dashboard,\u201d all optimizations above are valid best practices at the analysis level as well.",
        "Query builder": " To analyze a customer\u2019s journey by an ending event, select your single ending event which you want to analyze. From there, toggle the Starting With option and then select the Ending With option. By choosing the Ending With option, your customer journey will begin from an end event. ",
        "Filter where": " In the query builder, you can filter an event to include or exclude users who meet certain criteria; for example if they have specific event property. To create a filter, you may only use event properties and there is no limit to the number of filters you can apply to a single query row. The first filter you will encounter is the:is equal toclause. You may also filter in the following ways: Is not equal toContainsDoes not containIs definedIs not defined",
        "Query menu": " All Journeys queries may be customized further using the query menu. Moving left to right\u2026",
        "Repeat events": " Use the Repeat Events component to merge sequentially repeated events into one single event. For example, if you track a Page View event that fires with each page view on your website, your most common Journey is likely to be Page View to Page View to Page View to\u2026you guessed it, Page View! If you track meta-events such as the example above, you\u2019ll want to Collapse your repeated events into one single event. If you don\u2019t track meta-events such as the example above, you\u2019re fine to leave your Repeated Events as Expanded. This feature can impact query performance. For example:  Path from A (ignoring repeated): A \u2192 B \u2192 C \u2192 D \u2192 END Paths from B (ignoring repeated): B \u2192 C \u2192 D \u2192 END (3x) Paths from C (ignoring repeated): C \u2192 D \u2192 END (2x)",
        "Events with breakouts": "This menu bar item will display any events that are broken out by an event property. Here, you may delete any such events. To break out events by an event property, see theEvent Menusection.",
        "Excluded events": "This menu bar item will display any events that have been excluded from the visualization. Here, you may delete the excluded event to have it appear in the visualization again. To exclude events, see theEvent Menusection.",
        "Percentage threshold": "On the right of the query menu, we have two additional options: Percentage Threshold and Number of Steps. Both of these settings impact the height and depth of your Journeys visualization. The percentage threshold governs the height of your visualization. Each Journey label contains a percentage number. This percentage is the percentage of your total users who arrive at the selected event, at the selected step. In the Percentage Threshold menu item, you may select your percentage threshold. A lower percentage threshold means more possible Journeys. A maximum of 10 Journeys can be displayed in each step. At each step, we only keep the events that account for at least X% of the total paths at that step. This is another report feature that reduces noise by allowing you to control what defines a significant percentage of users performing the next step.  Please note that query time may increase as you lower the percentage threshold.",
        "Number of steps": "The number of steps governs the depth of your visualization. By default, Journeys displays 5 steps. You may add up to 15 total Journeys steps. Please note that query time may increase as you add more and more steps.",
        "Create an IAM Role for the Lambda": "Your AWS Lambda needs to have an Execution Role that allows it to use the Kinesis Stream and CloudWatch. (For more information on setting up IAM Roles, please seethe official AWS tutorial.) Go toIAM Managementin the Console and chooseRolesfrom the sidebar.ClickCreate role.For the type of trusted entity selectAWS Serviceand for the service that will use this role chooseLambda. ClickNext: Permissionsat the bottom of the screen.Now you need to choose a permission policy for the role. The Lambda needs to have read access to Kinesis and write access to CloudWatch logs - for that we will chooseAWSLambdaKinesisExecutionRole. Search forAWSLambdaKinesisExecutionRolein the search and mark the checkbox as shown below.ClickNext: Reviewat the bottom of the screen.On the next screen provide a name for the newly created role underRole Name, then clickCreate roleto finish the process.",
        "Create the Lambda Function": "The Lambda function can be created either directly through AWS Console or through other tools like the AWS CLI. For this integration, the recommended memory setting is 256 MB and because the JVM has to cold start when the function is called for the first time on a new instance, you should set a high timeout value; 90 seconds should be safe. As with the IAM Role, we will be using the AWS Console to get our Lambda function up and running.Make sure you are in the same region as where your Kinesis streams are defined. On the Console navigate to theLambdasection and clickCreate a function(runtime should beJava 8).Write a name for your function inName. In theRoledropdown pickChoose an existing role; then in the dropdown below choose the name of the role you created in the previous step. ClickCreate function.The Lambda has been created, although it does not do anything yet. We need to provide the code and configure the function:a. Take a look at theFunction codebox. In theHandlertextbox paste:com.snowplowanalytics.indicative.LambdaHandler::recordHandlerb. From theCode entry typedropdown pickUpload a file from Amazon S3. A textbox labeled S3 Link URL will appear. We are hosting the code through ourhosted assets. You will need to choose the S3 bucket in the same region as your AWS Lambda function: for example if your Lambda isus-east-1region, paste the following URL:s3://snowplow-hosted-assets-us-east-1/relays/indicative/indicative-relay-0.4.0.jarin the textbox. Take a look atthis tableto pick the right bucket name for your region. Make sureRuntimeisJava 8.Get theAPI Keyfrom step 4 from the Analytics UI.BelowFunction codesettings you will find a section calledEnvironment variables.a. In the first row, first column (the key), typeINDICATIVE_API_KEY. In the second column (the value), paste your API Key.b. The relay lets you configure the following filters:- UNUSED_EVENTS: events that will not be relayed to Analytics;\n  - UNUSED_ATOMIC_FIELDS: fields of the [canonical](https://github.com/snowplow/snowplow/wiki/canonical-event-model) Snowplow event that will not be relayed to Analytics;\n  - UNUSED_CONTEXTS: contexts whose fields will not be relayed to Analytics. Out of the box, the relay is configured to use the following defaults: To change the defaults, you can pass in your own lists of events, atomic fields or contexts to be filtered out. For example: Similarly to setting up the API key, the first column (key) needs to be set to the specified environment variable name in ALLCAPS. The second column (value) is your own list as a comma-separated string with no spaces. If you only specify the environment variable name but do not provide a list of values, then nothing will be filtered out. If you do not set any of the environment variables, the defaults will be used. Scroll down a bit and take a look at theBasic settingsbox. There you can set memory and timeout limits for the Lambda. As mentioned earlier, we recommend setting 256 MB of memory or higher (on AWS Lambda the CPU performance scales linearly with the amount of memory) and a high timeout time of 1 minute 30 seconds. kinesis As a final step, add your Snowplow enriched Kinesis stream as an event source for the Lambda function. You can followthe official AWS tutorialif you are using AWS CLI or do it directly from the AWS Console using the following instructions. Scroll to the top of the page and from the list of triggers in theDesignerconfiguration up top, chooseKinesis. Take a look at theConfiguretriggers section which just appeared below. Choose your Kinesis stream that contains Snowplow enriched events. Set the batch size to your liking - 100 is a reasonable setting. Note that this is amaximumbatch size, the function can be triggered with fewer records. For the starting position we recommendTrim horizon, which starts processing the stream from an observable start (Alternatively, you can selectAt timestampto start sending data from a particular date). Click theAddbutton to finish the trigger configuration. Make sureEnable triggeris selected.  Save the changes by clicking theSavebutton in the top-right part of the page.",
        "Validate Your Data": "Go to yourIndicative projectto check if you are receiving data. You can also go to thedebug consoleto troubleshoot the relay in real time.",
        "SDK Integrations": "We offer packaged integrations written in a variety of languages including Java and Python, as well as special integrations for iOS and Android apps. These integrations are designed to be fail-proof, asynchronous, and able to be modified as suits your needs. Please see our SDK documentation below. AndroidiOSJavaJavaScriptPythonReact Native",
        "Create a new group definition": "If you\u2019re using the classic UI, navigate toData Master > Group Identitiesfrom the left hand nav bar. If you\u2019re using the New Experience, SelectCustomer 360from the Overview page.ClickCreate Group.Use the dropdown menu to select the user attribute that will act as the source for your group ID, and clickNext.  Enter a name for your group definition. This is how you will find your new group in other mParticle features, like the Audience builder.Enter a group description.  ClickCreate Group Identity. After creating your new group identity, you are taken to the Group Identity Details page where you can begin adding group attributes. Any users with the same value for the user attribute you selected as the group identifier are added to an instance of this group. For any given group ID value, a group is only created if the group ID value is at least 5 characters long.",
        "Add attributes to a group": "To add user attributes to your group: ClickSelect Attributes.  Use the two dropdown menus to select the user attribute to add as a group attribute and its type, eitherBoolean_ororLatest.  To add additional attributes, clickAdd Attribute. You can add up to 10 group attributes.Finally, clickSavewhen you are done adding attributes. After users have been added to a group, they will appear in the User Activity View when searching for the group identifier or group attribute.",
        "Delete or modify a group definition": "To modify a group or its attributes: Navigate toData Master/Customer 360 > Group Identities.Select the group definition you want to modify.  To remove a group attribute, click thexdelete icon next to the attribute.To add another group attribute, clickAdd Attributeand follow steps 2 through 4 inAdd attributes to a group  To delete a group definition: Navigate toData Master/Customer 360 > Group Identities.Click the details icon on the right side of the group definition\u2019s row in the Group Identity page, and click \u201cDelete\u201d. ",
        "User profiles": "Consent state is maintained per person on the User Profile using the structure defined above. For testing consent, you can useUser Activity Viewto check that a consent was recorded correctly. Here is an example of how CCPA data sale opt-out will appear: ",
        "User identifiers": "User identifiers, referred to asuser identitiesin the IDSync API, are attributes like email addresses or customer IDs defined as key/value pairs. Identifiers are used to identify users whenever an IDSync request is received. The complete list of supported identifiers are: customer_idemailfacebooktwittergooglemicrosoftotherother_id_2other_id_3other_id_4other_id_5other_id_6other_id_7other_id_8other_id_9other_id_10mobile_numberphone_number_2phone_number_3 There are several subcategories of identifiers with some unique characteristics, described below.",
        "Login IDs": "Login IDs are used to identify one, and only one, MPID for a known user when resolving an identification request. If your account uses the Profile Link strategy, then the first time IDSync receives an identification request containing a login ID with no associated user profiles a new user profile will be created. To designate one or more identifiers as login IDs, contact your account representative directly or submit a request to mParticle support. A login ID identifies a single known user. In order to maintain the integrity of known identity records, a record with at least one login ID can only be returned if the identify request includes a matching login ID. Identity records Scenarios One way identity strategies handle new known users is by applying rules about what to do when a new login ID is received for the first time. For example, theProfile linkstrategy always creates a new identity record when a login ID is received for the first time. TheProfile conversionstrategy does not create a new identity record when a login ID is first received. The new ID is added to the existing identity record. Identity records Scenarios",
        "Immutable IDs": "Immutable IDs are identifiers that cannot be changed once they have been set. In order to maintain the integrity of known user profiles, the value of an immutable ID may not be modified to protect against identity theft. A profile with at least one immutable ID can only be returned if the identification request includes at least one matching immutable ID. Immutable IDs may be used as query parameters for the profile API.",
        "Unique IDs": "A unique identity (unique ID) is a setting that specifies that that user profile identifier must be unique. This means that only one mParticle user profile can have that value of the identifier. If a modify request to theIDSync APIwould result in two identity records sharing the same value of a unique identity, mParticle will add or update the identifier on the requested user profile and remove it from any other user profile to enforce uniqueness.Note that this doesn\u2019t mean all other identifiers are removed from the user profile. The history of that profile remains intact. But removing the conflicting identifier from the profile means it can no longer be used to lookup that profile. User profiles with no remaining identifiers are effectively \u2018orphaned\u2019. They will not be deleted, but can never be returned by an IDSync API request. User profiles A user signs up for your iOS mobile app with the emailed.hyde@example.com. The same person also independently interacts with your helpdesk, using a different email addressh.jekyll.md@example.com. This results in two user profiles being created, one for each email. Each has a unique mParticle ID: Scenarios",
        "Identity records": "Behind the scenes, mParticle maintains a user profile for each user. You can think of a user profile as a folder of data that describes all the events, user attributes, identities, attribution info, and device info for a user. User profiles help determine which users are included in different audiences, and they enrich incoming data with any relevant user information before forwarding it to a connected output. The main purpose of IDSync is to assign incoming data to the correct user profile. However, to identify users in real time, IDSync doesn\u2019t look at the entire profile, but at that profile\u2019s identity record. Think of an identity record as a label on the front of your folder of user data (the user profile). The identity record contains a list of all identifiers that can be used to look up a profile. Identity records always have a 1:1 relationship with their corresponding profile. There are two key points to remember about identity records: Some uses of IDSync force identifiers to be unique to a single identity record. Email addresses are a good example. SeeUnique Identitiesfor more information.The identity record might not contain every possible type of identifier available in a profile, but it will contain the identifiers that are specified in your identity hierarchy.",
        "Conversion vs. Conversion Over Time": " You can toggle a funnel between Conversion and Conversion Over Time analyses. Conversiontracks users through a multistep sequence of events, or \u201cfunnel\u201d. Users who complete steps within the funnel are counted only once.Conversion Over Timedisplays how the funnel performs on a historical basis. Users who complete steps within a funnel can be counted more than once if they complete the funnel over multiple intervals.  Choosing daily, weekly, or monthly determines the length of the funnel entry window. ",
        "Conversion over time between steps": "When Conversion Over Time queries are visualized as a table, you may view the 3 core conversion metrics (Conversion Rate, Average Conversion Time, and Converted Users) between each step in your funnel. With this feature, you can identify what steps in your funnel are driving drop-off or increased conversion, just like you can with a standard funnel analysis. Take the following funnel: Blog ViewDownload AppStart App  In the Table visualization, the three funnel metrics are displayed not only for the entire funnel, but for each step within the funnel:  The specific funnel steps shown in the table can be toggled using the Steps dropdown in the query settings bar: You may also toggle on and off the metrics displayed via the Metrics dropdown in the query settings bar. For each step, you can view: Total Conversion Rate(the percentage of users who completed that step compared to the previous one).Converted Users(the number of users who completed that step).Average Conversion Time(the average time it took to move from that step to the next).",
        "Getting started": "$ npm install react-native-indicative --save",
        "Mostly automatic installation": "$ react-native link react-native-indicative",
        "Manual installation": "In XCode, in the project navigator, right clickLibraries\u279cAdd Files to [your project's name]Go tonode_modules\u279creact-native-indicativeand addRNIndicative.xcodeprojIn XCode, in the project navigator, select your project. AddlibRNIndicative.ato your project\u2019sBuild Phases\u279cLink Binary With LibrariesRun your project (Cmd+R)< Open upandroid/app/src/main/java/[...]/MainActivity.java Addimport com.reactlibrary.RNIndicativePackage;to the imports at the top of the fileAddnew RNIndicativePackage()to the list returned by thegetPackages()method Append the following lines toandroid/settings.gradle:include ':react-native-indicative'\nproject(':react-native-indicative').projectDir = new File(rootProject.projectDir, \t'../node_modules/react-native-indicative/android')Insert the following lines inside the dependencies block inandroid/app/build.gradle:compile project(':react-native-indicative')",
        "Usage": "Find more onNPMandGitHub.",
        "Quality": "Quality reflects the overall accuracy of your prediction. Each time you create a Predictive Attribute, Cortex sets aside a portion of your data to compare the predictions to real-world outcomes. The results of these tests are converted into a score that indicate how well the model performed. The values in the Quality column correspond to the following scores: Excellent:0.85 - 1.0Very Good:0.75 - 0.85Good:0.65 - 0.75Average:0.55 - 0.65Below Average:0.5 - 0.55Unknown:A quality score could not be determined. The scores reflect the percentage of the predictions that aligned with what was observed in the real world. A score of 1.0, for instance, means that every prediction in the model was correct when compared with real-world data. Conversely, a score of 0.0 means that every prediction was incorrect.",
        "Status": "Each rule has a master switch in the Settings panel. If there is a problem with your rule, you can switch it off and it will be disabled for all connections until you enable it again. To disable, clickEditin the right sidebar and set theStatusslider toinactive.",
        "Last Calculation": "The time at which the most recent calculation was completed.",
        "View prediction details": "Clicking on the name of any of your Predictive Attributes will direct you toEnrichment / Predictive Attributes / {Name of Prediction}:  The tiles in theUser Likelihood Rangessection tells you about the users who fall into three default percentile ranges:  The default ranges displayed in this section are: Most Likely Users:90th percentile and above. (More likely to convert than at least 90% of other users in the group.)Somewhat Likely Users:70th - 90th percentile. (More likely to convert than at least 70% of other users, but less likely to convert than the top 10%.)Least likely users:0th - 10th percentile. (Less likely to convert than at least 90% of users in the group.) Each User Likelihood Range tile also displays: Total users in the rangeA predicted conversion rate for the rangeHow likely users in this range are to convert relative to the average across the group",
        "Custom Ranges": "Using theCustom Rangetile, you can specify a custom prediction percentile or fixed number of users. The tile will then display the total users, predicted conversion rate, and conversion likelihood relative to the average for the range specified: ",
        "Troubleshoot a failed pipeline": "The most common reason for a prediction to fail is because there are not enough users in the pipeline, and therefore not enough real-world training examples for the model to generate accurate predictions. This can happen for a number of reasons, including: The selected conversion window is too short.The conversion time frame you select is the lookback time that Cortex will use to analyze which behaviors lead to conversion. If not enough users converted within this time frame, Cortex will not be able to draw meaningful connections and create accurate predictions. Consider selecting a longer conversion time frame to increase the number of users in the pipeline.Exclusion criteria are too stringent.While usingexclusion criteriato narrow a prediction to a subset of users can help improve its accuracy, an overly restricted prediction can result in an inadequate number of total users and a failed pipeline. If you are using exclusion criteria in your prediction, consider loosening or eliminating certain  conditions.",
        "Visualization area": "Your aggregated Customer Journeys will be displayed in the visualization area, just below the Query Menu. Your Journeys chart contains a number of components. ",
        "Event": "Each event in your Journeys chart contains a label. This label displays the number of users in this particular event at this particular step, along with the percentage of users in this particular event out of users at this particular step. To see the number of users coming from one event to the other, hover over the path linking the two events.",
        "Drop off": "This is an aggregation of the users for which the event in the prior step is the last observable event.",
        "Other": "This is an aggregation of the users who do not meet the percentage threshold selection in the query menu.",
        "Begin an analysis": "First, you must select whether you would like to analyze users who performed an event, or users who are in a user segment. The default selection is performed an event. To toggle this, click on the dropdown at the top of the query builder.  Once you have selected your analysis type, begin inputting events in the query builder. If you have selected performed an event, you may input many events in the query builder. In this case, each user must have completed the specified events chronologically in the order that they were inputted.  You may select a different time zone from your project time zone on a per query basis by clicking on the globe icon on the top right of the query screen.  Then, select the date range in which users must complete the specified events. Users must have completed all events in the query builder within the specified date range.  Then, click the play button to run the query. The Users tool will return a list of users who have either performed the sequence of events, or users that are in the specified user segment, along with all of their user properties.",
        "Row order": "To be included in a User Insights list, users must have completed the events in the query builder in the specified order. To re-order the event sequence, click and drag a row in the query builder into the desired position in the sequence. ",
        "Searching results": "Search for users and their user properties by any matching value by using the search box in the upper right corner of the chart window. ",
        "User first lookup": "You can navigate directly to a user\u2019s profile by inputting their user ID, either authenticated or unauthenticated.  This feature drastically reduces the time associated with viewing a particular user\u2019s properties, event history, or event timeline.",
        "Access user insights from Segments, Funnels, and Cohorts": "Cross-Tool Compatibility allows you to recreate your analysis from one tool in another tool within the Analytics suite. User Insights may be used to view analysis results from Segmentation, Funnel, and Cohort and drill down into the behavior of individual users. To learn more about how User Insights complement each Analytics tool, view the articles below: Cross-tool compatibility in SegmentationCross-tool compatibility in FunnelsCross-tool compatibility in Cohort",
        "View Assisted Analysis": "To view the Assisted Analysis, select the icon highlighted below.",
        "Use Assisted Analysis": "To use the Assisted Analysis tool: 1. Run a Segmentation query. 2. Select the icon to the right of your query builder, which displays a menu on the right side of the screen. The first time you use it, you must accept the Terms of Service since this is a beta feature. 3. Choose an option: Insights: Summarizes the chart results into easily digestible insights.Suggestions: Provides suggestions for follow-up analysis or changes to consider implementing. The Assisted Analysis tool will then use AI to generate Insights or Suggestions depending on your selection. You can click Insights or Suggestions again to generate a new response. This third-party AI uses the query and resulting CSV output, not the raw event data. PII will not be sent to the third party unless included in the query or CSV output, for example, if an email address is used in the query filter.",
        "Disable Assisted Analysis": "To\u00a0 disable Assisted Analysis at the project level, click the Settings icon (gear) in the left navigation bar, then scroll down to\u00a0Privacy and Security. Under\u00a0AI Features, click Disabled.",
        "Export Query Results": "To download your results as a CSV file or PNG image file, simply choose the export icon located in the menu bar beneath the query builder, then select a file type. CSV exports are limited to the first 1,000 results. For detailed instructions on how to export results from each tool, see the following articles: Export Results in SegmentationExport Results in FunnelExport Results in CohortExport Results in Users",
        "Export User Data": "There are two ways to export user data from Analytics: Export user data directly from the Users tool: First, you mustcreate a query in the Users tool. Once your query is ready for export, simply select the export icon located in the menu bar beneath the query builder, and choose Download CSV. A CSV file will be emailed to the email associated with your account. There is no limit to the number of users that can be exported.  Export Event Data and Users from Segmentation, Funnel, or Cohorts: You can also download a user list from a Segmentation series or point, a specific step in your Funnel, or a Cohort cell. To download event data from a Segmentation query: Create a Segmentation QuerySelect a data point in the querySelect Download Data and Events to CSVSelect either In Entire Series or In this PointClick Proceed and you\u2019ll be emailed a download link By default, all properties are selected. The export limit is one million events. Read-only users do not have permission to run exports.  To download users: Run a query (Segmentation, Journeys, Funnel, Cohort)Select a data point in the visualizationSelect either Explore Users From Entire Series or From this PointYou\u2019ll get redirected to the Users tool, from which you can download users according to the instructions above",
        "Create Audience Output": "To add an audience output: Navigate toSetup > Directory, and click the card for your audience partner of choice. (You can filter the Directory to show only partners with an Audience Configuration.)Select an output and complete the Connections Settings dialog. This will be different for every integration. See theIntegrations Centerfor details for your integration.Complete theConfiguration Settingsdialog. Each partner will require slightly different information. Some require an API Key/Secret/Token, others require you to log in from mParticle using Oauth. See theIntegrations Centerfor details for your integration. Give the configuration a name and clickSave. You can view and update your configurations at any time by navigating to Setup > Outputs, and selectingAudiencetab.",
        "Connect Your Audience": "Once you have created at least one audience output, you can connect your audience to begin sending data to that vendor. In the Audience Editor, selectConnect Outputin the card beneath the audience you want to connect.Select the output to which you want to connect and follow instructions in theConnect Outputdialogue. This will be different for every integration. See theIntegrations Centerfor details for your integration.Make sure theStatusswitch is set toActive.ClickAdd connection.",
        "How Audience Forwarding Works": "When mParticle forwards an Audience to an output, only identity data is sent by default. While mParticle can collect various identity types for both devices and users, most audience partners accept only a limited set of identities based on their specific needs. For example, an email marketing partner may only accept email addresses, a push messaging partner may only accept push tokens, and a mobile advertising platform may only accept device advertising identifiers (IDFA for iOS and GAID for Android). When building your Audiences in mParticle, you don\u2019t need to worry about which identities a partner supports. Simply define your Audience criteria, and mParticle will forward all available identities that each partner accepts.",
        "Selective Attribute Forwarding": "When forwarding data to an Audience output, you have the option to selectively forwardUser Attributesbeyond the identities that are required for engagement. Forwarding Calculated Attributes like LTV or Predictive Attributes like propensity scores, for example, enables you to use richer data in your activation platforms. Not all integrations support this \u2014 when you connect your Audience you will see what is supported. In the last step of the process to connect an output, select which account and workspace level attributes you would like to forward to that particular tool: ",
        "Installation & Setup": "Download the library on GitHub After downloading the client, add theindicative.pyfile to your project and import Indicative. import indicative Before you start recording events, call theinit()method and pass in your project\u2019s API key: indicative.init('Your-API-key-goes-here') You should only have to do this once.",
        "Sending Events": "Next, to record an event, set up a dictionary object with your property names and value, and then call therecord()method. Its usage looks like this: Therecord()method takes three arguments: the event name, the user\u2019s unique ID, and a dictionary containing the event\u2019s property names and values. This method creates a JSON representation of your event and sends it to our API endpoint. Note that this is done synchronously.",
        "Supported warehouse providers": "You can use Warehouse Sync to ingest both user and event from the following warehouse providers: Amazon RedshiftGoogle BigQuerySnowflakeDatabricks",
        "Warehouse Sync setup overview": "Prepare your data warehouse before connecting it to mParticleCreate aninput feedin your mParticle account for your warehouseConnect your warehouse to your new mParticle input feedSpecify the data you want to ingest into mParticle by creating a SQL data modelMap your warehouse data to fields in mParticleForuser data pipelines, this mapping is done by your data modelForevent data pipelines, you must complete an additional data mapping stepConfigure when and how often data is ingested from your warehouse",
        "Observability for Warehouse Sync": "mParticle\u2019sObservabilityoffers support for tracing data ingested into your account through a Warehouse Sync pipeline. Simply select the pipeline feed you want to trace from the list of Feeds whencreating a new trace configuration. Trace configurations for Warehouse Sync pipelines will generate a new unique trace for each event batch generated by your pipeline within the timeframe you specify in your configuration. Like other trace configurations, pipeline traces display spans for each processing stage your data passes through once it\u2019s ingested into mParticle. Warehouse Sync traces do not currently provide row-specific details from your source database or details for diagnosing errors encountered during data ingestion. The current version of Warehouse Sync tracing is intended to help diagnose and troubleshoot issues encountered post-ingestion.",
        "1 Prepare your data warehouse": "Work with your warehouse administrator or IT team to ensure your warehouse is reachable and accessible by mParticle. Whitelist themParticle IP address rangeso your warehouse will be able to accept inbound API requests from mParticle.Ask your database administrator to perform the following steps in your warehouse to create a new role that mParticle can use to access your database. Select the correct tab for your warehouse (Snowflake, Google BigQuery, Amazon Redshift, or Databricks) below.",
        "2 Create a new warehouse input": "Log into your mParticle accountNavigate toSetup > Inputsin the left-hand nav bar and select the Feeds tabUnderAdd Feed Input, search for and select your data warehouse provider.  You can also create a new warehouse input from the Integrations Directory: Log into your mParticle account, and clickDirectoryin the left hand nav.Search for either Google BigQuery, Snowflake, Amazon Redshift, or Databricks.  After selecting your warehouse provider, the Warehouse Sync setup wizard will open where you will: Enter your warehouse detailsCreate your data modelCreate any necessary mappings between your warehouse data and mParticle fieldsEnter your sync schedule settings",
        "3 Connect warehouse": "The setup wizard presents different configuration options depending on the warehouse provider you select. Use the tabs below to view the specific setup instructions for Amazon Redshift, Google BigQuery, and Snowflake.",
        "4 Create data model": "Your data model describes which columns from your warehouse to ingest into mParticle, and which mParticle fields each column should map to. While mParticle data models are written in SQL, all warehouse providers process SQL slightly differently so it is important to use the correct SQL syntax for the warehouse provider you select. For a detailed reference of all SQL commands Warehouse Sync supports alongside real-world example SQL queries, seeWarehouse Sync SQL reference. Write a SQL query following the guidelines outlined below and theWarehouse Sync SQL reference. Make sure to use SQL commands supported by your warehouse provider.Enter the SQL query in the SQL query text box, and clickRun Query.ClickNext.  mParticle submits the SQL query you provide to your warehouse to retrieve specific columns of data. Depending on the SQL operators and functions you use, the columns selected from your database are transformed, or mapped, to user profile attributes in your mParticle account. If you use an attribute name in your SQL query that doesn\u2019t exist in your mParticle account, mParticle creates an attribute with the same name and maps this data to the new attribute. mParticle automatically maps matching column names in your warehouse to reserved mParticle user attributes and device ids. For example, if your database contains a column namedcustomer_id, it is automatically mapped to thecustomer_iduser identifier in mParticle. For a complete list of reserved attribute names, seeReserved mParticle user or device identity column names. Below is an example of a simple table and SQL query to create a data model: Table name:mp.demo.userdata Example SQL query: This SQL query selects the first_name, last_name, and email columns from the table calledmp.demo.userdata. In the next step, we will set up the mappings for this user data.",
        "5 Create data mapping": "After creating a data model that specifies which columns in your warehouse you want to ingest, you must map each column to its respective field within mParticle with a data mapping. To create a data mapping, first use the dropdown menu titledType of data to syncto select eitherUser Attributes & Identities OnlyorEvents, depending on whether you want to ingest user data or event data. To continue with our example user data table and SQL query from above, we\u2019ll selectUser Attributes & Identities Only: When mapping attributes and identities from your warehouse to fields in mParticle, you must always create auser_identitiesoruser_attributesobject first. You can then create the individual mappings for your identities and attributes within these entities, as shown in the next two sections. To map your user identities: ClickAdd Mapping.Under Mapping Type, selectObject.Under Field in mParticle, selectuser_identites.ClickSave.Within your newuser_identitiesobject, click the+button.Under Mapping Type, selectColumn.UnderColumn in warehouseenter the name of the column containing the identities you want to ingest. In this example, we\u2019ll use a column calledemail.UnderField in mParticleuse the dropdown menu to select the correct user identity in mParticle you want to map your identities to. In this example, we\u2019ll selectEmail Address.  ClickSave. To map your user attributes: ClickAdd Mapping.  Under Mapping Type, selectObject.Under Field in mParticle, selectuser_attributes.  ClickSave.Within your newuser_attributesobject, click the+button.  Under Mapping Type, selectColumn.UnderColumn in warehouse, enter the name of the column you want to map to mParticle. You can enter a specific column name, or$unmappedto select all currently unmapped columns. Entering$unmappedselects all columns that are not already mapped to mParticle. In our example, because we\u2019ve already mapped theemailcolumn to theEmail Addressfield in mParticle, it will be excluded. UnderField in mParticleenter the name of the field in mParticle that you want to map your column to. If you entered$unmappedfor your warehouse column selection, you can enter an asterisk (*) forField in mParticleto use each unmapped column name in your warehouse as the respective field name in mParticle. This allows you to map all of your attributes at once so you don\u2019t have to create a separate mapping for each individual attribute.  ClickSave. If you are ingesting event data instead of user data (as shown above), selectEventsunderType of data to synconce you reach the Review Mapping step. ClickAdd Mapping.  Under Mapping Type, selectObject.Under Field in mParticle, selectevents.  ClickSave.Within the neweventsobject, click the+button to add a new mapping.  Select the type of mapping you want to use from theMapping Typedropdown:Column- maps a column from your database to a field in mParticleStatic- maps a static value that you define to a field in mParticleIgnore- prevents a field that you specify from being ingested into mParticle The next steps will vary depending on the data you are ingesting and the mapping type you select. Following are several examples of how to use each mapping type. UnderColumn in warehouse, select the name of the column in your database you are mapping fieldsfrom.UnderField in mParticle, select the field in mParticle you are mapping fieldsto.ClickSave.  UnderInput Valueselect eitherString,Number, orBooleanfor the data type of the static value you are mapping.In the entry box, enter the static value you are mapping.UnderField in mParticle, select the field you want to map your static value to.ClickSave.  UnderColumn in warehouse, select the name of the column that you want your pipeline to ignore when ingesting data.ClickSave.  To add additional mappings, clickAdd Mapping. You must create a mapping for every column you selected in your data model.  When you have finished creating your mappings, clickNext.",
        "6 Set sync settings": "The sync frequency settings determine when the initial sync with your database will occur, and how frequently any subsequent syncs will be executed. Select eitherProdorDevdepending on whether you want your warehouse data sent to the mParticle development or production environments. This setting determines how mParticle processes ingested data. Input protection levels determine how data ingested from your warehouse can contribute to new or existing user profiles in mParticle: Create & Update: the default setting for all inputs in mParticle. This setting allows ingested user data to initiate the creation of a new profile or to be added to an existing profile.Update Only: allows ingested data to be added to existing profiles, but not initiate the creation of new profiles.Read Only: prevents ingested data from updating or creating user profiles. To learn more about these settings and how they can be used in different scenarios, seeInput protections. There are two sync modes: incremental and full. Incremental: Use this sync mode to only ingest data that has changed or been added between sync runs as indicated by your warehouse column you use as an iterator. The first run for incremental sync modes is always be a full sync.Full: Use this sync mode to sync all data from your warehouse each time you execute a sync run. Use caution when selecting this sync mode, as it can incur to very high costs due to the volume of data ingested. The remaining setting options change depending on the mode you select from theSync modedropdown menu. Navigate between the two configuration options using the tabs below: The value you select for Sync Start Date determines how much old, historical data mParticle ingests from your warehouse in your initial sync. When determining how much historical data to ingest, mParticle uses to the column in your database you selected as the Timestamp field in the Create Data Model step. After your initial sync begins, mParticle begins ingesting any historical data. If mParticle hasn\u2019t finished ingesting historical data before the time a subsequent sync is due to start, the subsequent sync is still executed, and the historical data continues syncing in the background. Historical data syncing doesn\u2019t contribute to any rate limiting on subsequent syncs.  After entering your sync settings, clickNext.",
        "7 Review": "mParticle generates a preview for Data Warehouse syncs that have been configured, but not yet activated. Use this page and the sample enriched user profiles to confirm the following: Your data model correctly maps columns from your database to mParticle attributesYour sync is scheduled to occur at the correct intervalYour initial sync is scheduled to occur at the correct timeYour initial sync includes any desired historical data in your warehouse After reviewing your sync configuration, clickActivateto activate your sync.",
        "View and manage existing warehouse syncs": "Log into your mParticle account.Navigate toSetup > Inputsin the left hand nav bar and select the Feeds tab.Any configured warehouse syncs are listed on this page, grouped by warehouse provider. Expand a warehouse provider to view and manage a sync.To edit an existing sync, select it from the list under a warehouse provider. This loads the Warehouse Sync setup wizard, where you can modify your sync settings.Connect a sync to an output by clicking the green+icon under Connected Outputs.Configure rules for a sync by clicking the+ Setupbutton under Rules Applied.Delete a sync configuration by clicking the trash icon under Actions.To add a new sync for a warehouse provider, click the+icon next to the provider.",
        "Manage a sync": "To view details for an existing sync, select it from the list of syncs. A summary page is displayed, showing the current status (Active or Paused), sync frequency, and a list of recent or in-progress syncs. To pause a sync, clickPause Sync. Paused syncs will only resume running on their configured schedule after you clickResume. To run an on-demand sync, clickRun Syncunder Sync Frequency. Use the Data Model, Mapping, and Settings tabs to view and edit your sync configuration details. ClickingEditfrom any of these tabs opens the respective step of the setup wizard where you can make and save your changes.",
        "Managing Users": "Admin Users can manage the access of other users in their mParticle Account from theUser Managementtab of their User Settings page:  To add a new user, provide first and last name, an email address, and select the user\u2019s permissions. SeeRolesfor more on permisisons. ",
        "Custom Access Roles API": "mParticle provides theCustom Access Roles APIwhich mParticle account admins can use to create custom sets of permissions, or roles. You can then assign these custom roles to users of your account through the UI described above or with an API call. Learn more in theCustom Access Roles APIdeveloper documentation.",
        "Authenticating to mParticle with SSO and SAML": "mParticle uses Auth0 to authenticate logins to mParticle\u2019s web UI. This allows you to create a SAML/SSO connection to an identity provider of your choice, such as Okta. Using SAML/SSO, or federated identity management and single sign-on authentication, improves your account\u2019s overall security and the security of your customers\u2019 data. To enable a SAML/SSO connection, you must collaborate with mParticle\u2019s support team: Contact mParticle support or your account representitive, and request an ACS (Assertion Consumer Service URL) and an EntityID.Provide mParticle with an SSO URL, an optional logout URL, identity provider domain(s), and signing certificate.mParticle will configure an SSO tenant using the details you provided in step 1.Use the SSO tenant provided by mParticle to implement your existing authentication system and policies.",
        "mParticle to Analytics mapping": "",
        "Permissions and access": "Organization admins automatically have access to all portfolios created within an account. However, user access to portfolios is not automatically shared with users who have access to the underlying projects. These users must be invited by another user with access to the portfolio to join.",
        "Impact to Analytics Features": "All existing project-level features and analysis types are available within portfolios with the following exceptions: Events and Properties metadata editing:You cannot edit event or property metadata (descriptions, categories, etc.) within theAnalytics Data Managerin a portfolio. This applies to both event and user properties. You can still view all events, properties, mappings, and autocomplete values, however.User Segments:While you can create new user segments within a portfolio, you do not have access to user segments created in underlying projects.Global Project Filters:Global Filtersset within an underlying project will not be shared with the portfolio.Annotations:Annotationsset within an underlying project will not be editable.",
        "Start using Portfolio Analytics": "Reach out to your customer service representative to enable Portfolio Analytics for your account.",
        "Data in mParticle": "mParticle collects two important kinds of data:",
        "Event data": "Event data is about actions taken by a user in your app. Some events are collected automatically by mParticle\u2019s native SDKs. These include the Session Start events you saw in the Live stream when you firstset up your input. Other events need to be captured by writing code in your app. Of these, the most significant are: Screen/Page Views- keep track of where a user is in your appCustom Events- track any actions the user might take in your app, such as clicking a button or watching a video.Commerce Events- track purchases and other product-related activity.",
        "User data": "mParticle also captures data about your user, including their identities, information about the device they are using and any custom attributes you set. As with event data, some user data, such as information about the devices they use, is captured automatically by mParticle\u2019s native SDKs. Two important types of user data must be captured by writing code in your app: User identitiesare unique identifiers for your user, like an email address or customer ID. These are different from the device identities collected automatically by the SDKs, which don\u2019t identify an individual person but a particular cell phone, browser session, or some other device.User identities help mParticle keep track of unique users of your app and allow you to track a user\u2019s activity over time and across different devices. To learn a lot more about user and device identities, read ourIDSyncguide. For now, you just need to know that a user identity is a way of identifying aperson, independently of thedevicethey are currently using.User Attributesare key-value pairs that can store any custom data about your user. The value of a user attribute can be:A stringA numberA listA boolean value (trueorfalse)null- attributes with anullvalue function as \u2018tags\u2019, and can be used to sort your users into categories.",
        "Capture User and Event Data": "To start capturing data you will need to go back to your app code. Inthe previous stepyou should have installed and initialized the mParticle SDK in at least one of your app platforms. This means you\u2019re already set up to capture Session Start and Session End events, as well as basic data about the device. Grab a friendly developer again, if you need one, and try to add some additional user and event data to your implementation. Here are a few things you might try, with links to the appropriate developer docs: Add a Customer ID or Email Address for a user.Android/iOS/WebCreate a custom user attribute that tells you something about a user. For example:status: \"premium\".Android/iOS/WebCreate a page or screen view event that captures the name of a page or screen being viewed.Android/iOS/WebCreate a custom event to track a user action in your app. Include some custom attributes. For example, the mPTravel app sends a custom event when a user views one of its content videos. The event is called \u201cPlay Video\u201d and it has two custom attributes: thecategoryof content, and the traveldestinationthe video promotes. Later on, you\u2019ll see how events like these can be used to target custom messaging.Android/iOS/WebCreate a purchase event - track a purchase using mParticle\u2019s commerce APIs.Android/iOS/Web",
        "Verify: Look for incoming data in the Live Stream": "Once you\u2019ve added code to your app to start collecting some basic data, start up a development build of your app again and trigger some events. Have another look at the Live Stream. You should start to see new event batches, with the individual events you have added to your app. ",
        "Segmentation": "You may create a user segment inSegmentationfrom any result that contains at least one user. To create a user segment, click on any data point, series, or table cell within your query results, then select \u201cCreate User Segment\u201d.  A user segment createdfrom a pointwill only include users from that particular data point.A user segment createdfrom a serieswill include all users from all of the data points in the selected analysis results.",
        "Funnel": "You may create a user segment inFunnelfrom any result that contains at least one user. To create a user segment for a funnel up to and including a particular step, click on the numerical user count located within the table along the bottom of the chart area, labeled \u201cCount,\u201d then select \u201cCreate User Segment\u201d.  You may also select the number displayed within the circle that represents users who complete the step within the chart area, then click on the relevant path, and then select \u201cCreate User Segment.\u201d  There are two options for creating a user segment when thefunnel query contains a group by: A user segment created from a breakout selection will contain users in that particular breakout.A user segment created from the entire step will contain all users in this step, regardless of breakout. In amulti-path funnel, you must select a single pathway to create a user segment.",
        "Cohort": "You may create a user segment from any result that contains at least one user. A user segment created from a cohort generation will contain users in that particular cohort. This includes all of the users in the table row.  A user segment created from a cohort interval will contain users in the cohort who also completed the target behavior in the interval. This includes only the users in the table cell. ",
        "\u201cCreate a user segment\u201d modal": "Use the Create User Segment Modal to review the criteria that comprise your user segment, create a name, description, and category for the user segment, and select whether the segment never changes (One-time) or is updated daily (Daily).  This User Segment is composed of users who: \u2026 : This section describes the criteria for selecting users to be included in the segment.Segment Name: Give your segment a name. It is recommended that the name be easily understood by any user within your organization.Description: Provide additional context and notes here, for example, to describe how this user segment should be used or to provide business-specific context. The description will appear within the data dictionary, available within each tool.Category: Define the category that your segment belongs to. For example, if the segment is related to marketing, type \u201cMarketing.\u201dOne-time vs. Daily: A user segment with an update cadence ofOne-timeis never updated. It contains only the users that meet a set of fixed criteria. If the update cadence isDaily, the user segment is updated daily to include all users that meet a set of relative criteria. Users may enter or exit a dynamic user segment as it updates over time.",
        "Group by": " Group byclauses, also referred to as \u201cbreakouts,\u201d allow you to segment data based on event properties, user properties and now user segments to better understand its context. In the case of user segments, only two values will be expressed when a \u201cBy\u201d clause is selected: [Query row name]: in [segment name]: Only includes users who are also in the selected segment.[Query row name]: not in [segment name]: Only includes users who are not in the selected segment.",
        "Manage user segments": "To manage user segments, click Settings in the navigation bar, then select \u201cUser Segments.\u201d Here, you will see all of the user segments created and saved by members of your project.  Segment Name: A descriptive name. Analytics recommends that the name be easily understood by any user within your organization.Description: Additional context and notes. For example, describing how this user segment should be used or providing business-specific context. The description appears in the data dictionary, available within each tool.Category: The category that your segment belongs to.Segment ID: A unique ID for use with theUser Segments Export API.Type: One-time or daily. A one-time user segment is never updated. It contains only the users that meet a set of fixed criteria. A daily user segment is updated daily to include all users that meet a set of relative criteria. Users may enter or exit a daily user segment as it updates over time.Creator: The user who created the user segment.Created (Time Zone): The time zone in which the user segment was created. Hover over a user segment to view two additional options: Open Query: Opens the original query that defines the cohort within the relevant Analytics tool.View Users: Opens a list of users included in the user segment within the Users tool.",
        "Manage Events": "To access events, open the Events and Properties data manager.",
        "Hide/Unhide Events": "Select the Archive icon to hide events. Once archived, events will not appear in any of the Analytics tools. They will also not appear in the user\u2019s event history. To unarchive an event, go to the filter in the upper right-hand corner of the data manager and choose the Archived Events icon. This will display events which have been archived. Click the three vertical dots on the right-hand side of the event\u2019s column, then clickActivateto unarchive the event.",
        "Rename, Define, and Categorize Events": "Under theDisplay Namecolumn, you may rename an event or event property. Events or event properties without a label will default to their original name.Under theDescriptioncolumn, you may add a description to an event. Definitions are useful for new users, and/or when your project contains similarly named events.Under theCategorycolumn, you may create categories and assign events and event properties to them. Creating categories helps keep your data panel organized.",
        "Joining Events": "Events & Properties settings allow you to join multiple events into one event, a \u201cjoined event\u201d. In a query, a joined event composed of Event A and Event B will represent auser who performed Event A or Event B. To create a joined event: Select the checkbox next to Event Type on the left side of the screen for each event you would like to join together. (If you don\u2019t see these boxes, you may need to request permissions from your project administrator).When the boxes for two or more events are selected, the Join Selected option will be available in the pop-up menu.Name the joined event and choose Save.",
        "Query Summary": "You may also view a written summary of your query builder for any dashboard analysis. To do so, select the three dots in the top right of any dashboard analysis, and choose Show Summary: When you select Show Summary, the analysis \u2018card\u2019 will flip to display the Query Summary:",
        "Calculations": "The following table lists all supported calculations. *Setting the date range toWithin the Lastcauses all calculations to be asynchronous. Be aware of the following before creating your calculation attributes: Calculated attributes require server-side forwarding. Therefore, this feature isn\u2019t available for kit-only integrations that solely support client-side forwarding.All timestamp values are in ISO 8601 format in the UTC timezone.Several calculations produce results with types that depend on the type of the event attribute selected; for example,FirstValue` returns a string if the selected event attribute is a string. All attribute values in our platform are stored as strings, including calculated attributes.Calculation speeds listed are after the values have been initialized.For unique lists, up to 100 values are calculated. The values are selected based roughly on the order in which mParticle received the data, though the ordering is not guaranteed. When viewing and using unique lists values (in User activity view, Profile API etc.), values are returned in alphabetical order.When using aliasing to transition from an anonymous to a known user profile, mParticle doesn\u2019t copy the calculated attribute or trigger a recalculation on the resulting profile.For aggregation CAs:More than one attribute may occur the same number of times, creating a tie. To break the tie, mParticle sorts the attribute name alphabetically and chooses the first attribute.After the first 100 values are collected for aMost FrequentorUnique ListCA, no more values are collected. ForMost Frequent, the frequency of the first 100 are continuously evaluated, but no new values are added. ForUnique List, mParticle keeps only the first 100 seen values. To trigger a re-collection of values for either calculation type, edit the CA definition or create a new one.",
        "Type conversions": "Some calculated attributes, likesum, require numeric event attributes to function. If you select an attribute that is not detected as the correct type, the platform will warn you about using those fields in the calculated attribute definition. Youcan still usethe calculated attribute despite the warning, and it will attempt to convert the string values into numerics. For example, if you pass the attributeamountin as a numeric string like\"34.32\", asumcalculation will still work correctly: the string\"34.32\"will be converted to the decimal value34.32. ",
        "Conditions": "When you define a calculated attribute, you can add conditional logic. For example, if you wanted to count the total number of times a promotion was clicked, but only for certain currencies, you could add the condition \u201cwhere the currency code is only AUD or EUR.\u201d The following conditions are available for all four categories of calculated attributes (Count, Aggregation, Occurrence, and List): ContainsDoes not ContainExact MatchDoes not MatchPatternExistsNot ExistsIs EmptyIs In List",
        "Commerce quantity fields": "The following behaviors affect commerce event attributes (commerce_events) in some calculated attributes:",
        "Product events": "The product action (product_action) and product impression (product_impression) attributes can be used in the quantity field for calculations. Note the following behaviors: Average uses the quantity field in a product array when averaging the value of the price field.First value picks the first product in an array if the product attribute is selected.Last value picks the last product in an array if the product attribute is selected.First timestamp is the batch timestamp.Last timestamp is the batch timestamp.Unique list can pick any field in an events or products array.Most frequent uses quantity for calculating the most frequent for non-numeric fields such as brand, category, or coupon code.",
        "Promotion events": "The promotion action (promotion_action) attribute can be used in the quantity field for calculations. Note the following behaviors: First value picks the first promotion in a promotions array.Last value picks the last promotion in a promotions array.First timestamp is the batch timestamp.Last timestamp is the batch timestamp.Unique list can pick any field in an events or promotions array.Most frequent can pick any field in an events or promotions array.",
        "Drive User Engagement Through Intelligent Segmentation": "Let\u2019s say you want to target users who installed your app to increase their engagement. There are two approaches to accomplishing this using Audiences: Rules-based:Taking this strategy, you would use audience criteria to build a customer segment that aligns with your target characteristics, like users who installed your app in the last 72 hours with fewer than three sessions.AI-powered:Using a predictive approach, you could simply define your business goal, then let Predictive Attributes to perform a comprehensive analysis of user data to predict which new users are most likely to take the engagement action you have defined. Once the Predictive Attribute has been generated, you could use it as an audience criteria to select users who are statistically most likely to engage with your app. No matter which approach you take, you can easily forward your audience to push notification and email partners.  Once these audience connections are established, mParticle instantiates a corresponding audiences both partners.  No coding is necessary.",
        "Drive app downloads": "Let\u2019s say you want to find more users like your currently highly engaged users and run an app download campaign in Facebook against that target audience. You start by defining your highly engaged users, using whatever criteria is important to you: lifetime value metrics, session activity, event activity, or any other data points you capture. Once your audience is defined, configure the Facebook integration and corresponding custom audiences in your Facebook account.  From there you can leverage the custom audiences like any other custom audience in Facebook. Because we want to target users that look like our highly engaged users, we will create a Facebook lookalike audience from our highly engaged user audience and run a Facebook app install campaign that targets that lookalike audience.",
        "Create an \u201cAnd\u201d Clause": "After using a Filter Where clause to filter an event, add another filter below the one you just created, or drag another event property or user property onto the Filter Where parameter to build an \u201cAnd\u201d or an \u201cOr\u201d clause. For example, you may want to look at user who did View Content where the Platform is equal to Desktop and the user\u2019s Marketing Channel is Affiliate. This would help understand how many users acquired through an affiliate are viewing content on their desktops.",
        "Create an \u201cOr\u201d Clause": "Alternately, use \u201cOr\u201d to view users who did View Content where the platform is equal to Desktop or the platform is equal to Android to see the total content views made by anyone who used either a Desktop or an Android device. This can help when looking for very granular behavior patterns.",
        "Changing an \u201cAnd/Or\u201d Clause to a \u201cBy\u201d Clause": "Within the \u201cAnd/Or\u201d dropdown menu, there also exists an option to instead create a\u201cBy\u201d Clause. After selecting the \u201cBy\u201d option in the dropdown menu, the clause will be re-applied as a \u201cBy\u201d clause breakout.",
        "In-Line \u201cAnd/Or\u201d Clauses": "You may also add And/Or clauses in-line within the filter dropdown using the symbols \u201d&\u201d (shift+7) and \u201d|\u201d (shift+) to represent \u201cAnd\u201d and \u201cOr\u201d respectively. Rather than create a new line in the query for each additional value in the row, click within the box and use an ampersand (and) or a pipe character (or) to quickly add And/Or clauses in the same line. For example, you may want to see how many content views came from the users on the platforms iPhone and Android. In the Filter Where clause: Select iPhoneClick inside of the drop down boxAdd an ampersand after iPhoneType Android",
        "Establish your Identity Strategy": "This guide hasalready coveredcollecting identities, such as email addresses, for your users. mParticle\u2019s IDSync feature gives you a lot of control over how you identify and track your users over time, and selecting an Identity Strategy is one of the most important decisions you need to make when implementing mParticle. Read our fullIDSync guidefor more.",
        "Add more sources": "For most mParticle clients, the primary sources of data are native and web apps, instrumented with the mParticle SDK, but you can also use mParticle to leverage other sources of data to build a more complete picture of your users: TheEvents APIcan be used to send supplementary server-side data.Our mainAppleandAndroidSDKs can also be instrumented in AppleTV and FireTV apps, and we publish independent SDKs forRokuandXbox.If you use a cross-platform development framework, you can use our libraries forReact Native,Xamarin,Unity, andCordovato interface with our native SDKs.Use Feeds to bring in data from other services.",
        "Explore advanced Audience features": "If you want to compare different messaging platforms or strategies, you can use mParticle to conduct anA/B Testby splitting an audience into two or more variations and connecting each to different outputs.The more specific your audiences, the more you are likely to need to create. If you have a large number of audiences to forward, useBulk Audience Connectionsworkflow to speed things up.",
        "Transform your Data": "One of the core benefits of mParticle is the ability to capture data once and forward it to multiple outputs. However, you probably don\u2019t want to sendallyour data toeveryoutput. mParticle provides you with a full set of tools to filter and transform your data. Use these tools to control the flow of Personally Identifiable Information (PII), to customize the data you send to each output and to control your costs. TheData Filterallows you to individually filter each data point for each output.User Splits allow you to test competing services by dividing your users into groups and connecting each group to different outputs.Forwarding Rulesallow you to block data to an output according to simple predefined rules.User Samplingallows you to send only a subset of your data to a given output. This is usually done to control costs for services that charge according to data volume or unique user counts.For advanced transformations, theRulesfeature allows you to host a custom function on AWS Lambda which can change almost any aspect of your data.",
        "Manage your GDPR Obligations": "If you have users in the European Union, you may have obligations as a Data Controller under the General Data Processing Regulation. mParticle provides tools, available as premium features, to help you manage two aspects of GDPR compliance: User ConsentData Subject Requests",
        "Know your limits": "Part of the purpose of mParticle is to allow you to maximize leverage of your data without compromising performance. In order to protect the performance of both your app and the mParticle dashboard, we impose certain limits on the data you can send. If you\u2019re a dashboard user, you can read a brief summary of the default limits,here. If you need the full picture, you can read ourdetailed guide.",
        "Manage Event Properties": "To access event properties, select any event. You may also select the Details tab while hovering the mouse over an event to access the event properties for each event, and vice versa.",
        "Archive/Unarchive Event Properties": "Select the Archive icon to archive event properties. Once hidden, event properties will not appear in any of the Analytics tools. To unarchive an event property, choose the Unarchive icon on the hover menu of an event property. This will unarchive it, making it re-appear in Analytics tools.",
        "Rename, Define, and Categorize Event Properties": "Under theDisplay Namecolumn, you may rename an event or event property. Events or event properties without a label will default to their original name.Under theDescriptioncolumn, you may add a description to an event. Definitions are useful for new users, and/or when your project contains similarly named events.Under theCategorycolumn, you may create categories and assign events and event properties to them. Creating categories helps keep your Data Panel organized.",
        "Data Types": "Within the properties menu, you may identify a property as a certain type to surface additional capabilities (like summing numbers). By default, properties will be set to Auto, which will automatically assign a data type. The following data types are available: Stringproperties enable the use of \u201ccontains\u201d based on query filters.Numericproperties enable the use of \u201cless than\u201d and \u201cgreater than\u201d based on query filters.Location (US States)properties enable the use of the Map Chart in Segmentation analysis.Geolocation (IP Address)properties are automatically collected to show your customers\u2019 country, subregion and city for integrations after May 15, 2019 using the Analytics SDK. For existing Analytics SDK integrations, go into project settings and turn on IP collection to use these properties. Note: IP Addresses cannot be retroactively collected.Date/Timeproperties enable the use of \u201cbefore\u201d, \u201cafter\u201d, and \u201cbetween\u201d based query filters.  This option will also display the underlying data as a human-friendly date, according to your project settings.",
        "Avoiding Duplicate Display Names": "When event properties are forwarded with names that match existing Display Names, the Display Names can become duplicated. This may lead to confusion in the Analytics and Segmentation interfaces, as multiple event properties with the same Display Name will appear, making it unclear which one to select. To ensure clarity and prevent ambiguity: Assign unique Display Names to event properties when configuring them.If duplication occurs, you can resolve it by editing the Display Names in the Analytics \u2192 Data \u2192 Events interface. While it is technically possible to assign the same Display Name to multiple properties, it is advisable to use unique names to differentiate between them.",
        "Dashboard settings": "To access your Dashboard Settings: Click the Saved section in the left-hand navigation menu and select a dashboard.SelectManagein the top right corner of your dashboard.SelectDashboard Settingsto open the dialog. From Dashboard Settings, you can perform the following tasks:",
        "Modify dashboard title": "In theDashboard Settingsdialog, type your desired dashboard\nname in the Title field. You can also rename your dashboard by clicking on the\ntitle in the top left corner of your dashboard.",
        "Modify dashboard description": "In theDashboard Settingsdialog, type a description in the\nDescription field. Your new description will be reflected in the top left corner of your dashboard.",
        "Change widget layout flow": "In theDashboard Settingsdialog, select a Widget Layout Flow. Free Flowlets you place your dashboard analyses wherever\nyou choose.Auto Flowautomatically compacts analyses to the top\nof your dashboard. Check out the Dashboard Visualization article for more information on organizing\nyour dashboard.",
        "Change dashboard layout mode": "In theDashboard Settingsdialog, choose how to display your\ndashboard: screen or print mode. Screen Modeis optimized for web browsers. This is the default\nlayout mode.Print Modeis optimized for printing your dashboard and\nfor scheduledreports. In print mode, you may choose to display the following: Show simulated paperShow titleShow descriptionShow date uploaded Check out the Dashboard Visualization article for more information on screen\nand print modes.",
        "Change widget color matching": "The Widget Color Matching feature within dashboard settings allows Analytics users to easily digest the widgets of their dashboards. Widgets that share the same series match visualization colors for easy comparison across the dashboard. Note that undefined values will always be black. To enable widget color matching, go toManage >Dashboard Settings, and then selectOn.  Once selected, widgets in the same series will match:  Note: Colors persist even when opening a widget from a dashboard. Although different widgets may query different events, series can still color match if they share the same breakout value. Notice that the Funnel widget and the Segmentation widget below share the same breakout values. Although both widgets measure different events, their overlapping breakout values share the same color.  Identical series are defined as the following: Segmentation: Identical series are defined by the Total Count Of/Users Who Performed selection, event name, and breakout valuesFrequency: Identical series are defined by frequency groupingsFunnel: Each step within a funnel is defaulted to a green color unless there are breakout valuesJourneys: Defined by event name",
        "Switch to full screen": "In theDashboard Settingsdialog, toggle full-screen mode settings. You may toggle the following: Show titleShow description You may also copy to your clipboard a direct link to a full-screen version of\nyour dashboard. For more information on full-screen mode, visit theOrganize Dashboardsarticle.",
        "Enable public access": "A dashboard may also be made publicly available. This means that anyone with\nthe Public Access Link can view your dashboard. To make a dashboard publicly available, from the Dialog Settings dialog, check the \u201cPublic Access\u201d checkbox. Then, simply click to copy your shareable link to your clipboard. You may\nalso generate a new public link by clickingReset Access.",
        "Filter public dashboards via API": "Analytics supports the ability to filter public dashboards programmatically via an API. You may create variants of your public dashboards by bulk adjusting each analysis\u2019 Date Ranges, Time Zones, Query Intervals, and Property Filters. The Dashboard Settings dialog provides the dashboard variant API endpoint: However, please see our Dashboard Filter API documentation to filter public dashboards via the API. This feature requires advanced instrumentation from a technical resource. Analytics uses an algorithm to balance data \u2018freshness\u2019 and efficient resource utilization when refreshing dashboards. When you apply a new filter to a dashboard or widget, please allow up to 15 minutes for changes to be reflected in the dashboard\u2019s variants. Subsequently, each request to render a dashboard or widget will display the most recently cached result before initiating an update.",
        "Dashboard privileges": "If you are on the Enterprise plan and are the Dashboard Owner, you can control\nwhether or not team members can edit a dashboard. All team members with the correct\nuser permissions can edit a dashboard by default. Dashboard privileges do not\noverride user permissions. To change the dashboard editing privileges for team members: Click theSavedsection in the left-hand navigation menu and select a dashboard.SelectManagein the top right corner of your dashboard.SelectDashboard Privilegesto open the dialog:SelectSelected Teammatesto show a list of all teammates\nwho have sufficient privileges:Click the checkbox next to each name you wish to add or remove, and then selectSave. Teammates are notified when their privileges are updated. Note the following: If you wish to change the editing privileges on a dashboard you don\u2019t own,\nyou can change the dashboard owner by makinga support request.Editing privileges don\u2019t override any other permissions or privileges in\nAnalytics. For example, if a team member has Read Only permissions, they won\u2019t appear in the list of teammates\nto select or exclude.You can duplicate a dashboard even if you can\u2019t edit it. You\u2019ll be the duplicated\ndashboard owner and can make any changes, including the dashboard privileges\nsetting.",
        "Duplicate dashboard": "To duplicate a dashboard: Click the Saved section in the left-hand navigation menu and select a dashboard.SelectManagein the top right corner of your dashboard.SelectDuplicate Dashboard. Select a name for your new dashboard, and select a folder to save your dashboard\nin.ClickDuplicateto finalize your dashboard duplication orCancelif you\u2019ve changed your mind. Note the following: You may also duplicate your dashboard by hovering in the Views dropdown.You can duplicate a dashboard even if you don\u2019t haveediting privileges.",
        "Delete dashboard": "To delete a dashboard: Click the Saved section in the left-hand navigation menu and select a dashboard.SelectManagein the top right corner of your dashboard.SelectDelete Dashboard. You will be asked to confirm if\nyou want to delete your dashboard because any analyses saved to a deleted dashboard\nwill not be recoverable. You may also delete your dashboard by hovering in the Views dropdown. ",
        "Rule execution": "Each of your Inputs, such as for your web, mobile, or server-to-server data, has an individually configured data pipeline, and each Input\u2019s pipeline contains the same key stages. Rules are therefore applied for a specific Input\u2019s pipeline, and it\u2019s up to you choose where in that Input\u2019s pipeline each Rule is executed. A single Input pipeline may contain multiple Rules each stage.",
        "mParticle pipeline stages": "Stage 1 - Input Data is received by mParticle for a specific Input (such as Web, iOS, or a custom server feed). Stage 2 - Storage and Processing The Input\u2019s data is stored and processed by mParticle, including: mParticle\u2019s profile system, which stores user data and enriches the Input\u2019s data based on the existing profile of that user.mParticle Data Master tool Stage 3 - Output The Input\u2019s data is sent individually to the mParticle Audience system and 300+ partner integrations. In this stage the pipeline actually branches out with a single Input potentially being connected to many Outputs.",
        "Rule application": "Rules are applied to a specific Input\u2019s pipeline. There are two places in the pipeline where rules can be applied: In between Stage 1 and Stage 2 Rules executed between Stage 1 and Stage 2 affect the data sent to both Stage 2 and then Stage 3, including the mParticle profile store, audience store, and all outputs. These are labeled \u201cAll Output\u201d Rules in your mParticle dashboard. In between Stage 2 and Stage 3 You can also apply a rule right before it\u2019s sent to aspecificOutput. This lets you mutate data to handle specific requirements or mappings that need to occur for a given partner integration.",
        "Rule requirements": "All rules accept an mParticle Events API batch object and can return a modified or null batch object.There are some differences in error handling and available fields depending on pipeline location. SeeRules Developer Guidefor details.A 200ms timeout applies to all rules. You can choose if a batch should be dropped or continue unprocessed by the rules in the case of a timeout.Rules are executed on the server and only act on data forwarded downstream server-to-server. A warning is shown in the dashboard if you set up one of the following rules:A rule for integrations that forwards data client-side via a kit.A rule for hybrid integrations that support forwarding via client-side and server-to-server.If you are using a rule to modify user identities or user attributes, you must include a \u201cUser Identity Change Event\u201d (user_identity_change) or a \u201cUser Attribute Change Event\u201d (user_attribute_change). SeeRules Developer Guidefor an example ofuser_attribute_changein a rule.",
        "Create a function in AWS": "mParticle rules are hosted in your AWS account as Lambda functions. To do this, you need to be able to provide an Amazon Resource Number (ARN) for your rule. See theAWS Lambda documentationfor help creating a function.  The Lambda functions used for rules must be hosted in the same AWS region as yourmParticle account. The name of the function must begin with \u201cmpr\u201dYour development rule must have an alias of \u201c$LATEST\u201dYour production rule must have an alias of \u201cPROD\u201d Your ARNs should look something like this: arn:aws:lambda:us-east-1:999999999999:function:mprmylambdafunction:PROD arn:aws:lambda:us-east-1:999999999999:function:mprmylambdafunction:$LATEST",
        "IAM user": "To connect to your AWS Lambda function, you must provide the AWS Access Key ID and Secret Access Key for anIAM user. In the IAM dashboard, add the following permissions policy for the user:",
        "IAM role": "You will also need to create a role in IAM. Assign this role the same policy document created above.  Assign this role to each Lambda function you plan to deploy as an mParticle rule. ",
        "Creating a rule in the dashboard": "Create a rule by navigating toData Master > RulesClickNew Rule.  Enter your Development and Production ARNs and clickTest. ",
        "Error handling": "When you first test a rule, you must select aFailure Action. This determines what happens if your rule throws an unhandled exception. There is no default action, you must select one of the following: If you chooseDiscard, an unhandled exception causes your rule returnnull, effectively dropping the batch.If you chooseProceed, an unhandled exception causes your rule to return the unaltered batch object, proceeding as if the rule had not been applied. Regardless of which option you choose, it\u2019s best practice to handle all exceptions in your code, rather than falling back on the above defaults. This is especially true if your rule deals with events, where an unhandled exception from just one event could lead to all events in the batch being dropped.",
        "Javascript syntax": "Your code must be a validLambda function. batchis the complete incoming batch object.contextis a required argument for Lambda functions, but is effectivelynullfor mParticle rules.",
        "Testing rules": "The first time you test a rule, you are asked to provide a name, description and failure action. After naming a rule, you can test it by using one of the sample templates provided in the Test rule dialog. You can also copy and paste batch JSON from your Live Stream. If you do this, be sure to pick a full batch to copy, not a single event.  ClickTestto run. Optionally, check a box to save your JSON template in local storage for future testing. You must enter validbatchJSON in the code editor. If there are any syntactical errors in your code, warning or error icons will display next to the line number with details of the problem so you can correct. After clickingTest, you can examine the JSON output from your function to see that the input has been modified as expected. After a successful test you can clickSaveto save the Rule. Due to recent updates in AWS Lambda, it may be necessary to wait one minute after a successful test in order to save the Rule. If your test fails, try examining thelogsfor any console output.",
        "Versioning": "When you first create a rule, by default it will only be applied toDEVdata. As well as testing a rule with sample JSON you should test the rule in your dev environment to make sure data reaching your output services is as expected. When you are ready to apply a rule to your production data, clickPromote to Prodon the rule page. This will create a \u201cv1\u201d production rule. Note that before a rule can be promoted to Prod, you must remove allconsole.log()statements. If you need to make changes, choose$LATESTfrom theVersiondropdown. All other versions are read only. Test your changes with your dev environment and, when you are ready, clickPromote to Prodto create \u201cv2\u201d of your production rule. Note that you can have a maximum of 50 versions per rule. If you have too many versions, select a version and click the trash can icon to the right of the version number to delete it.",
        "Logs": "To help you with troubleshooting rules, mParticle maintains logs for each rule where you can view all console output. From an individual rule page, select theLogstab. You can filter messages by date range or search for keywords. ",
        "Deleting rules": "From the rules listing, select theDeleteaction to delete the rule. If the rule is applied to any connections, it will be removed from those connections.",
        "Accounts, organizations, and workspaces": "mParticle creates a unique organization for you. It\u2019s the container for all data and metadata related to your mParticle. Within an organization, mParticle will create one or more accounts for you, and within each account, you can create one or more workspaces. Your choices for account and workspace setup are important because these choices affect identity and feature provisioning.  These three nested containers provide scoping and functionality for multi-brand and multi-geo use cases, as well as edge use cases. The scope and advantages of each are explained in the following sections. OrganizationMost mParticle customers have one organization which contains one or more accounts. However, some large companies have multiple organizations. No information is shared across organization boundaries. mParticle creates the organization(s) for you.A few features apply at the organization level, includingprofile strategies.You can think of the organization as representing your company.AccountEach organization has one or more accounts. Accounts often represent different functional groups or goals within an organization, for example regional divisions, or Sales, Marketing, and Customer Support. mParticle creates the account(s) for you.Some information is shared across accounts either by default or by enablement:Audiences can be shared across account boundaries withcross-account sharingenabled.mParticle users (people authorized to access your mParticle organization) are shared across accounts and workspaces.User data is sharedacross workspaces by default, but you can request it be shared across accounts.WorkspaceEach account contains one or more workspaces. A workspace is the basic container for data in an mParticle account.  mParticle creates your first workspace, but you can add more at any time.For most use cases, each workspace is its own domain, separate from other workspaces. Some information is shared across workspace boundaries:Audiencesare shared, allowing you to build an audience using data from more than one workspace.mParticle users (people authorized to access your mParticle organization) are shared across accounts and workspaces.Some mParticle accounts have over a dozen workspaces, while others have only one. How you organize data from your app ecosystem is entirely up to you.",
        "Using organizations, accounts, and workspaces": "Use organizations, accounts, and workspaces to manage multiple brands, regions, and to manage custom identity configurations and unique input/output requirements.  You can also use mParticle features likecross-account audience sharingormultiple workspace real-time audiences. To see which accounts and organizations are available: To display your current organization and account, log into mParticle and in the left navigation bar toward the bottom, click the buildings icon. If more than one organization or account has been created, you\u2019ll be able to search for it.To see your current workspace name, log into mParticle and in the left navigation bar, the square in the upper left corner displays your current workspace name. Click anywhere in the square to search for other workspaces or see the settings for this workspace. The Best Bags company sells handbags under several different names, and in several regions of the globe. They can use organizations and workspaces to provide differentiation when needed: Best Bags created workspaces that correspond to three regions: North America, APAC, and Europe. Each brand defines their own audiences and users within a separate organization, since most customers purchase from only one region.Best Bags created accounts that correspond to their different brands: BestieBags, BlingBags, and CarriageBest. In this way, they can create unique inputs and outputs for the same data sources and forwarding destinations, address different governance and compliance requirements, while still being able to share audience membership, since their customers may buy different brands at different times. You can also useidentity scopeto manage how user data is shared between workspaces and accounts. And if you need to share audiences across accounts, you can request that mParticle enablecross-account audience sharingfor your organization.",
        "Managing workspaces": "Click on the name of your current workspace in the top-left corner of the dashboard to open the workspaces menu. From here you can switch into any of your current workspaces, or clickSettingsto open the Workspace Settings page.  From the Workspace Settings page, you can: View daily, monthly and quarterly statistics across all workspaces in this account, including data from both development and production environments.Browse a list of all workspaces in your account.Download the Event Volume Report, which lists all events ingested in the selected timeframe. The report provides visibility into the calculated attributes and audiences created using those events. This is the same report you can download fromData Master > Catalog > Download Report.Create a new workspace - all you need to do is provide a name for the new workspace.Delete a workspace - this will also delete all the workspace data and connection settings. This action cannot be undone, so proceed with caution.Edit a workspace - view theApple App Transparency Tracking (ATT) Defaults, enable GDPR and CCPAregulations, and retrieve the workspace Key/Secret to use with theOpenDSR API. Note that you can\u2019t delete a workspace that is part of aMulti Workspace Audience. First delete or modify the multiworkspace audience, then you can delete the workspace.",
        "Working with web data": "mParticle handles Web data \u2014 collected from a browser client \u2014 a little differently from data collected from native apps. In most cases, data collected by the mParticle SDK is sent to mParticle, and then forwarded on to integration partners server-to-server. There are exceptions to this rule: in cases where a server-to-server integration cannot support all the required functionality of an integration partner, an Embedded Kit may be used. Embedded Kits are extra components added to the mParticle SDK that communicate directly with an integration partner from the app client. While direct communication between the client and partner is the exception for native apps, it is common for web data. A key reason for this is that most of mParticle\u2019s integration partners are not set up to receive web data server-to-server, as they rely on cookie data only accessible to the cookie owner. To support these integrations, the mParticle Web SDK uses the following workflow: On initialization, the SDK checks to see which Web integrations are enabled for your workspace.For each enabled integration, mParticle SDK will fetch the Partner\u2019s javascript and the mParticle wrapper specific to that Partner. For example, if you have enabled the Google Analytics integration, the mParticle SDK will fetch Google\u2019sanalytics.jssnippet and mParticle\u2019sGoogleAnalyticsEventForwarder.jssnippet. We fetch only the integrations that you have enabled in order to keep the page size to a minimum.Any supported events are mapped directly onto the equivalent partner method. For example, when the mParticle SDK logs a Page View it automatically calls Google Analytics\u2019pageviewmethod. To make it easier to work with web integrations, we provide the source code in a public repository, so you can work with the Integration Partner\u2019s documentation and see exactly how we map mParticle methods onto the Partner code. See themparticle-integrations organizationfor a complete list of client-side web integrations.",
        "Platform limits": "mParticle imposes limits on the number and length of attributes that can be associated with events and users. A quick summary of some of the most important limits is below. For more information, see our fullDefault Service Limitsguide. Events An event can have up to 100 attribute key/value pairs.Event names and attribute keys are limited to 256 characters.Event attribute values are limited to 4096 characters. Users A user can have up to 100 attribute key/value pairs.User attribute names, including user identities like email or Customer ID, are limited to 256 characters.A user attribute value can be a list. These lists are limited to 1000 entries.An entry in a user attribute list is limited to 512 characters.A user attribute value that is not part of a list is limited to 4096 characters. Note that Output Services often have their own limits, which can differ from mParticle\u2019s. When planning your implementation, check the documentation for your Output Services in theIntegration Centerto make sure you are complying with their limits.",
        "Tracking protection": "Browsers add third-party tracking protection for end users. The protections affect third-party trackers and their cookies and work in different ways. For example, Firefox Enhanced Tracking Protection (ETP) relies on a list of known trackers to decide what to block. Safari, Chrome for iOS and other browsers with the Apple WebKit engine use Intelligent Tracking Protection (ITP). ITP prevents the browser from loading cookies from a third-party domain. mParticle aligns with this privacy stance. Firefox Enhanced Tracking Protection (ETP)Apple WebKit engine and ITP",
        "User attributes and event attributes": "mParticle ingests data points that are composed of event attributes and user attributes.",
        "Timestamps and ingested data": "When mParticle ingests data, there are two timestamps associated with events: Each event batch has a timestamp.Each event in the batch may have a timestamp. If batches or events have a timestamp that is more than 15 minutes in the future, relative to the server processing the data, that timestamp will be reset to the current server UTC time. Attribute timestamps remain unchanged. If you load data using CSV Import, the batch timestamp is reset to the current server UTC time.",
        "Forward-looking statements": "mParticle strives to be as transparent as possible. Part of this transparency is to share information about products, features, or functionality that we expect to deliver in the future. Forward-looking statements are as accurate as possible given the knowledge at the time of publication. However, no purchasing decisions should be made on the basis of any forward-looking statement, and mParticle may withdraw or change the products, features, or functionality mentioned in such statements.",
        "Why use Predictive Attributes?": "Traditionally, marketers and product managers look to user events like website visits, page views, and journey completions to build out customer segments. While manual segmentation can certainly be effective, it is subject to the limitations and inefficiencies of human decision making. Predictive Attributes let you avoid the drawbacks of manual audience building. Once you define your desired conversion goal, Cortex\u2019s Machine Learning models will analyze thousands of behavioral signals to determine which users are statistically most likely to convert, letting you accelerate decision making and execute campaigns with confidence. Additionally, Predictive Attributes update automatically based on real-time customer behavior. Since your highest value customers are always changing, Cortex continuously recalculates Predictive Attributes to ensure that your campaigns stay focused on the users who most closely align with your campaign goals.",
        "How do Predictive Attributes work?": "Predictive Attributes are stored on the User Profile, and you can use them in the same ways you would any other behavioral or demographic User Attributes (like audience building or querying via the Profile API). The difference is that unlike regular User Attributes, which are captured as soon as your users take specific actions, Predictive Attributes need to be defined and generated withing a workflow in mParticle. Once they exist on the User Profile, however, there is functionally no difference between Predictive and regular User Attributes in how you use them to achieve your business goals.",
        "Types of Predictive Attributes": "Predictive Attributes fall into two main categories based on the business outcomes they enable: These predictions tell you how likely your individual customers are to take a specific action that matters to your business, like purchasing a new product or upgrading a subscription (to name just a few). These predictions tell you which specific offer among a specified set is mostly likely to result in a customer taking a defined action.",
        "When to use Predictive Attributes": "The most fruitful use cases for Predictive Attributes tend to have the following characteristics: Behavioral events are an important factor in generating a prediction.User Attributes alone (e.g. Customer Lifetime Value, subscriber status, etc.) are not sufficient to predict user behavior.The action occurs frequently.The more frequently an action occurs, the more behavioral data Cortex will likely have on the user base. (Like purchases and membership upgrades, for example.)The action is intentional and meaningful.Conversion actions like purchasing an item, subscribing, and upselling reflect meaningful consumer decisions, whereas product views, page visits, and other browsing behavior can be less intentional. Specific examples of use cases where Predictive Attributes are likely to improve campaign performance include: Non-subscriber to subscriber conversionSubscription tier increasesUpselling users on a premium offeringChurn preventionPredicting media engagement(e.g. which users are likely to watch a particular show)",
        "Access Usage & Billing": "To access the Usage & Billing page: Log into your mParticle account.Ensure you have an Admin & Complianceuser role.In the left navigation, click theSettingsicon and selectUsage & Billing.  The Usage & Billings page presents a list of your invoices for the current calendar year, sorted from most recent to oldest, with the currentmonth-to-date invoiceat the top. ",
        "View invoices from a different year": "To view invoices from a different year, click the button showing the year in the upper left, and select a different year. ",
        "Download an invoice": "To view an invoice for a particular month, clickDownloadwithin that month\u2019s row. Invoices are provided as Microsoft Excel .xlsx files and must be downloaded to be viewed. Both \u201cIn progress\u201d and \u201cCompleted\u201d invoices are provided as .xlsx files. ",
        "How invoices are formatted": "Each invoice file is organized into four tabs: The Definitions & Calculation Logic tab of your invoice lists the name of each product you used during the billing period and the product family it belongs to. The \u201cDefinition\u201d column defines the specific units used to meter the product and the \u201cHow It\u2019s Calculated\u201d column explains how those units are metered. The Overall Summary tab contains your Usage Summary, Credit Summary, and Credit Ledger: TheUsage Summarylists each product you used, the volume of your usage (in millions), the price in mParticle credits, and the total cost.TheCredit Summaryshows your current Credit Grant, or your credit balance.TheCredit Ledgerlists individual credit transactions, such as when credit grants are created or credits are spent. The Overall Summary tab is a report of your usage across all of your accounts and workspace. The Usage by Workspace tab breaks down your usage by each of your workspaces, denoted by the workspace ID under the \u201cWorkspace\u201d column. The Usage by Account tab breaks down your usage by each of your accounts, denoted by the account ID under the \u201cAccount\u201d tab.",
        "Invoice status": "Invoices will always display one of two status: In progressCompleted",
        "In progress invoices": "An invoice with theIn progressstatus provides a real-time report of your usage from the beginning of the billing period to the current date. These invoices always include\u201cmonth-to-date\u201dat the end of their name to help differentiate them from Completed invoices. In-progress invoices are intended to provide an estimate of your usage and credit consumption in real-time for the current billing period that hasn\u2019t completed yet. When the billing period ends, the invoice is markedCompleted.",
        "Completed invoices": "Invoices with theCompletedstatus provide a finalized report of your usage and how you are charged for eachbillable item. Invoices are not markedCompleteduntil the 2nd day of the following month. For example, your January invoice will not be finalized and markedCompleteduntil February 2.",
        "Analyze as Funnel/Cohort": " In order to interlope your customer path from the Journeys tool, and analyze it as a Funnel or Cohort query, select the Analyze as Funnel/Cohort menu item. Then, you\u2019ll be prompted to select an event in each previous step until you reach a singular customer journey. As you click on events going backwards, notice that your customer journeys will start to slim into a singular Journey. When you reach the starting event, your customer journey will load into theFunneltool, or theCohorttool, depending on your selection.",
        "Exclude event": " In order to exclude an event from your Journeys query, select the Exclude Event menu item. Any excluded events will not be displayed in the chart - they will be treated as if they did not happen. To re-include an event, click on the Excluded Events menu bar item. Events expanded or excluded will be shown in the UI.  Excluding events will also affect the query performance.",
        "Breakout event": "Let\u2019s say you currently track a Page View event, with a Page Title property. You\u2019re most likely going to care about Page View & Page Title combinations, rather than just the Page View event. In order to display event+property combinations in your Journeys visualization, click on the event in which to break out, and select the Breakout Event menu item. Then, select a property to breakout by. To undo this, click on the Events with Breakouts menu item. At query time, an \u201cevent\u201d in a path can include property values. ",
        "Script for Non-AMD Sites": "Please note, the instructions in this section are only valid for sites that don\u2019t use AMD (e.g. require.js). First, you\u2019ll need to asynchronously load our script into your site. Add this script in either your site\u2019s\u00a0\u00a0or\u00a0tag: This script tag asynchronously loads Indicative.js from our CDN and initializes the JavaScript code with your unique API key. You will need to set your API key in quotes where it says \u201cYOUR_API_KEY_GOES_HERE\u201d. You can find a list of all of your projects and appropriate API keyshere. To choose between your site\u2019s\u00a0\u00a0or\u00a0\u00a0tags, note the pros and cons of each. The\u00a0\u00a0tag will allow you to access the Analytics object earlier (on load), however your site will not load until everything in the\u00a0\u00a0tag is loaded. So, if you do not need the Analytics object immediately, we recommend putting this snippet in the\u00a0\u00a0tag. If you would like a version of the script that does not ask require.js, please reach out tosupport@mparticle.com.",
        "Building and Sending an Event": "Recording an event is easy and customizable. It can be as simple as: The above line will build and send an event named \u2018event-name\u2019 with a unique ID set as a random UUID. You can also add your own user IDs and important properties to every event to further enrich your data for more impactful analyses like in the example below. Some properties may be stored as a persistent cookie, so that every page can share some common properties or unique ID instead of explicitly passing them every time you build an event. Additionally, Indicative automatically tracks some properties by default; learn more.",
        "Validating Integration": "Open up theDebug Console in Analyticsto view all incoming events. You should expect to see your data in Analytics",
        "Additional Information": "For advanced JavaScript settings, please refer to our documentation. For a full list of our Analytics Object API, please click here.",
        "Callbacks": "Indicative also allows callbacks, which will be fired after a successful or unsuccessful stat post. You can include a callback function in any of the buildEvent methods, like so: With so many different ways to build an event, you\u2019ll have a lot of flexibility to build and send any custom events you need. For further references, refer to the Analytics object API table below.",
        "Stateful Variables": "We allow you to set stateful variables across every page. Stateful variables are stored as a persistent cookie, so every page will be able to share the same common properties and a uniqueID for the user triggering events on your site. Anywhere in your JavaScript, after Analytics was initialized, call: Indicative.setUniqueID(\"unique-user-id\"); This will allow you to log events without having to refer to a unique ID every time you build an event. Analytics also allows for stateful properties, as well, which can be added with the following calls: These properties will be appended to subsequent event calls. They will not override the properties passed into a buildEvent call, rather append to the list of properties. If a common property isn\u2019t applicable anymore, call: Indicative.removeProperty('propertyName'); This will remove a single property. It\u2019s just as easy to clear the entire common properties list: Indicative.clearProperty();",
        "How to Track Links": "Tracking\u00a0href\u00a0link clicks can be challenging, because once the page changes we lose our chance to fire an event. To solve this problem, we\u2019ve added a callback to our build object. Use the following function to track link clicks and then send the user to the linked page: To call this function in your HTML, set up a link like so:",
        "How to Track Web Sessions": "We\u2019ve implemented Web Sessions in our JavaScript SDK track users\u2019 web sessions with just a slight change to one line of your code.\u00a0If a user has no activity for 30 minutes (no events are fired locally), upon any new event activity, the JavaScript SDK will also fire a \u201cWeb Session\u201d event to indicate the start of a new Web Session. Note:\u00a0The window of inactivity is customizable, but defaults to 30 minutes, with industry standards.",
        "How to Integrate": "Where you see the line in this snippet: Indicative.initialize(apiKey); Alter it to read: If you want to alter the inactive session length, change the line to be this instead: where 5\u00a0signifies\u00a05 minutes.",
        "Automatic Tracking": "We automatically track the following properties: browser:\u00a0browseroperating system:\u00a0browser_osdevice:\u00a0browser_devicereferrer:\u00a0browser_referrerlanguage:\u00a0browser_languagepage title:\u00a0page_titleurl:\u00a0page_url We also automatically track marketing channels provided by UTM search parameters. You\u2019ll be able to see the following properties if you have users loading your page with UTM properties in the URL:\u00a0campaign_source,campaign_medium,\u00a0campaign_term,\u00a0campaign_content, and\u00a0campaign_name. We will also provide these channel properties in their own section of the data panel, titled as User Properties - UTM (category).",
        "How to Track Users Across Subdomains": "We support tracking user sessions across various subdomains through the use of our SDK. If the option \u2018cookiesOnMainDomain\u2019 is set to true, it will store the cookie on the root domain. Where you see this line in the snippet: Indicative.initialize(apiKey); Alter it to read: Whenever the cookiesOnMainDomain option is set to true, it is recommended that you include the base domain name. If it is not set, our SDK will attempt to figure it out by taking the last two tokens of the domain name, and in some cases it may be invalid values (e.g. .com.mx). To explicitly add the domain name, add the domainName to the initialization parameters. Indicative.initialize(apiKey, {recordSessions: true, sessionsThreshold: 30, cookiesOnMainDomain: true, domainName: \u2018example.com.mx\u2019 }) Warning:\u00a0Changing or enabling this optionmaybreak existing cookie tracking.",
        "Simple": "Use Simple Track By mode when each tracking property applies to each event in the funnel. For example, PetBox may want to track the user journey from Site Visit -> Open App -> Purchase Product tracked by Session ID. You may select up to 3 tracking properties in simple mode. Add a tracking property to your funnel analysis by hovering over the tracking line at the bottom of thequery builder, and selecting an event property from the \u201c+Select a Property\u201d dropdown. Please note that only properties that apply for all events selected in the funnel will appear in this dropdown. Once you have selected a tracking property, run the query for the changes to be applied.  A maximum of three tracking properties may be added to a funnel analysis. By using tracking properties, PetBox is able to ensure that each user journey in this funnel occurs whilst holding User IDandSession ID constant.  PetBox has now ensured that all events in the user journey are completed by the same user, and in the same session. Thus, this funnel analysis is counting user-property pairs, and not just users.",
        "Total count vs. users who performed": "Once you have added a tracking property to your Funnel analysis, you may select whether to return users or counts. This can be toggled to the left of the first event in your query builder.  Total Count ofreturns unique user-property pairs. Thus, a user may be represented in the Funnel more than once, provided that said user completes the funnel with more than one tracking property value.Users who performedreturns unique users. Thus, a user may only be represented in the funnel once. The user will be represented by thefirsttime they perform the events selected in the funnel.",
        "Integration Requirements": "Before starting the integration process, any customers who are self-hosting Rudderstack\u2019s Data Plane should confirm their current versions by navigating to their Rudderstack Data Plane URL. This URL can be found where they initialize their SDKherein the value for<DATA_PLANE_URL>. Once you have the URL, you can locateDATA_PLANE_URL/versionsto get the transform and server versions. Ensure those are the latest, and if not, update them. Self-hosted Rudderstack users should ensure they are running the following versions or later: rudderlabs/rudder-server: 05102021.075534rudderlabs/rudder-transformer: 05102021.062759 SeeRudderstack Software Releasesfor the latest software versions.",
        "Direct link to specific user activity timeline": "Analytics users may directly access a specific user\u2019s Activity Timeline by appending the User ID to the end of this url in the following fashion: app.indicative.com/#/users/detail/USER_ID_HERE An example link would be app.indicative.com/#/users/detail/6834fa-dfa12-4389. Using this direct link would lead to that user\u2019s specific User Activity Timeline: ",
        "Pin event properties": "You can pin additional fields to the expanded session view: See important event properties at a glance without having to expand each eventPersist these properties across sessionsEach person can choose their own items to pin To pin an event property in the user activity timeline: Navigate to the Users page by clicking the green plus sign and selectUser Lookup by Event:Select an event and run the analysis:Analytics displays all the users associated with that event:Click on a user to display event properties:Click on the Events (in this example, 80 Events), and for the event whose property you wish to pin, hover to the left of the label until you see the pin icon:Click the pin to pin that property.Now the property you pinned is displayed for every event: To unpin a property, click the pin icon next to any of the pinned events to deselect it. Note the following about event property pins: Pins are set at the user level and are persistent across sessionsPins are set at the project level and do not persist between projectsPins are limited to 5 per user per projectAll Analytics users across all plans should have access to pin, as long as they have access to the Users toolAnalytics doesn\u2019t support event-specific pinning. A pin applies to all qualified events.",
        "Export Users": "To download a list of users from an entire series, click on any data point or table cell within your results. Within the dropdown, select \u201cExplore Users.\u201d Once your Users query results have loaded, click on the export icon located in the menu bar beneath the query builder, then select \u201cDownload CSV.\u201d  To download a list of users from a single point, click on any data point or table cell within your results. Within the dropdown, select \u201cDownload Users in this Point to CSV.\u201d A list containing all users associated with this data point will be emailed to the address connected to your Analytics account. There is no limit on the number of users that can be exported. ",
        "Requests": "Since the best match strategy does not support login IDs, there are nologinorlogoutrequests, onlyidentify, for identifying a user based on your configured identity priority and the information in the request, andmodifyfor altering an identity for a given MPID.",
        "Calculate Using Rows": "To create a calculation using two rows, simply select the row name you\u2019d like to use, then drag and drop the row name into the first (top) or second (bottom) drop zone. You may switch drop zones by selecting the up and down arrows to the left of the calculator field. Remove a row from the calculator by selecting the red \u201cX\u201d icon before the row name. The calculator supports the four basic mathematical operations- addition, subtraction, multiplication, and division- and can also automatically calculate a percentage. Percentage and Division are based on the same calculation, with the difference being that Percentage does an additional calculation of multiplying the resulting value by 100. To change the operation for your calculation, simply select the relevant icon. Once you\u2019ve completed each field and selected an operation, choose Calculate to create a new query row containing your calculation. Selecting a calculation row will re-open the tool for editing. Calculations are visualized in the chart area alongside the underlying queries.",
        "Calculate Using Numbers": "To add a number to your calculation, simply select the first (top) or second (bottom) drop zone and enter the value. Numerical values can be used independently or in combination with a query row.",
        "Complex Calculations": "Only two rows may be used in a single calculation. If you\u2019d like to create a calculation with more than two rows, you\u2019ll need to use a calculation query row as a component of another calculation. For example, to calculate the sum of A+B+C, you may calculate A+B and then use this new row to calculate (A+B)+C. Changes in underlying query rows are calculated before calculation query rows, so your calculation will update automatically if its underlying data changes. You may hide the component queries by hovering the cursor over the row you\u2019d like to hide, then selecting Deactivate Row. This will not delete the row, but the hidden row will not appear in your results. Deactivated rows are grayed out in the query builder.",
        "How It\u2019s Done": "Take a look at the following example query: To decide which blog view events will be counted, Analytics will look at the timestamp on each blog view and check to see if there is a create profile event that falls in the 7 subsequent calendar days. If this is the case, the blog view will be counted in the analysis. Note: Did [not] Perform\u00a0clause events could have occurred outside of the main date range in the query builder. For example,\u00a0Subscribe\u00a0events in the chart window could have occurred within the Last 7 Days of the current date, as set in the date range menu at the base of the query builder. However, the\u00a0Email Clicked\u00a0and\u00a0Blog View\u00a0events will have occurred within the seven days prior to the\u00a0Subscribe\u00a0event. If\u00a0a\u00a0Subscribe\u00a0event occurred six days ago, and\u00a0Email Clicked\u00a0occurred six days prior to that, the\u00a0Subscribe\u00a0event will be counted even though\u00a0Email Clicked\u00a0occurred 12 days prior to today.",
        "Saving a users query": "Once you save your query, a pop-up window will appear. Use the dropdown menu to select the folder in which to save your Users analysis. ",
        "Breakout mode": "To understand which types of users are converting, you may apply property breakouts to your Funnel analysis. In Funnel, there are two types of property breakout modes:",
        "Per step": "An event property or user property breakout will be applied to each step individually for the entire group of users in each step. For example, a user may complete steps in a conversion funnel on a variety of devices. To explore which type of device the user used at each individual step, use Per Step. ",
        "Shared": "An event property or user property will be applied to a single step and then shared across the entire funnel. For example, some of your users may enter the top of the funnel through their mobile device, whereas others may enter through their desktop computer. To compare conversion rates through the entire funnel via mobile vs. desktop, regardless of the device used in subsequent steps, use Shared and apply your breakout to the first step in the funnel. ",
        "Attribution": "When applying a Group By clause to a funnel analysis, you must select whether the steps are broken out by thefirst,lastorallproperty attribution. The default selection is first.  First:Use the first observed property value to attribute users to a breakoutLast:Use the last observed property value to attribute users to a breakoutAll:Use all observed property values to attribute users to a breakout All of the above options are subject to the selected date range for your funnel query.",
        "Group by first or last": "When the funnel query is set to group byfirstorlastproperty values, the results are grouped by the first or last time the event was observed for each user. For example, when the query is set to \u201cgroup by last\u201d,  if a particular user performs the eventBlog Viewthree times during the selected date range, the third time they performed the event is represented in the groupings in all of the donuts (because the Funnel breakout mode is Shared). With the default (Shared) breakout settings, thisBlog Viewgrouping is applied across all of the subsequent steps in the funnel. If you wish to see the users\u2019 last action for each step, switch the breakout mode to Each Step in Settings.",
        "Group by all": "When the funnel query is set to group byallproperty values, additional comma separated breakout values are created that represent all possible combinations of behaviors that users might take. To illustrate how grouping by all users\u2019 events works, consider three users who performed the eventDownloadduring a given time frame, and are grouped by browser: User A performedDownloadfrom a Mac.User B performedDownloadfrom a PC.User C performedDownloadfrom a PC, and then from an Android device. When grouped by all values, the resulting funnel donut for these users would show three breakouts: one for Mac, one for PC, and one for PC | Android.",
        "Step 1. Initiate a download": "To download an audience from the the journey where it is defined: From the journey where you have defined an audience, click the three dot icon in the active audience that you wish to download.If the audience includes A/B Testing Variants, you can select which variants you want to download.Determine whether you want to download the full audience or an audience sample. Downloading a sample will likely take less time than downloading a full audience (depending on the audience size), and is useful for testing or troubleshooting.Select the identity types you want, then clickDownload.",
        "Step 2. Download the file": "The download takes some time to prepare. When your download is ready, mParticle sends you an email with the download link. The download is a ZIP file which, when extracted, will contain a CSV file for each audience or variant, plus amanifest.jsonfile, with metadata about the files.",
        "CSV format": "Audience CSV files have a row for each identity in the audience. Remember that a single user profile can have multiple identities and, therefore, multiple rows. The four columns show a Unix timestamp when the audience membership was retrieved for download, the mParticle ID of the profile, the identity type, and the value: The manifest file is in JSON format.",
        "Data Forwarding and Connections": "Data about an event, including individual attributes, may be forwarded from the input to an output in a variety of ways:  Server-side: Data is forwarded from the web or client app to mParticle servers, stored there, and from there is forwarded on to an output or destination such as Braze. For these types of forwarding, no client setup is needed, because the client communicates directly with mParticle.Client-side: Data is forwarded from the web or client app directly to the output, unseen by mParticle servers. For this type of forwarding, you must set up your client, usually by adding a kit to the platform SDK of the client app. Some integrations allow you to choose either server-side or client-side when you configure a connection.",
        "Connection Workflow": "No matter which data flow your integration uses, a connection is required. A connection is the combination of an input, an output, and the configuration information required to make the connection work. Most of the configuration information is specified in the mParticle UI, but some values may need to be fetched from the output, and you may need to add a kit to an SDK for some client-side integrations. The Connections screen controls how event data from your inputs (iOS, Android, Web, Feeds, etc) is forwarded to your output platforms. You must set up a separate connection for each input-output-configuration combination. For each connection, you have several opportunities to cleanse and filter your data, to ensure that each output receives the data you want it to receive, in the correct format.  Each output has its own requirements, so the process for setting up each connection is a little different. However, all connections require these basic steps: Create a connection.If needed,add a kit to the SDK for your input platform.Activate the connection.Verify that data is being forwarded.",
        "Create a Connection": "Data flows once an input and output are connected and the connection is active. For an overview before you create your first connection, view the following video:  1. Select an input Navigate toConnections > Connectin the mParticle UI.From the list displayed, select the input you want to configure. If the list is empty, go toSetup > Inputsto create an input. 2. Apply \u2018All Outputs\u2019 transformations Once you have an input selected, you can set up transformations that are applied to all outputs connected to that input. ClickAll Outputsto see options. There are two transformations that can be applied here: RulesUser Splits 3. Select an Output Once you have selected an input, you will see a list of available outputs that can receive data from your selected input. If this list is empty, go toSetup > Outputsto create some outputs. The following video shows how to create an output:  The mParticle UI may indicate that you need or may need to add a reference to a kit in your platform dependency configuration. You can do this after you create the connection but before you change theConnection Statusto active. SeeAdd a Kitfor more information. 4. Complete Connection Settings Complete any settings that apply to the connection. These will be different for every output but can include: Credentials or Account/Workspace identifiersWhat user identifiers and attributes should be sent. You must choose aUser Identification(identity type) or data may not flow.Encoding to be used for identifying dataHow custom attributes should be mappedHow to handle attributes specific to the OutputThe minimum platform version of your input that the connection will forward data from.If you set theMin Platform Versionconnection setting, then mParticle will only forward data from inputs with anapplication_versionthat is equal to or greater than the version you set. This allows you to create a separate connection for different versions of your app.The version number you set for the Min Platform Version connection setting must only contain numeric characters (0through9) and decimals (.) Using any non-numeric characters when setting your Min Platform Version will cause the connection to fail. 5. Apply \u2018Specific Output\u2019 transformations The second set of transformations apply only to your selected Output. ClickSpecific Outputto see options. Specific output transformations include: Event Filter- note, this is not part of Connections Screen but should be configured before the next step if needed.Specific Outputs RulesForwarding RulesCustom MappingsUser Sampling",
        "Add a Kit": "When you configured your output in step 3, the mParticle UI may have indicated that you do need or may need a kit added to the SDK for your app or web pages: If so, check the integration documentation for your output. If a kit is required, follow the instructions for adding the kit to your input platform dependency configuration.",
        "Activate the Connection": "After you have completed the required settings, set up any transformations, and added a kit (if needed), you are ready to activate the connection: Navigate toConnections.Select the input for your connection.Click the output you are ready to activate.Click the Connection Settings gear icon.Click theConnection Statusslider so it displaysActive.ClickSave. Very large data volumes may take up to 48 hours to process. To reduce processing time, reduce the number of sessions your account sends to fewer than 200,000 per day.",
        "Verify Your Connection": "Verify that data is flowing. Check in the mParticle UI and in your downstream app or system (output). Wait for the time indicated in the mparticle UI to ensure your connection has been activated. Additionally, some outputs such as Google Analytics have their own processing delays. Check the \u201cData Processing\u201d section of [the integration documentation(/integrations) for your output.OpenData Master > Live Streamand select the following values:The input fromInputsThe output fromOutputsInMessage Direction, selectBoth In and Outto check whether events are being forwarded.InDevice, leave the default valueAll Dev Dataunless you are verifying the flow to a device in production. In that case, choose the relevant device.Check that you have chosen aUser Identification(identity type).Events should be listed as they occur:  If you don\u2019t see events being forwarded,troubleshoot your connection. To verify that data is arriving n the downstream system: In mParticle, look inData Master > Live Streamand select an event.Search for that event in your downstream system. If you don\u2019t see events being forwarded,troubleshoot your connection.",
        "Troubleshooting Connections": "Follow these steps to troubleshoot an event connection: Make sure you have waited for the time period specified in the mParticle UI before troubleshooting further.CheckActivity > System Alertsfor any fatal errors or warnings and resolve them.If your events are not appearing in the output although the mParticle Live Stream suggests that the connection is active, follow the steps inVerify your connectionto ensure your connection is working. Although Live Stream has indicated that events are forwarded downstream, there might be issues downstream that prevent successful forwarding. The next steps will help you find this type of problem.Does your connection depend on a kit? Does your connection use a kit to forward data downstream? Has the kit been included in your application? If yes, check your application for HTTP requests directed to the partner. Have they succeeded or are they reporting errors?Does your connection use batch forwarding? Some outputs use batch forwarding. You might have to wait longer for events to arrive in these systems (approximately 10 minutes or after several event batches have been collected). Still not sure what\u2019s wrong? ContactmParticle Support.",
        "All Outputs Transformations": "SeeRulesfor more information on all-output rules.",
        "Specific Output Transformations": "mParticle lets you customize the data that you send to each output. There are many reasons to do this, including: Filtering out personally identifiable information (PII);Filtering out data containing company insights you don\u2019t want to share with a particular service;Filtering out events that you don\u2019t need to track in a particular service;Filtering out information from places or customer types you don\u2019t want to track in a particular service;Enriching the data you send to a service with extra user info from an external source;Reformatting your data to match what a particular service accepts.",
        "The Data Filter": "Unlike other transformations, the data filter exists on its own page, separate from the Connections screen. A data filter allows you to decide which events/attributes you want to send to each output. By default, all event attributes are enabled when you first activate a connection. From the event filter you can: Decide whether new events and attributes should be forwarded by default.Turn forwarding on/off for each event, by event name.Turn forwarding on/off for attributes of each event, by attribute name. SeeThe Data Filterfor more information.",
        "Forwarding Rules": "Like the event filter, forwarding rules let you filter out events from being sent to an Output. But where the event filter is based on event and attribute names, forwarding rules look at values, which lets you build some more complex conditions. There are several types of forwarding rules. Attribute:Attribute rules take an event attribute name and a value. You can choose to either not forward events that match the rule, or to only forward events that match the rule, excluding all others. Greater than / less than comparisons are not possible. Matching is case sensitive and exact.  If an attribute is criteria for the forwarding rule, but is omitted from the source payload, it is treated as if the attribute key exists and the value doesn\u2019t match.Attribution:Attribution rules filter events according to Publisher information. You can choose to exclude events attributed to a specific publisher, or forward only events attributed to that publisher.Consent:Data privacy controlsallow you to filter events based on whether a user has given consent to a particular data collection purpose.ID Sync: ID Sync rules allow you to only forward data from logged-in users. A logged in user is one with at least oneLogin ID, as defined by your Identity Strategy.",
        "User Sampling": "User Sampling is applied to a single output and sends only a subset, or sample of your data to an output. The main reason to do this is to control costs on services that charge by volume of data. Data is sampled on a user level, not an event level - if you select a 50% sample, mParticle forwards all data received from half of your users, not half of each user\u2019s data. ",
        "Specific Output Rules": "SeeRulesfor more information about specific-output rules.",
        "Custom Mappings": "Some services allow your incoming events to be translated into events specific to the service. For example, if you have a custom event named \u201cNextLevel\u201d, typically this event would be forwarded as a custom event to a service. With custom mappings, you can specify that this event be forwarded to a service using their specific event name. For example: For partners that support custom mappings, the output\u2019s events are listed on the left side of theCustom Mappingstab. For each event, you can then select an mParticle event and associated attributes to map to the partner\u2019s event.  The following integrations support custom mappings: AgilOneAlgoliaAmazon Mobile AnalyticsAppLovinAppsFlyerCriteoFacebookFiksuGoogle Ads Enhanced ConversionsGoogle Analytics for FirebaseGoogle Analytics 4 (GA4)IterableNCR AlohaOptimizelySimpleReachSnapchatTikTok If an event has a Custom Mapping for a particular connection, it will be displayed with an icon in the Event Filter  If you turn off forwarding of an event with a Custom Mapping, the mapping information will be deleted.",
        "mParticle Forwarder Module": "The final and most crucial transformation step is the mParticle Forwarder Module itself. After all your other transformations have been completed, the forwarder module turns your data into messages to the output in its preferred format. Each integration has its own forwarding module. Settings for the forwarder are derived from three places: Some data is handled based on hard-coded settings. For example, any device information (such as device model or operating system type) the output service accepts is usually forwarded without the user needing to set anything up.Some data is handled according to the connection settings. For example, in the Mixpanel settings, you can choose whether you want to forward session events, and decide what they should be called.Anycustom mappingsyou have created. Based on these settings, mParticle transforms your data into a format accepted by the output. This can involve extensively reformatting the data. For example, Mixpanel\u2019s API accepts events, with attributes given as a flat set of key-value pairs. To fit this structure, a single mParticle eCommerce event with four products will be transformed into four Mixpanel Events - one for each product - with common attributes, such as user and device info, repeated for each event. The documentation for each integration will tell you what you need to know about how data is transformed to be accepted by the Output service.",
        "Best Practice for Transformations": "mParticle provides many opportunities to transform and enrich your data. It is often possible to perform the same transformation in more than one place. For example, if you wanted to drop all Application State Transition events for a given output, you could use the event filter, or you could write a condition in an output rule. There are advantages to each choice. The event filter can be used by anyone with the appropriate access to your organization in the mParticle Dashboard, so it is easy to update and maintain. Writing a rule gives you much finer control over your data, but rules may be difficult for non-developers to understand or alter. Make the necessary transformations to your data in as few steps as possible. The fewer times you alter your data, the easier your integration will be to troubleshoot and maintain.",
        "More Help": "To find out more about event integrations in mParticle, view the following video: ",
        "Profile data": "Data about users are stored as attributes of individual profiles. These attributes include identities, device types and IDs, and several custom attributes such as membership status and demographic information. An attribute value may not be current, depending on how often it is updated. For more information about how profile data is associated with users, seeStore and Organize User Data.",
        "Data retention limits": "The maximum period that mParticle stores profile and event data is governed by your long-term data retention policy, which is defined in your contract. Usually, the long-term data retention policy is the same for event and profile data. However, you can have different ranges for added control and flexibility for events and profile data.\nFor example, during org and account setup, you can set long-term data retention for events at two years and reduce the time profile data is available to 12 months. Another factor may affect the data that is available for audiences: the Real-Time Audience Storage Lookback specified in your contract. Typically it is set to 30, 60, or 90 days, but can be changed. It is also overridden by Unlimited Lookback. SeeData retention and Unlimited Lookbackfor details.",
        "Data retention and Unlimited Lookback": "Unlimited Lookback is a premium feature that extends your audience and calculated attribute lookback to your long-term retention for events. Without this feature, audience lookback is limited to the Real-Time Audience Storage Lookback specified in your contract. Data retention for personalization features (audiences, journeys, and calculated attributes) behaves differently depending on whether or not your account has the Unlimited Lookback feature enabled: Unlimited Lookback:Uses long-term data retention for events.All events in the date range for your long-term data retention for events are available for data evaluations.In the criteria builder, the number of days you specify for recency can go back to the long-term data retention for events.When a personalization feature (journey, audience, or calculated attribute) is activated, it is initialized using data available for the entire range specified by your long-term data retention for events. For the initialization of personalization items, a longer lookback usually means larger data volume, which may incur an additional expense.To help with cost estimation, you can set an estimated lookback value. This informational value alerts anyone creating a calculated attribute when a specified date range exceeds that organization\u2019s expected date range.Without Unlimited Lookback:mParticle uses the Real-time Audience Storage Lookback for real-time evaluation, which is specified in your contract and is often set to 30, 60, or 90 days.The behavior described for Unlimited Lookback features is not available.",
        "Examples with no Unlimited Lookback": "If your long-term retention for events is set to two years, you can view events that occurred up to two years ago.If your long-term retention for profiles is set to seven days, if a user has no activity for more than seven days, then the profile data for that user expires and isn\u2019t available for calculated attributes, journeys, or real-time profile enrichment.",
        "Examples with Unlimited Lookback": "If your long-term retention for events is set to two years, audiences, journeys, and calculated attributes are initialized using data up to the long-term retention for events and long-term retention for profiles.When choosing a recency or frequency value in the criteria builder for audiences or journeys, you can specify a value up to the long-term retention for events.",
        "Date range measurement": "mParticle determines the beginning of a date range for retention purposes differently for event and profile data: Event data: The event batch timestamp (timestamp_unixtime_ms) added to the top level of every batch, representing the time the batch was received by mParticle. Note this is different from the timestamp associated with an individual event.Profile data: The last time a profile was updated by an inbound data stream. For example, a profile may be created, attributes may be added, updated, or deleted, or other profile information may change. Each change triggers a timestamp change. Events and profiles need different ways of calculating age because an event isn\u2019t usually updated. Since several processes may modify a profile\u2019s timestamp, the date is measured differently than event data.",
        "Date range example": "Assume the following facts: A screen view event (screen_view) occurred and is time-stamped 1657934165001 (6 June 2023, at 21:29:24).This event was received by mParticle (ingested) in a batch time-stamped 1657934165102 (6 June 2023, at 21:30:55).Your long-term data retention for events is two years. In this example, mParticle keeps the event available until 1749270655 (6 June 2025, at 21:30:55).",
        "Group ID": "When you create a group definition, you select a user attribute that becomes the group ID. Any user attribute can be used as a group ID, as long as it exists in your data catalog. Every Monday, on a weekly recurring basis, mParticle processes all profiles and groups together any profiles who share the same value for a user attribute that is a designated group ID. For example, if you create a group definition withaccount_idas the group ID, and three users all have the attributeaccount_idwith a value of1234, then all three users are added to a group that is identified by the group IDaccount_ID:1234. Group IDs must be unique hexadecimal or numerical user attributes within the following constraints: Minimum 1 characterMaximum 32 charactersIncludes only the alphanumeric charactersa-zand0-9 When you create a group definition, you are defining a set of criteria that is used to create multiple group instances, one for each value of the group ID. Using the example above, if mParticle ingested data for two more users who each had the attributeaccount_idwith a value of5678, they would be added to a separate group identified byaccount_id:5678.",
        "Group attributes": "A group attribute is a user attribute given to all members of a group. The value of a group attribute is the same for every profile in the group, but the value of a group attribute is calculated based on one of four possibleaggregation functions. When viewing a user profile, you can tell the difference between group attributes and other user attributes by a prefix equalingnameof a group ID. For example, all user profiles in a group with the group IDaccount_id:1234and the group attributepremium_subscriberwill display the new attributeaccount_id:premium_subscriber. This naming convention prevents any conflicts between group attributes and user attributes. For example, a user may have a premium subscriber group attribute, showing that they are in a group with a premium subscriber, even though individually they are not marked as a premium subscriber. Any user attribute that exists in your data catalog can be used as a group attribute.",
        "Group attribute aggregation logic": "The value of a group attribute is calculated by aggregating the values of each original user attribute in the group. The aggregation results from one of four functions that you specify when first creating a group attribute. These four aggregation functions are: Continue reading below for detailed descriptions and examples of each aggregation logic option. Group members only inherit aboolean orgroup attribute when another member has the same user attribute with a value oftrue. Only boolean user attributes can be used asboolean orgroup attributes. For example, imagine a group with a source attributehas_dog. Let\u2019s say this group contains three users and none of them had a dog when they joined the group, so each user\u2019s instance ofhas_dogequalsfalse. If a new user with thehas_dogattribute set totruejoins the group, then the other members will all have theirhas_dog groupattribute set totruethe next time the grouping job runs. The value of alatestgroup attribute is set to the most recently updated instance of the user attribute in the group. All members of a group with alatestgroup attribute inherit that attribute upon joining the group. User attributes of any data type can be used aslatestgroup attributes. For example, imagine a household group with two members: John and Jane. The group has alatestgroup attribute calledstreet_addresswhich is set to1234 Main Street. Next, a new user, Cindy, joins John and Jane\u2019s household group. Cindy\u2019s profile would inherit the group attributestreet_address:1234 Main Street. The value of asumgroup attribute is equal to the sum of all instances of the user attribute in the group. Only non-negative numbers or integers can be used assumgroup attributes. The value of anaveragegroup attribute is equal to the average of all instances of the user attribute in the group. Only non-negative numbers or integers can be used asaveragegroup attributes.",
        "Group Identity API": "In addition to the mParticle UI, you can also create and manage group definitions programmatically using the Group Identity API. For more information, see theGroup Identity API reference.",
        "How audiences are forwarded": "In mParticle, an audience is a set of users who match a given set of criteria. When mParticle prepares to forward an audience, it is broken down into a series of messages about audience membership. Each message contains: The name of the audienceAn identity that can be used for targeting, such as an email address, a device identity or a social media identity.Whether that identity is being added to, or removed from the audience. mParticle then translates these messages into a format that can be read by each audience output partner, and forwards them via HTTP API. Each output deals with audience information a little differently, depending on their data structure, but there are two main patterns.",
        "Direct": "Some audience output partners allow mParticle to either to directly create an audience (some call them \u2018lists\u2019, or \u2018segments\u2019) via their API, or at least to manage the membership of an existing audience. The end result will be an \u2018audience\u2019 in the partner system, containing as many identities from the original mParticle audience as the output can accept. mParticle will continue to update the membership of the audience in the partner system as users are added and removed. Email marketing and social media platforms are usually in this category.",
        "Indirect": "Not all audience output services have a concept of \u2018audiences\u2019 that mParticle can map to. Others don\u2019t allow their audiences to be directly managed via API. In these cases, mParticle usually forwards audiences as some kind of user attribute or tag. Push messaging and other mobile-oriented services often fall into this category. As an example,Braze, has it\u2019s own audience feature, called \u2018Segments\u2019, but it does not allow mParticle to create segments via API. Instead, for each Braze-supported identity in the audience, mParticle sets a tag on the user, named after the audience. You can then easily find matching users in Braze by searching for that tag. The catch here is that it is often necessary for the output service to already have a record of the users you want to target. For this reason, this type of audience integration usually works best when paired with a matching event integration.",
        "Example - Connect an audience to Mailchimp": "Just like event outputs, each audience output will follow a similar setup process, with the exact prerequisites and settings being different for each. This tutorial forwards an audience to Mailchimp as an example. You can follow the same steps with a different output, or create afree Mailchimp accountto follow along exactly.",
        "Create a Mailchimp List": "mParticle sends audiences to Mailchimp via itsList API. For this to work, You need to have already created a list in my Mailchimp account, and you need to know the List ID. You can give your Mailchimp list the same name as the mParticle audience you want to forward. . You\u2019ll also need to create a Mailchimp API Key, which you can do from theExtrastab of your Mailchimp Account Settings. ",
        "Add the Mailchimp output": "Navigate to theDirectoryin the sidebar. Locate Mailchimp and select theAudienceoption.Complete the Configuration Settings. You\u2019ll need theAPI Keyyou created in Mailchimp. All audience outputs will need different settings. This example sets theEmail Typeto \u201cHTML\u201d and disables theDouble Opt-InandDelete on Subscription Endsettings.ClickSave.",
        "Connect your Audience": "Navigate toAudiencesin the left column and open any audience page. This example uses the \u201cPotential Parisians\u201d audience, created in the previous tutorial. Click theConnecttab.ClickConnect Output.Select your Mailchimp configuration and complete theConnection Settings. Again these will be different for every output. For Mailchimp, you just need the List ID of your Mailchimp list. ClickSave.",
        "Verify: Check your list in Mailchimp": "The simplest way to check that your Connection is working is to see if your Mailchimp list is showing subscribers. For most audience outputs, mParticle begins forwarding data immediately and continues to update audiences in near real time. For some outputs, however, the design of the output partner\u2019s API requires that we queue audiences messages and upload at a regular interval. In these cases, we make a note of the upload criteria in the docs for that output. mParticle forwards to Mailchimp in realtime, and you should be start to see results in the mailchimp dashboard within ten minutes. Open theListstab in your Mailchimp dashboard. Find the list you used to set up the connection. If you see a positive subscriber count, your connection is working. ",
        "Query Notes in Tool": "Query notes can be found to the right of the query builder in any analysis tool, just below theData DictionaryandProperties Explorer:  When you click on the Query Notes icon, a pop-out will appear on the right hand side of your screen.  Click into the notes area to add any text or images that are relevant to your query. If you add any additional query notes, don\u2019t forget to save your query before navigating away. If you do not modify your query notes, the form will be pre-filled with a written summary of your query.",
        "Audiences Landing Page": "The Audiences landing page is a central hub from which you can view and manage all of your audiences in mParticle. To access it, selectSegmentationin theOverview Map, then clickAudiencesin the left-hand navigation. ",
        "Audiences Table Columns": "The columns of the audiences table display key information about each of yourAudience Strategiesandindividual audiences: Size:The count of MPIDs in the audience.(Note: This value is only displayed at the Audience level; Audience Strategies will show a \u201d-\u201d instead.)Total audiences:The total number of audiences that are part of the Audience Grouping.Activated audiences:The total number of audiences connected to an active output.Connected outputs:The total number of distinct tools receiving audience data.Adds:The number of MPID additions to this audience over the last 24 hours.(Only shown at the Audience level.)Drops:The number of MPID drops from this audience over the last 24 hours.(Only shown at the Audience level.)Volatility:The change in the audience calculated as:(adds + drops) / size in MPIDs.(Only shown at the Audience level.)Last updated (UTC):Allows sorting by time created. Some statistics, such asSize, Adds, Drops, and Volatility, are only displayed at theAudience leveland not forAudience Strategies/Groupings. If a value isn\u2019t applicable at the Strategy level, it will be displayed as\u201d-\u201din the table. To customize the columns that are visible in the table: ClickView Columns.Toggle on/off the columns you want to view.ClickSave.",
        "Audience Strategies vs. Audiences": "Individual audiences are contained within folders calledAudience Strategies. The main table in the Audiences landing page displays all of your audience strategies and their associated audiences. Click the+icon to the left of an audience strategy name to expand and hide the audiences it contains:  Note: The example above shows an A/B test, which produces individual audiences for the control and test variants.",
        "Audience Tags": "As you continue to create new audiences, you can use tags to keep them organized and allow team members to easily see the purpose of each audience at a glance. For example, you can use tags to group your audiences by campaign type, giving them names like retargeting, lead gen, and product launch. Once you have applied tags, you can use tag names as search queries to return all audiences that have that particular tag applied.",
        "Create a new tag": "On the Audiences landing page: Click the+icon next to the name of an Audience Strategy to expose its audiences.Click theTagsicon in the row for that audience.Enter the name of your new tag in the text bar.Note: Tags have an 18-character limit.Click the name of the new tag to apply it to the audience.  To add additional tags, select the name of the tag(s) that have already been applied, open the dropdown menu, and click the additional tags you would like to add.",
        "Update an existing Audience Strategy / Audience": "To update an existing Audience or Audience Strategy, click on its name in the audience table. This will display the Audience Builder modal (if you selected an audience) or the Audience Strategy Editor (if you selected an audience strategy). Here, you can update the inclusion criteria for this particular audience, or update / add features like new paths, additional audiences, or A/B tests. At any time after an Audience Strategy has been created, you can edit its data inputs. Click on the top node in the Audience Strategy, which has the heading \u201cAudience Strategy Inputs.\u201dAdd or remove inputs as you see fit, then clickUpdate.",
        "Share Audiences between accounts": "You can share audiences between your organization\u2019s accounts, with detailed control over what data is shared. Share data broadly or restrict it to only what\u2019s needed for a campaign. This feature does not affect data shared with third-party tools. Navigate to theAudienceslist page.Open theActionsmenu for the audience you wish to share.SelectShareto open the sharing modal. In the modal, you can view the accounts the audience is shared with and their sharing permissions. There are four sharing permission levels: In the sharing modal, select the+icon.Choose the account and the desired permission settings. To view audiences that have been shared with an account, navigate toSegmentation, then selectShared Audiencesin the left-hand navigation.",
        "Add or Remove Teammates": "To add a new teammate, select \u201c+ New Teammate\u201d at the top right of the table, then make your selections. Select \u201cAdd Teammate\u201d to send an invitation directly to their email address. To remove a teammate, locate the user within the table, then click \u201cRemove from Project\u201d or \u201cRemove from Organization\u201d at the far right. You may also use the check box to select one or more teammates, then click \u201cRemove Selected from Project\u201d or \u201cRemove Selected from Organization\u201d to remove them.",
        "Edit Roles": "To edit the role of a teammate, locate the user within the table, then click on their corresponding Role. You may select from the available options, then confirm your selection.",
        "Teammate Status": "A teammate with \u201cPending\u201d status has received an invitation to join Analytics, but has joined the organization and / or project. The user should check their email inbox to complete their registration. A teammate with \u201cActive\u201d status has joined the organization and / or project. No further action is required.",
        "account": "Each customer has at least one mParticle account, which contains one or more workspaces. All accounts for the same customer are contained within an organization (org). These three logical containers control different types of scope. For example, The Profile API is set to workspace scope, while the Platform API is set to account scope.",
        "act-as feed": "A feed you can configure as if it comes from an iOS, Android, or Web platform (Act as Platformoption in the Feed Configuration). Data from the feed can be forwarded to any output that supports the specified platform type.",
        "alias": "Aliasing is a feature that allows clients to associate anonymous customer events to post-sign up events.  Functionally, aliasing performs a copy operation from a source MPID to target MPID. The source MPID is unchanged and still accessible in the system. This feature is supported by the Profile Link and Profile Conversion identity strategies.",
        "ARN": "Amazon Resource Name. A complete ARN is required for partner lambda integrations and some Amazon Redshift integrations.",
        "attribute": "A key-value pair that provides additional information about an event, user, or product. For example, a custom event Play Video might have the attribute ofcategorywith a value ofdocumentary.",
        "audience": "A set of users connected to an integration for the purpose of engaging those users. Audiences may bereal-timeorstandard: real-timeaudiences are populated based on recently received data.standardaudiences are populated from historical data. If the type of audience isn\u2019t specified, then the reference is likely toreal-timeaudiences unless stated otherwise.",
        "audience real-time lookback window": "A date range for how far back you can look to create real-time (not standard) audience segments, apply event enrichment of profiles, and to keep calculated attribute values up to date after initiatilization. Most lookback windows are 30, 60, or 90 days. Lookback windows are defined in the service agreement and are sometimes referred to as \u201chot storage.\u201d Contrast withdata retention.",
        "AWS": "Amazon Web Services. mParticle accounts are assigned to an AWS region that provides optimal performance.",
        "batch": "The basic processing unit for all mParticle data. A batch contains data about a single user of your app, on a maximum of one device. And includes an array of events along with information about the user and device. You can inspect raw batches in JSON format in the Live Stream and User Activity View.",
        "Beta release": "An early release of mParticle products or features. Seereleasesfor more details.",
        "calculated attribute": "A read-only user attribute with a value that is automatically calculated as new event data is received. Examples of calculated attributes include a total count of events, aggregation of events, the discrete occurrence of events, or lists of unique event attributes.",
        "certified partner": "A company that isa certified solutions partner or technology partnerwith mParticle.",
        "channel": "The type of input by which a batch reached mParticle. Not to be confused with platform. For example, a batch for the Android platform can arrive via three different channels: the SDK, the server-to-server Event API, or an \u2018act as\u2019 partner feed.",
        "client-side": "Data forwarded directly from a device or web browser to an integration partner. Client-side integrations often require a kit to be included with the mParticle SDK. Some client-side kits have a configuration option to work in tandem with a server-side integration. Contrast withserver-side.",
        "cold storage": "Seedata retention.",
        "commerce events": "A special mParticle event type that tracks actions related to products and promotions. Examples of commerce events are Add to Cart, Purchase and Refund.",
        "configuration settings and connection settings": "Settings for event and audience integrations are split into two sections: configuration settings and connection settings. Configuration settings define an output and are reused for each connection.Connection settings are specific to the input (platform, feed, or audience) being connected.",
        "connection": "A configuration that defines how data flows into mParticle (input) or is forwarded out of it (output).",
        "consent": "mParticle lets you track a user\u2019s consent for their data to be captured. Consent is tracked according to a predefined consent framework. mParticle supports the GDPR and CCPA frameworks.",
        "Cortex": "*For existing customers, the appointment of Deel, Inc. is effective as of August 30, 2024. For new customers, the appointment of this subprocessor is effective immediately. **For existing customers, the appointment of Pivoting Owl, Inc. (Thena) is effective as of August 4, 2024. For new customers, the appointment of this subprocessor is effective immediately.",
        "credentials": "A key and secret used to access the mParticle Events API.",
        "custom event": "An event type that can capture any type of user activity in your app. A basic custom event contains a name, a custom event type, and a free-form map of attributes. See alsoatribute.",
        "custom feed": "A feed from any data source including the mParticle Events API. Contrast withact-as feed,unbound feed, orplatform input.",
        "custom mapping": "The relationship between a custom event, screen view, or commerce event and the corresponding event in the integration partner.",
        "data map": "A definition of how one data model equates to another data model in either the same or a different data store.",
        "data model": "A definition of how data objects are structured. In Warehouse Sync, a data model may include a data map.",
        "data plan": "A codified set of expectations about the extent and shape of your data collected with mParticle. Data plans contain data points and metadata: a plan name, plan ID, version, and description.",
        "data point": "An event, user attribute, or user identity that is unique within an mParticle workspace, defined for each type of data received from an input.",
        "data privacy controls": "A set of mParticle features for working with consent and data subject requests.",
        "data retention": "The maximum period of time that mParticle stores profile and event data. The duration of the time period is governed by your long-term data retention policy, which is defined in your contract. Contrast withaudience real-time lookback window.",
        "data type": "The type of data contained in an attribute value. mParticle supports the following data types: string, number, boolean, and date.",
        "data warehouse": "A type of integration partner, such as Snowflake, Google BigQuery, and Amazon Redshift.",
        "development (DEV)": "Seeenvironment.",
        "device application stamp": "A unique identifier generated for each unique device the first time it is seen on a given platform in an mParticle workspace. Some event outputs use the Device Application Stamp (DAS) as part of a fallback strategy when other identities are not available.",
        "device standard": "Term for the device used to access your app or website. Examples of devices include an iPhone, an Android phone, a web browser, or an XBox.",
        "DSR": "From the GDPR specification, a data subject request.",
        "Early Access (EA) release": "An early release of mParticle products or features. Seereleasesfor more details.",
        "environment": "Each event batch is associated with an environment: eitherdevelopment(DEV) orproduction(PROD). All development data can be inspected in the Live Stream to enable debugging. You can also create separate event outputs to handle development and production data.",
        "event": "An event is a tracked user action. Examples of events are a user loading a page, clicking a button, or opening an email. Every analysis in Analytics starts with at least one event to analyze behavioral patterns. You can think of events as the \u201cwhat\u201d that a user has done.",
        "feed": "A stream of data into mParticle from either your own data source or a partner. Seeact-as feed,custom feed,platform input, andunbound feed.",
        "field transformation": "Specifically for Warehouse Sync, a field transformation is a data map between an external data source and mParticle\u2019s JSON schema. Field transformations define which individual key:value pairs of data in an external data source correlate to which key:value pairs of data in mParticle.",
        "filter": "A definition that blocks a data point from being forwarded to a particular output.",
        "forward": "Send data from an input to an output.",
        "Generally Available (GA) release": "The release of mParticle products or features that are typically available to all customers. Seereleasesfor more details.",
        "GDPR": "The General Data Protection Regulation is a set of regulations passed by the European Union. mParticle provides two features to help clients manage their obligations under the GDPR: Consent Management, and Data Subject Request processing.",
        "hot storage": "Seeaudience real-time lookback window.",
        "IAM": "AWS Identity and Access Management. Using a custom AWS Lambda functionARNto apply rules in mParticle requires the configuration of an IAM User and IAM Role.",
        "identity priorities": "The order of precedence for matching user profiles. See alsoidentity strategyandIDSync.",
        "identity strategy": "The strategy that determines which user profile to add data to when the current user (known user) can be identified, and what to do when the current user can\u2019t be identified (anonymous user). You are assigned an identity strategy when your org is created. See alsoidentity prioritiesandIDSync.",
        "IDFA": "Identifier for advertisers on iPhones. An Apple IDFA is similar to an advertising cookie, in that it enables an advertiser to understand that a user of a particular phone has taken an action like a click or an app install.",
        "IDSync": "A set of mParticle features for managing how you identify your users across devices:identity strategy,identity priorities, and the Identity API.",
        "input": "The configuration that defines how a partner sends data to an output. Inputs may be one of several types: Platform inputs capture data sent by mParticle partners from an operating-system-specific device or the web. For example, \u2018iOS\u2019, \u2018Android\u2019, or \u2018web.\u2019Feeds capture data sent by mParticle partners using feed integrations. There are several types of feeds:act-as feed,custom feed, andunbound feed.",
        "install": "A data point tracked by many mParticle partners, representing the action of a user installing the app on their device. In mParticle, an install corresponds to an Application State Transition event, of typeApplication Launch, where the attributeis_first_runistrue.",
        "integration": "The flow of data from one of mParticle\u2019s partners to another. Types of integration include: event, audience, data warehouse, feed, data subject request, and cookie sync. Also referred to asintegration partnerorintegration service.",
        "kit": "A component you add to an mParticle SDK that communicates directly with an integration partner from the app client. Usually the kit includes some or all of the partner\u2019s own client-side SDK. Kits are not the same as SDKs. Also referred to as embedded kits. Kits are typically not needed for server-side integrations.",
        "mapping": "Each of mParticle\u2019s integration partners uses a slightly different data structure, with different names for key data points. Mapping is the process of transforming mParticle data into a format that can be used by a partner, and vice versa. For some integration, mapping is customizable. For example, if a partner only collects one user ID, you may need to decide which mParticle identity type to map to the partner\u2019s user ID. See alsocustom mapping.",
        "Metered Integration": "An integration type in which mParticle runs a secondary processing service for mapping data. These integrations consume credits and are enabled by the mParticle account team.",
        "MPID": "A unique identifier (64 bit signed integer) that each user is assigned in mParticle to aid in processing identity and profile data.",
        "MAU*": "Monthly active users.",
        "MTU": "Monthly tracked user, a measurement used in mParticle billing. An MTU is any profile stored in mParticle that has been updated or has generated at least one tracked event within a calendar month. Contrast withVBP.",
        "organization (org)": "Each customer of mParticle is assigned an org, which contains one or more accounts. An account contains one or more workspaces. Different features of mParticle are scoped to org, account, and workspace.",
        "output": "The configuration that defines how a service receives data from an input via either mParticle servers or directly from the client.",
        "partner": "Apps and services that can receive data from, or forward data to, mParticle via an integration. Downstream partners are connected by an output configuration to mParticle, and upstream partners are connected by an input configuration. Also referred to as \u201cintegration partner.\u201dA company that isa certified solutions partner or technology partnerwith mParticle.",
        "pipeline": "Generally speaking, a pipeline is a data definition describing the data that flows continuously from a source to a destination. Cortex machine-learning pipelines transform raw data into machine learning predictions. These pipelines are used in the CDP to create user predictions.Warehouse Sync pipelines ingest predefined selections of data into mParticle from databases in external warehouses. Warehouse Sync pipelines can be configured to run automatically according to a schedule, or they can be configured and run manually.",
        "platform input": "An operating system such as iOS, Android, Roku, or the web that serves as an input. Contrast withact-as feedorcustom feedorunbound feed.",
        "product": "mParticle representation of a physical or virtual product or service that your users can buy. Products are referenced in Commerce events.",
        "production (PROD)": "Seeenvironment.",
        "premium feature": "A feature of mParticle that requires an additional license. Submit a request tomParticle Supportto request a premium feature.",
        "profile": "A complete record of what you\u2019ve learned about a given user over time, across all channels, continuously updated and maintained in real time as new data is captured.",
        "purchase": "A type of commerce event captured when a user of your app buys one or more products.",
        "real-time audience": "A set of users connected to an integration for the purpose of engaging those users. Real-time audiences are populated based on recently received data. Contrast withstandard audience.",
        "releases": "mParticle has two types of releases: BetaAn initial release of products or features thatmParticle expects to make generally available.mParticle typically offers Beta release functionality free of charge to customers who want to test and provide feedback on future functionality.General Availability (GA)A release of products or features that have been fully tested and validated for scalability, quality, and usability. Any product or feature not labeled Beta or Early Access (EA) in documentation is a GA release. GA features are rolled out to customers over a period of time.GA release functionality is available for purchase to all customers.",
        "rule": "Rules allow you to cleanse, enrich and transform your incoming data before it is forwarded.",
        "screen event": "An event type used for tracking navigation within an app.",
        "SDK": "A code library created and maintained by mParticle to track data in your native and web apps. Note that the preferred terminology varies between platforms. This includes native SDKs for iOS and Android, a JavaScript snippet on Web and various libraries, modules, and plugins used for mobile development frameworks like Xamarin and React Native.",
        "server-side": "Data forwarded from mParticle servers to an integration partner, rather than directly from a client (such as a mobile device). Server-side integrations typically do not require that a kit be added to the mParticle SDK.",
        "server-to-server": "A channel for incoming data such as the Event API.",
        "standard audience": "A premium feature that enables you to define and build audiences based on long-term historical data. Contrast withreal-time audience.",
        "UAV": "SeeUser Activity View.",
        "unbound feed": "A feed that can\u2019t be configured to behave as if it came from a specified platform (there is noAct as Platformoption in the Feed Configuration). Contrast withact-as feed.",
        "user": "The person or system who caused an event to occur. Users may be anonymous or known.Someone who has access to the mParticle system is an mParticle user.",
        "User attribute change (UAC)": "A user attribute change event: anevent_type : attribute_change_event. An SDK uploads an event whenever a user attribute changes to denote new attributes, changing attributes, and removed attributes. This allows for calculation of the current user attribute state for each event within an mParticle upload.",
        "User Activity View (UAV)": "The page in the Activity section of mParticle that allows you to view a detailed summary of data associated with a single user.",
        "user profile": "Seeprofile.",
        "VBP": "Value-based pricing (VBP), an alternative toMTUmParticle billing. VBP is aligned with a customer\u2019s usage and scales as the customer\u2019s needs grow.",
        "workspace": "A workspace is the basic container for data in an mParticle account. An account has one workspace already created; more can be created at any time. These logical containers control different types of scope. For example, the Profile API is set to workspace scope, while the Platform API is set to account scope.",
        "event property": "Event properties describe the context of an event. For example, event properties for the event \u201cButton Click\u201d could include the device type used to perform the event, the time zone the event was performed in, or the web browser through which the event was performed.",
        "user property": "Analytics isn\u2019t just about tracking events, it\u2019s also about tracking user. User Properties are the properties associated with the user performing an event, such as demographic factors, an email address, or the marketing channel through which the user was originally acquired. While event properties can differ from event to event, user properties are associated with every event performed by a given user.",
        "query builder": "Every analysis in Analytics is built in thequery builder. Here, you may combine events, event properties, and user properties to create and visualize an analysis.",
        "query row": "Aquery rowis a section within a query, and contains events, event properties, and/or user properties that will determine the analysis. A query may be composed of one or more query rows.",
        "value": "When an event property or user property is broken apart into its components, these components are referred to as values. For example, when looking at the property \u201cPlatform\u201d, \u201ciPhone\u201d and \u201cAndroid\u201d are potential values.",
        "numeric value": "A value that contains only numbers. Numeric values may be used in calculations. For example, values under the property \u201cPurchase Type\u201d containing specific price information are numeric values.",
        "string value": "A value that contains letters, numbers, or other characters. String values are not used in calculations. For example, a User ID consisting only of numbers is a string value.",
        "widget": "Awidgetis a module on adashboardwithin Analytics. Widgets provide both access to and results from saved analyses. Any analysis built inSegmentation, andCohortmay be added to a dashboard as a widget.",
        "Period vs Interval": "If you select\u00a0Period, Analytics will compare your query results with the results from the immediately preceding date range. For example, if your query\u2019s date range is Last Full Week, then Analytics will calculate the results for the last full week and also the previous full week \u2014 for example, \u201cthere were 18 fewer signups last week than there were the week before last week.\u201d Both results will be displayed within the chart area, with the previous range appearing in gray. If you select\u00a0Interval, Analytics will compare each interval within your results to the immediately preceding interval. For example, if your query displays daily intervals, then Analytics will calculate the difference each day \u2014 for example, \u201cthere were 12% more fewer signups on Friday than there were on Thursday.\u201d",
        "Count vs Percent": "If you select\u00a0Count, Analytics will calculate the net change in absolute numbers \u2014 for example, \u201cthere were 56 more purchases this week than there were last week.\u201d If you select\u00a0Percent, Analytics will calculate the net change as a percentage \u2014 for example, \u201cthere were 35% more purchases this week than there were last week.\u201d",
        "Trends Display": "The net change is displayed in the upper right corner of the results field. If your query results are higher than the previous period, then the text will be green and the arrow will point upward. If your query results are lower than the previous period, then the text will be red and the arrow will point downward. This indicates the direction of travel. Hover over the result to see a definition of the previous date range. To move this display, simply click to toggle between the upper right and upper left corners of the chart area. If your query contains multiple rows, you may toggle between trends by hovering over the relevant row name in the chart legend or by hovering over the relevant data points in the results field. If you\u2019re using a table visualization, sometimes you will see a dash \u2019-\u2019 in the change column (represented by \u0394). That does not necessarily mean that there is no change between the two date ranges, but rather that there isn\u2019t enough data for Analytics to provide you with the change. Circumstances that could produce this result include: The data point did not exist in the previous period.The data point did exist in the previous period, but was not among the top/bottom breakouts for the previous period.",
        "List view": "You can use the Catalog\u2019s List view to: View a centralized listing of all the data points.Spot data points which are duplicated, inconsistently named, etc.Identify and eliminate unnecessary or redundant data points The list view displays six main categories: Custom EventsScreen ViewsCommerceUser InformationApplication LifecycleConsent ",
        "Date Range, Search, and Filter": "You can filter the list view to display specific data points: Date Range: show data points that have been seen within a selected date range.Search: show data points with a matching name or description.Filter icon: show data points that match the criteria you specify:Input/App Version: show data points that have been seen for the selected inputs/app versions.Environment: show data points that have been seen in thedevorprodenvironments.Channel: show data points that have been seen for the selected channel. Channel is distinct from input and describes how a data point arrived at mParticle. For example, a data point may arrive from the client side, server side, or from a partner feed. Valid channels include:SDKFeedServer to ServerPixel Combine date ranges and filters with your search terms to quickly browse and explore data points. Setting a filter will also clear any current category selection.",
        "Details view": "The details view gives you detailed information on an individual data point, including environments the event has been captured for, and when an event was last seen for each platform. Users withadminaccess can annotate data points in the following ways: Tags: a freeform list of labels you\u2019d like to associate to the data point.External Link: a link to your wiki or any other resource containing documentation about the data pointDescription: a custom text field where you can describe the data point, expected attributes, how it\u2019s used, and any other relevant information.Additional Names: a list of alternate names the data point is known by. For example, legacy names or names from a partner feed.",
        "Data Point Attributes": "Your event data points may include attributes, and the details view shows every attribute name that has ever been seen within the given data point. You can see the total volume received in the last 30 days, when the attribute was last seen, and thedetecteddata type. The supported detection types are: StringString-listBooleanNumberDate-time ",
        "Stats view": "For data points, the stats view shows two important groups of statistics for a selected date: Inputstats show how many instances of the event have been received, by platform and channel.Outputstats show the volume sent to each output, as well as the delta between the number of events received and outgoing messages sent. This delta can be useful for troubleshooting, but note that the difference between volume sent and received usually doesn\u2019t indicate a problem. Expansion of eCommerce events can cause multiple messages to be sent to an output for a single event. Likewise, filtering or an output partners minimum requirements can cause mParticle not to forward every event we receive. ",
        "User continuity": "A common scenario for a media or ecommerce app goes something like this: User downloads an app but has not yet registered an account.User browses around in the app. Visits screens A, B and C. Data collection begins, but since the user has not yet registered, this activity is stored against an anonymous user profile identified only by an anonymous ID (such as a device ID).User decides to register for an account, creating a new logged-in user profile, and continues to use the app while signed in. Visits screens D and E, and buys product F. How should the data from this interaction be organized? There are two basic approaches: Link the new logged-in user profile with the original anonymous one. This approach yields a continuous view of the user journey.At the moment of user registration, create a new user profile and keep the post-signup activity completely separate from the pre-signup activity. There are compelling business and legal arguments for and against each approach. By choosing the first approach, you have a chance to preserve a complete history of a user\u2019s experience with your app. This might be invaluable for improving your funnel. However, you also introduce the possibility of mingling data from several users into a single profile. For example, on a shared device, multiple users might access the app in a pre-signup state. The second approach sacrifices the possibility of collecting a user\u2019s entire history under a single continuous view. However, you can be sure that the data from your logged-in users is never mixed up with data from a different user. Quarantining anonymous data from known user data may also be required by law. IDSync is designed to let you make smart decisions about user continuity that fit the needs of your app and to give you transparency into how user profiles are created and updated.",
        "Cross-device tracking": "Users often interact with an app ecosystem through more than one device. For example, users might interact with an eCommerce app through both a native app and a web browser, or view media content on a web browser, a native app, or a Roku channel. Many apps will want to track events and lifetime value for a user across all platforms, but others will prefer to keep data for each platform separate. IDSync allows mParticle to support both use cases, and to harness 3rd party data to decisively link data generated from your apps with data from other sources, like CRM Feeds.",
        "Cross-app tracking": "Your product ecosystem may be spread not just across multiple platforms, but also multiple apps. Needs for tracking users across multiple apps will vary depending on your business model. For example, a gaming organization might publish dozens of individual games and want to track their user\u2019s LTV across all their apps. By creating workspaces for each app group under the same mParticle account, you can allow them to share a pool of users, and create only one profile per known user, no matter how many of your apps they use. Alternatively, you might wish to define different groups of users for different apps within the same ecosystem. For example, you might have one app for vendors and another for buyers, with a completely different set of metrics for each group. IDSync allows mParticle to support either use case.",
        "Customer experience personalization": "Personalization of customer experience (CX) is a top priority for marketers. Personalization reduces friction and increases conversions by presenting relevant in-context content that increases customer awareness, engagement, and satisfaction. The Immutable Identity Setting enables marketers to use the mParticle Profile API to get the most up-to-date real-time user identities, device identities, user attributes, and audience memberships. The Profile API uses either an identifier with Immutable Identity set or the mParticle Identifier to match a user profile. Additionally, IDSync Search allows marketers to query User Profiles by any known identifier, such as email, mobile phone, or device identity, and return all matched user identity values including the mParticle ID. The mParticle ID can then be used with Profile API to get the values necessary to personalize the customer experience.",
        "Privacy compliance": "The ability to provide evidence that demonstrates that your organization is in regulatory compliance is important to every Chief Privacy Officer and corporate information security executive. GDPR and CCPA data privacy controls and traceability are core to mParticle\u2019s user profile data policies. In addition, the IDSync Search capability can verify that a matching User Profile exists. It can also be used after a GDPR or CCPA User Profile Delete Request has been processed, to validate that the process has completed successfully and thereby validate compliance.",
        "Mutable identities": "Different user identifiers have different lifespans and degrees of specificity. A Customer ID or a social media ID permanently identify a single user, while an IP Address or Session ID may not be sufficient to identify a single user and can change at any time. Other identifiers fall somewhere in between. Email addresses, for example, do identify a single user, but a user may change their email address over time. IDSync gives you the tools to update identifiers for a User Profile without losing that user\u2019s history.",
        "Identity translation": "With mParticle managing all available identities for a user, you\u2019re freed up to focus on your data. One messaging service requires an email address while another needs Push Tokens? Don\u2019t worry about it. Build your messaging audiences in mParticle based on any criteria you need and mParticle will forward the correct identities for each service, as long as they are available.",
        "Save a Cohort to a Dashboard": "If you select \u201cSave into Dashboard,\u201d a pop-up window displays a preview of your cohort query. Here, use the dropdown menu to select the dashboard in which to save your cohort query. If you want to save your query to a new dashboard, you can create a dashboard from the save window. ",
        "Opting in / out": "To opt in to Journeys 2.0, navigate to eitherAudiences > JourneysorAudiences > Real-time, and hover over the text \u201cLEGACY EXPERIENCE\u201d next to the top breadcrumb navigation. Then, click \u201cTry the New Experience\u201d in the modal that appears upon hover:  Once opted in to Journeys 2.0, you can revert to the legacy Audiences experience at any time by hovering over \u201cNEW AUDIENCES EXPERIENCES\u201d in the top navigation and clicking \u201cRevert to Legacy Experience\u201d: ",
        "Journeys landing page": "After opting in, theAudiences > Journeyssection of the UI will display a table that includes every Audience and Journey you have created, sorted byLast Updatedby default.  The Journeys 2.0 landing page represents two main types of customer segments: Single-step JourneysRows with aTotal Audiencesvalue of 1 represent both single-step Journeys that you have created in the past, as well as legacy Real-time Audiences that have been migrated here as single-step Journeys.Multi-step JourneysRows with aTotal Audiencesvalue greater than 1 are multi-step Journeys, which are represented in this view as nested Audiences. Click on the \u201d+\u201d icon next to the name of a multi-step Journey to expand it to expose the individual milestones (or Audiences) the Journey contains. Clicking any of the nested Audiences will bring you to the Journeys canvas, where you can update this or any Audience contained within the Journey.",
        "Audiences vs. Milestones": "Functionally, a Milestone and an audience are the same. Both represent a targeted segment of customers that you can forward to integration partners, run A/B splits on, and otherwise leverage in campaigns. Their only difference is how you create them in mParticle: A Milestoneis an individual step within a journey that you have defined to capture a meaningful moment in the customer lifecycle (i.e. a free trial signup, an email interaction, or an ad click). Each Milestone creates an audience of customers who fit the criteria you have set for it.An audienceis a customer segment that you have created.",
        "Update an existing Journey / Audience": "To update a journey (either single- or multi-step), expand it by clicking the \u201d+\u201d icon to the left of its name, then click on one of the milestones nested beneath it. This will display the Milestone editor within the Journey canvas, where you can update the inclusion criteria for this particular Audience:  Closing the Milestone editor will display the full Journey canvas, which you can use to add additional Journeys features likenew paths,additional milestones, andA/B tests.  At any time after a Journey has been created, you can edit that Journey\u2019s data inputs. First, click on the top node in the Journey which has the heading \u201cJourney Inputs\u201d:  Next, add or remove inputs as you see fit, then clickUpdate. ",
        "Create a new Journey": "The Journeys canvas provides a flexible and powerful tool for creating targeted customer segments. Let\u2019s look at two of the main ways you can use Journeys to create audiences:",
        "Single-Milestone Audiences": "As mentioned above, creating a Milestone within a Journey is functionally equivalent to a creating a Real-time Audience. By using the selection criteria and logical operators within a Milestone, you can create highly targeted customer segments for your campaigns. Let\u2019s explore how to do this. At the Journeys landing page (Audiences > Journeys), click theNew Journeybutton in the top right-hand corner of the screen.  At theCreate Journeymodal, name your Journey, select your inputs, and clickCreate. In the Journey canvas, click the \u201d+\u201d icon followed by theMilestoneoption to create your first (and only) Milestone in the Journey.  For each audience you create, you can select the environment(s) from which you want users in that audience to come from (Production, Development, or both).  Test data in Development may overwrite Production data in partner systems.If you send dummy or anonymized data to your Development environment, you should select the Production environment for any audiences you forward to partners to prevent unwanted overwriting. For example, you send test customer profiles to Development environment with real customer IDs but anonymized emails for privacy purposes, the anonymized emails can overwrite real ones in any system you forward them to.Parent and child audience environment should align.If a parent audience pulls its members from Development and its child pulls its members from Production, the child audience will be empty (and vice versa). In the example below,Wait list Subscribers(child) andInterested and Engaged(parent) both use the Development environment, which is why the child audience has been populated with users. The other child audiences (Lapsed SubscribersandAbandoned Cart Subscribers) both use the Production environment which conflicts with their parent\u2019s environment, and therefore have no users. As you define your audience criteria, a list of suggested matching values will appear based on what you\u2019ve entered. This feature works both when building new audiences and fine-tuning existing ones, helping you save time, reduce manual effort, and improves accuracy. To use this feature, you must have one of the following standard Roles:User,Admin,Audiences-only,Support, orAdmin+Compliance. Alternatively, you can create a Custom Role with any of the following tasks:audiences:draft,audiences:edit,catalog, oraudiences. In the Milestone editor, you can begin adding criteria to target the users you want in your audience. Once you have added your first criterion, you can use the Boolean operatorsAnd,Or, andExcludeto create logical relationships with subsequent criteria.   Continue adding and combining operators to hone in on the precise user segment that matters to your business goals. The Milestone below, for example, uses three criteria and two Boolean operators to create a segment of customers who are on iOS 10.0, have started at least three sessions within the last seven days, who arenotlocated in New York City. ",
        "Nested audiences from multiple Milestones": "Now let\u2019s see another approach to using  Journeys and Milestones to create targeted customer segments, which is is equivalent to creating nested audiences in the legacy Real-time Audiences experience. To explore how this works, let\u2019s first look at a nested audience created with the legacy Real-time Audience builder as an example:  This customer segment uses membership in three separate Audiences\u2013\u2013App Downloaders, Recent Users (Last 30 Days), and Engaged with Ad\u2013\u2013as its membership criteria (which is what makes it a \u201cnested\u201d audience). Now, let\u2019s see how we can recreate this segment as a Journey. First, create a new Journey, then click the \u201d+\u201d icon under the top (and only) node in your Journey to create a new Milestone. For this Milestone, set membership in the first audience from your nested audience (App Downloaders) as the sole inclusion criteria:  Now, create two more Milestones for the remaining two audiences in the original nested audience:  This Journey is now effectively the same customer segment as the nested audience created with the legacy Real-time Audience builder, with the added advantages of being able to add A/B Tests, new paths, and partner integrations at each individual Milestone (or nested audience), and the ability to visualize each Milestone that comprises the segment in one place.",
        "Audience Insights": "TheInsightstab in the audience builder gives you quick visibility into the customers included in your audience. This helps you better understand who you are targeting and ensure your audience meets your business goals. TheInsightstab displays four metrics: Compare your audience size over different time periods to track growth trends and fluctuations. The size metric shows: Daily average audience size per weekCustom date range comparisons (e.g., Last 30 days)Percentage changes between periodsPartial week data clearly marked Analyze the distribution of users across different attributes to understand your audience demographics and characteristics: View user distribution by various attributes (e.g., Location)See percentage breakdowns for each attribute valueCompare relative sizes of different user segmentsFilter and sort distribution data as needed Assess the potential reach of your campaign across different user identifiers: Track identified users per identity type (e.g., Email Address, Customer ID)View coverage percentages for each identifierCompare reach across multiple identification methodsMonitor identity match rates and coverage gaps Find audiences that share users with your current audience to optimize targeting and prevent duplicate messaging: See other active audiences containing the same usersView overlap percentages between audiencesCompare audience sizes and shared user countsIdentify potential audience consolidation opportunities Note:Numbers shown in the Insights tab are sampled by default. For precise figures, use the\u201cRe-run analysis\u201doption to perform a full audience analysis. Each section of theInsightstab helps you make informed decisions about your audience targeting strategy and campaign optimization. Use these metrics to refine your audiences and ensure they align with your marketing objectives.",
        "Activate a Milestone in a campaign": "Once you have created a Milestone that you want to forward to an external tool for use in a campaign, click theConnect Outputbutton in the Audience tile under the Milestone in your Journey, then follow the steps to connect that audience to any of your connected outputs. ",
        "Selective User Attribute forwarding": "You can optionally include additional User Attributes, beyond identities, when forwarding to each Audience output. This enables you to use richer data in your activation platform, such as LTV, lead score or propensity to convert. In the last step of the process to connect an output, select which account and workspace level attributes you would like to forward to that particular tool: ",
        "Organize journeys / audiences with Tags": "As you continue to use create new audiences, you can use tags to keep them organized and allow team members to easily see the purpose of each audience at a glance. For example, you can use tags to group your audiences by campaign type, giving them names likeretargeting,lead gen, andproduct launch. Navigate to the Journeys homepage.Click the+icon next to the name of a Journey to expose its milestones.Click theTagsicon in the row for that milestone.Enter the name of your new tag in the text bar.Note: Tags have an 18 character limit.  To apply your tag, click the name of the new tag below the text bar. To add additional tags, select name of the tag(s) that has already been applied, open the dropdown menu, and click the additional tags you would like to add. Tag names can be used as search queries to return all audiences that have that particular tag applied.",
        "Share Audiences within Journeys between accounts": "You can share Audiences within Journeys between your organization\u2019s accounts, with detailed control over what data is shared. Share data broadly or restrict it to only what\u2019s needed for a campaign. This feature does not affect data shared with third-party tools.",
        "Adjust sharing settings": "At the Journey\u2019s homepage, open theActionsmenu for the Audience you wish to share.SelectShareto open the sharing modal.  In the modal, view accounts the audience is shared with and their sharing permissions. There are four sharing permission levels: Permission Levels| Level | Access Details |\n| --------- | -------- |\n|Owner| Full access to the audience, including editing, audience updates, and connecting outputs. Admins can set permissions. |\n|Private| The audience is invisible to the receiving account. |\n|View only| Visible to the receiving account but cannot connect to outputs. |\n|Usable| Visible to the receiving account and can connect to outputs. Audience definition cannot be edited. |",
        "Share audience with new account": "In the sharing modal, select the+icon.Choose the account and the desired permission settings.",
        "View shared audiences": "To view audiences that have been shared with an account, navigate toSegmentation, then selectShared Audiencesin the left-hand navigation.",
        "Apply a Filter Where Clause": "In order to apply a filter where clause, hover over the query row in which you would like to filter, and select +filter where. Then type to search for a property to filter, and then select your desired property.",
        "Property Value Type": "There are three different property types to consider when using filter where clauses. Numeric:A value that contains only numbers. Numeric values may be used in calculations.String:A value that contains letters, numbers, or other characters. String values are not used in calculations.Date/Time:A value that represents a date or time. Date and Time values may be in the ISO 8601, Unix Time Seconds, or Unix Time Milliseconds format. Property types can be toggled in theEvents and Properties Manager. The first 50 property values auto-populate when choosing Select a Value.",
        "Numeric": "When filtering by a numeric value, select one of the following options: Is equal todisplays data for values equal to that specific property value.Is not equal todisplays data for values that are not equal that specific property value.Is greater thandisplays all data for values greater than the selected property value.Is less thandisplays all data for values less than the selected property value.Is greater than or equal todisplays all data for values that are greater than or equal to the selected property value.Is less than or equal todisplays all data for values that are less than or equal to the selected property value.Is defineddisplays all data where there are values for the selected property value.Is not defineddisplays all data where are no values for a selected property.",
        "String": "When filtering by a string value, select one of the following options: Is equal todisplays data for values equal to that specific property value.Is not equal todisplays data for values that are not equal that specific property value.Containsdisplays data where the string contains the selected property value.Does not containdisplays all data that does not include the selected property value.Is defineddisplays all data where there are values for the selected property value.Is not defineddisplays all data where there are no values for a selected property. If you are filtering by string values, and you have selected either contains or does not contain, you may combine multiple property values. You may select to combine property values using Or or by using And. For example, PetBox may choose to analyze Email Clicked, filtered where Browser Name contains Chrome or Safari. This use case would not work with the and filter where. However, PetBox may choose to analyze Email Clicked, filtered where Marketing Channel contains Social and Content.",
        "Date/Time": "When filtering by a date/time value, select one of the following options: Is equal todisplays data for values equal to that specific property value.Is not equal todisplays data for values that are not equal that specific property value.Is beforedisplays data for values that occur before a specific date or time.Is afterdisplays data for values that occur after a specific date or time.Is betweendisplays data for values that occur in between two specified dates or times.",
        "Combining Filter Where Clauses": "You may also combine multiple filter where clauses. To do so, simply hover over the query row in which you are analyzing, and select \u201c+filter where\u201d again. Then, you must select whether to combine the filter where clauses using and or or. If you choose and, then users must satisfy both filters in order to count in the analysis. If you select or, then users may satisfy only one of the filters in order to count in the analysis.",
        "Access the Annotation Manager": "There are three ways to access the annotation manager. Choose Manage Data in the main menu bar on the left of your screen, then select Annotations on the top menu bar. Annotations settings allows you to view, edit, archive, delete, or subscribe to your existing annotations, add a new annotation and manage categories.From any tool, select the flag dropdown in the top right corner of the chart area. This opens the in-widget annotation manager.From any dashboard, select +New on the top menu bar, then Annotation Widget. This creates a standalone Annotation Timeline widget, displaying annotations throughout the history of your project. ",
        "Add an Annotation": "To add an annotation from the Annotations settings, select +Add Annotation at the top of the table to the left of the search bar. To add an annotation within the in-widget annotation manager, select +New in the top right corner of the manager or click anywhere along the timeline at the bottom of the manager. The Add an Annotation screen contains seven fields. Category and Annotation Name are required fields. Description, Select Events, Select Subscribers, Start Date, and End Date are optional fields. The optional fields will automatically populate with default contents which can be customized at your discretion. ",
        "Manage Annotations": "Use categories to organize your annotations by theme, by team, by product, or by the events that are associated with the annotation. Assign a different color and label for each category. To edit an annotation, select the flag icon in the annotation manager, then choose the pencil icon to open the Edit Annotation screen.",
        "Annotation Timeline Widget": "The Annotation Timeline widget displays all of the annotations throughout the history of your project. It is a standalone widget displayed alongside the other widgets on a dashboard. To add an Annotation Timeline widget, select +New in the dashboard options located at the top right corner of the dashboard, then choose Annotation Widget. The Annotation Timeline widget will be created at the bottom of your dashboard. You may relocate or resize the widget just like any other dashboard widget. You may also interact with the widget to view, edit, delete, or subscribe to your existing annotations, add a new annotation and manage categories. Please note that the widget displays only annotations attached to events that are present in queries on the dashboard.",
        "The Query Builder": " Within the query builder, you\u2019re able to construct a complex data search using a variety of field types. Every Segmentation query begins with an event. Start by setting whether you\u2019d like to calculate the total count of events or the number of users who performed a particular event. Additionally, you may categorize your results by using theGroup Byfunction or narrow your\u00a0search by using a Filter Where function. TheGroup Byand Filter Where function are akin to SQL \u201cgroup by\u201d and \u201cwhere\u201d clauses. You can use event properties, user properties or user segments with any event you select in your segmentation. Event properties are tied to a single instance of an eventGiven a scenario where a user downloads an app on iOS today and then downloads it on Android tomorrow: if the analysis looks at \u201cUsers who performed App Downloads grouped by event property Platform\u201d, the count would be 1 for iOS and 1 for Android.User properties can be chosen on any event regardless if it came in with an event since it\u2019s part of the user\u2019s profile.User properties have a chronological control and can be configured to be measured as the first value or last value. Using the example above, assume user property Platform is set to the last value. If the analysis looks at \u201cUsers who performed App Downloads grouped by user property Platform\u201d, the count would be 1 for Android and 0 for iOS. This is because Android is the last value seen for platform for that user.User Segments cross check user identities from their segment memberships.Special Note: User Segments using a specific time frame can be used with Filter Where to show members of a User Segment to help understand users who performed an action in two different time frames. For example, a User Segment looking at the count of users who were active yesterday can be contrasted with the count of users who were active this month, creating a DAU/MAU analysis. Finally, you can create a combination of events by using additionalFor Clauses(+Did [Not] Perform).",
        "Frequency Query": "You can also run aFrequencyquery to group users into different segments based on the number of times they performed a particular event.",
        "Additional Query Builder Settings": "All queries within Analytics are fully customizable using Settings such as thedate range and interval. Event queries may display results on a per-interval basis or as a cumulative count. There are six different chart types to choose from: Line chartArea chartBar chartStacked bar chartTableMetrics Optional settings includeAnnotations, which mark milestone events, and Data Labels, which communicate the numerical values at each point in the chart. For more advanced analysis, you may create queries with multiple rows, displaying and comparing information of distinct events within the same results. You may also create calculated queries to explore, for example, proportions or percentages using theCalculator. Finally, you may compare results to a previous interval by using theTrendsfunction. Certain analyses run in the past can change based on what user properties are set to since user properties can change over time. A common scenario where changes are seen when running analysis at different points in time, is a direct result of looking at users who were aliased since then. Note on Aliasing: Aliasing runs once on a daily basis and links unknown users to known users. The count of unique users triggering an event seen before the aliasing process may decrease after aliasing finishes processing. You can read more by visiting our docs onuser aliasing.",
        "Helpful Tools": "A few helpful tools are located to the right of the query builder: Data DictionaryProperties ExplorerQuery NotesAssisted Analysis",
        "Receive email notifications": "To start receiving email notifications, emailsupport@mparticle.comand request that you begin receiving failure alerts. Once enabled, you will be notified once per load if the load is at least three standard deviations over the average load time over the last 7 days To add additional people who receive notifications,submit a request.",
        "Stop email notifications": "Respond to an email notification with \u201cunsubscribe\u201d in the body of the email orsubmit a request.",
        "About SSO": "Single Sign-on (SSO) allows a user to log into Analytics using any third-party identity provider that supports SAML 2.0. Examples include Azure Active Directory, Okta, OneLogin, Auth0, and others. SSO is available for Enterprise customers only. The walkthroughsin the examples sectioncover just a handful of the providers that are supported by the platform. Specific steps and processes may differ, but the overall setup process is similar for most identity providers. If you don\u2019t see your provider\u2019s instructions here and have additional questions, please contact Analytics support for further assistance. Note:Users are required to initiate login from Analytics in order to capture all of the information needed to securely complete authentication. Identity Provider initiated login is not supported at this time.",
        "SSO Settings Within Analytics": "As an Organization Admin, navigate to Organization Settings and thenSingle Sign-Onto view, enable, and edit Enterprise Single Sign-On.  With SSO enabled, users do not have to be added to use Analytics app before logging in \u2013 accounts will automatically be created for them as long as they are provisioned in the identity provider.",
        "Add a Folder": "To add a new folder, click on the Add a New Folder icon located in the bottom left corner of the dashboard menu, below your existing folders. You must provide a name for your new folder. If you are a Pro or Enterprise customer, you must select whether the new folder is public or private. ",
        "Create a Dashboard": "A Default Public dashboard is created automatically for every new Analytics user. There are several ways to create a new dashboard. First, you can create a new dashboard from the Saved menu. In the bottom right corner, select Create a Dashboard. Next, provide a name for your new dashboard, provide an optional description, and select a folder to save the dashboard in.  You may also create a new dashboard from any existing dashboard. To save a new dashboard, click the New icon in the top right corner, then select Dashboard. Note that Growth customers are limited to 25 dashboards containing 20 analyses each. Enterprise customers have unlimited dashboards containing 50 analyses each. ",
        "Manage your Dashboard": "View ourManage Dashboards documentationdocumentation for more information about how to customize your dashboard.",
        "Goal/Business Question": "Are cat food purchasers also purchasing cat toys?",
        "Create a Segment of Cat Food Purchasers": "We first need to create auser segmentfor cat food purchasers in Segmentation. You can do so by selecting thePurchase Productevent and forFilter Where, selectProduct Category. In the text box to\nthe right ofis equal to, you should type \u201dFood\u201c.If the start of the query row readsTotal count ofinstead\nofUsers who performed, you should make that change as well.\nYour final results should look like this:Use04/01/xxxx to Todayin your date range selector and\nrun the query. Click on any point and hover overCreate User Segment. Be sure to then click onFrom entire seriesto get the users from the full date range.Below is theCreate a User Segmentmodal. Name the segment\nand provide a description so we can remember what we saved in the future.\nSelect a category or create a new one by typing the name in so you can easily\nfind your user segment in the future. Note theOne-time/Dailytoggle. In this case, we want the user segment\nto update with the most recent results (refreshed daily), so we will selectDaily.",
        "Examine and Extract for Cross-Selling": "All of the prep work is done. You have set up a user segment of cat food purchasers\nand are ready to do your analysis.",
        "Back to our question: Is this group purchasing cat toys?": "Let\u2019s build out our query. We want to see the number ofUsers who performedPurchase Product, usingFilter WhereProduct Categoryis \u201dToys\u201d\nthis time, instead of \u201cFood\u201d.Remember, we want to see if cat food purchasers are buying cat toys so the\nonly thing missing from this query are the cat food purchasers. Click on\nthe newFilter Whereselector and find yourCat Food Purchaserssegment.Now run the query and let\u2019s take a look at the results.",
        "Export your users to send a targeted campaign": "You can export your users to engage with them outside of Analytics in one of\ntwo ways: Download aCSVand upload it to your marketing tools.Connect to ourSegments API(available for Pro and Enterprise users) to make API calls to your automation\ntools. We hope this tutorial gives you ideas on how you can use Analytics to analyze\nyour data and achieve actionable insights from it. If you have any questions\nor comments, please reach out tosupport@mparticle.com.",
        "Query Builder": "To begin a Funnel query, select either Conversion orConversion over Time. Conversion tracks a single customer journey funnel within a defined date range.Conversion over Time plots the funnel on a chart and compares the funnel across time intervals. The unit of measurement in Funnel is always Users who Performed. The first event of a funnel is required \u2014 a user must complete this event to enter the funnel. You may categorize your results by using theGroup Byfunction or theFilter Wherefunction. You may also combine multiple events using a For clause.  Select a different time zone from your project time zone on a per-query basis by toggling the globe icon on the top right of the query screen.  You may choose to visualize your funnel either as a multipath donut or as a stacked bar. Multipath donuts are optimized for comparing multiple funnel paths in the same visualization, whereas stacked bars are better for viewing drop-off. To switch between the two, select the visualization dropdown from the menu bar.  There are a number of available functions for each query row, including Group By, Filter, and Did [Not] Perform. While hovering the cursor over each query row, a number of options appear in the top right corner. You can create aCustom Eventfrom a query row that contains a filter where clause. Duplicate your row for speedier analysis, or minimize your row if it\u2019s taking too much room on your screen. Finally, you \u2026  After completing the first query row, you may add an unlimited number of steps to represent a full user journey. Name your steps on the right hand side of each query row, and reorder the rows by dragging and dropping. Once you have selected all of the events in your journey, select the run query button within the query visualization. ",
        "Customize your funnel": "All funnel queries can be customized further using the menu. Here, you can select thedate rangefor your query. Users in your funnel must enter the journey within the specified date range. You can also establish aConversion Limit. A conversion limit requires all users to complete the funnel within a defined time limit. The limit can be applied to the entire funnel or between each step.  In the settings dropdown, further customize your analysis by selecting aBreakout mode,Funnel DirectionorConversion Precision: Breakouts may be applied to a single step and then distributed throughout the funnel, or they can be applied to each step of the funnel independently.A Forward funnel means that a conversion rate is calculated for each step in the funnel. A Reverse funnel means that the contribution from the preceding step is calculated as the contribution rate for each step in the funnel.A \u201csequential conversion precision\u201d means that users must complete each step in the funnel in chronological order, with distinct timestamps.Finally, an \u201capproximate conversion precision\u201d means that events in the funnel may occur within the same second of the preceding step and still count as converted.  You can also create aMultipath Funnel, and make one or more steps in the funnel optional. To do this, click the pin icon next to a step in the query builder or in the visualization. The results will display multiple paths that a user may complete. You may then analyze multiple user journeys in the same visualization. In the settings dropdown is a setting for Path Exclusivity. In aninclusivemultipath funnel, users completed the steps in the selected funnel, andmay or may nothave completed the optional steps.In anexclusivemultipath funnel, users completed the steps in the selected funnel, anddid notcomplete the optional steps. ",
        "Anonymous and known user profiles": "A user who opens your app and is tracked by mParticle is referred to as the current user. mParticle stores data from the current user\u2019s session in a user profile. IDSync automatically searches for the best profile to use immediately after the current user begins a session. Depending on your identity strategy, if a profile cannot be found using the available user identifiers then mParticle creates a new profile. All user profiles can be either known or anonymous.",
        "Known profiles": "Known profiles have at least one login ID, which is a unique identifier like a customer ID, email address, or phone number. Known user profiles can only be returned in response to an identity request if the request includes at least one matching login ID.",
        "Anonymous profiles": "Anonymous profiles do not have any login IDs. Unless a new user supplies a login ID, they will always be given an anonymous profile.",
        "Transitioning from anonymous to known": "When a user supplies a login ID, IDSync transitions their profile from being anonymous to known. The default behavior for how data collected with the anonymous profile is carried over to the new known profile and whether or not the same MPID is used for the new known profile depends on your identity strategy.",
        "Default IDSync configuration and the profile conversion strategy": "The default IDSync configuration uses the profile conversion strategy. If you have explicitly selected the conversion strategy or your account uses the default configuration, then the appearance of a new login ID adds the login ID to the existing anonymous profile. This means that the new profile is now considered known, but it keeps the same MPID. Any historical data collected with the anonymous profile persists to the known profile.",
        "Profile link strategy": "If you are using the profile link strategy, the appearance of a new login ID results in the creation of a new profile with a new MPID. While the profile link strategy does not carry data from the anonymous to the known profile by default, you can configure your app to execute an alias request which (if successful) will attribute data from the anonymous (or source) profile to the known (destination) profile.",
        "Make an alias request": "The general process for making an alias request is the same regardless of the SDK you are using. To learn how to make an alias request with a specific SDK, refer to the SDK documentation forWeb,Android, andiOS. Remember that the mParticle SDKs always maintain a persistent \u201ccurrent user\u201d, or the user actively engaging with your app. Data from the current user\u2019s session is being associated to a profile, which is either known or anonymous. An alias request includes: MPID of the destination profile: the known profileMPID of the source profile: the anonymous profileStart time: only data collected after this time is aliased to the destination profileEnd time: only data collected up to this time is aliased to the destination profile If you do not specify the start and end time, then all data collected for the source profile will be aliased to the destination profile up to the point the user submits a login ID or your app otherwise submits an alias request. Alias requests are most often made when a user creates or logs into an account, or whenever they provide an identifier configured as a login ID in your account\u2019s IDSync settings. However, you can submit an alias request using the SDKs at any time.",
        "Supported identity strategies": "Aliasing is only available to accounts configured to use either the default identity strategy, the profile link strategy, or the profile conversion strategy.",
        "User profile requirements": "For an alias request to be successful: The source profile must not have been the source profile for a previous alias request with an overlapping start or end date.The source profile must not have been the destination profile for a previous alias request.The destination profile must not have been the source profile for a previous alias request.",
        "1. A user first downloads your app or opens your website": "The initial identification request includes only the device IDs collected automatically by the mParticle SDKAn anonymous user profile with the MPID of 1234 is createdAny events and attributes captured for the user are stored against this profile",
        "2. The user creates an account": "When the user creates an account, a login identity request is sent, including at least one login ID (e.g. an email address)A new known user profile is created with the MPID of 5678The login request returns objects containing information on the previous and current users. At this point, any user attributes or products in the cart (for ecommerce) captured for the anonymous user can be copied to the known user profile",
        "3. An alias request is sent": "The alias request contains four pieces of information:The source (anonymous) user profile MPIDThe destination (known) user profile MPIDA start date (optional) - only events collected after this date are copied to the new profileAn end date (optional) - only events collected before this date are copied to the new profile. The default value is the time the alias request is submitted. If the alias request meets the validation requirements, it will be processed after a 24 hour delay. This delay allows for any late-arriving events from the source profile to be included.",
        "Results of a successful alias request": "A successful request will result in a202 acceptedresponse. Errors are only returned in the cases of failed authorization or exceeded rate limits.",
        "Information from the source profile updates the destination profile": "The first seen date (a value helpful in the mParticle Audience Builder) of the source profile overwrites the first seen date of the destination profile.All events captured for the source profile, between the start date and end date (up to a 90 day period), will be copied to the destination profile.Any install attribution information captured for the source profile will be copied over to the destination profile.",
        "Not all information is automatically copied": "The following information is not copied as a result of an alias request: User identifiers and device IDs are not copied to the destination profile. However, the destination profile should already contain the same device IDs as the source profile, since it should have originated from the same device.User attributes and calculated attributes are not automatically copied as part of an aliasing request.If you are using Data Privacy Controls, consent information is not copied. You need to reobtain consent information from your users after a successful alias request. The mParticle SDKs provide a method for copying user attributes, identities and consent data any time the current user profile changes. For more information see theSDK docsfor iOS, Android, and Web.",
        "Status messages are added to both profiles": "A status message will be added to the source profile indicating that it has been aliased and noting the mParticle ID of the destination profile.A status message will be added to the destination profile, indicating that it has been merged and noting the mParticle ID of the source profile.",
        "General": "The organization display name is defined by the Owner of an organization during their initial account creation. To change your organization display name, click General, then type your new name into the text field, then click Save. ",
        "Projects": "The Projects screen within Organization Settings displays a list of existing projects, and provides the option to open or delete a project using the three-dot menu on the right side of the page.",
        "Default Project Access": "With Default Project Access, select which projects users will receive access to. Existing users retain access to their current projects unless otherwise modified. Project access from an invite takes precedence over Default Project Access. ",
        "Organization Administrator": "Organization administrators have access to all features, all organization projects, organization settings, and project settings within their organization. Only organization administrators may access the organization settings within the Settings menu.",
        "Adding Teammates": " Within the Teammates section, by selecting the New Teammate option, you may invite new teammates, and assign them to projects and a group",
        "Project Permissions": " Within Project Permissions you are able to change a teammate\u2019s group in any of their projects by ticking the checkbox next to the respective project name.",
        "Groups & Feature Permissions": " You can control teammate access permissions for different sections and features of Analytics within the Groups & Feature Permissions section. These groups can then be applied to teammates within the Teammates section withinProject Settings. There are three default groups: Admin, Member, and Read Only. Please note that these default groups cannot be edited or deleted. Default Groups settings (Growth and Enterprise only):There is a no limit to the number of teammates that can be assigned to these default groups Admin:For teammates in leadership rolesFull access to all projects and all features of AnalyticsMay add new teammates to projectsMember:For all general teammates within an OrganizationAccess to assigned projectsMay not access Organization Settings, Project Settings, or TeammatesMay not add new teammatesRead Only:For viewers not associated with an OrganizationView only access to assigned projectsMay not access Organization Settings, Project Settings, or TeammatesMay not use any of the features and tools of AnalyticsMay not add new teammates Growth:Pro users have access to the default three groups. Enterprise:Enterprise owners have access to the default three groups, and access to custom groups. Organization Owners and Organization Admins may create new groups by choosing New Group.",
        "Custom Groups": "  By creating a new group, you may change the access to specific features, settings, and support as shown below:",
        "Export results from a users query": "First, you mustcreate a query in the Users tool.Once your query is ready for export, simply click on the export icon located in the menu bar beneath the query builder, and select Download CSV. A CSV file will be emailed to the email associated with your account. There is no limit to the number of users that can be exported. ",
        "Export users from segmentation, funnel, or cohort": "You can also download a user list from a Segmentation series or point, a specific step in your Funnel, or a Cohort cell. To download a user list from Segmentation, you must firstcreate a query.Once your query is ready for export, click into the point or cell from which you wish to download data. A menu will appear. Then, select \u201cDownload Users in this Point to CSV\u201d. A CSV file will then be sent to the email associated with your account. If you wish to download users from an entire series, you must select \u201cExplore Users\u201d -> \u201cFrom Entire Series\u201d from the aforementioned menu. You will then be redirected to our Users tool, from which you can download users according to the instructions above.  To download a user list from Funnel, you must firstcreate a query.Once you have done so, simply click into the funnel step from which you wish to download data. A menu will appear. Then, select \u201cDownload Users to CSV\u201d. A CSV file will then be sent to the email associated with your account.  To download a user list from Cohort, you must firstcreate a query.Once you have done so, simply click into the point or cell from which you wish to download data. A menu will appear. Then, select \u201cDownload Users to CSV\u201d. A CSV file will then be sent to the email associated with your account. ",
        "Identity resolution": "Every piece of data collected is attributed to a user. This data attribution is stored in a user profile. Identity resolution is the process of determining which user profile incoming data should be added to. This process also includes: Determining if the current user has an existing user profile (or if the user is known).Deciding what to do with the collected data if the user does not have an existing user profile (if the user is anonymous).",
        "How mParticle identifies users": "At a high level, there are three steps in the mParticle identity resolution process:\nAn identification request is made via one of the mParticle platform SDKs or the HTTP API. The identification request includes all available user identities, or identifiers, such as a customer ID, email address, or phone number. mParticle iterates through your account\u2019s identity priority in ascending order, comparing the identifiers included in the request with each identifier in your identity priority. Remember, your identity priority is a list of identifiers organized according to their ability to confidently find the right profile for the user in question. For example, a customer ID or email address is more likely to be unique to a single user than a device ID, because a device ID could be shared by multiple users. Depending on the identity strategy configured for your account, mParticle returns the user profile matching the identifiers provided in the request. If mParticle finds an existing user profile for the user you are trying to identify, then this profile is returned to the SDK or API and the event and user data collected will be attached to this profile. If no existing profiles match the supplied identifiers (according to your identity priority), then mParticle will either create a new user profile to use, or it will do nothing. Whether or not a new profile is created (and how data is attached to that profile) is determined by your identity strategy.",
        "Example: tracking a new user through a signup flow": "Let\u2019s look at an example using the profile conversion identity strategy. Remember that the profile conversion strategy is designed to create a complete record of a user\u2019s journey through a common signup funnel. The following example is broken down into 5 basic stages, beginning with a new user navigating to an ecommerce app and ending the creation of an account while purchasing a product.",
        "1. A new user opens your app": "Imagine that a new user navigates to your ecommerce app and begins to browse different products. Let\u2019s say that the user does not have an account, nor do they create one.",
        "2. mParticle attempts to identify the user": "As soon as the user opens your app, an identity request is automatically made to mParticle to look for a matching user profile. The identity priority for this example is: Customer IDEmail addressUsernamePhone number Since the user hasn\u2019t logged in or created an account yet, no customer ID, email address, or username is provided with the identification request.",
        "3. mParticle creates an anonymous profile for the user": "mParticle iterates through the identity priority, but since no identifiers are provided that match any existing profiles, mParticle creates a new anonymous user profile based on the user\u2019s device ID. Like all user profiles, this new anonymous profile is assigned a unique MPID (mParticle ID). mParticle continues to collect data about the user\u2019s behavior and stores it in the new anonymous profile.",
        "4. The user signs up for an account": "Imagine that the user picks out a product they want, they add it to their cart, and then they begin the purchase flow. Before they complete their purchase, they create an account.",
        "5. mParticle adds the login ID to the anonymous profile, making it a known profile": "At this point, mParticle has finally received a login ID (the username or email address provided when the user signs up). Since the profile conversion strategy is designed to track users through the entire signup funnel (starting with anonymous browsing and ending with account creation), mParticle does not create a new user profile based on the newly supplied login ID. Instead, mParticle adds the provided login ID to the existing anonymous profile created in step 3 that was used to store the user\u2019s browsing activity leading to the final purchase and account creation. This guarantees the creation of a complete record of the customer\u2019s journey, providing more valuable insights into your customer\u2019s behavior and your app\u2019s performance.",
        "Creating a \u201cDid [Not] Perform\u201d Clause": "To use a Did [not] Perform clause, select an event from the Did [not] Perform data dropdown. This article contains information about modifications that will provide you with more ways to understand your users.",
        "Did vs Did Not Do": "After selecting an event in the Did [not] Perform dropdown, decide whether you want to understand the data of users whodidthis event ordid not dothis event. For example: Show the total count of events where users who did\u00a0Subscribe\u00a0alsodidEmail Clicked\u00a0at least once within the prior 7 days.Show the total count of events where users who did\u00a0Subscribedid not doEmail Clicked\u00a0within the prior 7 days. Note: Switch from \u201cTotal count of\u201d to \u201cUsers who performed\u201d to examine the count of users rather than the count of events, but keep in mind that this is the dropdown that determines whether the query returns an event count or user count. When the query is set to \u201cTotal count of\u201d, users may be counted in the results even though they explicitly meet the did not perform clause criteria. This is because, in \u201cTotal count of\u201d mode, the query may count a user multiple times if they satisfy the query criteria. To illustrate this, consider a query measuring \u201cTotal count of Subscribe for users who did not do Email Clicked within the prior 1 hour\u201d. A user who logs on for the first time at 12pm and performs Subscribe will have their action counted because they did not perform Email Clicked within the prior hour. If the same user then performs Email Clicked at 1:30pm, and then performs Subscribe again at 2pm, their action will not be counted twice because this instance does not meet the criteria, as they did do Email Clicked within the hour prior.",
        "Adjusting Event Count": "After adding an event using the Did [not] Perform clause, you can adjust for the number of times users performed this event, such as greater than or equal to one time, less than five times, or greater than three times. For example, you can view the event count for users who did\u00a0Subscribe, and\u00a0also did\u00a0Email Clicked\u00a0greater than three times within the seven days prior to subscribing.",
        "Adjusting Date Range": "After adjusting the event count, customize the date range. The first menu allows you to select whether the Did [not] Perform\u00a0clause event should have occurred prior to the initial event, subsequent to the initial event, or between two selected calendar dates. When using between, the second menu displays a calendar, allowing you to select a specific date range. When using within the prior/subsequent, the second menu allows you to select a preset or custom count based on minutes, hours, days, weeks, or months.",
        "Multiple \u201cFor\u201d Clauses": "Multiple Did [not] Perform\u00a0clauses may be added by selecting additional events from the Did [not] Perform dropdown. As events are added, you can adjust the event count and date range of each additional\u00a0Did [not] Perform\u00a0clause. When using a relative date range, the date range is always relative to the date of the base event in the query andnotthe other\u00a0Did [not] Perform\u00a0clauses. For example, you could view the event count for users who did\u00a0Subscribe\u00a0for users who also did\u00a0Email Clicked\u00a0two times or more within the prior seven days,andalso did\u00a0Blog View\u00a0three times or more within the prior seven days. This will show users who clicked email links and viewed content within seven days before subscribing.",
        "Difference between Funnel and a Did [not] Perform Clause": "The most common method of analyzing a user journey is to use theFunnel tool. The Funnel tool is specifically designed to measure user journeys, and is highly customizable through features such asConversion Precision,Optional Steps, andTracking Properties. However, user journeys combined with aConversion Limitcan be recreated in aSegmentationquery combined with a did [not] perform clause. However, these two methods can yield different results. Consider the following queries:",
        "Did [not] Perform:": "These two queries are measuring the same user journey - a user who does\u00a0Email Clicked, and then within 24 hours also does\u00a0Subscribe. However, the funnel query shows a considerably lower amount of converted users. This is due to the following: A\u00a0Funnel\u00a0query only counts a user once. Therefore, the funnel tool will only consider thefirstinstance of an event.A\u00a0Segmentation\u00a0query will count users more than once. Therefore, the segmentation tool considersallinstances of an event. Consider the following example: User performs\u00a0Email Clicked\u00a0at 10:00 AM on 2/27User performs\u00a0Email Clicked\u00a0at 13:00 PM on 2/27User performs\u00a0Subscribe\u00a0at 11:00 AM on 2/28 The funnel tool will look at the first instance of\u00a0Email Clicked\u00a0(at 10AM), and see that the user did not complete Subscribe within 24 hours of that instance of\u00a0Email Clicked.\u00a0This user is therefore not counted in the funnel analysis. The segmentation tool will look at all instances of\u00a0Email Clicked, and therefore see that the user performed\u00a0Email Clicked\u00a0at 13PM, and\u00a0Subscribe\u00a0within 24 hours of that event. This user is therefore counted in the segmentation analysis. So, when comparing user journeys in Funnel to user journeys in Segmentation, the user count will always be lower in Funnel due to the fact that the Funnel tool only counts each user once, and looks at the first instance of the initial event.",
        "Posting Announcements": "Users with the Project Announcements permission enabled are able to post project-wide announcements. In order to post or edit an announcement, users must select the Edit button in the bottom right corner of the widget. The widget is flexible, so you can post text, tables, link to other areas of the app (like a new dashboard for example), and more! Once you save your changes, the widget will be updated immediately for all of your teammates to see. You can control who can edit and view announcements in theGroups and Permissionspanel of Organization Settings.",
        "Count vs. percent": "If you select \u201cCount,\u201d Analytics will calculate the net change in absolute numbers. For example, \u201cthere were 56 more users who completed the funnel this week than there were last week.\u201d  If you select \u201cPercent,\u201d Analytics will calculate the net change as a percentage. For example, \u201cthere were 35% more users who completed the funnel this week than there were last week.\u201d ",
        "Trends display": "The net change is displayed in the upper left corner of the results field and in the table at the bottom of the results field.  If your funnel results are higher than the previous period, then the text will be green and the arrow will point upward. If your funnel results are lower than the previous period, then the text will be red and the arrow will point downward. This indicates the direction of travel. Hover over the result to see a definition of the previous date range. In the table visualization, sometimes you will see a dash \u2018-\u2019 in the change column (represented by \u0394). That does not necessarily mean that there is no change between the two date ranges but rather, there isn\u2019t enough data for Analytics to provide you with the change. Circumstances that could produce this result include: The data point did not exist in the previous period.The data point did exist in the previous period, but was not among the top/bottom breakouts for the previous period. Trends are not available for conversion over time funnels.",
        "Segmentation Group By Clauses": "You may apply a Group By function to each row in your segmentation query. This\nwill categorize results by each Breakout. Breakouts can be hidden from your visualization by selecting them by name in\nthe field below the visualization area. You may view a single Breakout by double-clicking\non its name in the same field.",
        "Group By Numerical Properties": "InSegmentation, query rows whose events have numeric string properties can be grouped by the sum of or the average of event properties. For example, you can find the sum of all purchase prices of Paid Purchases within the last 7 days. ",
        "Multiple Group By Clauses": "Any query row may be broken out using multiple properties. For example, if Open App is broken out by the properties Browser Name and Marketing Channel, the resulting Breakout will include all possible combinations of these values. In this example, the combinations would include: Chrome | EmailChrome | DirectFirefox | EmailFirefox | Direct And so on.",
        "Funnel Group By Clauses": "You may apply a Group By function to categorize users via event property, user property, or user segment.  To isolate each group\u2019s conversion rate, select the corresponding slice of the donut in the visualization area. The Total Conversion Rate and Average Conversion Time displayed in the top left corner, and the conversion rate in the path between each step, will change to reflect only users that are in the selected breakout. ",
        "Cohort Group By Clauses": "Cohorts are defined by a shared generation or a shared property. A generation is a unit of time, such as a month. A monthly cohort would include all users who entered the cohort during that month. A property is a characteristic or attribute, such as device type. Cohorts defined by device type would include all users with an iPhone, all users with an Android, etc. ",
        "Breakout Display Options": "Queries with a breakout are limited to display a maximum of 50 breakout results. Use the selector in the query builder to show the top or bottom 5-50 results. If there are less than five breakout results, the menu will not appear.  To view more than 50 breakouts, export the data from the tool as a CSV file: Exporting Results in SegmentationExporting Results in FunnelExporting Results from Cohort",
        "Rows in the Query Builder": " Query Rows represent user actions. A row may contain any combination of events, event properties, and user properties. Rows in the query builder are numbered for easy identification in analysis results.",
        "Create a Query Row": " Every row begins with an initial event. To add an event to a query builder, you can click on+ Select an Event Once an event is added, additional clauses and filters may be added: Understand Breakouts with By ClausesUnderstand Filters With Where ClausesModify Filters With And/Or Clauses",
        "Label a Query Row": " Each query row may have a custom annotation added to it. This allows you to organize your analysis, improve readability, and ensure that data points in analysis results are appropriately labeled. These labels remain a part of the query when saved, added to a dashboard, or shared via URL.",
        "Collapse the Query Builder": " To collapse the query builder, choose the collapse icon. It is located on the right hand side of the query builder at the bottom of the query rows. Collapsing the query row does not affect the analysis, it only reduces the size of the query builder. To expand the query builder again, select the expand icon.",
        "Save a Query Row as a Custom Event": " Some query rows may be saved as custom events. Query rows may be saved as custom events if they consist of a single event and/or contain one or more Filter Where clauses. Use custom events to save query rows that you create frequently. To be saved as a custom event, a row: Must contain 1 event, and no For clausesMay contain an unlimited number of Where clausesMust not contain By clauses Once saved, the new event will appear in the Data Panel.",
        "Duplicate a Query Row": " When creating variations of an existing row, Duplicate in the query row menu enables you to easily clone rows for modification.",
        "Deactivate a Query Row": " Hiding a query row excludes it from analysis results, but does not delete it. This can be useful for viewing analysis components individually, or tentatively excluding rows as you progress through an analysis. To deactivate a row, go to the right of the event and select the deactivate button. You can reactivate the row by selecting the Activate button.",
        "Delete a Query Row": " Deleting a query row will permanently remove it from the query. To delete a row, navigate to the right side of an event and choose the delete button.",
        "Export users": "To download a list of users from a funnel, click on any data point or table cell within your results. Within the dropdown, select \u201cExplore Users.\u201d If your query contains a series, you may choose to view users from a single data point or from the entire series. \u201cFrom this Point\u201d creates a list of users from only one interval whereas \u201cFrom Entire Series\u201d creates a list of users from all of the intervals within the date range. Once your Users query results have loaded, simply click on the export icon located in the menu bar beneath the query builder, then select \u201cDownload CSV.\u201d Users interoperability is not available for metric visualizations.  To download a list of users from a single point, click on any data point or table cell within your results. Within the dropdown, select \u201cDownload Users in this Point to CSV.\u201d A list containing all users associated with this data point will be emailed to the address connected to your Indicative account. There is no limit on the number of users that can be exported. ",
        "All users who entered the funnel": "To access a list of all users who entered the funnel during the date range, click on or in the circle representing Step A in the results field. Within the dropdown, select \u201cDownload Users in this Point to CSV.\u201d  Alternatively, you may click on Column A in the table at the bottom of the results field. Be sure to select the row labeled \u201cOverall\u201d to view all users who entered the funnel.  If your funnel contains a breakout, you may choose to view users from a single breakout or from the entire step. \u201cFrom Breakout\u201d creates a list of users from only one breakout whereas \u201cFrom Entire Step\u201d creates a list of users from all of the breakouts. ",
        "Users from subsequent steps": "To access a list of all users who completed a step other than Step A, click on or in the circle representing the relevant step in the results field. Then, click to select the relevant path from the table within the Path Insights dropdown. Within the dropdown, select \u201cDownload Users to CSV.\u201d If your funnel contains a breakout, you may choose to view users from a single breakout within the step whereas \u201cFrom Entire Step\u201d creates a list of users from all of the breakouts within the step.  Alternatively, you may click on the relevant column in the table at the bottom of the results field. To access a list of users from only one breakout within the step, select the relevant cell, taking into account which step (represented by a column) and which breakout (represented by a row). To access a list of users from all of the breakouts within the step, select the row labeled \u201cOverall.\u201d ",
        "Users from a single path in a multistep funnel": "To access a list of all users who completed a single path within a multipath funnel, click on or in the circle representing the last step in the results field. Then, click to select the relevant path from the table within the Path Insights dropdown. Within the dropdown, select \u201cDownload Users to CSV.\u201d The relevant path will be highlighted in the results field upon hover.  It is not possible to select a single path within a multipath funnel from within the table at the bottom of the results field.",
        "Export results": "To download your results as a CSV file, simply click on the export icon located in the menu bar beneath the query builder. The download will contain a CSV version of the table at the bottom of the results field.  To download your results as a CSV file in a conversion over time funnel, simply click on the export icon located in the menu bar beneath the query builder, then select a file type. You may choose to export your analysis results as a CSV file or as a PNG image file. CSV exports are limited to the first 1,000 results. ",
        "Step one: create a calculated attribute": "Within theData Mastersection of your dashboard\u2019s side navigation panel, selectCalculated Attributes, and then select+ Calculated Attribute.Enter the Calculated AttributeNameand an optionalDescription.Select theCategoryof calculation. SeeCalculation Categoriesfor details.ClickNextto display the criteria section.Define the data used to run the calculation.a. Select values from the drop-downs to define and add criteria as needed.For Value Based Pricing customers, only events set toPersonalizecan be used as criteria in a calculated attribute.  Events set to other tiers are grayed out.Some operations require a specific data type to run a calculation. When a selected attribute is incompatible with the operation, a warning message is displayed. If you want to force the use of a specific event attribute, you can continue past the warning and activate the calculated attribute. For example, if you pass in the purchase amount as a string, you can force it to be a number for use in a sum calculation.b. ClickNextto select a date range.To choose the date range for your calculation, click on the Date Range drop-down:SelectSincefrom the dropdown for calculations that must be made from a specific start date within the audience retention period defined in your subscription plan.SelectWithin the lastfor calculations that must be made over a specific rolling time period. Enter a number and specify a time unit ofDaysorWeeksSelectAll Timeto use all the data available per yourlong-term data retention policy. Therefore, selectingAll Timemay incur additional expenses due to larger data volumes.You can't specify a period longer than the period specified in your subscription plan. If you need a longer period of time to match your long-term data retention policy, you can request enablement of [Unlimited Lookback](/guides/platform-guide/data-retention/#data-retention-and-unlimited-lookback).You can seed the calculated attribute with historical data if you selectSinceorAll Timefor your date range. To seed, add the date you want to start using incoming mParticle data\u2014it can be a date different from the last date of seeded data. If you don\u2019t want to seed this calculated attribute, click the X on the right to remove it. For more information about seeding, seeSeed a calculated attribute.Saveyour changes.",
        "Step two: activate a calculated attribute": "A calculated attribute must be activated before mParticle starts calculating its values across your users. To activate a calculated attribute: If it\u2019s not already open, go toData Master > Calculated Attributesand click the calculated attribute to open it.SelectActivate.Once activated, the calculated attribute is immediately available across the mParticle platform. You can then create audiences or set up connections and data filters. When activated, mParticle computes and initializes the value for the calculated attribute. Depending on the date range, volume of data in your workspace, and definition complexity, calculations require different amounts of time before they are available across your customer profiles. While calculating, the UI displays progress of the initialization.",
        "Optional step: using the Calculated Attributes feed": "You can use the default behavior for forwarding calculated attributes forwarding, or you can use\nthe special Calculated Attributes feed.",
        "Forward calculated attributes in event batches (default behavior)": "mParticle automatically enriches incoming batches with active calculated attributes for that user. Like regular user attributes, you can restrict which outputs receive them usingdata filters. ",
        "Forward calculated attributes in the Calculated Attributes Feed": "The Calculated Attributes feed allows you to send calculated attributes downstream whenever they change, without an event from the user. This feed is especially useful for keeping calculated attributes with asynchronous calculations synchronized throughout your stack, and for sending calculated attributes downstream alongside kit integrations. If your output partner supports the Calculated Attributes Feed, the input appears once you have activated a calculated attribute. When a new connection is made to this input, calculated attribute values for users who have not been seen since their asynchronous calculated attributes were calculated are sent. This feed sends an update when calculated attributes change (both synchronous & asynchronous); it does not send user attributes. To control which downstream system receives these updates, connect specific platforms to receive the calculated attribute updates. You can also filter out calculated attributes you do not wish to forward using the platform filters page.  The calculated attributes feed is available with the following partners: Amazon KinesisAmazon Kinesis FirehoseAmazon RedshiftAmazon S3Amazon SNSAmplitudeApache KafkaBrazeGoogle BigQueryGoogle Cloud StorageGoogle Pub/SubMicrosoft Azure Blob StorageMicrosoft Azure Event HubsSlackSnowflakeWebhook",
        "View calculated attributes in Live Stream, User Activity view, or Profile API": "Calculated attributes can be viewed alongside other user attributes in theLive Streamand theUser Activity view, and are accessible via theProfile API. ",
        "Use calculated attributes in audiences": "Use calculated attributes in the Audience builder by selectingUser > Calculated Attributes. Calculated attributes appear as \u2018string\u2019 types at first and then automatically switch to the correct type as they are computed across many users.  You can build audience criteria with a calculated attribute even if it is calculating. After the attribute values are completed for each user, their audience membership is updated. ",
        "Seed a calculated attribute": "Seeding allows you to pass in historical values for calculated attributes that mParticle will build upon as new data arrives without passing in all the raw events. Seeding allows you to seamlessly transition from your own calculations to mParticle\u2019s. To use seeds with a calculated attribute: Define a calculated attribute in the mParticle platform before sending any seeds.The calculated attribute can be in either draft or active state. However, mParticle recommends the calculated attribute be activated after seeds for all users have been sent to mParticle. Once a calculated attribute is activated, mParticle starts calculating it, and thus some users may show inaccurate values until all seeds have been received.Send seeds via theCalculated Attributes Seeding API.Calculated attribute creation or update takes up to five minutes to be included in the mParticle cache. Therefore, if you send seeds immediately after creating or updating a calculated attribute, mParticle may send a NOT FOUND error. Once mParticle has received seeds, mParticle combines them with calculated attribute results based on live data received after the cutoff date.",
        "Calculated attribute changes and seeding": "After seeds have been sent to mParticle, any of the following changes make the previously received seeds invalid and are deleted from mParticle. Calculated attribute nameCalculated attribute calculation type, such as from sum to countSeeding cutoff dateDeleting the calculated attribute",
        "Updating seeds": "To update the seeds after you have sent them to mParticle, send the updated seeds to mParticle again. mParticle overwrites previously received seeds.",
        "Historical data loads require recalculation": "If you load historical data usingthe historical endpoint of the HTTP API, after loading, you must recreate new calculated attributes or update the definition of an existing CA to trigger a recalculation.",
        "Adding Queries to a Dashboard": " In Segmentation, Funnel, and Cohort, you can add a query to your customer journey dashboard by navigating to the Save to Dashboard button in the top right-hand corner. In the Funnel tool, you will be given a few options for how your analysis will display in the dashboard. Select as many as you would like displayed in your dashboard, and each will become a separate widget. ",
        "Rename and Delete Widgets": "First, find the three-dot menu to the upper right of the analysis in a dashboard. Choose Settings to rename the dashboard. Select on the name of the widget to edit the individual analysis.  ",
        "Setting Up Scheduled Reports": "Once you have built your dashboard to your liking, now is when we suggest you schedule reports to be sent on a cadence of your choosing. Users can schedule their dashboard results to be sent to other Analytics users, executives, or external partners and stakeholders. Once a report is created, your dashboard will refresh at the selected date and time, and a PDF will be sent to the selected individuals. In addition to the PDF snapshot, Analytics users may access your dashboard to view results in real time. Any teammate with access to your Analytics project can create a Scheduled Report for an existing dashboard. Now, let\u2019s dive into how to create a Scheduled Report. To create a scheduled report, you must first create a dashboard to send it from. If you already have a dashboard that you\u2019d like to use, navigate to that dashboard using the View dropdown in the top navigation menu. In this case, we will use a previously built dashboard called \u201cTest Dashboard\u201d for the purposes of this demonstration. Once you have selected your desired dashboard and changed your dashboard layout mode to Print Mode, click to open the Reports dropdown in Dashboards settings in the top right of the dashboard. Here, you may view your existing Scheduled Reports or create a new Scheduled Report as shown below:  You are now ready to build a dashboard and share it with your team, congratulations!",
        "Inputs, outputs, and connections": "  ",
        "Audiences": "Consent state can be used to create conditions in the Audience Builder to check a users\u2019 consent state as a requirement for audience inclusion or exclusion. For example, for CCPA you may want to include only users who have NOT opted out of data sale, by adding a criteria like this:  For GDPR, you may want to include only users that have an opt-in consent for a given purpose, shown here as \u2018Advertising\u2019: ",
        "Additional topics": "      You can also visitthe mParticle YouTube channelto view a wide variety of mParticle topics.",
        "Analyzing as a funnel or cohort": " In order to analyze your Journeys query as a Funnel or Cohort, select the event you\u2019d like to start with, and select the Analyze as Funnel/Cohort menu item. Then, you\u2019ll be prompted to select an event in each subsequent step until you reach a singular customer journey. As you click on events the customer journey will be highlighted. When you reach the starting event, your customer journey will load into theFunneltool, or theCohorttool, depending on your selection.",
        "Reverse journeys": "The Reverse Journeys behave the same as regular Journeys. The difference is that they don\u2019t move in chronological order, they move backwards. So its starting step will be the ending step on a regular Journey. A thing to keep in mind is that these reverse journeys are slower to run than regular journeys.",
        "Connection Setup": "Log in to your Snowflake account.Enter theAccount Infointo Analytics.Account Infois everything to the left of.gcp.snowflakecomputing.com/\u2026Enter theWarehousename.Enter theDatabasename.Click into Warehouses and copy theSchema.Enter theTablename.ForAuto-Generated Password, we randomly generate a password for you to use. If you would like to create your own password, please replace the autofilled value in that field.",
        "User Modeling (Aliasing)": "After some basic checks, we can define your users within your data. For more information on User Identification (Aliasing), please refer tothis article. If you choose to enable Aliasing:Unauthenticated ID- Input the field used to identify anonymous users.Authenticated ID- Input the field used to identify known users.If you choose to disable Aliasing, pressDisabled:Unauthenticated ID- Enter the field used to identify your users. All users must have a value for this field. If you have a non-null value that represents null UserID values, please click on theShow Advancedbutton. In this field, please enter these non-null values. ::: success\nAfter this step, we will perform additional checks on your data with the user model that you provided. The checks are: User Hotspot (Is there a single UserID that represents over 40% of your records?)Anti-Hotspot (Does your data have too many unique userIDs? A good events table contains multiple events per user)Aliasing\n- Too many unauthenticated IDs for a single authenticated userID\n- Too many authenticated IDs for a single anonymous ID\n:::",
        "Get some more data": "Up until this point, you\u2019ve been testing your account with a single development build of your app. This works well to establish basic data throughput. The Audiences feature allows you to target segments of your users based on their activity or attributes. So to effectively use Audiences, even at the testing stage, your app needs multiple users! If you\u2019re not ready to enable the mParticle SDKs in your Production app yet, you can either spin up multiple development environments, or try using theEvents APIto supply some test data in bulk.",
        "Create your Audience": "The mPTravel app lets users watch video content about travel destinations. This tutorial creates an audience to allow mPTravel to target users who view content about a paticular destination with deals for that destination.",
        "Create Criteria": "To define an audience, you need to specify some selection criteria. ClickAdd Criteria.Choose the type of criteria you want to create. Except for theUserstype, which is covered below, these criteria all correspond to mParticle event types. ClickEventsto target custom events.There are three distinct aspects of an event criteria that you can define:Event name- mParticle populates a dropdown list based on all event names received for the workspace. This means that you can only select events that have already been captured by mParticle. This example targets the \u201cPlay Video\u201d event name.Attributes- you can refine your criteria further by setting attribute conditions. This example targets only instances of the Play Video event where the \u201ccategory\u201d attribute has a value of \u201cDestination Intro\u201d and the \u201cdestination\u201d attribute has a value of \u201cParis\u201d.Note that this example creates anExact Matchcondition, but there are other types of condition to explore. For example, if you set \u201cdestination\u201dContains\u201cFrance\u201d, then you could match events with a \u201cdestination\u201d of both \u201cParis, France\u201d and \u201cCannes, France\u201d.The types of condition available depend on what kind of data an attribute holds. For example, an attribute that records a number value will haveGreater ThanandLess Thanconditions. mParticle automatically detects what type of data an attribute holds. However, you can manually set the data type by clicking the type symbol.Don\u2019t change the data type unless you really know what you\u2019re doing. If you force the data type to beNumber, and all your attribute values are strings, your condition will always fail! As long as you\u2019re sending the same type of data consistently for each attribute, you shouldn\u2019t have to worry about it.Recency / Frequency- Sets how many times the user needs to trigger a matching event, and in what time period, in order to meet the condition. If you don\u2019t specify anything here, the default forRecency / Frequencyis \u201cGreater than 0 events in the last 30 days\u201d.When you\u2019re happy with your criteria, clickDone.",
        "Add Multiple Criteria": "You could save this audience right now and target all users who have watched mPTravel\u2019s Paris content in the past three days. But, what if you have some extra special limited deals that you want to save for your premium members? You can\u2019t just tell everyone! You need to add a second criteria. Whenever you have multiple criteria, you need to decide how to evaluate them together. There are three options: And- both conditions have to be true for a user to be added to the audienceOr- a user will be added to the audience if either condition is trueExclude- a user will be added only if the first condition is true, but the second is false. Exclude is great for use cases like abandoned cart targeting. You can select users who triggered an Add to Cart event, then exclude users who triggered a Purchase event. To target users who watched Paris content, AND are premium members, chooseAnd.  This is a good opportunity to look at theUsercriteria type, as it\u2019s a little different. Where the other criteria match users who have triggered a particular event, theUsercriteria looks at all other information you might know about your users: the type of device they use, where they live, their custom user attributes, etc. This example targets users with a user attribute of \u201cstatus\u201d, with a value of \u201cPremium\u201d. When you\u2019ve added as many criteria as you need, clickSave as Draftto come back to your definition later, orActivateto start calculating.  When you activate the audience, you\u2019ll be asked if you want to set up an A/B Test. SelectNofor now, to go to the Connections Screen.",
        "Check that size is greater than zero": "After you finish defining your audience you will be taken straight to the Audience Connection screen. Connecting an audience will be covered in the next section. First, check that your audience definition is working as expected. Start by selectingAudiencesfrom the left column to go to the main Audiences page. Audiences take time to calculate, so if you\u2019ve only just activated it, you\u2019ll probably see aSizeof 0 for your audience. Mouseover the pie chart to see how far along the calculation process is.  After a while, as long as you have users that match your criteria, you should start to see the value of theSizecolumn increase.  If the audience is 100% calculated, and your size is still zero, there may be an issue with your conditions.",
        "Download to verify individual memberships": "In some cases, it might be enough just to know that your audience is matching users. However, if you know specific identities of users who should match your criteria, you can check that they matched by downloading your entire audience in CSV form. Follow the instructionshereto download your audience.",
        "Common uses of data privacy controls": "Data privacy controls are flexible and customizable, allowing you to build any data flow or consent-based logic you need. Use mParticle\u2019s data privacy controls to help comply with CCPA\u2019s \u201cdo not sell my data\u201d requirement by collecting users who opt-out and blocking those users\u2019 data from flowing to any \u2018data sale\u2019 output by: Recording a CCPA data sale opt-out as a user consent (more information below)Identify which outputs count as \u2018data sale\u2019 and apply the below forwarding rule to themApplying a forwarding rule of: Do not forward if CCPA Data sale opt out is present GDPR defines consent as one method of lawful data processing. One common setup is to: Define a processing purpose of \u2018marketing\u2019Prompt users for affirmative consent for \u2018marketing\u2019Identify which outputs would perform \u2018analytics\u2019 processingApply a forwarding rule of: Only forward user data if GDPR Consent for \u2018marketing\u2019 is true",
        "Data privacy and the mParticle platform": "Once enabled and configured, data privacy work with the mParticle platform to ingest and pass on consent state: Define categories of data collection called consent purposes.Store the consent state in a user\u2019s profile.Control data flow based on stored consent.Send user consent state to your integrations (outputs). ",
        "Enabling data privacy controls": "Data privacy controls save user consent decisions and applies them to data flows. Enable GDPR and/or CCPA compliance features on your workspace fromWorkspace Settings>Workspace>Regulation.  For GDPR, create a set of purposes fromPrivacy > Privacy Settingsin the dashboard. For CCPA, once it is enabled in your workspace, the purposedata_sale_opt_outis automatically created. The SDKs and mParticle UIs facilitate using this purpose, so you don\u2019t need to hardcode it anywhere. ",
        "Consent properties": "The mParticle format for a single record of a user decision on a privacy prompt,.consent, is ourconsent_stateobject. This is used for both GDPR-style opt-in consent and for CCPA-style opt-out. For each user or workspace, consent state can be stored for each possible combination of regulation and purpose. For each purpose, the following fields are supported. All fields are optional, exceptconsented,timestamp_unixtime_ms,regulationandpurpose. Theregulationandpurposefields are built into the structure. Be sure to include your privacy and compliance experts when deciding how to implement optional fields.",
        "Example consent state": "Consent state can be logged via the HTTP API simply by including a consent state object in a batch, mirroring the structure of the user profile (above):",
        "Collecting consent state": "For detailed definitions of how to report consent state, review the sections of our API and SDK references that cover data privacy controls: Web SDKiOS SDKAndroid SDKAMP SDKHTTP API Additionally, ourintegration with OneTrustallows you to ingest customer consent states into mParticle.",
        "Connections and forwarding rules": "Consent state can be used to create forwarding rules that selectively filter data based on a users consent state, in real time and per-person. For example, you can choose to only forward data from users who have given consent for a particular purpose. For CCPA, you may want a forwarding rule to apply a data sale opt-out. In this example, users\u2019 who have a consent state oftruefor the CCPA purpose ofdata_sale_opt_outwill NOT have their data forwarded (if the consent state is missing or false for that purpose, data will flow): For GDPR, you may want a forwarding rule to only send data when a single purpose is consented: If you set up a Forwarding Rule for an embedded kit integration, the iOS and Android SDKs will check consent status for the user on initialization. If the rule condition fails, the kit will not be initialized. Note that kits are only initialized when a session begins or on user change, so if consent status changes in the course of a session, while mParticle will immediately stop forwarding data to the kit, it is possible that an embedded kit may remain active and independently forwarding data to a partner from the client until the session ends.",
        "Forwarding consent state to partners": "When the consent state of a profile changes, that change can be communicated to mParticle event integrations. If theconsent_stateobject on an incoming event batch contains changes from the existing profile, mParticle adds a \u2018system notification\u2019 to the batch for each consent state change before the batch is sent to incoming forwarders. This notification contains the full old and new consent state objects: There are currently two ways that consent state changes are forwarded to mParticle event integrations: Some partners accept raw event batch data from mParticle, mostly for data storage or custom analytics use cases. For these partners, mParticle will forward the \u2018system_notifications\u2019 object with each relevant event batch. Forwarding of system notifications can be turned off with a user setting. Integrations that can currently receive the system notifications object include:Amazon KinesisAmazon S3Amazon SNSAmazon SQSGoogle Pub/SubGoogle Cloud StorageMicrosoft Azure Event HubsSlackWebhookmParticle is working with other partners to support forwarding consent state changes as a Custom Event. These events contain the new consent state information as custom attributes, a custom event type of\"Consent\", and an event name of\"Consent Given\"or\"Consent Rejected\". These consent events are forwarded to supporting partners as standard custom events.{\"data\":{\"event_name\":\"Consent Given\",\"custom_event_type\":\"Consent\",\"custom_attributes\":{\"consented\":\"true\",\"document\":\"location_collection_agreement_v4\",\"hardware_id\":\"IDFA:a5d934n0-232f-4afc-2e9a-3832d95zc702\",\"purpose\":\"location_collection\",\"location\":\"17 Cherry Tree Lane\",\"regulation\":\"GDPR\",\"timestamp_unixtime_ms\":1523039002083},\"event_type\":\"custom_event\"}}Partners that currently accept these custom consent state events include:AmplitudeSnowplowMixpanelGoogle AnalyticsSalesforce DMP \u201cGDPR Consent Change\u201d is  listed as a data type in theIntegrations directoryand we will update this list as more partners add support. Please reach out to your success manager if you would like to distribute consent to an additional partner.",
        "Data subject requests": "mParticle helps you respond todata subject requestsas mandated by the GDPR and CCPA regulations. You can search for integrations that support data subject requests in theIntegrationspage. Search on categoryData Subject Request.",
        "Ingest GPC signals": "The California Consumer Protection Act (CCPA) and the upcoming CPRA (California Privacy Rights Act) require that users can signal their privacy choices. In support of that requirement, you can ingestGlobal Privacy Control (GPC) signalswith mParticle. Browsers append the GPC signal to HTTP requests and make it queryable via the DOM. This signal is designed to convey a person\u2019s request to websites and services to not sell or share their personal information with third parties, perthe Global Privacy Control specification. This opt-out is at the browser level, allowing users to turn on the GPC signal for all or specific websites. The workflow for ingesting and forwarding GPC signals via SDK or Events API: ",
        "Sample code for GPC": "This sample code show two options: mapping to a GDPR purpose and mapping to a user attribute."
    },
    "Lytics": {
        "Quick Start": "Developer Quickstart1. Install the Lytics Tag2. Content Setup3. Surface Personalized MessageBuilding ProfilesDefault AttributesDefault SegmentsGuides & InspirationLead CaptureContent Recommendations",
        "Account Management": "What is Vault?Accessing AccountsMonitoring Metrics and AlertsJob AlertsMonitoring Audit LogsExporting Audit Logs or Alert JobsMonitoring MetricsUsage MetricsManaging UsersSingle Sign-OnAccount SettingsAccount DetailsJavaScript Tag ConfigPersonalization APIContent ServicesSecurityAI & Modeling ControlsSchema ControlsData PoliciesPlatform LimitsPrivacy and Data ProtectionImpact of Browser Tracking ChangesComplianceAuthorizationsAccess Tokens",
        "Key Concepts": "Identity ResolutionConsent & PrivacyComplying with the Digital Markets Act (DMA)Client & Server Side CookiesContent AffinityContent CurationTopic TaxonomyArchitectureIntegration PatternsJob ProcessingLytics Platform Data Flow and AccessCDI and CDP Implementation StrategyLytics Zero Copy with Cloud Connect",
        "Pipeline & Profiles": "Data PipelineJobs (Data Sources and Exports)Data StreamsStream Routing APITemplatesCreating Templates Using the UICreating Templates using Lytics APIUsing the Jsonnet LibraryTemplate ExamplesSchema ManagementFields & MappingsAdvanced Mapping FunctionsMeta FieldsQueries & LQLIdentity Key RanksSchema VersionsPlanning & TroubleshootingLytics Profile SchemaSchema CopilotIdentityMerge StatisticsIdentity ExplorerIdentity Rules",
        "Warehouse Access": "What is Cloud Connect?Connecting WarehousesData Models & QueriesActivating Data ModelsCloud Connect Troubleshooting and FAQs",
        "Audiences & Activation": "User ProfilesUnderstanding Profile HealthFinding a UserAudiencesAudience GroupsPrebuilt AudiencesReportsWhat are Reports?ComponentsManaging ReportsDashboard ReportContentTopics & AffinitiesTopicsAffinitiesUsing Topics & AffinitiesClassificationEnrichmentRecommendationsDocumentsContent CollectionsCreating Content CollectionsViewing, Using & Managing CollectionsInterest EnginesDefault Interest EngineCustom Interest EnginesCollaborative FiltersLookalike Models & AudiencesGetting StartedBuilding Lookalike ModelsEvaluating Lookalike ModelsCreating Lookalike AudiencesImproving Lookalike ModelsActivationDestinationsGoalsExperiencesBehavioral Scores",
        "Tutorials": "Leveraging User ProfilesAccessing Profiles Client SideAccessing Profiles Server SideWorking with Anonymous ProfilesGet Started with Lytics SegmentsInline Content RecommendationsData Collection & OnboardingWorking with Custom DataWorking with Web DataCollect Mobile Data with Firebase + GTMData ManagementProfile Stitching Best PracticesMigrating from Queries to Conductor SchemaLQL & Data Import BasicsUse CasesAcquire New Customers with LyticsBest Practices for Personalizing Your Ad or Search Landing PageCapture More Information from Qualified LeadsContent Modularization in Email with Lytics AudiencesCustomize your Web Experiences with Branded ImagesDeliver Targeted ContentDrive Email Capture & Engagement with ContentDrive Mobile App Downloads With LyticsEngage Qualified Users with Targeted AdsEnhance Personalized Messaging with User Profile FieldsGrow Your Email Marketing ListImport an Audience from your Data WarehouseImprove Ad Campaign Metrics with Predictive TargetingIncrease Conversions with Lytics and Facebook LookalikeKeep Visitors Engaged with Content Recommendation ExperiencesLeverage Lookalike Models and Predictive AudiencesLookalike Models: Conserve marketing spend on engaged usersLookalike Models: Convert anonymous users to known usersLookalike Models: Convert single purchasers to multi-purchasersLookalike Models: Determine which subscribers are likely to churnMobile Messaging with Lytics Webhooks and Serverless FunctionsOptimize Remarketing SpendPersonalize the Messaging of your Website Based on AudiencePopulate Your Website With One-to-One Content RecommendationsPromote Relevant Content to Users based on their InterestsReach the Right People Using LyticsRetain Existing Customers With LyticsPersonalize your Iterable emails With Lytics content recommendationsBuild a Custom Personalized Experience With Video ContentUnlock Additional Web Personalization Features with Lytics API OverridesHow to Use GTM Tags to Modify Lytics Pathfora Widget BehaviorStyle your Lytics Web Experiences to Fit Your Brand Guidelines",
        "SDKs & Tools": "WebJavaScriptOn-site PersonalizationSubresource Integrity (SRI)MobileiOSReact NativeAndroidChrome Extension",
        "Integrations": "Lytics Integration OptionsAcousticAdobeAdRollAirshipAmplitudeAmazon AdsAmazon KinesisAmazon PinpointAmazon RedshiftAmazon S3Amazon SQSAnsiraBigCommerceBlueKaiBlueshiftBrazeBrevoCampaign MonitorCheetah DigitalClearbitContentfulCordialCriteoCustomer.ioDatabricksDotdigitalDriftEpiserverFullContactGIGYAGoogle OptimizeGoogle Ad Manager (DFP)Google AdsGoogle BigQueryGoogle Cloud Pub/SubGoogle Cloud StorageGoogle DriveGoogle Marketing: Analytics, DV360, CM360Google Cloud OperationsGoogle Tag ManagerHubSpotiContactInsiderIterableJebbitKlaviyoLeadsquaredLinkedInLiveRampLocalyticsLookerLotameLytics File ServiceLytics MonitoringMailchimpMailgunMandrillMappMapp: BlueHornetMarketoMaropostMediaMathMetaMicrosoftMicrosoft AzureMicrosoft TeamsMixpanelNetSuiteNew RelicOmedaOneSignalOneTrustOracle Marketing Cloud: EloquaPinterestPostUpRadarRedditResponsysRetention ScienceSailthruSalesforceSalesforce DMP (Krux)Salesforce Marketing CloudSalesforce PardotSegment.comSelligentSendGridShopifySitecoreSlackSnapchatSnowflakeSparkPostSurveyMonkeySwrveTaboolaTealiumTikTokThe Trade DeskUnified ID 2.0VersiumWebhooksWebhook TemplatesWistiaX AdsYahoo AdsZapierZendeskZuoraContentstack",
        "Partners": "Partner TypesLytics Communication",
        "Legacy": "InsightsImporting External ExperiencesIDP-initiated SSO (legacy)Web PersonalizationIntroductionDrive Traffic CampaignCollect Leads CampaignPresent a Message CampaignRecommend Content CampaignCampaign ReportingCampaign ManagementBuilding Audiences with Campaign Data",
        "Introduction": "Reports can consist of one or manyComponents. Think of aComponentas a method of visualizing data. EachComponentprovides different ways of displaying and drilling into your data. Supported Components: TheSizecomponent allows you to display the sizes of one or more Audiences as a line chart or numeric value. This component helps compare the sizes of different Audiences (ex: for A/B testing).TheCompositioncomponent allows you to understand and chart the distribution of values for a field across your audience in various ways, including Bar, Pie, Line, and Table view.TheAudience Overlapcomponent allows you to understand the intersection or difference in audience members between two or more audiences.TheData Flowcomponent allows you to visualize the flow of data that connects your Providers, Audiences, and Destinations.",
        "Before You Begin": "Before you set up webhooks to go to Lytics make sure you've completed the following prerequisites: Have a Lytics API token with theData Managerrole ready. You can read how togenerate a new API tokenif you do not already have one.Go through theauthorization processand have your authorization ID ready. If you are not currently using webhooks in Contentful, it may help to read through the Contentful documentation onwebhooksandwebhook transformationto familiarize yourself with these concepts.",
        "Getting Started Checklist": "Getting started with Lytics is quick and easy! In just a few minutes, you'll be able to set up Lytics and start personalizing your website. We've focused this guide on the 3 essential steps to ensure a positive experience for you and your customers: 1. Install the Lytics tag on your site.2. Ensuring site content and Lytics are syncing.3. Create your first personalized message.",
        "Digging Deeper": "After completing the initial checklist outlined above, it's time to explore further avenues for enhancing and utilizing your profiles to their fullest potential. We've broken additional guides into two core focuses:",
        "Building Profiles": "Here, we'll gain a comprehensive understanding of all available out-of-the-box attributes. Discover how to tag your site and integrate other sources to create robust and comprehensive profiles. This section is divided into:",
        "Default Attributes & Segments:": "Profile AttributesAudience SegmentsContent Collections",
        "Site Activity & Conversion Tagging:": "Capturing Website Activity (coming soon)Capture Website Conversion Activity (coming soon)",
        "Using Profiles": "Here, we'll explore leveraging out-of-the-box personalization SDKs and APIs to deliver optimal user experiences. Discover how to harness Lytics' tools and integrations to create tailored experiences that resonate with your audience. This section covers:",
        "Guides & Inspiration": "Surface a lead capture form only to unknown visitors.Surface content recommendations based on interests.Surface a promotional message to high-momentum visitors. (coming soon)Sync profiles & audiences to GA4 or meta. (coming soon)Personalize your site based on behaviors and stored attributes. (coming soon)",
        "SDK Documentation": "WebJavaScript SDKPersonalization SDKMobileiOS SDKAndroid SDKReact Native SDK",
        "Build an Audience of High Value Purchasers in Lytics": "For purchase data, this example uses an account that has been connected with Shopify data via Lytics' Shopify import. You may be importing your purchase data to Lytics via some other method such as the Javascript Tag, CSV, or one of our other commerce integrations. When building your audience the custom rule you choose may differ for your account. Follow the steps outlinedin this guideto build source audiences in Lytics. The steps that follow in this guide provide specific instructions on constructing a high value purchasers audience. In the audience builder, selectCustom Rule.If you have imported data via Shopify, search for and select the fieldLifetime Order Price. This field contains the sum of all purchase amounts made by the user.  From the rule optionsLifetime Order Pice must...selectbe at least. In theThese valuestextbox enter a price threshold.  ClickAdd Condition.Verify the user count of your audience. Facebook expects the source audience to be anywhere from 1,000 to 50,000 users. You may need to adjust the price threshold or add additional rules to generate an audience of the appropriate size and specificity. ClickCreateto save your audience.",
        "Using your Lytics Audience in a Lookalike Ad Campaign": "Follow through with the steps below to use your high value purchasers audience for the source of a Facebook lookalike ads campaign: Export to FacebookCreate a Facebook Lookalike AudienceAssign your Lookalike Audience to an Ad ",
        "What is a Cookie": "An HTTP cookie (web cookie, browser cookie) is a small piece of data that a server sends to a user's web browser. The browser may store the cookie and send it back to the same server with later requests. Typically, an HTTP cookie is used to tell if two requests come from the same browser\u2014keeping a user logged in, for example. It remembers stateful information for thestatelessHTTP protocol.",
        "First-party": "First-party cookies are created by the host domain \u2013 the domain the user is visiting. These cookies are generally considered good; they help provide a better user experience and keep the session open. This means the browser can remember key information, such as items you add to shopping carts, username, and language preferences.",
        "Third-party": "Third-party cookies are those created by domains other than the one the user is visiting at the time and are mainly used for tracking and online-advertising purposes. They also allow website owners to provide certain services, such as live chats.",
        "Session management": "Logins, shopping carts, game scores, or anything else the server should remember.",
        "Personalization": "In the \"Personalization\" section of the extension, users gain access to a log of all active Lytics experiences and campaigns, along with their associated details. This feature proves invaluable when debugging the configuration of experiences or ensuring that overrides have produced the expected results. Users can easily track the performance and behavior of each personalized element on their website by providing a comprehensive overview of active campaigns and experiences, including their parameters and settings. This functionality streamlines debugging and empowers users to fine-tune their personalization strategies for optimal effectiveness.",
        "Tracking": "Recording and analyzing user behavior.",
        "How does Lytics use cookies?": "Cookies are one of the preferred methods for maintaining a user identifier in the browser, both known and anonymous. This gets associated with all inbound events captured by our Javascript tag as they interact with customer websites. Specifically, our Javascript tag stores a string of digits and characters that are used as a unique ID commonly referred to as our_uidor, in some casesseeridas a first-party cookie.",
        "Where do cookies come from?": "In web development, the client-side refers to everything in a web application displayed on the client (end-user device). This includes what the user sees, such as text, images, and the rest of the UI, along with any actions an application performs within the user's browser. When someone refers to a \u201cclient-side cookie,\u201d they are generally referring to a cookie that is created and/or managed via a common client-side programming language such as Javascript. Like with client-side, server-side means everything that happens on the server instead of on the client. In the past, nearly all business logic ran on the server-side, including rendering dynamic webpages, interacting with databases, identity authentication, and push notifications. When someone refers to a \u201cserver-side cookie,\u201d they are generally referring to a cookie that is created and managed using one of the many common server-side programming languages such as NodeJS, PHP, Python, etc.",
        "Cookie Ingredients": "Defines the cookie name. Generally, the cookie name is the primary way of retrieving a cookie and its associated value and attributes. The stored value for the cookie. This can include any US-ASCII character excluding a control character, whitespace, double quotes, comma, semicolon, and backslash. Indicates the maximum lifetime of the cookie as an HTTP-date timestamp. SeeDatefor the required formatting. Indicates the number of seconds until the cookie expires. A zero or negative number will expire the cookie immediately. Defines the host to which the cookie will be sent. Indicates the path that must exist in the requested URL for the browser to send theCookieheader. This indicates that the cookie is sent to the server only when a request is made with theHTTPS:scheme (except on localhost) and, therefore, is more resistant toman-in-the-middle attacks. Forbids JavaScript from accessing the cookie, for example, through theDocument.cookieproperty. Note that a cookie created withHttpOnlywill still be sent with JavaScript-initiated requests, for example, when callingXMLHttpRequest.send()orfetch(). This mitigates attacks against cross-site scripting (XSS). Controls whether or not a cookie is sent with cross-origin requests, providing some protection against cross-site request forgery attacks (CSRF).",
        "Who\u2019s taking away my cookies?!?": "In June 2017, Apple introduced a new privacy feature calledIntelligent Tracking Prevention (ITP). This same feature was officially released in September 2017 with Safari 12 and iOS 11. Since then, the ITP has evolved and introduced several subsequent versions leading us to the current state (as of July 2022), which has many impacts on marketing but most notably: ITP blocks all third-party cookies by default. (ITP 1.0/1.1)ITP can grant exceptions to third-party cookies with Storage API. (ITP 2.0)ITP caps all first-party cookies set with JavaScript to 7 days or 24 hours. (ITP 2.1/2.2)ITP caps first-party cookies set by the server using CNAME cloaking to 7 days. (ITP 2.3) As a result of the privacy and security efforts in general other browsers such as Mozilla\u2019s Firefox(ETP) and Google Chrome have followed suit in announcing and/or implementing their security and tracking protocols, which continue to impact tools such as third and first-party cookies that have long been a staple in providing the data necessary for marketers to personalize their communications effectively.",
        "What can I do to replace my cookies?": "With each iteration on more stringent privacy-related changes comes a wave of workarounds or alternate approaches to maintaining access to behavioral data essential to marketing:",
        "1. Deploy a strong first-party identification strategy.": "There is no replacement then a strong identification strategy. Creating a relationship in which a user will openly share their identity through a login or some other authenticated means will always result in the highest level of certainty on identity, which leads to the best level of personalization. However, many use cases focus on anonymous users or users who have not built up the relationship necessary to unlock this level of authentication.",
        "2. First-party client-side cookies have changed but are still a viable solution.": "Though third-party cookies are effectively dead, first-party cookies are still viable for many use cases. Not only do they offer a very simple off-the-shelf type of implementation for leveraging, they also have a long shelf life, assuming that an anonymous user interacts frequently enough to overcome the 7-day expiration window.",
        "3. First-party server-side cookies offer an extended expiration window.": "Over the past 12 months, there has been a surge of interest in server-side cookies. This method for setting cookies currently is not affected by the ITP changes that impact client-side cookies, most notably the automatic expiration at seven days. Rather, server-side cookies can live for long periods, leading to a higher quality identification for anonymous users. The downside, however, is they are far more difficult to leverage than the client-side. They require a much more technical integration with whatever server-side technology is used to power the web asset and may not be accessible in the same manner as client-side cookies.",
        "Getting Technical with Server Side Cookies": "In general, regardless of the specific attribute settings used when leveraging server-side cookies, they currently are not impacted by the 7-day expiration window that client-side cookies fall victim to. However, Apple has made it clear that they have additional plans to extend some of the client-side cookie limitations to the server-side, and the most important attribute in that discussion is theHttpOnlyattribute. https://webkit.org/blog/9521/intelligent-tracking-prevention-2-3/ As a CDP and technology leader, we always aim to help our customers future-proof their implementations. As such, even though a non-HttpOnly server-side cookie offers an easier means to bypass current client-side restrictions, it is our recommendation to consider investing to leverageHttpOnlycookies set by the server-side to prevent any potential impacts of the next few iterations of ITP. Below we\u2019ll explore the two options and demonstrate the key differences.",
        "Server Side Cookie without HttpOnly": "The following example in Node.js demonstrates a sample snippet for setting the cookie server side. Most server-side languages have existing methods to make this very easy. In the case of creating a cookie for Lytics to leverage, you will also need to generate a unique ID which can be done in a variety of ways. In the case of Node, you may consider using therandomUUID() method of the Crypto interface. A simple Google search can lead you or your developer down the road of generating a unique ID that best fits your use case. Once you have that unique ID, you simply set the cookie using the pre-defined Lytics name our Javascript tag is looking for. Alternatively, the tag can be configured to use any custom name. Since non-HttpOnly cookies are accessible out of the box client-side there is no additional lift necessary. Our tag will pick up the cookie and use that ID as the identifier. It is important to note that any UID changes must also be managed server-side as setting the cookie client-side will engage the 7-day expiration max. In this example, we simply raise an alert with the cookie string and do not show the actual implementation of the Lytics tag.",
        "Server Side Cookie with HttpOnly": "Setting HttpOnly to true comes with an additional level of complexity but benefits from following the stringent and recommended guidelines of Apple ITP, which in theory will go farther in the way of future-proofing. Much like the above example, we\u2019ll set a cookie, in this case using Node, in the same way. The only difference here is settinghttpOnlyto true. This means that the cookie is secure but will no longer be accessible out of the box by Javascript. Rather, you\u2019ll have to implement an alternative method for surfacing that ID to Javascript so that it can be passed to the Lytics Javascript tag and used during collection/resolution. All code examples in this document are purely for demonstration. Any customer facing implementation should follow the guidance of our customer facing technical teams and the technical experts on our customer\u2019s end. These examples in production represent HIGH risk as documented.",
        "Accessing Data from Server Side": "By default, Lytics returns a user's profile and any surfaced fields to the browser. This, however, is not always sufficient when it comes to marketing use cases. An alternative approach is to do the lookup based on the user cookie, utilizing the server side language of your choice. First, read the browser's cookies to get the visitor's_uidin order to identify them, make a request to thePersonalizationto get the visitors current profile, and then surface the profile as a JavaScript object.",
        "Identifying the Visitor": "Identifying the user can be done in a variety of ways. By default, Lytics sets a cookieseeridthat is used to identify a user. This is then surfaced in a user profile as_uid. The_uidfield is what the Lytics JavaScript tag uses for web based identity resolution. That said, if users are logged in or identified by another known key this can be used in place ofseeridin the following examples. Since we already have a cookie, all we need to do is read those to get the user profile from the Lytics API.",
        "Get Visitor's Profile": "Once you have an identifier, make a GET request to our personalization API. In this example, the cookie value is used to build a call to the personalization API for three fields:last_make,last_model,last_color. If you are using a custom key value pair for identification, replace the$fieldnameparameter with the field name of your identifier in Lytics and$valuewith the value of that key.",
        "Surface User Data in the Browser": "Next, parse the response and either inject the variables directly into your template or surface the profile as a JavaScript object.",
        "Collections List": "On theCollectionspage, located in theContentmenu, you\u2019ll find a list of the Collections available in your account. By default, Lytics automatically generates three key Collections: Documents With Images: This collections includes all documents with aprimary_imagefield, typically sourced from  meta tags with anog:imageproperty.All Documents:Includes every document that Lytics has collected from your content sources.Default Recommendation Collection: Features documents that have both images and Topics, and is designed for use in onsite RecommendationExperiences.",
        "Collection Summary Page": "When you click on a Collection from the list, you\u2019ll be taken to the Collection Summary page, which includes the following details: Collection Definition: This section outlines theSegment definition, along with an AI-translated, human-readable version of the criteria.Experiences Using this Collection: A list of all active Experiences that utilize the Collection.Collection Report:ASize Report Component: displays the number of documents in the Collection over timeComposition Report Components: provides detailed breakdowns of the Collection's Topics, Authors, URL Paths, and HTTP Status codes",
        "Documents Tab": "TheDocumentstab displays a subset of up to 100 documents from the Content Collections. Each document card includes the following details: primary_imagetitledescription Additionally, each card provides links to the document\u2019s URL and itsIdentityview.",
        "Recommendations Tab": "TheRecommendationstab offers a variety of tools and resources to help you create, test, and implement content recommendations within Lytics. This section is designed to streamline the process of using Lytics' powerful Recommendation engine, enabling you to generate personalized content experiences for your users. Create a New Recommendation Experience: This section provides a direct link to create a new Recommendation Experience. An Experience in Lytics allows you to serve personalized content, such as articles, products, or promotions, to users based on their individual affinities and behaviors. You can use the intuitive interface to quickly set up and customize these Experiences, ensuring that your users see relevant content that drives engagement.Documentation for theContent Recommendation: For those looking to dive deeper into programmatic access, Lytics provides a comprehensive guide to the Content Recommendation API. This API allows developers to fetch content recommendations on-demand, enabling integrations with third-party systems or custom user interfaces. The documentation provides detailed information on how to authenticate, structure API calls, and interpret the responses.Code Snippets for Multiple Languages: Lytics simplifies the process of integrating content recommendations into your system by providing ready-made code snippets. These snippets are available in several popular programming languages, including:Bash,JavaScript,PythonandGo.  Whether you're a seasoned developer or just getting started, these code snippets will help you quickly integrate the Lytics Content Recommendation API into your project. You can copy and paste the provided code to get up and running in minutes. Recommendation Playground The Recommendations tab also features a \"playground\" where you can test and preview recommendations for individual users. By entering a user's unique identifier (such as their user ID), you can see real-time results of what Lytics would recommend for that specific user. This feature is particularly useful for validation and testing purposes.After clicking theRecommendbutton, Lytics will return a list of personalized content suggestions for the user. Each recommendation will be displayed in a table, showing: URL: The link to the recommended content.Image: A thumbnail or visual representation of the content.Title: The title of the recommended content.ID: The unique identifier for the recommended content item. This playground offers a hands-on way to understand how Lytics' recommendations align with user interests, providing a deeper level of insight and confidence before rolling out personalized content to your audience.",
        "Managing Content Collections": "When working with Content Collections in Lytics, you have several management options available via the ... menu located next to the Collection title at the top of the page. These options allow you to efficiently duplicate, edit, delete, or refresh (re-enrich) your collections, ensuring that they remain relevant and aligned with your evolving content strategy. Duplicate: SelectingDuplicatecreates a copy of the existing collection and takes you directly to the Collection Builder. Here, you can modify the parameters of the duplicated collection to fine-tune the content being gathered. This is especially useful if you want to create a new Collection that is similar to an existing one but with slight adjustments\u2014such as targeting a different subset of content or users. Duplicating saves time by allowing you to leverage your previous work instead of starting from scratch.Edit: ClickingEditopens the Content Collection Builder for the selected collection, enabling you to change the criteria or settings that define the collection. Whether you need to update filters, or adjust the logic behind what documents are included, editing lets you easily refine the collection over time. As your content strategy shifts or new types of content are added to your site, this option ensures that your collections remain up-to-date and aligned with your goals.Delete: TheDeleteoption permanently removes the collection from your account, freeing up resources and clearing clutter from the interface. However, there are some restrictions:You cannot delete a collection if it is being used in any legacy Personalize campaigns, Experiences, or workflows. Before deleting, you'll need to first remove the collection from any campaigns or processes that rely on it.A warning message will notify you if the collection is in use elsewhere, preventing accidental deletions and ensuring you don't disrupt any live campaigns or workflows.\u26a0\ufe0fWarning: Be cautious when deleting collections that may be tied to essential processes. Always ensure that they\u2019re no longer in active use to avoid unintended disruptions in personalization or recommendations.Re-Enrich: TheRe-Enrichoption allows you to force Lytics to rescan and re-process all documents within a collection. This is particularly useful when you\u2019ve made significant updates to your content, such as changing meta-tags, adding new metadata, or updating page structures. Re-enriching ensures that Lytics captures the most current data and classifications for the documents in the collection. This process will refresh the topics, keywords, and metadata that power recommendations and user affinity models, helping to keep your content classification accurate and relevant for personalization.",
        "Client Side Integrations": "A set of client-side integrations is facilitated by the core Lytics SDK. Each of those integrations is either deployed or disabled based on a series of checkboxes under the \"Client Side Integrations\" section. Each of those options follows the pattern outlined below. Client-side integrations supported out of the box: Amazon DSP:Allow Lytics to pass the current visitor's_uidto Amazon DSP for improved identity resolution.Amazon DSP Confirmation Events:If false, confirmation events will not be sent to the amazon_dsp stream upon successful sync completion.Criteo:Allow Lytics to pass the current visitor's_uidto Criteo and receive a Criteo GUM ID for improved identity resolution.Google Ads Partner API:Allow Lytics to use the Google Partner API to make Google Ads calls.Google Analytics 4 (GA4):Allow Lytics to pass the current visitor's_uidand audience membership to GA4.Google DV360:Allow Lytics to pass the current visitor's_uidto Google DV360 and receive a unique DV360 ID for improved identity resolution.Krux:Allow Lytics to pass the current visitor's _uid to Krux for improved identity resolution.Lotame:Allow Lytics to pass the current visitor's _uid to Lotame for improved identity resolution.Taboola:Allow Lytics to pass the current visitor's _uid to Taboola and receive a unique Taboola ID for improved identity resolution.The Trade Desk:Allow Lytics to pass the current visitor's _uid to The Trade Desk and receive a unique Trade Desk ID for improved identity resolution.Yahoo Ads:Allow Lytics to pass the current visitor's _uid to Yahoo and receive a unique Yahoo ID for improved identity resolution. In addition to those outlined above, many automatic client-side integrations are included within the Lytics core SDK. These integrations automatically sync enabled profile data with other tools if those tools have been configured on your site, too. For instance, Lytics will sync audience membership for the current visitor with Meta if the Meta pixel is also present on the page. To prevent these syncs, each integration can be blocked by adding the proper key to the \"Integrations Blocklist\" as outlined below: Available client-side integrations and their associated slugs:",
        "Simple webhook payloads": "One of the primary use-cases for templates is for shaping the request payload for outgoing Webhook Exports from Lytics. For example, say you are exporting a Lytics audience calledfrequent_visitorsto a service that expects the following JSON in the request body: We could create the following template in Lytics, and select it in our Webhook Export job configuration to send the correct request, using thefirst_name,last_name, andemailfields on your Lytics profiles (seeherefor details on using thelytemplates.libsonnetlibrary): Once created, this template can be selected from theLytics Templatedropdown menu in thewebhook job configuration.",
        "Usingassertto skip events that have empty required fields": "Many times when sending webhook events, certain fieldsmustbe set or the webhook endpoint will respond with an HTTP error, causing the webhook job to fail. This simple template uses the Jsonnetassertfunction to signal that email must be set before sending. These events will be omitted from sending and will be logged with the included assertion messageemail must be set.",
        "Using templates for dynamic URLs": "You can also use a template to dynamically generate URLs for webhook requests instead of hard-coding one in your jobconfiguration. Say you are exporting Lytics profiles to a service that expects requests to the following endpoint where{email}is the user's email address: We could create the following template, which extracts the email from each Lytics profile and includes it in the URL: Once created, this template can be selected from theWebhook URLdropdown menu in thewebhook job configuration.",
        "\ud83d\udcc8": "The new customizable Lytics Reports do not replace the reporting capabilities you are familiar with in Decision Engine. They are simply another configurable tool you can use to understand your audiences.",
        "Creating your First Report": "To create your first report, click theCreate Newbutton.  You will then be directed to the creation wizard, where you can name your report and provide an optional description. Once you've set up your Report, you will be redirected to the Report page,  where you can addComponentsto your Report by clicking theAdd New Componentbutton below.  To learn more aboutReportsandComponents, check out the following links: Creating ComponentsManaging Reports",
        "Overview": "Google Cloud Pub/Subis a fully-managed messaging service that lets you send and receive messages in real-time between independent applications. This integration supports both importing and exporting data using Cloud Pub/Sub in real-time. Importing data results in a continuous update of new users or existing user profiles supplemented with activity data. This differs from other import integrations that usually only import once every hour or every day. You can then use this data to build and refine your existing Lytics audiences to power better, cross-channel campaigns. Exporting data results in a continuous update of new and existing user profiles on a Pub/Sub Topic of your choice.",
        "Authorization": "If you haven't already done so, you will need to set up a Campaign Monitor account before you begin the process described below. Navigate to Campaign Monitor in the Jobs section of Lytics.Navigate toAuthorizations.ClickAdd new authorization.In theEmail or Usernamebox, enter your Campaign Monitor email or username.In thePasswordbox, enter your Campaign Monitor password.ClickLog in.  ClickAllow access.In theDescriptionbox, enter a name for your authorization.ClickAuthorize.You are now ready to start a workflow with Campaign Monitor.",
        "\ud83d\udcd8": "NOTE: Thehttps://api.lytics.io/api/user/ssoURL should only be used for Okta. Other SSO providers use the default URLhttps://api.lytics.io/api/user/verifyauth",
        "Import Audiences & Activity Data": "Data Extensions can be used to store a wide variety of information in Salesforce Marketing Cloud. The Data Extension Import allows you to bring that data into Lytics to get a more complete picture of your users.",
        "Integration Details": "Implementation Type:Server-side IntegrationImplementation Technique: RESTAPI IntegrationwithAudience Trigger IntegrationFrequency:Real-time Integration, with an optional one-timeBackfillof the audience after setup.Resulting Data: Lytics users that are a member of the selected audience(s) are exported to Segment via Identify or Track events. This integration utilizes theSegment APIto send user data. Once the user initiates the job, it will run a backfill of users if configured to do so. Segment will then receive real-time updates when a user enters or exits the choosen audience. For each user the integration will: Create anIdentifyorTrackevent payload with the Lytics user field data selected according to job configuration.Append theaudience_{audience_name}as boolean (true or false) field if the event type selected at configuration isIdentify. Heretruespecifies the Enter event andfalsespecifies Exit event.If theTrackevent was selected at configuration, it will append the fieldeventwith a value of\"enter\"or\"exit\".Send the Event to Segment.",
        "Fields": "Segment supports various fields for Identify or Track payload. Please refer to Segment'sIdentifyandTrackdocumentation for more information. As mentioned above, theaudience_{audience_name}andeventfields are added in addition to the default payload depending on the event type. Lytics allows you to send additional fields as part oftraitsfor Identify events andpropertiesfield for Track events.",
        "Configuration": "Follow these steps to set up an export job for Segment. If you are new to creating jobs in Lytics, see theDestinationsdocumentation for more information. Note:Before running this job you will need to set up a Source in your Segment account, this may be aCustom Sourceor aLegacy Project Sourcethis will be the source where Lytics writes data in Segment. You will need to know your write key for your source. SeeSegment's documentationfor directions on how to obtain your write key. SelectSegmentfrom the list of providers.Select theExportjob type from the list.This job does not require an authorization.Enter aLabelto identify this job you are creating in Lytics.(Optional) Enter aDescriptionfor further context on your job.Select the audience to export.Enter your Segment write key inSegment.com Write Keyfor the target Segment source.Finding your write key.Select theUser IDfield to match users in Segment.  SeeSegment's documentationfor more information about user IDs.Select theAnonymous IDfield to match users as anonymous in Segment. See Segment's documentation aboutAnonymous IDsfor more information.Note:User ID or Anonymous ID must be set.Select theEvent type.Select the userExport Fieldsto export. By default, all fields will be sent.SelectExisting Usersto add users who already exist in the selected Lytics audience.ClickStart Export. ",
        "Export Audiences": "Export your Lytics audiences to Airship to use Lytics-powered insights in your mobile campaigns. Use your advanced data science driven audiences in Lytics as your push targets in Airship.",
        "Experiences": "Lytics Experiencessupport Salesforce Marketing Cloud journeys based on your data extensions or triggered API events. These Experiences are best kept as simple email journeys with a single email send action. Any actions beyond the first email send will not be counted in reach and conversion metrics. To manage the full customer lifecycle, you can create multiple Salesforce Marketing Cloud Experiences, each with a single email send, in the Lytics Canvas.",
        "Experience Import": "Like all Experience enabled providers, you canimport Experiencesfrom Salesforce Marketing Cloud to Lytics. During the import process, you will be asked to select an authorization. Read theSalesforce Marketing Cloud authorization documentationfor more information. ",
        "Tactics": "Tactics are determined by the journey'sentry source.Salesforce Marketing Cloud Experiences in Lytics support the following tactics: API Event- Send responsive campaigns in real-time using API-driven entry to a journey. This option supports Delivery Optimization from Lytics.Data Extension- Send scheduled emails (e.g. newsletters and event notifications) using data extension-driven entry to a journey.",
        "Activation": "Activating Salesforce Marketing Cloud Experiences will export users and populate a Data Extension, but the exact method depends on the Experience tactic.",
        "Blast Tactics": "For blast Experiences, simply assign the list exported to the associated Iterable Campaign once the list has populated. This should occur within a few minutes of activation if you do not have Delivery Optimization enabled. Open up the campaign in Iterable, and navigate to theSetup step.UnderSend Lists, select the list you configured your Experience to sync to.Save your changes and proceed to theLaunch step.Once you're ready to deploy, opt toSend Campaign Right NoworSchedule Campaign for Later.",
        "Triggered Tactics": "For triggered Experiences, you will utilize anIterable workflowto deliver your Experience as users enter or exit the list. You will need to create and configure the workflow before importing the Experience to Lytics, and enable the workflow after the Experience has been activated in Lytics. Open up the workflow associated with your Experience in Iterable.Make sure your workflowStart Whencondition is set toSubscribed to List, and choose the list you configured for your Experience in Lytics.For your second node, in your workflow select the execution (send email, send in-app, send SMS, or send push) and make sure your campaign is selected.Connect the nodes in your workflow, or add additional nodes for delay or additional logic.Save your workflow.Once you're ready to deploy, toggle the switch to enable users to begin receiving their Experience.",
        "Metrics": "Salesforce Marketing Cloud metrics are collected through theRetrieve APIto populate thereach and conversion metricsfor Salesforce Marketing Cloud Experiences.  Metrics from the SalesForce Marketing Cloud are mapped to Lytics as follows: Reach- Impressions are counted by the number of users who have opened the email sent by the Experience. Lytics will retrieve all theOpenEventsfrom Salesforce Marketing Cloud for the Experience.Converted- Conversions are counted by the number of users who have clicked a link in the email sent by the Experience. Lytics will retrieve all theClickEventsfrom Salesforce Marketing Cloud for the Experience. These events are also mapped to the Lytics user fieldsReached with SFMC ExperienceandConverted on SFMC Experience, which are available in the audience builder so that you can create audiences of users who have been reached by or converted on your Salesforce Marketing Cloud Experiences.",
        "Configure Webhooks": "Configuring webhooks in Contentful will allow you to send content to Lytics when it's published. You can send custom topics through this integration in addition to the standard Lytics topic extraction process using natural language processing. Imported Contentful entries can be included as part ofContent CollectionsandRecommendationsto power marketing campaigns personalized with one-to-one content.",
        "Including Identifiers in Links": "You may want to pass a unique Sailthru identifier from links within your email to help enable cross-channelidentity resolution. This will help Lytics identify users across data streams and merge their email and web activity. The easiest way to pass a unique identifier from Sailthru click-through links into Lytics is to modify your email template to include a hashedemailin each URL in the email being sent.",
        "Requirements": "Have access to an active Lytics account.Ability to add JavaScript to your website via tag manager or CMS.Entry-level knowledge of JavaScript. (Don't worry; it is mostly \"copy and paste\" here.)",
        "Adding an encoded email to Iterable Links": "Follow these steps to add a Base64 encoded email as an identifier to your email links in Iterable. Ensure that the Iterable integration isimporting audiences and activity data(default).Log in toIterableand navigate toTemplates > Templatesand select your email template.SelectAdvanced Optiontab.SelectEnable Custom Link Parameters.Set the following in the custom parameters:Key =encoded_emailValue ={{#base64}}{{email}}{{/base64}} Now all email clicks will pass theencoded_emailto your website when users click the links, and Lytics will automatically grab the encoded_email, decode it and stitch it to the user's web behavior via our JavaScript web tag.",
        "About the Lytics SDK for React Native": "iOSVersion Support:14.0+ (deployment target > 14.0)Xcode Version Required:15.0+AndroidVersion Required: 8+",
        "Step 1. Install the Lytics SDK & OS Specific Dependencies:": "Install the core React Native Library",
        "Step 2a. Install for Android (only if targeting Android devices)": "Add the following permissions to your AndroidManifest.xml file: This permission is required to send events to the Lytics API: This permission is technically optional but highly recommended to allow the library to determine the best time to send events to the API.",
        "Step 2b. Install for iOS (only if targeting iOS devices)": "Install native iOS modules:",
        "Step 3. Initialize the SDK": "There are several configuration options outlined below. At a minimum, however, you must initialize the SDK with a validAPI tokenwith data access to your Lytics account.",
        "Step 4. Identifying a User": "Once we have initialized the SDK, it is a best practice toidentifythe user anywhere they provide additional strong identifiers, such as a username or email, upon login. This ensures your profiles have the highest potential for properly unifying across your various data sources as soon as they identify themselves. Do note that this step is optional. By default, the Lytics SDK will create an anonymous identifier for all users and leverage that to merge disparate sources as we do with our traditional web-based JavaScript SDK.",
        "Step 5. Tracking Activity": "Once the user has been identified (optional), we can begintrackingvarious activities throughout their visit. Typical activities include screen views, clicks, purchases, etc. A full scope of event tracking flexibility can be found in the technical documentation section below.",
        "Technical Documentation": "The following outlines the full definition of capabilities supported by the Lytics SDK for iOS.",
        "Identifying Users": "Lytics.shared.identifyis used to emit strong identifiers and attributes associated with your app user. This method provides a means to store the primary identifiers on the device itself so that all future event tracking from that device automatically is appended with information necessary to maximize the effectiveness of your identity resolution strategy. For eachLytics.shared.identifycall, you have the option to only store that data to the device or emit it to a Lytics event stream for mapping to profiles. Emitting all event types to a Lytics event stream is always our recommendation. Exampleidentifycall where we set pass a custom stream, set of identifiers, and attributes:",
        "Tracking Events": "Lytics.shared.trackemits activity data for a particular app user to a Lytics even stream. Each track call will automatically be appended with the strong identifiers stored on the device to ensure the highest potential resolution against Lytics user profiles.Lytics.shared.trackcalls can include a variety of optional parameters and data associated with the interaction as defined below: Exampletrackcall where we track a purchase event and its associated properties: Lytics.shared.screenis an additional method for appending device information to the traditional track call. This is an extension of the above definition and results in including the following information with all events: Operating System Version (UIDevice.current.systemVersion)Device Model Name (UIDevice.current.name)identifierForVendor (UIDevice.current.identifierForVendor)orientation (UIDevice.current.orientation)userInterfaceIdiom (UIDevice.current.userInterfaceIdiom)",
        "Data Privacy Controls": "Depending on your application's particular needs, various data privacy controls are available by default. Below you'll find the optional methods and examples of scenarios where some or all may be most applicable.",
        "Consent": "Lytics.shared.consentis a specific event type for collecting critical information related to consent, such as location, document, etc. As a best practice, aLytics.shared.consentevent should always be leveraged alongside theLytics.shared.optIn()event to ensure proper documentation of consent:",
        "Opt In": "Lytics.shared.optIn()sets the state of a user's activity tracking preference. By default, all users are opted in. This default setting is fully configurable at the top-level SDK configuration. In either case, an explicit call toLytics.shared.optIn()will result in activities being tracked and sent to a Lytics stream.",
        "Opt Out": "Lytics.shared.optOut()is the opposite of the aboveLytics.shared.optInmethod and will prevent activity data from being emitted to Lytics after it has been called until an additional.Lytics.shared.optIn()call has been made.",
        "General": "Both of these attributes are automatically populated based upon the_evalue in thejstag.sendpayload. By default Lytics will collect apvevent for each page view and this will automatically populate thefirst_seenandlast_seenattributes. Below is an example of collecting a custom event that would populate these attributes as well. This attribute is automatically populated with a count of events per hour for the user. This attribute is automatically populated with a count of events per hour of the week for the user. This attribute is automatically populated with the last time an event was received in any stream for the user.",
        "IDFA (Identifier for Advertisers)iOS": "IDFAoradvertisingIdentifieris an alphanumeric string that is unique to each device. Collection of this identifier requires explicit confirmation from the app user, which the Lytics SDK for iOS can trigger. In addition, upon confirmation, the IDFA will be treated as a strong identifier and included with all outbound events emitted to Lytics streams for improved resolution. To access the IDFA from iOS devices you must add the theNSUserTrackingUsageDescriptiondependency to your appsInfo.plistfile. Open your Info.plist file in Xcode.Right-click on a blank line and select Add Row from the context menu.In the newly added row, start typing NSUserTrackingUsageDescription to select it from the auto-complete list. This will be the key.In the Value column of the same row, enter a string that describes why your app needs to track the user. The description should be clear and user-friendly, as it will be displayed in the system prompt to the user.",
        "GAID (Identifier for Advertisers)Android": "GAIDthe Google Advertising ID (GAID) is a unique identifier assigned to each user's device by Google for Android devices. It's used for tracking and targeting purposes in advertising and analytics applications. GAID allows advertisers and developers to collect data on user behavior and preferences in a way that respects their privacy preferences. Users have control over their GAID, with the ability to reset it or opt out of personalized advertising, ensuring user privacy. This system is analogous to Apple's IDFA (Identifier for Advertisers) on iOS devices. To access the GAID from Android devices you must add the Google Play Services dependency as follows to yourbuild.gradlefile:",
        "Utility": "Several utility-type methods are available for flushing queues, resetting stored data, and performing general maintenance or debugging.",
        "Event Queue Flushing": "Occasionally, it may be desired to force a flush of the event queue. This is common after a series of events, before sign-out, etc. This method will bypass any queue caps or timers and result in all queued events being emitted:",
        "Stored Property Flushing": "Strong identifiers and attributes are stored locally on the device. In general, there are not likely to be many use cases where it is necessary to delete all stored data and reset the SDK. By doing so, the resolution will become much more complex, and all identifiers, even anonymous, will no longer be accessible. That said, in the case of a shared device where a user is logging in and out or in the more common case of a test device, the following method is available to delete all data stored by the Lytics SDK for iOS:",
        "Access Stored Identifiers & Attributes": "Be it for testing or presenting a profile to the end user, the complete stored device-level profile is available with the following method. Do note, this is only the stored data as a result ofLytics.shared.identifycalls and does not represent the complete user profile from the Lytics system and cross-channel sources.",
        "API Reference": "The Lytics SDK for iOS is optimized to leverage the fewest number of APIs possible. As such, all event collection is managed by leveraging the coreLytics event collection API.",
        "Troubleshooting": "When enabled, the.debuglog level will result in additional logs to aid troubleshooting and issue resolution. Don't hesitate to contact customer support if you have trouble implementing the Lytics SDK for iOS.",
        "FAQ": "Please visit ourcommunity sectionfor FAQs and SDK-related discussions.",
        "Types of partners:": "Subcontractor Implementations- A partner who implements a Lytics customer that has been acquired directly by Lytics.Subcontract Technical Account Management- A partner who helps aid a Lytics-acquired customer with technical management and strategy components as an extension of professional services.Referral Partner (Enterprise or Mid-Market)- Under a reseller agreement partners can bring their customers to Lytics to be supported by licensing and provide implementation or technical account management services at their discretion. Partners interested in becoming resellers and who do not have an agreement with Lytics can contact the Lytics commercial team to create one.",
        "Cookie Consent Management": "When it comes to managing cookie consent primarily on the client side, the common approach is to use explicit opt-out methods. One popular tool for cookie consent management is OneTrust. By utilizing OneTrust's cookie consent management, you can configure it to read the consent opt-out and prevent loading any configured tags. This means that the Lytics tag, responsible for sending data downstream and storing cookies client side, will be effectively disabled. To learn more about OneTrust's cookie consent management, visit their website:OneTrust Cookie Consent.",
        "Customer Consent Management with Lytics": "In addition to cookie consent management, Lytics offers a comprehensive platform for managing customer consent. Integrating Lytics with OneTrust can elevate your customer consent management to new heights. Lytics enables you to gather and handle customer consent data effectively, providing you with actionable insights and enabling personalized experiences while respecting consumer preferences. To learn how to leverage Lytics for managing customer consent, we have created a dedicated resource:Using Lytics to Manage Customer Consent. This guide will walk you through the process of utilizing Lytics alongside OneTrust to ensure compliance and build stronger customer relationships.",
        "Privacy Rights Automation": "As a consumer in California, you hold certain rights concerning your personal data, including the right to access and request the removal of information stored by companies. OneTrust offers efficient solutions for handling Data Subject Access Requests (DSARs), Data Erasure Requests, and automating privacy rights management. Data Subject Access Requests can be sent from OneTrust to the LyticsGet User API endpoint. This endpoint requires an API token to authorize the request and the by-field of the profile (typically email address). Any fields on the user profile can be returned and included in the reporting sent back to the consumer. Requesting the deletion of a consumer profile in Lytics can be triggered through the LyticsDelete User API endpoint. This endpoint again requires an API token to authorize the deletion request, as well as the email address (or other appropriate by-field) of the consumer profile to identify what to delete. This process returns a deletion request ID, which can be used to monitor the process completion. As the deletion involves removals from data archives anywhere else that the data may be retained, this can take up to 14 days in some cases. Here's an overview of how OneTrust handles DSARs: DSAR SubmissionOn your customer webpage, you can provide a URL link that directs users to a OneTrust-hosted web form dedicated to handling DSARs.This web form, residing on a dedicated subdomain, allows individuals to submit their DSAR requests by providing their email addresses and indicating their preference for information deletion.Request ProcessingOneTrust receives and queues DSAR requests, processing them in a systematic manner.Once a request is processed, the OneTrust system can perform the following actions:Submit deletion requests to Lytics and other connected systems and tools to ensure the removal of relevant information.Retrieve information from various configured connections, such as Lytics, CRMs, and databases, to generate comprehensive reports for compliance purposes.",
        "Integrating OneTrust with Lytics": "To further enhance your privacy rights automation and consent management, OneTrust allows you to configure a custom webhook integration with Lytics. This integration enables seamless communication between the two platforms and streamlines the following key functions: Delete ProfileEndpoints: The LyticsDelete User API endpointcan be used to send the profile deletion requests to Lytics from a OneTrust webhook.Required fields: The Delete User Endpoint requires a profile identifier or by-field that Lytics uses to identify the profile being deleted. When integrating with OneTrust for privacy management automation, it is recommended to use email as the identifier.API Key: Generated from the account settings in Lytics.Verify Deletion StatusThis step is optional but can be set up to check the status of profile deletion.The relevant endpoint and details can be found in the Lytics API Documentation for Profile Deletion.Retrieve ProfileEndpoint: The LyticsGet User API endpointallows you to retrieve profile information from Lytics and include it in the DSAR reports generated in OneTrust.Required fields: Similar to the delete profile configuration, the profile identifier or by field type and value should be specified (e.g., email).API Key: Generated from the account settings in Lytics.Optional fields: You can specify which fields you want to return when retrieving the profile information.",
        "Implementation Considerations:": "Implementing automation of consumer profile reporting and privacy regulation management should be done with careful consideration of the entire environment and systems which consumer data may be retained or synced from. Timing of syncs between systems, availability of complete data for reporting, and archive retention periods are all important to keep in mind when planning, designing, and implementing these types of automations.",
        "Google Ads Customer List Export": "Export your Lytics audiences to Google Ads Customer Match to reach and re-engage with your customers across Google Search, Google Shopping, Gmail, and YouTube. Learn more aboutGoogle Ads Customer Match. Refine your targeting efforts using Lytics audiences containing rich information on user behavior and content affinities across channels. This workflow is primarily used for targeting audiences across Google Ads inventory, including Search, Shopping, Gmail, and YouTube. It is ideal for advertisers focused on paid search campaigns, shopping ads, or video ads directly within Google\u2019s core ad system.",
        "Match Email & User Info": "To perform a customer match viaEmail, fill out the following fields: From theEmail Fieldinput, select the Lytics field that contains the email address of your users.Or, optionally if your Lytics account has SHA256 emails instead of raw emails, from theSHA256 Emailinput, select the Lytics field that contains the SHA256 hashed email address.From thePhone Numberinput, select the field that maps to the user's phone number.From theFirst Nameinput, select the field that maps to the user's first name.From theLast Nameinput, select the field that maps to the user's last name.From theCountry Codeinput, select the field that maps to the user's two letter country code.From theZip Codeinput, select the field that maps to the user's zip code.",
        "Match Mobile ID": "To perform a customer match viaMobile ID, fill out the following field: From theMobile IDinput, select the field that contains the mobile ID of your users. If Mobile ID is selected, it must be theonlyfield selected.",
        "Match User ID": "To perform a customer match viaUser ID, fill out the following fields: From theUser IDinput, select the field that maps to your uploaded user ID.",
        "Enhanced Lead Conversion Export": "Enhanced conversions for leadsuses first-party user-provided data from your website to measure sales and transactions that happen off your website. If you run lead-generation campaigns to drive offline sales, enhanced conversions for leads can help you understand the impact of your ad spend. This version of offline conversion tracking doesn\u2019t require the use of the Google Click ID (GCLID). Instead, it uses user-provided data from your website leads to measure conversions. You may need your Google rep to enable this feature for your account. To use this integration you will need to set up some things in your Google Ads account, seeGoogle's documentation forsetup details. Integration DetailsFieldsConfiguration",
        "Swrve API": "To import audiences, Lytics uses the standard Swrve API authentication, requiring the following credentials:API KeyandPersonal Key. You can find more information on creating a Swrve API keyshere. In theLabeltext box, enter a name for the authorization.(Optional) In theDescriptiontext box, enter a description for this authorization.Enter yourAPI Keycredential.Enter yourPersonal Keycredential.",
        "Swrve S3": "To import user activity data, you must firstset up a raw data exportin Swrve, which exports to a S3 bucket. You will need the following credentials: AWS S3Access Token,Secret Token, andBucket. In theLabeltext box, enter a name for the authorization.(Optional) In theDescriptiontext box, enter a description for this authorization.Enter yourAccess Tokencredential.Enter yourSecret Tokencredential.Enter yourBucketcredential.",
        "Import Activity Data": "Import data from your AWS Kinesis Data Streams to use Lytics data science scoring and Insights to build rich, behavioral audiences.",
        "Import Audiences": "Importing user and activity data from Mailchimp results in new users or existing user profiles supplemented with Mailchimp campaign data. You can use this data to build and refine your existing Lytics audiences to power better, cross-channel campaigns.",
        "Responsys Interact": "Responsys Interact you will need the following credentials, username, password and endpoint URI . In theLabeltext box, enter a name for the authorization(optional) In theDescriptiontext box, enter a description for this authorizationIn theUsernametext box, enter your Username credential.In thePasswordpassword box, enter your Password credential.In theEndpoint URItext box, enter Endpoint URI should look like either, login2.responsys.net, login.rsys8.net, or AccountToken-api.responsys.ocs.oraclecloud.com.",
        "Responsys SFTP Server": "Responsys SFTP Server you will need the following credentials. In theLabeltext box, enter a name for the authorization(optional) In theDescriptiontext box, enter a description for this authorizationIn theHosttext box, enter your Host credential.In thePortnumeric field, enter your Port credential.In theUsernametext box, enter your Username credential.In thePrivate Keytext box, enter your Private Key credential.(optional) In theFoldertext box, enter Folder path to place files.",
        "Responsys SFTP Server with PGP": "Responsys SFTP Server with PGP you will need the following credentials. In theLabeltext box, enter a name for the authorization(optional) In theDescriptiontext box, enter a description for this authorizationIn theHosttext box, enter your Host credential.In thePortnumeric field, enter your Port credential.In theUsernametext box, enter your Username credential.In theSFTP Private Keytext box, enter your SFTP Private Key credential.(optional) In theFoldertext box, enter Folder path to place files.In thePGP Private Keytext box, enter your PGP Private Key credential.In thePGP Private Key Passphrasetext box, enter your PGP Private Key Passphrase credential.",
        "Responsys: Export Audience": "Integration DetailsFieldsConfiguration",
        "Responsys: Import Activity Data": "Integration DetailsFieldsConfiguration",
        "Responsys: Export Audiences to Campaign": "Integration DetailsFieldsConfiguration",
        "TL;DR": "This document outlines how to leverage Lytics' real-time personalization engine to identify visitors who lack the strong identifiers necessary to link web-based behavioral insights with other activation channels such as email.",
        "How does Lytics make content recommendations?": "Lytics Interest Engines provides a sophisticated approach for businesses to gain and leverage insights into which content is most likely to resonate based on behavioral patterns. This is achieved through a detailed enrichment and affinity generation process in real-time. While full details are available in Lytics' comprehensive documentation, the core process can be distilled into four key steps: Tracking Interactions: Monitoring visitor behavior in real-time to understand their engagement with the site.Content Analysis & Topic Extraction: Using Google NLP and proprietary algorithms, Lytics automatically analyzes all the content on your website, translating it into a detailed topic graph.Interest Scoring: Assigning scores to reflect a visitor's level of interest in different topics, updated dynamically as new interactions occur.Content Recommendation: Suggesting content that aligns with the visitor's demonstrated interests, leveraging real-time insights for timely and relevant engagement. This streamlined framework enables real-time personalization, enhancing user experience by continuously adapting to evolving visitor interests.",
        "Why is it important?": "Accurate measurement of attribution and conversions is paramount to assessing the effectiveness of your marketing efforts.With the challenges posed by the deprecation of third-party cookies, particularly impacting top-of-funnel ad campaigns, lead capture becomes a crucial data collection and targeting strategy.In an era of short attention spans, every interaction must be hyper-relevant and efficient, making lead capture instrumental in delivering personalized content.As marketing budgets face constraints, each dollar spent must yield maximum potential, and lead capture aids in optimizing resource allocation.Building comprehensive consumer profiles representing behavioral patterns over time is essential to get the most out of the latest advancements in AI.",
        "See it in action.": "Lytics' real-time personalization engine lets you first identify the current web visitor, programmatically review their profile, and ultimately decide if a campaign should be delivered to their browser to close gaps in that profile. ",
        "Instructions": "Create a table calledlytics_custom_audience_pushwith the above schema.Write your query. The query output must have three columns: the audience name, the audience value, and the customer id.Under query settings, setlytics_custom_audience_pushas the destination table for your query results.Lytics pulls data fromlytics_custom_audience_pushon a rolling basis, so you should see the custom_audiences field in Lytics populate within roughly 6 hours (max).",
        "Install Lytics Core SDK": "Before executing this use case, your website must be properly configured with the core Lytics SDK (JavaScript tag). Please refer to ourinstallation documentationto confirm the tag is present before continuing.",
        "Interest Scores & Content Collections": "Your content must be classified before Lytics can offer content recommendations or insights. Typically, this classification process takes 24 to 48 hours, though it may extend if your catalog is particularly large. To ensure the health of your content graph and interest engines, there are a few key areas to focus on:",
        "Verify Interest Scores on Profile": "Ensure the Lytics Developer Tools Chrome extension is installed and Enabled.Visit the domain(s) that have the Lytics JavaScript SDK installed.Open the Chrome extension and navigate to theProfilesection at the bottom.From theSummarytab, you will see a section labeledInterests. Here, we will list any topics and their associated level for the user. If your user gets a message \"No interests are available,\" proceed to the next step to ensure content has been classified.",
        "Verify Content has Been Classified": "From the Lytics interface, navigate toDecision Enginefrom the product switcher at the top left.Using the left-hand menu, navigate toContent>Collections.From the list of collections, there is likely only one; look forAll Contentin the list and the rowsSize. This refers to the number of documents classified; it should be greater than one and reflect your total catalog.For greater detail, you can also visitContent>Web Classification.At the top of that page will be a summary dashboard of all content that has been successfully classified and cataloged. Most notably, theAll Documentscount under theURL Pathcomponent. I have nolytics_content_enrichstream. (coming soon)My content is not showing up at all. (coming soon)My content is being classified, but none of my documents have images. (coming soon)",
        "Select a content collection.": "We will use the default collection outlined below for this exercise, but you may also create a custom collection with a subset of your content to recommend.",
        "Activate Required Attributes & Segments": "As an admin user of Lytics, you canenable or disable any attributesfrom being surfaced to the web via our JavaScript SDK. Depending on how your account has been configured, you may need to allow some of the required attributes for this particular use case. Required Attributes:",
        "Configure Web Experience": "To execute this use case, we will launch a simple lead capture form at the bottom left of your site. The lead capture will focus on collecting email addresses for the user and use a generic \"dark\" theme along with some boilerplate messaging. For best results, we recommend leveraging an offer of some sort that will resonate with your customers to entice them to \"sign up\" and identify themselves. In return, you'll gain much knowledge about this visitor, strengthen your consumer record, and unlock the ability to connect in channels outside of just the web.",
        "Example 1: Content Recommendation Modal": "",
        "Example 2: Inline Content Recommendation": "In this example, we'll use a generic Drupal Block with pure HTML and CSS, but the same approach can be taken using WordPress or a third-party tag manager such as Google Tag Manager.",
        "Topics": "The Topics filter lists all the topics present in your account. You can select one or multiple to filter the content by, and use the search bar to find specific topics. Building a collection filtered by topic can be especially helpful when running a topic specific campaign.",
        "Affinities": "The Affinities filter lists all the curated Affinities in your account. Since Affinities are a group of related topics, this option can save you time when building collections for a campaign targeting users interested in several related subjects or products.",
        "Resources": "Creating Content Collections: Create your first Content CollectionViewing, Using & Managing Collections: Learn more about Content Collections.",
        "Managing Groups": "Creating GroupsThe Audience Group Management feature offers a user-friendly interface for creating new groups. Users can easily access the group creation functionality from within the audience management experience. The following steps outline the process for creating a new group: Locate and click the+icon, located on the right side of the Audience Group menu.Provide a name and optionally, a description, for the new group.Save the group configuration to create the new group. Editing GroupsThe Audience Group Management feature allows users to modify and update the definition of their audience groups. This ensures that groups accurately reflect the desired characteristics and targeting criteria. Here's how to manage an existing group: Locate the target group that requires modification in the Audience Group menu.Click on the edit-icon for the desired Group.Make the necessary changes to the group's name or description.Save the modifications to update the group's configuration. Deleting GroupsThe Audience Group Management feature provides users with the ability to delete individual or multiple audience groups as needed. The following steps outline the process for deleting groups: Identify the group(s) that need to be deleted.Click on the \"Delete\" icon located next to the edit icon to initiate the deletion process.Confirm the deletion by acknowledging any prompts or confirmation dialogs.",
        "Bulk Actions": "The Bulk Actions feature allows users to perform actions on multiple audiences simultaneously, enhancing the audience management experience. While the functionality initially supported only bulk deletion of audiences, it has been reworked to also include bulk-adding audiences to a group. This documentation provides a comprehensive guide on utilizing the Bulk Actions feature. Bulk Deleting AudiencesThe Bulk Actions feature enables users to delete multiple audiences at once, streamlining the deletion process. Follow these steps to perform a bulk delete:Select the audiences that need to be deleted. Users can choose multiple audiences using checkboxes or other available selection methods.Locate the \"Bulk Actions\" button, typically found in the toolbar or within the audience list view.From the dropdown menu, select the \"Delete\" option.Confirm the deletion by acknowledging any prompts or confirmation dialogs. Bulk Adding Audiences to a GroupIn addition to bulk deletion, the Bulk Actions feature now supports bulk-adding audiences to a group. This enables users to efficiently organize and manage their audiences. Follow these steps to perform a bulk add:Select the audiences that need to be added to a group. Users can choose multiple audiences using checkboxes or other available selection methods.Locate the \"Bulk Actions\" button, typically positioned in the toolbar or within the audience list view.From the dropdown menu, select the \"Add to Group\" option.Specify the target group to which the selected audiences should be added.Confirm the bulk add action to add the selected audiences to the specified group.",
        "Prebuilt Audience Groups": "Out of the box, Lytics provides a set ofPrebuilt Audience Groups, a set of predefined groups that offer convenient categorization and management of audiences within the Lytics platform. These prebuilt Audience Groups are designed to save you time and provide quick access to commonly used audience segments. The \"All\" prebuilt Audience Group includes all audiences available in your Lytics account. It provides a comprehensive view of all the audiences you have created, allowing you to quickly access and manage any audience within your account. The \"All\" group ensures that no audience is overlooked and serves as a central hub for audience management.The \"Your Audiences\" prebuilt Audience Group displays all audiences that the authenticated user owns. This group is personalized to your account and shows audiences that you have created or have been shared with you as the owner. It offers a convenient way to view and manage the audiences that you have direct ownership of, enabling focused audience management.The \"Recently Created\" prebuilt Audience Group consists of all audiences that have been created within the last 30 days based on timestamps. This group helps you identify and manage audiences that have been recently created.The \"Recently Updated\" prebuild Audience Group consists of all audiences that have been modified or edited within the last 30 days based. By grouping these audiences together, you can easily keep track of the most actively modified segments within your Lytics account.The \"Unused\" prebuilt Audience Group includes all audiences that are not associated with an experience, downstream workflow, model, or any other integration within the Lytics platform. This group assists in identifying audiences that may no longer be in use or require attention. By consolidating these audiences into the \"Unused\" group, you can efficiently evaluate and manage audience relevance and utilization.",
        "Using Audience Groups in the Audience Builder": "When creating new Audiences, Audience Groups can be accessible in the Audience Builder. Under theExisting Audiencestab, theFilter by Groupoption allows you to display only the audiences that exist in a specific group. This can streamline your Audience building process. The image below shows how one can access theAudiences Groupsvia theAudience Builder.",
        "Managing Ranks": "Lytics makes managing ranks easy. Press the up or down arrow for a specific identifier to move it up or down in the list. ",
        "\ud83d\udea7": "Lytics supports Single Sign-On (SSO) by using Google Cloud Identity Platform as a service provider using SAML protocol. Lytics integrates with Identity Providers (IdPs) in such a way that the Service Provider (SP) initiates SSO.IDP-initiated SSO has been deprecated and is no longer supported by Lytics.",
        "Email: Event Quota Alerts": "Monitor the event quota in your Lytics instance with real-time alerts to an email address.",
        "Email: Alerts": "Monitor jobs, authorizations, audiences, and queries in your Lytics instance with real-time alerts to an email address.",
        "File Size": "Lytics uses theMarketo Bulk API, which allows an import of activity and leads data via large CSV files. However, Marketo restricts the amount of data Lytics can import daily. The default value of this limit is 500 MB per day. For example, if your Marketo account generates a 700 MB file whileimporting leads and activities, the first 500 MB will be imported the first day and the remaining 200 MB will be imported when the daily limit resets on the following day. If daily updates are critical for your use cases, spikes in activity volume may affect how timely Lytics can import data. In such scenarios, you can increase the daily limits for your Marketo account by contacting yourMarketo customer success manager.",
        "Filter Field": "When data is imported from Marketo, it is queried with one of the fields: createdAt: Only new leads will be received. Leads with updated fields will be ignored.updatedAt: Each time the import runs it will receive leads based on the time they were updated. NOTE: Importing based onupdateAtis preferred so that any new changes are received. However,updatedAtis not available as a filtering field for all Marketo accounts. IfcreatedAtis the only filter field available, this means that any updates made to existing leadswill notbe reflected in Lytics. See Marketo's documentation regardingBulk Lead Extract Filtersand contact their support to discuss gaining access to theupdateAtfilter.",
        "Step 1. Install the Lytics JavaScript Tag": "In most cases, installing the Lytics JavaScript Tag version 3 is as easy as copying and pasting a few lines of code to your source code or Tag Manager. Full instructions and the tag itself are available from the web app.",
        "Verifying Your Installation": "The simplest method is using the Lytics web app verification checker. Visit your tag installation instructions, and you will find an installation checker in the top right. Be sure to visit your site after installing the tag, as this checker requires data to be fully collected and may take a few minutes to verify. A basic check can be done using your preferred browser's JavaScript developer console. Consult your browser's documentation for access. Once you have enabled the JavaScript console, use the following command to return the installed tag version. If a version number is successfully returned, the Lytics tag has been installed correctly and has begun collecting visitor data. ",
        "Step 2. Tracking Activity": "With an instance ofjstaginitialized, you can easily begin collecting data about your users by firing ajstag.send()call to the SDK as demonstrated below.",
        "Additional details": "url: The generated URL should never be changed as it will prevent your data from being sent to Lytics.cid: When using the tag generated from your Lytics account, your ID will be included and should not be altered.loadid: By default, Lytics will store a first and third-party cookie representing a user. Theloadidflag allows Lytics to use the third-party cookie if available, thus pulling cross-domain behavior into a single, unified user profile.stream: When collecting data across various domains, it is common to use a custom stream for each domain, which will require proper LQL in place to surface the collected data as user fields. Consult customer support for more details.serializer: This is set todefaultby default. During migration from v2 to v3 you may opt to use the legacy serializer which serializes data passed to Lytics in the same way as version 2 of the tag. It will ignore the \"grandparent\" namespace on deeply nested variable names. You should only opt to use thelegacyserializer if the data you're passing is deeply nested and you previously used v2 of the Lytics JavaScript tag.",
        "Example": "To create our sample route rule, we make the following call. If successful, the response body will include the full route rule object, including a system-generated ID which can be used for subsequent calls against the object:",
        "Data Collection": "To maintain compliance with data privacy regulations, businesses need a clear and flexible path for collecting consent data. Lytics offers a range of SDKs and data collection APIs to support this effort. When approaching consent data collection, it is essential to consider the following: The source of the data and any inherent limitations. Can it be real-time, or does it need to be batch?The granularity of the consent. Is this a simple boolean, on or off, or is this something more granular, like opting into a particular communication method?Additional context related to the consent that may be important for your needs. Are there specific forms that are being consented to or a unique source? To address these considerations, Lytics has developed a suggested schema to get you started. Of course, this can always be customized viaConductorto meet your specific needs.",
        "What data is collected?": "The followingraw fieldsare included along with any collect event managed by the Lytics JavaScript Tag.",
        "Page Views": "By default, an initial page view will be captured each time the browser fully loads a new URL. This ensures that you are collecting the base behavior out-of-the-box. Alternatively, you can capture a page view manually at any point during a session by calling thepageViewmethod.",
        "Events": "Aneventis any action or activity a user performs, any update to a profile, or any export out of the subscription services.",
        "Advanced Options": "In addition to a JavaScript object payload, all Lytics JavaScript Tag collection methods accept three optional parameters. These should only be implemented by advanced users who fully understand the implications of using custom streams within Lytics or require feedback on asynchronous actions.",
        "Accessing Visitor Profiles": "In addition to collecting behavioral data related to a user's visit, the Lytics JavaScript Tag can also be configured to deliver real-time profile information back to the browser. By default, Lytics only returns the current audiences the visitor is a member of. However, by surfacing user fields, additional data from the visitor\u2019s profile can be returned. Listed below are the core methods available for understanding your website visitors.",
        "Lytics Anonymous ID": "Each time the Lytics JavaScript Tag is loaded, it will attempt to identify the visitor. This is done through a variety of means, the first of which is the first party_uidcookie. A unique identifier is generated and stored in the browser. This_uidis then included as part of the outbound payload in all collect methods from the JavaScript Tag aiding in identity resolution. In some cases, a developer must access or manipulate this ID. In those cases, there are two methods available. Manipulating the_uidis an advanced feature and can significantly impact how your profiles are compiled. Use at your own risk. Because getting the unique identifier can be an asynchronous process, you must pass a callback function with the request. Upon completing the lookup, this callback function will be called and passed an attribute that represents the visitor\u2019s unique identifier.",
        "User Attributes & Audience Membership": "Because loading of the current user's profile is done in real-time, a request to our profile API is required. This request is handled asynchronously as to never negatively impact your web properties. As such, it is recommended that access to this profile is always handled through our callback. This ensures you receive the expected result and do not run the risk of race conditions or other browser issues related to asynchronous requests. For use cases that can't support an asynchronous call, please see audiences cached in local storage below for an alternative option. To initialize a callback simply add a listener on theentityReadyfunction: Once the profile has been loaded, your callback function will be called and passed adataobject. By default, this data object will only contain the audiences the visitor is a member of and have also been enabled for API access. In addition to audience membership, any surfaced user fields will also be available on the object. In the example above the three audiences in thesegmentsarray have been enabled for API access and theemailuser field has been explicitly surfaced for the account. By default, Version 3 of the Lytics JS Tag caches the visitor's audience membership in local storage instead of a cookie. A forthcoming integration has been planned to support writing a cookie as well to facilitate some edge use cases but in the case of zero latency needs, local storage represents a better, more scalable storage solution for this type of data. In order to access the cached values on page load before the Lytics JavaScript tag has loaded the profile, you might implement something like:",
        "Custom User Identifier": "In some cases it is desired to use an identifier different than the Lytics cookie, for exampleuseridfor a logged in user. This configuration option is available by definingbyFieldKeyandbyFieldValueon theentitykey of the core Lytics JavaScript Tag configuration.",
        "Single Page Apps": "Some web properties will be built as Single Page Apps (SPA). SPAs shouldalwaysbe configured and managed by an experienced developer. They represent a series of unique behaviors, many unique to particular frameworks, that can impact how the Lytics JavaScript Tag needs to be configured to support the desired use cases. Version 3 of the Lytics JavaScript Tag has been built with SPAs in mind, along with other significant improvements. For most SPAs, there are two important things to consider as routes change: What represents the view of a newpage.When should the visitor\u2019s profile be reloaded or web-based integrations such as Facebook be updated. Each of these actions are handled independently with two simple commands.",
        "Hosting the Lytics JavaScript Tag": "While it's possible to download and examine the core Lytics JavaScript Tag (JS Tag) source files (latest.js,latest.min.jsorlio.js), storing or serving these file(s) locally or from another domain is not recommended. Referencing the Lytics JS Tag files from Lytics' servers (i.e.,<https://c.lytics.io/api/tag/someid/latest.js>) ensures that you get access to new features and product updates as they become available, giving you the most reliable collection and delivery of your data. That said, in some extreme cases it may be a requirement to serve all JavaScript files from your own secure environment. In these cases, we've created this document to outline the benefits, drawbacks, and a suggested approach.",
        "What are the benefits?": "Compliance with rigid security requirements.",
        "What are the drawbacks?": "Voids previously agreed upon Service-level agreements (SLAs) and support agreements due to the introduction of new and non-Lytics controlled infrastructure.Feature enhancements and bug fixes are made frequently to the Lytics JS Tag. Those that host their tag outside of Lytics will need to manually update the source to benefit from said improvements.Leveraging client-side integrations will require manual configuration while having limited support from the Lytics team.Additional Services hours or agreement to assist with initial installation and setup issues.Many account settings will no longer function as designed since configurations are not able to be updated dynamically.The core Lytics JS Tag endpoint has various levels of caching and protections to ensure it is always served no matter load or conditions. By hosting outside of the Lytics architecture, all caching and scalability concerns must be managed by the customer. Given these benefits and drawbacks, if your organization still wants to proceed with hosting the Lytics JS Tag, see the following steps to get started.",
        "Where should I start?": "Before you begin, ensure you are installing version 3 of the Lytics JS Tag. Version 2 can't be hosted outside of Lytics, which means you will be restricted to version 3 of the tag. Getting up and running with a hosted version of the Lytics JS Tag comes down to three main steps: Accessing the source code and default configuration.Navigating to the source code is relatively simple. The URL can be found by visiting the Tag Installation section of the Lytics app. The tag sample on that page will include ajstag.initfunction. One line below that you will see asrcdefinition and a URL that looks something like//c.lytics.io/api/tag/.... Copying and pasting that URL into a new browser window will result in the core JavaScript tag to be loaded. Most browsers will allow you to File -> Save the source from this point to your local machine.Hosting the source code and default configuration.The output file that was created above will need to be hosted on a public server within your infrastructure. Unfortunately, there is little to no guidance Lytics can offer here other than to be sure the hosting solution can handle the necessary load generated by your web asset(s).Initializing the tag.Return to the Tag Installation section and copy and paste the entire script tag as you normally would. ThesrcURL in theinitconfig will need to be updated with the URL of your now hosted JavaScript tag. You'll likely end up with something like this: Once your tag has been hosted and initialized properly, we recommend you complete the verification steps to ensure everything has been installed and is working properly.",
        "Subresource Integrity": "This document has moved.",
        "Working with Tag Managers": "Tag Managers make it easier to work with vendor JavaScript (JS) tags like Lytics for large, dynamic web assets. Lytics can be integrated through virtually every Tag Manager on the market today. Because each Tag Manager is different, the level of integration and development lift can vary so please consult the documentation for your preferred Tag Manager for specific details. That being said, installation of the Lytics JS Tag is generally as simple as copying and pasting the snippet of code from theinstallation instructionsin the Lytics app to a container within your Tag Manager, and then publishing to your live instance from there.",
        "Installing using Google Tag Manager": "Google Tag Manager(GTM) is recommended as a flexible and transparent Tag Manager. Follow the steps below to install the Lytics JS Tag using GTM. Log into yourGTMaccount.ClickNew Tagand thenTag Configuration.Under the Custom header, selectCustom HTML.Copy theLytics JS Tag snippetand paste it here.Name itLytics JavaScript Tag.In the Triggering section, selectAll Pages.ClickSave.Publish the container by clickingSubmit.Navigate to your website and click on several pages to trigger sample event. Wait a few minutes for the traffic to be received.Check theTag Installation Statusat the top right of theinstallation page.If you are not seeing traffic received, try repeating steps 9-10 before trying the basic troubleshooting steps.",
        "Is the tag loaded?": "The core JavaScript tag has a helper method that will returntrueif all Lytics resources have been loaded properly. Open theconsole in Chrome Developer toolsthen copy and paste the following to verify that the tag has finished loaded.",
        "Is data being sent to Lytics? What data? Why?": "The Lytics Jstag sends data to Lytics after every page view or on a custom trigger configured by the client. Those \"sends\" are captured in the network tab in developer tools. When the data is sent, how it is sent, and if it was sent successfully are all stored within the network call metadata. To start select the network tab and filter on\"lytics.io\". Once the list is populated, select a line that us structured like the one highlighted below:  The request URL can be found at the top of the \"Header\" section. This url indicates that data is being sent to Lytics via the /c endpoint.  The actual attributes that were sent to Lytics can be seen in the \"Payload\" section as seen below:  Testing To test the sending method there is also the jstag.send() function from the console and that will send a single payload to the account configured as the cid in the initialization of the tag. Please see the example below for syntax:  The network call should then show an event sending to Lytics:  _Finally, validate the event made it to Lytics by verifying the event has come in the UI via the stream/pipeline:",
        "Is my visitor's profile being retrieved correctly?": "The Lytics Jstag will use the/personalizeendpoint to retrieve the entity on every pageview. This ensures that the most relevant information is considered when deciding what the consumer experience should be while they are navigating the webpage. To start select the network tab and filter on\"lytics.io\". Once the list is populated select a line that us structured like the one highlighted below:  The request URL can be found at the top of the \"Header\" section. This url indicates that data is being retrieved from Lytics via the /personalize endpoint.  Additionally the preview tab, as seen below, can be very useful in understanding what information has been pulled in to update the consumers profile client side. ",
        "What sort of data is being stored client side? How is this controlled?": "Several types of consumer data are stored client side: Consumer AttributesConsumer behavior Consumer attributeswill be actual data from the profile like first_name, last_name etc... This information is stored on the entity itself and can be accessed for personalization. The Lytics user has full control over what attributes appear client side. _In this example _uids has been allow listed in the account settings to surface client side. _Account Setting:  Console in Dev Tools:  Consumer behaviorsexpose the different audiences a consumer is currently in and current experiences/campaigns they qualify for. Both of these behaviors can be found in the payload returned when calling jstag.getEntity in the console as seen below: Experiences  Audiences ",
        "How can I verify that the correct audiences are being returned?": "You can see the audiences the current user is a member of using the following in yourChrome developer console: This callback will return a list of audiences the user is a member of and should only be called after the user profile has loaded.",
        "Best Practices": "Simultaneous Implementation:Understand the end-state requirements for your data in the CDP to guide the setup of your CDI, avoiding redundant reconfiguration. Feature Allocation:Assign primary tasks (e.g., identity resolution, segmentation) to one platform to reduce complexity and maintenance challenges.",
        "Why aren't my numbers lining up?": "If you're troubleshooting numbers such as user counts within Lytics,check out this FAQon why your numbers may differ and our acceptable margin of error described below.",
        "Acceptable Margins of Error": "It is common to perform quality checks against audiences in Lytics vs. your other sources of truth. In a successful implementation, Lytics can reconcile users across channels and merge profiles, which makes one-to-one comparisons tough. However, below are quality checks that can be performed to ensure that the numbers you are seeing in Lytics match up as expected with other common data sources: Web Active Countsagainst Web Analytics for defined periods (for the difference in filters). Be sure you are comparing the same:Time period/date rangeWeb PropertyTaking into account employee traffic/botsComparing only audiences with a web cookieNumber of Subscribersby List (for profile merging -- e.g., a user may have more than one email).Build various audiences in Lytics based on file sources and compare numbers to original data. Google Audience/Web Analytics often reports higher numbers than Lytics. One main reason is Google Analytics reports how much unique traffic made it to a page. Lytics reports how many user profiles visited a page across browsers and devices. For any additional validation, you can export raw events or audience membership to Google Big Query, Amazon S3, or another data warehouse to query against. Please contactLytics supportfor assistance understanding or troubleshooting these numbers.",
        "\ud83d\udc4d": "Lytics audience segments apply predefined rules to each user profile as they update. Membership in these segments is maintained in real-time and can trigger subsequent actions when users enter or exit the segment.",
        "Available Attributes": "The following attributes are all available out of the box with no customization necessary in all Lytics pricing tiers. Do note that any attributes flagged asComputedcan not be edited directly but are computed based on various factors, including other non-computed attributes.",
        "Identifiers": "Identifiers in Lytics Schema serve to stitch profiles together. For instance, if you pass anemailalong with the _uid, all events that have only been associated with either identifier will be merged into a single comprehensive profile. Default identifiers are listed below.  Some jobs in your data pipeline may add new identifiers to your schema, likeshopify_customer_idorsalesforce_contact_ids.",
        "Details": "The Details section displays all the information about setting up your Connection, including the authorization and configuration settings.",
        "Meta": "Lytics automatically generate this and represents the oldest event associated with the user. This is automatically generated by Lytics and represents the last time the user was modified. This is automatically generated by Lytics and represents the last time the users scores were updated. This is automatically generated by Lytics and represents the number of aliases associated with the user. This is automatically generated by Lytics and represents the number of days the user has existed. This is automatically generated by Lytics and represents the number of events associated with the user. This is automatically generated by Lytics and represents the number of streams associated with the user. This is automatically generated by Lytics and represents the names of the streams associated with the user. This is automatically generated by Lytics and represents whether the user has been flagged as a bot or not.",
        "Behavior": "In the \"Behavior\" section, you'll find a graphical representation of Lytics' ninebehavioral scores. These scores offer valuable insights into the user's engagement patterns across channels, helping you tailor marketing strategies and campaigns to their preferences.",
        "Interests": "Interests entail understanding the topics a user is interested in based on their interactions, cross-referenced by deep programmatic analysis of their online activities. This allows for tailored content recommendations and targeted messaging aligned with the user's preferences and engagement history.",
        "Intelligence": "The intelligence tab gives a snapshot of key information about an individual user. At the top, a bar is displayed to help you quickly understand user engagement across all channels:Likelihood to re-engage,Current engagement level, andFrequency of user interactions.  Each of these boxes summarizes the data science-powered behavioral scores into practical terms. The individual behavior scores and the user\u2019s content affinity are displayed below. This rich data based on a user\u2019s actual behavior is available thanks to the predictive analytics built into Lytics. ",
        "Activity": "The \"Activity\" section overviews the user's activity over time. You can track when they are most active and identify the best times to communicate with them. This historical data helps optimize the timing of your interactions for maximum engagement.",
        "Web": "This attribute is automatically populated with the domains the user has been active on. This attribute is automatically populated with the first time the user visited the site and sends data to thedefaultstream. This attribute is automatically populated with the last time the user visited the site and sends data to thedefaultstream. This attribute is automatically populated with the number of_pvevents recieved for the user. This attribute is automatically populated with the referring domain for the user. This attribute is automatically populated based on the user agent of the browser. This attribute must be turned on in your Lytics account to be collected. This attribute is automatically populated with the number of visits the user has had based on presence of the_sesstartkey in an event. This attribute is automatically populated with the city the user visited from based upon GeoIP. This attribute is automatically populated with the country the user visited from based upon GeoIP. This attribute is automatically populated with the region the user visited from based upon GeoIP. Form data is a wildcard attribute that allows you to pass a number of key value pairs that all get stored under theform_dataattribute. This is useful for capturing form submissions.",
        "Machine Learning at Your Fingertips": "Creating aLookalike Modelis as simple as selecting aTargetandSourceaudience, to drive users from the source to the target (think [unknown --> known], [1-time purchaser --> repeat purchaser], etc). Once configured, Lytics will train a model based on the data accessible on user profiles, and output meaningful insights and predictions to help you grow your target audience. Some common use cases for Lookalike Models include: Unknown to Known: among my unknown users, find users who are likely to sign up for an emailExpand Reach: among my users who\u2019ve purchased one item, find users who are likely to become repeat purchasersChurn Prevention: find customers who are at risk of churning",
        "Built by Data Scientists, for Marketers": "Lytics' Lookalike Models incorporate modern techniques to ensure your models outperform traditional marketing methods. When building models, Lytics applies feature-reduction techniques and trains a set of Random Forests and Logistic Regression models, followed by a suite of model-tuning and cross-validation to determine the best configuration and hyper-parameters. Unlike traditional statistical modeling approaches, Lytics Lookalike Models update user scores in real-time, so you can start targeting users not only when the model is built, but also as their behavior changes or new users are added. This means that a prediction for a user will update after every event (ie: page click, email open) which helps ensure that you are targeting the best users. Rather than using a static list, predictive Audiences (Lookalike Audiences) built from Lookalike Models provide a dynamic pool of users that will respond best when they are ready for ads or other marketing messages. You can also adjust your targeting criteria to make the best tradeoffs between reach and accuracy to maximize your marketing budgets. Lytics also provides robust APIs for Lookalike Models, so your engineering team can quickly create and test models via the Lytics API.",
        "Import Contacts and Activity": "Import contacts and email activity from HubSpot to drive better engagement through Lytics.",
        "\ud83d\udcd8Cloud Connect in Action": "Check out thesolution architecture developedwith Google which leverages BigQuery and Analytics Hub in addition to Cloud Connect to create a rich pattern for secure data collaboration and activation.",
        "Accessing Cloud Connect": "Cloud Connect is available to all Lytics customers. If you don't currently have access via the product switcher at the top of the primary navigation, contact support or your account management team.",
        "Common Use-cases": "Cloud Connect unlocks many use cases enabling marketers to segment their customers based on any attributes stored in their database. A few examples include: Time Window: All users who did not log in last month.Joins (B2B): All users associated with accounts without feature \"x.\"Lifetime Value (LTV) or Rollup: All users with a premium subscription have purchased at least two products.",
        "\ud83d\udcd8Need a warehouse?": "Don't currently have access to a data warehouse? Lytics has you covered with Lytics Warehouse. We provide a simple data warehouse on top of Google BigQuery to all of our customers as part of Conductor. Simply reach out to account manager or support for access and information.",
        "How it Works": "Lytics has developed the Cloud Connect product to address this, which serves as a materialized layer atop the cloud warehouse and facilitates direct segmentation and activation. This model allows customers to employ either SQL or AI to craft consumer audiences, which can subsequently be activated downstream. Importantly, this method does not involve long-term data storage within Lytics but instead generates a transient profile context that disappears once the Cloud Connect model is deleted.",
        "Topic Taxonomy": "Using data science techniques to look at the topic overlap between classified content, Lytics will programmatically build a topic taxonomy. In addition to programmatically building this taxonomy, Lytics dynamically adjusts the taxonomy as new content gets published. The topic taxonomy is stored as a weighted and directional graph. Although this structure may be daunting to look at\u2014and even more daunting to try and utilize by hand\u2014Lytics uses it when determining content recommendations. ",
        "Topic Relationships": "Topics are determined to be related by evaluating how they occur together and how they occur independently. When two topics occur together frequently (meaning in multiple documents), it is safe to assume they are related. In addition to the relationship existing, Lytics will determine thedirectionof the relationship. Is \"Cookies\" a subtopic of \"Baking\" or is \"Baking\" a subtopic of \"Cookies\"? The co-occurrence of topics determines the relationship. The independent occurrence of a topic determines the direction. Since the topic \"Cookies\" frequently occurs with \"Baking,\" but \"Baking\" frequently occurs without \"Cookies,\" \"Cookies\" must be a subtopic of \"Cookies.\" To draw this example further, \"Baking\" may be a subtopic of \"Recipes,\" \"Recipes\" may be a subtopic of \"Cooking,\" and \"Cooking\" may be a subtopic of \"Hobbies.\" This is important because it allows Lytics to make affinity inferences. A user with a high affinity for \"Cookies\" may be interested in general \"Baking\" content.",
        "Weighting Relationships": "The weight between relationships is incredibly important to consider when making affinity inferences. For instance, Michael Jordan played baseball professionally for one dark year in the 90s. This means there are valid relationships between Michael Jordan and Baseball and Michael Jordan and Basketball. Most people who know anything about sports or pop culture know Michael Jordan as a basketball icon. The way this gets reflected in taxonomy is through relationship weights. The weight between Michael Jordan and Basketball is strong, while the weight between Michael Jordan and Baseball is weak. By having weights, affinity inferences can use those weights as thresholds for building further relationships or recommending content. A user who has shown interest in Michael Jordan is more likely to be interested in Basketball than they are to be interested in Baseball. Unless they are interested in Sports Icons From The 90s, they might be more interested in Ken Griffey Jr. A rich topic taxonomy will surface this nuanced information.",
        "The Graph Representation of the Taxonomy": "A graph is the ideal data structure for topic taxonomies and taxonomies in general. Since each topic can have many subtopics and be the subtopic of many things, the correct way to structure this data is with a graph. Leaders in data modeling use this approach. Notable examples are Facebook'sSocial Graphand Google'sKnowledge Graph.",
        "Export Audiences (Lifecycle Optimizer)": "SailthruLifecycle Optimizerallows you to automate email campaigns based on user events, such as entering an audience. By exporting your Lytics audiences in real-time, you can activate automated interactions with your users immediately as changes to their profiles are processed.",
        "Set up Lifecycle Optimizer": "Before configuring an export job in Lytics, you will need to set up Lifecycle Optimizer in your Sailthru account. Login to your Sailthru account and navigate toCommunications>Lifecycle Optimizer.Set up a Lifecycle Optimizer flow schedule by declaring whatActionshould happen when a user joins a specified list (onlyList Joinedoption will work in theEntrypart of the Lifecycle Optimizer flow).The list should be namedLyticsfollowed by the audience name you want to export from Lytics. Either create a new list corresponding to the audience from theUsers>Listsor use an existing list. See the SailthruLifecycle Optimizerdocumentation for more information.",
        "Add encrypted email to Sailthru links": "Ensure the Lytics Sailthru importHash Algorithmoption is set to 'SHA-256' (default).Log in toSailthruand navigate toCommunications > Templatesthen select your email template.Select theAdvancedtab.Copyemail_hash={sha256(email)}into theAuto-Append Link Parametersfield. Now all email clicks will pass the encrypted SHA256emailto your website when users click the links, and Lytics will map the encrypted email to the user's profile.",
        "Privacy and Data Protection": "As a service provider and data processor, Lytics assists its customers in enhancing security and meeting privacy and data protection obligations, including the European Union\u2019sGeneral Data Protection Regulation(GDPR) andCalifornia Consumer Privacy Actof 2018 (CCPA). Listed below are the compliance-enabling functionalities Lytics provides. As used below, personally identifiable information (PII) has the same meaning in the CCPA and is meant to include \u201cpersonal data\u201d as defined in the GDPR. Please consult your company\u2019s legal counsel or privacy professional about what privacy and data protection requirements your company must comply with. Lytics recognizes that its customers are the data controllers of the PII, which Lytics processes on their behalf. Each Lytics customer maintains control over which PII sources and destinations to use with Lytics and the types and content of PII shared between its sources and destinations. Lytics does not sell PII or supplement its customers\u2019 respective PII with third-party data except via customer-directed integrations.",
        "Access Control": "Restrict access to personal information by role. Role Based Access Controls(RBAC):Account Admins can easily add and remove account users. Lytics has various defined user roles with respective permissions.Single Sign On(SSO):Add SSO to your user login process to enhance security.Multi-factor authentication(MFA):Add MFA to your user login process to enhance security.Restrict access to PII:You can indicate any user fields that contain PII via the private fields account setting. These fields will be hidden for anyone who does not have Admin, Data Manager, or User Search roles. You should verify with Lytics Support that the field hiding in the segment scan is also enabled for your account to ensure these fields are also hidden there.",
        "Data Mapping": "Map personal information processed by Lytics, including sources and destinations. Audit your data schema:Use theSchema Auditfeature to see what user fields are being populated, the data contained, the source(s), and if that data is being used in audience definitions.Determine third-party data sources:You can see the third-party data sources from which you send data to Lytics using the Lytics UI by navigating toData>Data Streams. The \"default\" stream will contain your web data unless otherwise configured. You will see your other integrations in the list of stream names using the drop-down menu at the top right. Each stream page will show you the last time Lytics received data.Determine third-party data destinations:You can view the activity history for a data destination using the Lytics UI by navigating toData>Integrations. Click on the tile for the integration in question. If you're already running the integration, you will automatically be taken to the overview page that shows a list of running imports and exports and the history of events for those works.",
        "Notice and Consent": "Manage user consent and preference data.",
        "Obtaining Customer Consent": "Lytics customers are responsible for obtaining consent for collecting and transferring PII to Lytics for processing. You can use the Lytics JavaScript Tag to collect PII about customers' online behavior. One consent mechanism is to implement a custom tag and trigger in your Google Tag Manager account and assign that trigger to the Lytics JS tag. See how to manage consent with Google Tag Manager. For sites not using Google Tag Manager, customer consent on the web can be managed in several ways. Consent triggers can be listened to by adding a snippet of custom JavaScript to your site. Another alternative is to use a cookie-consent solution, many of which exist. Github has documented afree solution, including demos. The Lytics JS Tag can be configured to consume triggers from any of these solutions to manage consent for your customers. Recording Proof of Consent:Schema fields may be established for the purpose of storing customer consent.Privacy Policy Notice:When you use a Lytics modal to collect user information, you should include a link to your organization\u2019s privacy policy regarding the treatment of the PII collected. A link can be added to any modal created using the Experience Editor.Age Gating:If you have collected accurate age data, you can build audiences that target or exclude certain ages.",
        "Responding to Consumer Requests": "Respond to the data subject (consumer) requests in compliance with regional and state privacy and data protection requirements. Personal Data Access:Using thefind a userfeature, enter the identifying details provided by the consumer to locate their profile. The profile \"created\" date refers to the earliest date Lytics collected any data on this user.Personal Data Correction:If user profile data requires correction, you must send the corrected data to Lytics, which will be remapped to convert the resulting user profile information.Determining Categories of Personal Data Collected:You can use the Lytics UI to obtain information about the categories and specific pieces of PII collected on a consumer in the past 12 months. Again using the find a user feature, you can view the fields of populated data and determine the appropriate consumer PII categories to disclose to a requesting data subject/consumer.Personal Data Portability:We support the export of profile information via the Lytics UI or APIs. An individual\u2019s profile data from Lytics will be downloaded as a JavaScript Object Notation (JSON) file. JSON is a common, machine-readable file format.Personal Data Deactivation/Suppression:You can establish audiences to enforce consumer suppression and \u201cdo not market\u201d choices and prioritize those choices when establishing marketing journeys for your consumers. These audiences can be exported from Lytics to your downstream tools or \"data destinations.\"Personal Data Deletion:We provide a method for deleting users via the Lytics UI. Our API may also be used for this purpose. This will send a deletion request to the Lytics platform, which will process the request for the customer identifier provided.",
        "SOC 2 Audits": "An independent auditor has examined Lytics platform controls and confirmed they are in accordance with the Service Organization Controls (SOC) 2 Type II Trust Services Principles for Security, Availability, and Confidentiality. You can learn more about our SOC 2 Type II examination in thisblog post. Lytics will continue to engage independent auditors to conduct SOC 2 Type II audits regularly and make our audit report available to our customers and prospects under an NDA. In addition, we retain independent security firms to conduct regular penetration tests and vulnerability scans on our systems and code, respectively. Our underlying cloud services provider, Google, also submits to regular, multiple independent audits, including SOC 2 Type II audits.",
        "Safeguards and Transfers": "Lytics Data Protection Safeguards:Lytics and its data hosting partner, Google, have implemented numerous safeguards to protect the PII that Lytics processes on its customers' behalf. External auditors audit these safeguards on an annual basis. For more information regarding these safeguards, please ask your Lytics account manager.Transfers of Personal Data from EU:Lytics participates in the EU-US and Swiss-US Privacy Shield Frameworks regarding collecting, using, and retaining personal data from European Union member countries and Switzerland. We have certified with the U.S. Department of Commerce that we adhere to thePrivacy Shield Principles. Please let your account manager know if your organization requires Lytics to enter into the EU Standard Contractual Clauses regarding data transfer from the EU to the U.S.",
        "Web Personalization Introduction": "",
        "What does this product do?": "Lytics Web Personalization is a robust product that allows you to present tailored website messaging tospecific users of your audience. It's easy to create lead gen forms, recommended articles, and promotions,which can be displayed in-line, as overlays, or announcement bars. More advanced users can leverage thePathforaSDK to create richer personalizations that go beyond what is possible in the Lytics UI.",
        "Why would I use this tool?": "Some common uses of Lytics Web Personalization are: Present an email capture form to anonymous users, but hide it from users who have already shared their email address.Show users the right call-to-action based on their behavioral and event-based characteristics.Let eligible users know about a targeted sale or promotion.Keep your audience engaged by serving them content that is personalized to their browsing behavior.",
        "How does this tool work?": "Lytics Web Personalization is, at its core, a product that allows you to serve different web experiencesto users from different audiences. With the campaign editor, you can design a widget, pick an audience,pick display criteria, and then publish your campaign. Once your campaign is running, Lytics will reporton conversions and impressions. You can even create sub-audiences based on campaign conversion data, allowingyou to string campaigns together for richer personalization.",
        "What types of campaigns are available?": "The four types are: Drive Traffic CampaignCollect Leads CampaignPresent a Message CampaignRecommend Content Campaign Thanks to the expressive nature ofAudiences, these fourdifferent campaign types offer a lot of mileage.",
        "Keep in Mind!": "Alerts can be configured for specific workflows, or for all workflows running in the account.Consider having one general alert to an Operations Channel or Team for all alerts, or alerts for specific workflows managed by particular teams Consider which statuses to includeAlerts can be configured for statuses aside from failures. Workflows like critical batch import workflows can send a confirmation on successful syncs as well. Understanding errorsIf a source or destination job has failed, Lytics will show the latest error message on the Conductor Diagnostics Dashboard and on the Logs tab of the Source/Destination Job Summary interface, and allow the job to be restarted if needed. The most detailed information for troubleshooting can be accessed from the Job Logs API.",
        "What is a \u201csync\u201d in the context of Cloud Connect?": "A sync refers to the process of running SQL queries configured on Data Models against data warehouses and then updating Lytics profiles with any changes that have occurred since the previous sync.",
        "Will my SQL query only be run once per sync?": "Most of the time, yes. However, if an interruption occurs during the sync (either due to a processing or network error), we\u2019ll attempt to resume the sync from the place where we left off. If that occurs, we\u2019ll need to rerun the query.",
        "Why are the sync times slightly different than what I\u2019d expect?": "On the Data Model Details page, we display two timestamps related to the sync for your Data Model: \u201cLast Sync on\u201d and \u201cNext Sync On\u201d. The \u201cLast Sync On\u201d timestamp displays the exact time when your Data Model was last synced. However, the timestamp shown for \u201cNext Sync On\u201d will vary slightly from the time when your model will actually sync. In an effort to reduce the number of syncs needed, we set the \u201cNext Sync On\u201d timestamp in a way where more models will end up synced together. Then every hour, we check to see which models are ready to sync - if \u201cNext Sync on\u201d is in the past, we sync the model. Ultimately, you should think of your selected sync frequency as the number of times per day your model will sync (e.g. a model with a sync frequency of four hours will sync six times per day). The \u201cNext Sync on\u201d timestamp gives an estimate as to when the next sync will occur rather than the exact time.",
        "Why can\u2019t I create a Data Model that returns no rows?": "We use the columns returned by the SQL query to set up our internal schema. If no data is returned by the query, we don\u2019t have the required information to correctly set up our schema. Note that once the schema is set up, your Data Model will continue to sync regardless of how many rows are returned.",
        "Why can\u2019t I create/update a Data Model when my schema has unpublished changes?": "Creating a Data Model modifies your Conductor schema - at a minimum, a true/false \u201cmembership\u201d field will be added to denote whether a profile falls into that Data Model. In addition, one field is added for each activated field that\u2019s selected. In order for these fields to be added, your Conductor schema cannot be in a draft state. When updating a Data Model, we\u2019ll attempt to update your Conductor schema if any activated fields are being changed. For this reason, if you try to edit your activated fields and your schema has unpublished changes, we\u2019ll reject the update.",
        "Can I select any field as the primary key?": "The UI allows for selecting any column that\u2019s returned by your SQL query as your primary key. However, it\u2019s important that the selected primary key field contain unique values. If there are duplicate values, we cannot guarantee that the values for your activated fields will be correct. You should choose a field whose values will match to the values of an existing \u201cby\u201d field in your Lytics schema.",
        "Can I select any field as the Lytics key?": "No; only \u201cby\u201d fields can be selected. Further, once a Data Model has been created for a certain primary key, all subsequent Data Models which use that primary key must also use that same Lytics key.",
        "Sync Failures Related To Collation": "Cloud Connect requires that the column you use as the primary key of your data model queries be UTF-8 collated. If your column is collated differently, this can cause your data model syncs to fail. Most database providers allow columns to be converted to a particular collation within a query. If your primary key column is collated differently, consult the documentation of your database provider to ensure a compatible collation: SnowflakeBigQueryAzureRedShift",
        "Universal Schema": "Lytics' universal schema delivers a set of intelligent attributes that areuniversal to every profilethat gets created, regardless of data source, age, or their anonymous or known status. These attributes include rich behavioral models, AI-derived interests, and identity resolution metadata, leading to more comprehensive insights on all of your profiles.  (Not just the known ones!) Universal attributes provide a common vocabulary with which to think about how you might engage with each and every individual \u2014 what they're interested in, what their engagement has been, or how they're likely to engage in the future.",
        "Metafields": "Metafields encompass all system-level information that provides insights into the health and breadth of the profile. This includes data such as creation date, last update timestamp, source information, and other metadata associated with the profile's management and maintenance. Metafields offer a behind-the-scenes view of the profile's overall status and administration.",
        "Behavioral": "Behavioral attributes comprise a multidimensional set of scores that provide insight into an individual's behavior over time. These insights are invaluable when personalizing experiences based on changes in behavior or behaviors indicative of high likelihood. For instance, you might want to present a premium offer to users exhibiting higher momentum than usual. Behavioral attributes enable targeted and timely interventions tailored to user actions and patterns.",
        "_split": "Random splits are assigned by algorithmic hashing and result in an even distribution across profiles. There are 100 possible splits (0-99), so creating an audience where_split = 44would result in a random subset of 1% of profiles.  A random split where_split < 50would result in a subset of 50% of profiles.",
        "_split2": "Having access to two random split assignments allows you to target 100*100, or 10k different subsets of your audiences. This means you can create splits that are capable of targeting down to 0.01% of profiles.",
        "segment_prediction": "Lytics' SegmentML models provide scores for modeling custom outcomes in Lookalike models.  Each active model attaches a score (between 0 and 1) to a profile with its name.",
        "segment_prediction_percentile": "Similar tosegment_predictionvalues, the percentiles provide the ability to create audiences and target users based on theirpercentileinstead of their raw score.  This enables you, for example, to create audiences of users with the top 10% of scores (by usingsegment_prediction_percentile > 90).",
        "Common Schema": "Commonfieldshelp to structure your profiles in a way for developers to easily plug into them, and for marketers to easily use them in personalization. Commonmappingshelp to translate your source data into a format that can be used by common fields. These fields and mappings help to provide consistency in your profiles, regardless of the source from which the data came. For example, your CRM might have\"tax_exempt\": \"true\", while your data warehouse might has\"tax_status\": \"exempt\", and your ESP might have\"is_tax_exempt\": 1.  In Lytics' Common Schema, this would simply be represented as\"tax_exempt\": true.",
        "Pre-defined Mappings": "For your convenience, Jobs managed in your Data Pipeline will automatically create schema fields and mappings to implement Lytics Common Schema in your data so that you can get to more use cases more quickly with ourInspiration Library. Some fields, likename, are automatically part of every Lytics account. Other channel-specific fields will be added when you add channel-specific data sources to your account. For example, adding a Shopify import job to your data sources would add the common schema fieldpurchase_total, among others.",
        "email": "Extract email address from \"Bob <emailaddress@lytics.io>\" format, note that the extracted email addresses are converted to lowercase and checked to ensure the existence of@symbol as well.",
        "email_sha256": "The SHA256 hash of an individual's email address.  In the event that your account has an email address present, the hash is automatically computed so as to be compatible with external systems that require the SHA256 hash for integration.  In the event that you don't want any PII in your account, you can pass the hash directly instead of the raw address.",
        "external_id": "Optional, and if provided, this will usually come from your proprietary, first-party database.  This will be used for profile stitching and should therefore be unique to an individual.",
        "hashedurls": "A history of an individual's most recently visited URLs, helpful for powering Lytics' Interest Engines. A siphash of each URL's sanitized value (without query parameters) is used to index the map. hashedurlsare used in Lytics' standard Interest Engine, but you can create additional Interest Engines with other sets of URLs, product IDs, or atomic content IDs from your CMS.",
        "needs_message": "The \"needs message\" score evaluates how likely an individual is to engage on a channel without any messaging. A higher score would indicate that an individual is less likely to return within a typical engagement window (relative to other individuals in a data stream).  A lower score would indicate that they are more likely to return within a typical engagement window.",
        "name": "An individual's name may take a few forms, depending on the source, and this field may sometimes be just the first name, a combination of names, or completely empty.",
        "Geo": "Geolocation attributes are inferred from geolocation on network activity.",
        "Device": "Device attributes describe the devices that have been associated with an individual and their activity.",
        "device_advertising_id": "This may be a Google Advertising ID (GAID) or an Apple Identifier for Advertisers (IDFA).",
        "device_advertising_ids": "These may be a Google Advertising ID (GAID) or an Apple Identifier for Advertisers (IDFA).",
        "browser_hash": "While not a default part of your identity graph, this field may be promoted to an identifier to perform identity stitching.  This should be performed with caution as it is not guaranteed to be unique and can result in stitching individuals with similarly configured browser.",
        "locale": "An individual's locale is usually a combination of an individual's language code and country code.",
        "language": "An individual's language code, standardized by ISO-639-1 two-letter codes.",
        "user_attributes": "User Attributes are helpful for storing custom attributes about an individual without having to create and map additional user fields.",
        "form_data": "Form Data is helpful for storing custom form data from an individual without having to create and map additional user fields.",
        "hourly": "hourlyutilized the timestamp of the event being collected, and does not require any mappings.",
        "hourofweek": "0-167 Integer representing the hour of the week (12:00-12:59am || 00:00:00-00:59:59 on Sunday is 0 hour)",
        "consent_status": "The marketing tracking and activation channels to which an individual has expressly given consent.  This typically includes the following values: \"email_marketing\", \"sms_marketing\", \"push_marketing\", \"cookies_necessary\", \"cookies_performance\", \"web_functional\", \"web_targeting\", \"web_social\".",
        "consent_status_ts": "The time at which the consent has most recently been confirmed.",
        "consent_status_source": "The source of the consent status.",
        "Lookalike Model View": "The Lookalike Model summary view is divided into four distinct tabs, each offering valuable insights about your model: The initialSummarytab provides an overview of the model, including its configuration, key features, and associated audiences.TheAudiencestab lists all Audiences that incorporate your Lookalike Model.TheConfigurationtab outlines the settings and parameters used to construct the Lookalike Model.TheDiagnosticstab presents statistical metrics such as R-squared and AUC scores, offering a detailed evaluation of model performance.",
        "Model Summary": "The model summary dashboard highlights the essential metrics of your model, including Accuracy, Reach, and the features it leverages. It also enables you to create impactful predictive audiences, using Lytics' predefined options or custom definitions tailored to your needs. TheSummarysection at the top of the image above features three key tiles. The left tile displays the number of audiences utilizing this model, along with the sizes of the Source and Target audiences involved.The middle tile highlights the model\u2019sConfidence,Accuracy, andReachmetrics.Lytics determines modelConfidencebased on the model\u2019s ability to make accurate predictions -- predictions from unhealthy models should not be trusted. Models are considered unhealthy if the accuracy is low (with an R2value less than 0.1), or if the model is overfit (with an R2value greater than 0.975).Accuracy([0 - 10]) measures how precisely the Lookalike Model's predictions align with actual outcomes during training. Models with higher accuracy are ideal for narrowly targeting top lookalike candidates, making them well-suited for the later stages of your marketing funnel.Reach([0 - 10]) reflects the relative size of the audience addressable by the Lookalike Model. Models with higher reach are better for broadly targeting larger user groups, making them more effective for earlier funnel stages.The right tile shows the total number of features and provides a breakdown of their types.",
        "Model Exploration": "TheModel Explorationsection provides detailed insights into your model and visualizes how users in yourSourceaudience are categorized based on their likelihood to convert to theTargetaudience. The left side of the image above displays the audience sizes for the Source, Target, and Lytics' predefined audiences: Unlikely: users in theSourceaudience who have asimilarity scoreunder the 25th percentileSomewhat Likely: users in theSourceaudience who have asimilarity scoreabove the 50th percentileLikely: users in theSourceaudience who have asimilarity scoreabove the 75th percentileHighly Likely: users in theSourceaudience who have asimilarity scoreabove the 90th percentile Clicking any predefined audience updates the Venn diagram on the right, providing a dynamic visualization of audience overlap.",
        "Advanced Model Exploration": "TheAdvanced Model Explorationfeature provides a deeper dive into the relationship between theSourceandTargetaudiences. When open, it displays a detailed chart that highlights the overlap in predictions between the two audiences. A slider bar within this feature lets you dynamically adjust thesimilarity score threshold. As you move the slider, the chart updates in real-time to show only users in theSourceaudience whose similarity score meets or exceeds the selected threshold. This interactive tool not only offers a clear visualization of how your model performs but also enables you to refine the threshold to better suit your predictive Lookalike Audience. By fine-tuning this setting, you can precisely target the users most likely to convert, enhancing the effectiveness of your predictive audiences.",
        "Creating new Lookalike Audiences": "TheCreate new Lookalike Audiencebutton, located above the Venn Diagram, provides a convenient way to build predictive audiences directly within the Lookalike Model interface. Clicking the button opens a modal with two editing options: Quick Editor: Use the streamlined inline audience-builder to quickly create a new audience without leaving the current page.Advanced Editor: Select this option to open the standard Audience Builder in a new tab, offering full access to advanced configuration tools and features.",
        "Feature Importance and Correlation": "TheFeature Importance and Correlationcharts highlight the key features influencing user conversions from the Source to the Target audience. The Importance chart ranks these features by their relative importance, as determined by the model, from most to least significant. The Correlation chart ranks features by their correlation to the Target audience. By analyzing this data, you can gain valuable insights into the characteristics shared between your Source and Target audiences. These insights can help you refine your audience-building strategy, including the option to incorporate specific high-impact features into new audience definitions for improved targeting.",
        "Lookalike Audiences": "Once your model becomesActive, you can start creating Lookalike Audiences\u2014predictive audiences that utilize your Lookalike Model. The Audiences tab serves as a centralized hub, displaying all your existing Lookalike Audiences and providing the option to create new ones directly from the interface.",
        "Model Configuration": "TheConfigurationtab displays the settings used when creating the model.",
        "Model Diagnostics": "TheDiagnosticstab provides an advanced view into a Lookalike Model's performance and can help provide assurance that the model has a robust statistical foundation. This information is helpful for technical users looking to gain extra insight into the data science behind this Lookalike Model. R2: A measure of how successfully a model is able to predict its desired outcome.  It can be interpreted as the percentage of variability in outcome for which the model accounts.  A perfect model would have an R2value of 1, and an ineffective model would have a value of zero.Mean Square Error: A measure of how closely a model\u2019s predictions matches actual outcomes.  The scale of MSE has no inherent interpretability.  When comparing two similar models, a lower MSE would indicate a better fit.False Positive Rate: The percentage of \u201cfalse positives\u201d in a model -- that is, percentage of users in the source audience that the model misclassified as users in the target audience.  While the false positive rate shouldn\u2019t be too high, a large false positive rate tends to yield a model with higher reach.False Negative Rate: The percentage of \u201cfalse negatives\u201d in a model -- that is, percentage of users in the target audience that the model misclassified as users in the source audience.  A lower false positive rate means that a model succeeds in correctly identifying the latent characteristics of the target audience.Overall Error Rate: The total percentage of misclassifications that occurred during model training.Accuracy: A measure derived from a model\u2019sR2value, rounded to the nearest decile.Reach: A measure derived from a model\u2019sspecificityvalue, typically ranging from 0 to 10.  A larger value of reach indicates a larger proportion of the source audience that behave similarly to the target audience.AUC: A measure of the area under a receiver operating characteristic curve (or \u201cArea Under the Curve\u201d).  ROC curves are used to determine the optimal decision threshold for a given model.Decision Threshold: The \u201coptimal\u201d model prediction value to use as a cutoff when constructing an audience definition.  This threshold is calculated assuming that a false positive should have the same \u201cpenalty\u201d as a false negative.",
        "Activating Lookalike Models": "When you're ready to start using your model, you canActivateit to calculate and assignsimilarity scoresto your user profiles. Once activated, the process may take up to an hour to fully calculate and save the scores. Deactivatinga model stops the scoring process, meaning that neither new users nor existing users will be scored by the model moving forward. However, any users who already have scores will retain those values, even after the model is deactivated.",
        "The Pathfora JS SDK": "Pathfora JS is a lightweight SDK for displaying personalized modules on your website. Lytics web campaigntools serve as an interface for the Pathfora API, but advanced users may wish to interact directly withPathfora to achieve deeper customization. You will be able to track any campaign created with Pathforawithin Lytics. Read Pathfora JS documentation \u00bb",
        "Define Source and Target Audiences": "Begin building a Lookalike Model by selecting yoursource and target audiences. The desired outcome for thetarget audiencein this case is \"multi-purchasers.\" Thesource audiencewill be \"single purchasers\" who have shown that they are likely to convert and become repeat purchasers.",
        "Build Lookalike Model": "In the Lytics UI, go to theLaboratorysection and clickCreate New Modelat the top right. On the configuration page, you'll select the source and target audiences that you previously defined. By default, theAuto Tuneoption is turned off. If you\u2019re not getting enough performance out of your models, try Auto Tune, which will select the default parameters for you. SelectModel Training Onlyif you're in an exploratory phase and not ready to target users based on this model's predictions. Learn more about the configuration options in ourModel Builderdocumentation. Depending on the sizes of your source and target audiences, your Lookalike Model may take a few minutes up to a few hours to build. On yourModel Dashboard, you'll see a \"Building\" status until the model is complete.",
        "Create Predictive Audience": "Once your model is built, view theModel Summaryto determine if it's ready for use in your campaign. You mustActivatea Lookalike Model before creating a Predictive Audience, which will evaluate users and write the model's prediction scores to user profiles. This process can take up to three days for large audiences.  On the summary page, clickCreate Predictive Audiencein the Model Usage section, which will open the audience builder pre-populated with your model's predictions. The default model decision threshold of 0.5 is used, but you canadjust the decision thresholdas needed to reach more users or be more accurate to only target users who are most likely to make additional purchases. ",
        "Next Steps": "After creating a Predictive Audience of single purchasers most likely to become multi-purchasers, you may want to target these users with discounts, loyalty programs, referral programs, or emails to drive sales.",
        "Select Provider": "This integration leverages our Webhook integration to connect with and retrieve data from Ansira's endpoints. Begin by selecting theWebhooksprovider tile. ",
        "Select Method": "Next, select an authorization method. If this is your first time creating an authorization for Ansira, selectOAuth 2.0 Client Credentials Grant. ",
        "Configure Authorization": "Next, we'll configure the authorization to use a key and secret provided by Ansira. Please configure all fields as outlined below and leave any additional fields blank unless you fully understand the implications.",
        "Create Webhook Template": "Webhook templates are created usingjsonnet. These templates provide a flexible way to reformat the outbound payload of a webhook request to meet the recipient's requirements. In this case, we'll create a template that provides the proper formatting of email and source ids for the AnsirauserAPI. This step may appear to be technical. As such, if you need assistance, please get in touch with your technical account manager or primary point of contact. Jsonnet is highly customizable. In the example above, we pull theemailfrom the profile and hard code asourceCodekeyNameIf the desire is for thekeyNameto be dynamic, this could also leverage the.event.getmethod to pull the value from each profile. If you use the dynamic method, the field name in the event.get() function is the field name that has been defined in the Lytics schema, as seen in the example below:",
        "Configure  Webhook": "In the final step, we will combine it by configuring our webhook destination to use the auth, templates, and audience to enrich a Lytics profile. It is important to ensure the auth is selected for the corresponding webhook trigger for entrances and exits. Because they have different structures, the data will not be updated properly in insider if this step is not followed. This integration leverages our Webhook integration to connect with and retrieve data from Ansira's endpoints. Begin by selecting theWebhooksprovider tile. Next, select theAudience Triggers Webhookjob type. This webhook job will pass the response of each request to the configured data stream to be mapped back to a user's profile. There is a list of fields available to configure for the Lytics webhook. The details for those configurable fields can be found in the webhook documentation. Below are the required fields specific to the Lytics and Insider workflow. Select one or more audiences to enrich with Ansira data. We highly recommend following the suggested approach in theaudienceof this doc to prevent invalid or unnecessary calls to the Ansira API. Select the template that was created in the previous step. This should be Exit or Entrance, depending on the JSONNET template it is linked to. Do not select any fields. This will default to syncing the entire profile based on the defined template.",
        "Amazon Ads: Export Audience": "Send Lytics user profiles toAmazon Adsto reach and connect with your customers.",
        "Amazon DSP: Sync Cookies/Email": "Send Lytics user profiles toAmazon DSPto reach and connect with your customers. Integration DetailsFieldsConfiguration",
        "Amazon DSP: Conversion API Export": "Send Lytics user profiles toAmazon DSP Conversion APIto improve your ad campaign performance. Integration DetailsFieldsConfiguration",
        "Amazon Marketing Cloud: Advertiser Data Upload": "\u200bThe Amazon Marketing Cloud export Uploads an audience of users to an Amazon Marketing Cloud dataset. This dataset can then generate insight reports based on your Lytics audience.",
        "Steps/Pattern:": "The AMC Instance is checked to see if the dataset exists. If it does not, a new one is created. If it exists, the columns are validated to contain the selected fields.The Audience is scanned. For each profile:a. The identifiers are normalized and hashed.b. The hashed identifiers and selected fields are written to a CSV file in the AMC instance's upload bucket.A load job is started in AMC to load the file into the configured dataset.If the export is configured toKeep Updated, it will wait until the next export cycle, where it will start at step 1.",
        "Importing External Experiences": "You can import existing marketing initiatives currently managed by your channel providers, which enables you to quickly gain insights about the campaigns you\u2019re already running. You can then take action by connecting these Experiences to your Lytics audiences. While you can monitor Experiences and activate audiences for them within Lytics, the management of external Experiences still happens inside your channel tools. If you want to stop, delete, or edit the execution of an Experience, you need to do so within the provider. If you choose to delete an Experience in Lytics that still has a matching campaign in your channel tool, it will show up on the import list and can be re-imported.",
        "Centralize cross-channel reporting": "Lytics serves as a central hub to monitor your cross-channel marketing, making it easier to understand and improve the performance of  your campaigns. For example, instead of logging into Facebook to check your ad campaign metrics and Iterable to check your email newsletter open rates, you can monitor the performance of both tools within your Lytics dashboard. To start monitoring external Experiences on Lytics, follow the batch import steps.",
        "Monitor Experiences via batch import": "On your Lytics dashboard, navigate toExperiences, and selectAdd Experiences > Import.",
        "Select your provider": "The Lytics Canvas connects out-of-the-box with select channel providers. Over time, more providers will be added to this list. If you want to use a provider that is not currently available, you can use the \"Generic Experience\" workflow to export Lytics audiences to any of your integrated providers. ",
        "Authorize": "Next, you will be prompted to select an authorization for your chosen provider. Existing authorizations will be shown. If you have multiple accounts within a particular channel such as Facebook, you will choose which account to import from. If you don\u2019t have an authorization, you will be prompted to add one before continuing. For Facebook, there is a second step for authorizing Ad Set ID after you\u2019ve selected your account ID.",
        "Import Experiences": "Finally, select the Experiences you want to import into Lytics (up to 10 at a time). The total number of Experiences you can bring into Lytics is only limited by the amount in your connected provider. However, to have meaningful and accurate reporting, it's recommended to only import Experiences that will add value to your use cases.  Completed experiences, such as an Ad Set that previously \"ended\", will not show on the list of importable Experiences. But if you have Ad Sets in Facebook that are saved as drafts, paused, or currently running, those will be available to import.",
        "Connect Experiences with Lytics audiences": "Once you start monitoring external Experiences, you can activate them by adding a Lytics audience. This lets you enrich your existing campaigns with Lytics behavioral audiences, content affinities, and delivery optimization.An example use case is to conserve Facebook ad spend by only targeting \u201cCurrently Engaged\u201d users who have a high affinity for the content of a particular campaign. Leveraging data science under the hood, Lytics audiences can bring immediate value to your existing marketing campaigns. If you have already imported an external Experience, you can activate it from the Experience summary view by clickingEditin the top right.  If you want to activate an Experience that you haven't imported yet, follow the single import steps below, which will guide you through the Experience editor.",
        "Activate Experiences via single import": "To activate an Experience that you haven't imported yet, you will follow the workflow of creating a new Experience in Lytics. From the Experiences list view, clickAdd Experiences>New. Select your chosen provider and thenImportthe campaign, ad set, or journey. This example will continue with importing a SendGrid campaign.  Next you will select an Authorization for your chosen provider as described above.  Then you will select the campaign or ad set to import. Note that here you can only import one Experience at a time. ClickImport 1 Experienceto continue.",
        "Complete the Experience Editor steps": "Finally, you will complete the following Experience Editor steps: Target: configure your target audience for this Experience.Configure provider: choose how the audience for your Experience will be exported to the third-party tool.Configure delivery: option to automatically determine when to deliver messages to individual users on the third-party tool. Upon completion, your Experience will be ready to publish. ",
        "Surface a Simple Message": "Lytics comes with our Personalization SDK called Pathfora. Pathfora allows you to easily surface simple lead capture and messaging modals or content directly inline. Fulldocumentationfor Pathfora is available, but initially, let's surface a welcome message to ouranonymous visitoraudience. Alter the Pathfora configuration to your liking.Install the Pathfora configuration onto your site via your preferred tag management method.Refresh the page and be greeted with your new welcome message targeted at anonymous visitors! Lytics and Pathfora provide a great deal of flexibility. If you are ready to dive deeper, please explore some of our other popular use cases: Surface a Promotional Message to High Momentum Visitors (coming soon)Surface a Lead Capture form Only to Unknown VisitorsSurface Content Recommendations Based on Interests (coming soon)Sync Profiles & Audiences to GA4 or Meta (coming soon)Personalize Your Site Based on Behaviors and Stored Attributes (coming soon)",
        "Import Contacts & Activity": "By importing your Dotdigital contacts and their activity into Lytics, you'll be able to apply Lytics powerful insights to your email and multichannel campaigns.",
        "Export Contacts": "Export Lytics audiences to a Dotdigital Engagement Cloud address book, which can then be used for campaigns using any of Dotdigital's supported channels.",
        "Export Multiple Audiences": "Export multiple Lytics audiences to existing Dotdigital address books, which can then be used for campaigns using any of Dotdigital's supported channels.",
        "Export Modified Contacts": "Use the Export Modified Contacts job type to keep your contacts' data fields up to date in Dotdigital.",
        "Remove Contacts": "Remove or unsubscribe users that enter the configured audience in your Dotdigital account. Optionally you can resubscribe users when they exit the audience, which can be useful for building an audience that has opted out of communications.",
        "Event Integration": "The Radarevent integrationwith Lytics imports Radar's location context data in real-time which surface as user profile fields which can be used in the Lytics audience builder. Lytics Audiences using Radar data can be used to activate location-based experiences in real-time.",
        "About the Lytics SDK for Android": "Approximate Installation Size:35 KB (315 Methods)Android Version Support:8+Android Studio Version Required:v2021.3.1 or Higher",
        "Step 1. Add the Lytics SDK & Dependencies:": "You can add the Lytics SDK to an Xcode project as a package dependency. From the File menu, select Add Packages...Enterhttps://github.com/lytics/ios-sdkinto the package repository URL text field and click Add PackageAdd the Lytics package product to your application target",
        "Step 2. Configure the SDK": "A variety of top-levelconfigurationoptions are available to ensure the SDK can be tailored to the needs of your particular application. To get started, however, we need only add your valid Lytics API token to theLytics.shared.startinitialization call as outlined below:",
        "Step 3. Identifying a User": "Once we have initialized the SDK, it is a best practice toidentifythe user anywhere formally they provide additional strong identifiers, such as a username or email, upon login, for example. This ensures your profiles have the highest potential for properly unifying across your various data sources. Do note, this step is optional. By default, the Lytics SDK will create an anonymous identifier for all users and leverage that to merge with disparate sources as we do with our traditional web-based JavaScript SDK.",
        "Step 4. Tracking Activity": "Once the user has been identified (optional), we can begintrackingvarious activities throughout their visit. Typical activities include screen views, clicks, purchases, etc. A full scope of event tracking flexibility can be found in the technical documentation section below.",
        "GAID (Identifier for Advertisers)": "GAIDoradvertisingIdentifieris an alphanumeric string that is unique to each device. Collection of this identifier requires explicit confirmation from the app user, which the Lytics SDK for Android can trigger. In addition, upon confirmation, the GAID will be treated as a strong identifier and included with all outbound events emitted to Lytics streams for improved resolution. The Lytics SDK for Android offers two methods to collect GAID and handle the opt-out of GAID. To present a required modal asking for tracking access and storage/inclusion of the device-specific GAID upon confirmation, call the following method: To disable the storing and emitting of the user's device-specific GAID, call the following method: To support collecting the Android Advertising ID, add the following to the application's gradle dependencies: implementation 'com.google.android.gms:play-services-ads-identifier:18.0.1' Additionally, declare a Google Play services normal permission in the manifest file as follows: <uses-permission android:name=\"com.google.android.gms.permission.AD_ID\"/> After confirming with the user and getting their consent, enable Advertiser ID collection viaLytics.enableGAID(). The user's Android Advertising ID will be sent with each event's identifiers. Note, the user can disable or change the Advertising ID via the Android system privacy settings.",
        "Anonymous Visitors": "Lytics considers anonymous visitors to be users identified only by a_uid. A_uidis a Lytics tracking value assigned to a cookie. Since cookies can be cleared, aren\u2019t shared across browsers or computers, and aren\u2019t strictly associated with an individual, they are unreliable. For these reasons, Lytics considers these visitors to be anonymous.",
        "The User Profile of an Anonymous Visitor": "Anonymous visitors can still have rich behavioral data. Every page they visited, the number of times theyvisited, the time they visited, and behavior patterns for data science models. A User Profile is nothing morethan the set of known User Field values associated with the_uidof the Anonymous Visitor.",
        "Identifying Anonymous Visitors": "Since all information about anonymous visitors relies on the continued use of a cookie, it is in a marketer'sbest interest to convert these unknown users to known users. Cookies are temporary, but email addresses are not. Note that when a user converts from anonymous to known, all information we knew about them carries with them.When their_uidbecomes associated with an email address (or any other By Field), both Profiles (the_uidprofile andthe email address profile) merge through a process called Identity Resolution.",
        "API Key": "Use this authorization method for Zendesk User Imports and User Exports. To connect to Zendesk using an API Key, you will need three pieces of information: Your Zendesk Email Address: This is the email address that you login to Zendesk with.Subdomain: This is what comes after://and before.zendesk.comin the URL when you are using Zendesk.API Key:  To obtain your API key, navigate to your Zendesk API Settings. Enable token access to the API and copy your token:  Once you have retrieved your API Key: Enter the following:Zendesk Email AddressAPI KeySubdomainDescription: a sentence to help you identify the authorization. You are now ready to start a ZendeskImportorExportjob.",
        "Connect API Key": "Use this authorization method for Zendesk Connect Audience Exports. Enter the following:API Key: your Zendesk Connect API Key. This is the private key which can be found in your Zendesk Connect account under Settings > Environments.Description: a sentence to help you identify the authorization. You are now ready to start aZendesk Connect Audience Exportjob.",
        "Import Tickets and Users": "Import Zendesk users and tickets to leverage Lytics Insights and behavioral scoring to make sure you're reaching the right users with the right messages.",
        "Export Users": "Exporting your Lytics audiences to Customer.io will create or update user data in Customer.io.",
        "Including Lytics Audience Information": "The first step in adding Lytics audience information to a Zendesk user is to create a new field in Zendesk to receive this information. Start by clicking the gear Settings icon in Zendesk at the left of the screen. Then click onUser Fieldsunder the \"Manage\" heading, and click on theTextfield type to create a new field: You can give the field any title and description that you like, but the Field keymustbemember_of_lytics_groupfor Lytics to find it.  ClickCreate Fieldand you're ready to get Lytics audience information into Zendesk. Start a Zendesk Export like in the section above and use the audience selector to tell Lytics which lists you want to be visible in Zendesk.After you clickStart Export, your Zendesk users will be updated with the names of the Lytics audiences they belong to. Lytics will update this information daily, along with any other fields you selected in the configuration steps above.",
        "Zendesk Connect Audience Export": "Zendesk Connect manages proactive customer communication across channels, to deliver better customer experiences at scale. Export Lytics audiences to Zendesk Connect to reach your desired audiences on Zendesk Connect channels.",
        "Add your first Experience": "The set up for each Experience is slightly different, so whether you are creating an on-site Lytics Experiences or connecting to one of our out-of-the-box Integrations, the simplest way to get started with experiences is to create a new Experience and allow our Experience Editor to navigate you through each of the steps.",
        "Experience List View": "Within theExperiencestab in the Lytics app, you will find a list of all of your Experiences. From this view you can Add ExperiencesBrowse ExperiencesView the Status of your ExperiencesNavigate to Experience summary pages to work with individual Experiences See below for more information around each of these activities.",
        "Adding Experiences": "Create a new Experience within theExperiencestab by clicking theAdd Experiencesbutton near the top of the page. Selecting theNewoption will guide you through the Experience Editor steps for any Experience (Lytics,Out-of-the-Box Integrations, orGeneric) If you want to import external marketing initiativesfrom out-of-the-box integrations that support Experiences, you can do so within theExperiencestab as well. Click theAdd Experiencesbutton near the top of the page and select theImportoption to be guided through the Experience Editor steps for these supported integrations.",
        "Browsing Experiences": "Search: The easiest way to find an Experience is to use theSearchbox to the right of the Experience list. As you enter your search term, the Experiences list will display any matching Experiences. Filter: To the right of the Experience list you can filter Experiences based on their... Channel: The channel is associated with the Provider you selected when creating the experienceIntegration: The Provider you selected when creating the ExperienceStatus: Life-cycle and health of the ExperienceType: Whether or not the Experience is associated with a Goal Sort: By default, the list of Experiences is sorted byName. If you choose to sort by dateLast modifiedinstead, your preferences will be saved for the next time you visit.",
        "Experience Statuses": "An Experience status indicates what life-cycle the Experience is in and the health of whether it is exporting a Lytics audience to a third-party tool in real-time. Below is a complete list of Experience statuses: Draft:A complete, or partially complete Experience that has not yet been activated.Active:An Experience that can be seen by your users (and has not passes its end date).Paused:An Experience that was activated but has been paused. To edit an Experience that has been running, you must pause it first.Scheduled:An Experience that will be activated at a start date set in the future.Externally Managed:Animported Experiencefrom a third party tool that hasn't been configured with a Lytics audience. Complete the configuration by editing the Experience in the Experience editor.Ended:An Active Experience that has passed its end date.Error:An Experience that failed to export audience data to the appropriate tool.Warning:An Experience that encountered issues when attempting to export audience data to a third party tool. It may recover on its own, or proceed to an error status after a number of failed attempts to export data.",
        "Experience Summary View": "Once you have created or imported Experiences they will be populated as a list within theExperiencestab. Click on an Experiences from the list to enter the Experiences summary. From this summary page you can... See an overview of the ExperienceActivate, pause or resume ExperiencesEdit an existing ExperienceDelete an existing Experience See below for more information about each of these activities.",
        "Experience Overview": "Within the Experience summary you will find important information about your Experience. The summary includes: Experience StatusExperience Performance ReportingExperience Intelligence Report(for accounts with the Lytics View feature enabled)Sample Audience Members (a small sample of users in the target audience that are eligible for the experience)Activity (recent activity around changes made to the Experience)Details (an overview of the configuration of the Experience)",
        "Activating Experiences": "From the Experience summary page you are able manage the activation of individual Experiences using the following buttons on the righthand side of the summary page. Activate: An Experience has not been previously activated, and is currently marked with the Draft status, can be activated.Pause: Campaigns currently marked as Active can be pausedResume: Any previously activated Experiences that are currently paused can be resumed",
        "Editing Experiences": "Within theExperiencessection of the Lytics app, you can edit an existing Experience by clicking on the desired Experience. This will navigate you to the Experience summary page. Here you can click on theEditbutton to enter the Experience Editor for this Experience.",
        "Deleting Experiences": "To delete an experience from Lytics, click on the Experience you wish to delete from within the Experiences list. You will be navigated to the Experience summary page where you can delete the experience from the options menu. You will be asked to confirm that you wish to delete the experience, click theDelete permanentlybutton to complete the deletion.",
        "Experience Editor": "The Experience editor is used to create, save, edit, and activate marketing Experiences. Combined with audiences created in theaudience builder, sophisticated Experiences can be built to deliver the right message to your users at the right time. To enter the Experience editor, you can eitheradd an experienceoredit an experience. The available steps of the Experience editor are dictated bythe Experience provideryou select.",
        "Experience Providers": "Lytics:Lytics Experiences allow you to create personalized campaigns on your website to capture leads, recommend content, drive traffic, etc without having to connect Lytics to an additional tool. The Lytics Javascript tag must be installed on your website for you to take advantage of this feature.Out-of-the-Box Integrations:Experiences added to Lytics from these providers allow you to monitor performance and activate audiences from the Lytics UI; however, the management of external Experiences still happens inside your channel tools.FacebookGoogle AdsIterableMailchimpMappSalesforce Marketing CloudSendGridThe Trade DeskGeneric:These experiences simply offer the ability to include a placeholder Universal Experience within the Goals Canvas and export a Lytics audience, with all of the nested Goal Stage filter logic, to any tool you choose.",
        "Lytics Experiences": "Use Lytics Experiences to personalize a website with modals, slide outs, sticky bars, etc. This does not require connecting to another tool.",
        "Activating your new Lytics Experience": "Once you have completed the steps in the Experience Editor and saved your Experience you canActivate it from the Experience summary page. You will be directed to this page once you exit the Experience Editor.",
        "Out-of-the-Box Integration Experiences": "You can import existing marketing initiatives currently managed by your channel providers, which enables you to quickly gain insights about the campaigns you\u2019re already running. You can then take action by connecting these Experiences to your Lytics audiences. To import an experience, click theAdd Experiencebutton in the Manage Experience page. You can select either the New or Import options in order to import experiences from our out-of-the-box integrations. Both options will direct you to the same flow.",
        "Activating your new Out-of-the-Box Integration Experience": "Once you have completed the steps in the Experience Editor and saved your Experience you canActivate it from the Experience summary page. You will be directed to this page once you exit the Experience Editor.",
        "UTM Tracking": "In addition to tracking default UTM variables when users visit your website, Lytics supports two custom UTM parameters specific to tracking clicks from out-of-the-box Integration Experiences. You may be recommended by the LyticsExperience Insightsto add UTM parameters to an Experience. The Lytics insights system will check the target URL of your eligible imported Experiences to see if it has these parameters configured and advise you on which Experiences are missing them. For most email providers click information is collected automatically through webhooks or APIs, but for ads this is the only way of recording user attribution. Lytics highly recommends using these UTM parameters in your ad Experiences as it will not only allow you to build and target audiences of users who have converted on your ad in Lytics, but it will also inform Lytics decisioning and insights as the platform builds a better understanding of which users respond to which Experiences. Use the following custom UTM parameters in any links coming from your external Experiences. These variables allow us to link the website visit with a click from the Experience. Thus recording what users clicked on what Experience. The steps for adding these parameters may differ by provider, but generally speaking you can add them directly to the target URL of your Experience, for example: https://www.lytics.com?utm_lytics_source=facebook&utm_lytics_experience=23847 Please refer to the providers that have specific docs on how to add these parameters: Facebook The UTM parameter is recognized by Lytics and mapped to the user fieldConverted on Experience (URL Param)which is available in the audience builder so that you can create audiences of users who have clicked on your Experiences. The key of this map field will contain theExternal IDof your Experience and the value will be the number of times the user clicked on the ad set.  If you find that the number of users in an audience utilizing this field does not match the conversion count you are seeing on your Experience summary page, it is important to rememberthe source of these numbers is different. Usually for ad providers, metrics are aggregate numbers imported directly from the API of the ad tool. If you added the tracking URL parameter to your ad after publishing, you may see a smaller user count when utilizing the user field.",
        "\u2757\ufe0f": "Automated classification requires a valid robots.txt file on your configured domain(s). In addition, we highly recommend using your production URL rather than a development URL for classification. This will greatly reduce confusion when it comes to recommendations and moving to production.",
        "Generic Experiences": "Generic Experiences allow you to export your Lytics audience to any tool you choose. Since you will not be able to monitor the performance of this Experience from Lytics, it's primary use cases are to act as a placeholder for.. An Experience of anintegrationthat is not built into the Lytics Goal CanvasAn Experience of a custom implementation usingwebhooksorinline web personalization. Generic Experiences allow you to produce audiences with all the nested targeting rules of a Stage within a Goal to make the audience mutually exclusive of the other Stages.As such, Generic Experiences should only be used for Experiences included in a Stage of a Goal.",
        "Activating your new Generic Experience": "Once you have completed the steps in the Experience Editor and saved your Experience you will need to navigate to the created audience and export it to the integration of your choice. Navigate to the created audience in the Experience summary page by clicking on Help and then the link found within theFinish setting up your Generic Experiencesection. Export the audience from the Audience summary page.",
        "Experience Performance Reporting": "On the Experience summary page, you will find Experience performance reporting. This reporting contains three metrics for the campaigns you\u2019ve created or imported into Lytics. Use these metrics to establish a baseline of your campaign performance and measure growth over time.",
        "Experience Intelligence Report": "In addition to the Experience Performance Reporting, the Lytics View feature can be enabled in your account to give you additional information about your Experiences via an Experience Intelligence Report. The Experience Intelligence Report, once enabled, will add the following modules to your Experience summary page (as well as in the Goal Intelligence Report for Experiences within Goals): Experience InsightsGoal Attribution(for Experiences used within Goals)Content RecommendationsData Science Performance Simulation",
        "Experience Insights": "Experience Insights help you track and understand campaign performance. This information can answer, \u201cWhich campaigns are resonating with my users?\u201d and \u201cAre my marketing tactics driving engagement or conversions?\u201d Experience Insights allow you to refine campaigns based on current metrics proactively. Since Lytics Insights are generated and refreshed every week, you can take action to improve a campaign while it\u2019s running. Experience Insights include the following types:",
        "Goal Attribution": "The Goal Attribution module displays metrics around how conversions on your Experience relate to conversions on your Goal. The percentage compares the total number of people reached by the Experience and those who converted. ForFacebook Ads with UTM parametersincluded, Lytics will capture conversion data at the user level. Use this metric to determine which campaigns to invest more or less in based on how they contribute toward your larger marketing Goal.",
        "Content Recommendation": "The Content Recommendation module surfaces content that is likely to resonate with your users based on their past behavior. Lytics analyzes the content on your website through the Lytics JavaScript Tag. The relevancy percentage is calculated by determining the total number of users with various content affinities and comparing this to the total number of users within the audience that visited your site. Create and deliver content that aligns with your users\u2019 interests to drive engagement and conversions.",
        "Data Science Performance Simulation": "The Data Science Performance Simulator module displays \u201csimulated\u201d conversion rate metrics that compare how your campaign audience performed against Lytics out-of-the-box,behavioral audiences. This gives a risk-free preview of how effective your campaigns could be when leveraging behavioral scores and custom models powered by Lytics data science. You can activate Lytics data science audiences and custom models in your campaigns by upgrading from the free trial. SeeData Science Introductionto learn more.",
        "Behavioral Analysis": "Lytics constantly tracks events from many data sources: web, email, Salesforce, and custom sources as well. Looking at this data directly is not useful; it needs to be summarized. Lytics allows for simple summaries by means of user fields and for use in audiences, but there is more to be learned from all this behavioral data. Behavioral analysis asks many questions and computes the answer. Questions such as: When did the user last visit?When did they visit before that?Is it unusual that they visited right now?During their visit, what were they up to?Did they visit a lot of pages?Did they stick around for a long time?How does that compare to other users?Are they going to come back?Is this a good time to encourage more interaction? After finding the answers to these questions, for every user, Lytics uses these answers to inform nine low-level scores that get assigned to every user who has shown enough behavior for a confident assessment. These scores are computed automatically, they are kept up to date continuously, and they are assigned relative to other users in the account.",
        "Behavioral Scores": "There is a lot to learn looking at user-level behavioral data. Lytics evaluates behavioral data automatically in real-time and reports nine scores for each user.  These scores each represent a distinct behavioral quality and can be composed to build rich audiences. Please note, users must have behavioral data to make confident measurements before they have scores. If a user was added to Lytics via email upload, for instance, they would have no scores. Note:If you are already a customer of Lytics, you can see the following explanation of each score alongside your own data in the app on the Scoring page.",
        "Quantity": "Quantitymeasures a user's cumulative activity over their lifetime of brand engagement. The more activity the user registers, the higher the score. This score measures the user relative to all other users. It is a common tactic to target a user based on the number of times the user has visited a website or has performed some other behavior. This becomes increasingly difficult as marketers add more data sources to their stack, and the user continues to engage over time. Quantity takes into account a user's behavior on all data sources and measures that relative to how the most and least active users are engaging \u2014 so a user's score will always be between 0 and 100. You can think of this as a test score. What is the point of scoring between 0 and 100? This is how we can ensure that any audience created with a score will always stay relevant. Perhaps 1,000 page visits seems like a lot for a user now, but the number will only grow larger as your site grows older and the amount of content you have increases. Another benefit of having a bounded score range is the ability to see the complete distribution of all users.  This is an example of how scores look like across an entire audience. The x-axis is the score (ranging from 0 to 100; 5 to 95 in the example for clarity) and the y-axis is the number of people who have that value as their score.",
        "Frequency": "Frequency describes when and how often that data is sent or received. It attempts to answer common questions such as: \"When will I see data in my tool?\" and \"Can I keep my data up-to-date?\" Lytics integrations are either real-time or scheduled.",
        "Recency": "Recencymeasures how recently the user's general interaction has been. More recent activity means a higher score. This score measures the user's recent activity relative to the user's past activity. Without scoring, this would be achieved by looking at the last time a user visited. Although better than nothing, that approach is kind of crude. Maybe an at-risk user opened your email by accident? It'd be an expensive oversight to assume that the user had recent activity and didn't need any nurturing.",
        "Intensity": "Intensitymeasures the depth of a user's typical interaction with your brand. More sustained intense/deep usage means a higher score. This score measures the user relative to all other users. The behavior a user exhibits during a single session is very telling of them as a consumer. If they have high interaction in a session (high intensity) they are more likely to be a deeper researcher or more curious. If they have low interaction in a session (low intensity) they are more likely to be casually browsing or engaged with a certain piece of content, but not your overall brand.",
        "Momentum": "Momentummeasures the rate at which users are interacting with your brand. Users who are interacting more than usual with your brand will have a higher score. This score measures the user's recent activity relative to the user's past activity. It's easy to confuse how momentum and recency differ, but they are actually very different. Universally speaking, we've found them to have a 5% correlation. Recency measures absolute recency of activity, but just because a user has recent activity, doesn't mean they're not at risk of churning. If a user maintains a constant rate of activity, their momentum score will be 50. If they are more active than they used to be, their momentum will be greater than 50 and might warrant a loyalty offer. If they are less active than they used to be, their momentum will be less than 50 and might warrant a win-back campaign.",
        "Propensity": "Propensitypredicts how likely a user is to return with subsequent activity. Users exhibiting positive interaction patterns are more likely to return and have higher scores. This score measures the user relative to all other users. There are many reasons why users churn \u2014 changing interests, competition from competitors, bad experience, etc. \u2014 but from a data perspective, attrition of any kind starts to look similar. Propensity employs an ensemble of statistical models to identify any patterns it can find for detecting how and when attrition starts to occur. With time, it's able to find more patterns in your data and become increasingly accurate in identifying when users start to exhibit those behaviors.",
        "Consistency": "Consistencymeasures the regularity or stability of a user's engagement pattern. Users who engage with your brand at a regular cadence will have higher scores. For example, a user that registers behavior every 7 days will have high consistency, and would have the same consistency as a user who registers behavior every 30 days. As users' behavior starts to vary \u2014 sometimes every 7 days, sometimes every 30 \u2014 the users' consistency score will decrease. Consistency scores can be paired with propensity scores to build more accurate Lookalike Models to predict customer churn and target users with win-back programs before they bounce. Learn more about how to reduce churn using Lytics Lookalike Models.",
        "Maturity": "Maturityis a normalized measure of how long a user has registered behavior. This indicates how \"old\" a customer is relative to your other users. A user who has registered behavior over 5 years will likely have high maturity. A user who registered behavior over 3 years, but hasn't registered any in 2 years will have less maturity. A user who registered behavior over the most recent 3 years will have the same maturity as the user previously mentioned. As an example, you could target users with high maturity scores inviting them into a loyalty rewards program via ads, emails, and in-app notifications. You may also want to consider suppressing engaged users who are likely to make their next purchase without additional advertising. Low maturity users, on the other hand, could be served more onboarding or educational content to nurture them into high-value, long-term users.",
        "Volatility": "Volatilitymeasures how stable vs. sporadic a user is interacting with your brand. It represents the stability of the volume of data that a user is generating, and serves as a slightly more nuanced version of the intensity score. Consider a user where 100% of their daily sessions are considered \"intense\". Their intensity score would be 100, but the score doesn't yield any information regarding the volatility of a typical session.",
        "Utilizing Scores": "Each Lytics score is accessible as a Custom Rule in the Audience Builder. They can be added to any audience definition as an intelligent filter when the size of the audience is larger than desired. For example, when crafting an audience to be used to buy ads against, the size of the audience is critical. The size can be arbitrarily shrunk by taking 10% of the matching users, or it can be intelligently shrunk by creating a threshold with a Lytics score such as Propensity or Momentum. This way, the best fit users remain. Interfacing with scores directly can be difficult. They are low-level building blocks that require expertise to use to their fullest. To make these scores more readily usable, Lytics offers out-of-the-box Behavioral Audiences that use scoring under-the-hood.",
        "Customizing Data Sources": "Lytics scores work best on behavioral data \u2014 that is, data that was generated by a user, like a web view or email open, rather than on list imports or other non-behavioral data. Every supported 3rd party integration is already configured to include only behavioral data in user scores. By default, any custom integration is assumed to be non-behavioral. If you're using a custom integration that you want to contribute to behavioral scores, contact your account manager to update the setting.",
        "Behavioral Audiences": "Since using Lytics behavioral scores directly requires a level of understanding of the data science at play, Lytics also offers out-of-the-box behavioral audiences. These audiences are essentially blends of scores that can be used alone or as a rule in a custom audience. For example, Casual Visitors describes users who come and go without showing much activity per session. The definition using Lytics scores isUsers with an intensity score less than 25. Now, instead of needing to know exactly what \"intensity\" means in this context, or what the significance of the number 25 is, the audience Casual Visitors can be used as a building block.  In this example audience, the Casual Visitors characteristic is being used to filter the users who have a high affinity for \"Computing\". Note that the number of people who have a high affinity for \"Computing\" is 276,046, but the number of people who have a high affinity for \"Computing\"andqualify as a Casual Visitor is 185,747. Reducing the size of an audience is a great way to make a campaign more efficient. Additionally, splitting an audience based on behavioral properties is a great way to introduce different modes of communication for different archetypes of users. For instance, Casual Visitors may not stick around long enough to answer their own questions, try engaging them with a slideout on early page visits. The opposite of Casual Visitors, Deeply Engaged Visitors, are probably determined to find that information on their own and would find a popup to be annoying. Splitting the audience keeps both archetypes engaged without accidentally detering anyone.",
        "The Full Set of Out-of-the-box Behavioral Audiences": " Frequent Users:Users who interact with your brand a lot. Definition:Frequency Score > 65.Infrequent Users:Users who interact with your brand occasionally. Definition:Frequency Score < 35.Deeply Engaged Users:Users who show a lot of activity when they do interact with your brand. Definition:Intensity Score > 75.Moderately Frequent Users:Users who occasionally interact with your brand. Definition: scorefrequency > 34 and score_frequency < 76.Casual Visitors:Users who show little activity when they do interact with your brand. Definition:Intensity Score < 25.Likely To Re-engage:Users likely to come back based on their past activity patterns. Definition:Propensity Score > 75.Unlikely To Re-engage:Users not likely to come back based on their past activity patterns. Definition:Propensity Score < 35.At Risk Users:Users whose interaction behavior is changing for the worse. Definition:Momentum Score >= 10 and Momentum Score <= 30.Binge Users:Users who show a lot of activity when they do interact with your brand. Definition:Frequency Score <= 20 and Intensity Score >= 50.Perusers:Users who visit often but rarely interact deeply with your brand. Definition:Frequency Score >= 70 and Intensity Score <= 20.",
        "Lytics Behavioral Audiences": "The follow out-of-the-box audiences are prefixed with \"Lytics\" and can be found in the Audiences list. Lytics New: Users who were created within the past 7 days. Definition: _created > \"now-7d\".Lytics Disengaged: Users who show minimal or no activity for a prolonged period of time. Definition:Frequency <= 5 and Intensity = 0 and Momentum = 0 and Quantity <= 3 and _created < \"now-7d\".Lytics Previously Engaged: Users who are currently disengaged with your brand, but had been previously. Definition:Momentum <= 10 and NOT Lytics Disengaged and NOT Lytics NewLytics Highly Engaged: Users who engage most frequently and consistently of your users. Definition:Quantity >= 50 and Frequency >= 50 and Intensity >= 25 and Momentum >= 40 and NOT Lytics New and NOT Lytics Previously Engaged.Lytics Currently Engaged: Users who are currently engaging with your brand. Definition:Momentum > 10 and NOT Lytics Highly Engaged and NOT Lytics Disengaged.",
        "Taking it further": "Remember that each of these out-of-the-box behavioral audiences can be recreated with the audience builder using Lytics scores user fields. Mastering Lytics scores will open up the possibility of new combinations of score thresholds that result in new behavioral audiences to be used as building blocks in campaigns.",
        "Web Personalization Campaign Reporting": " The campaign report is the control center for a published campaign. Here, you can find out how the campaignis doing, find out what kind of campaign it is, edit the campaign, pause the campaign, or duplicate the campaign. When you publish a campaign, Lytics begins reporting on conversations and reach, and tracks the conversionrate of a campaign over time. If you have create a two variations of your widget, the conversions and reachwill be tracked separately for each variation. Conversions are defined a little differently for each campaign type. Here are the definitions for each campaign type: Content Promotion: User clicks through to the promoted content.CTA: User clicks on the CTA.Lead Gen: User submits lead gen form.Present a message: No conversion. In addition to real time reporting data, the campaign report includes information about the type of campaignbeing run, the appearance of the widget, the event history, and recently converted users.",
        "Key Sections": "The Identity Explorer is organized into several key sections, each offering a unique perspective into the consumer:",
        "Identity": "In the \"Identity\" section, you can explore the relationships between unique identifiers that establish connections across the various channels the consumer engages with. The \"Identifier List\" provides a detailed overview of the keys and values used for linking, while the \"Identity Graph\" visually represents these relationships. This section helps you understand how the consumer is identified and linked across platforms.",
        "Profile Fields": "The \"Profile Fields\" section encompasses an exhaustive catalog (feature store) of attributes along with their corresponding values linked to the customer's profile. This encompasses default schema fields from connected tools, custom attributes, and many inferred or machine learning-based attributes generated by Lytics. This section offers an expansive overview of the consumer's data, facilitating precise targeting and personalized interactions.",
        "Searching & Filtering": "The \"Profile Fields\" view can be filtered by either typing in a query or selecting any combination of pre-built filters. We find it most effective to filter by the field categories to simplify the view during exploration.",
        "Raw Data": "For developers and data enthusiasts, the \"Raw Data\" section offers a JSON representation of the complete profile.",
        "Content Affinity": "The \"Content Affinity\" section provides a graphical representation of product and content-based affinity scores for the user. It reveals the user's preferences and interests, enabling you to make data-driven recommendations and content suggestions.",
        "Deleting Profiles": "In support of GDPR Profile Administrators may also delete a single profile or download the profile data to provide to a consumer directly if requested.",
        "Create an Audience of High Activity Users": "Effective campaigns begin with an effective audience. The goal here is to collect email addresses of visitors to your site that are not already signed for your email list. The following audience combinesmultiple rulesto target users with aquantity score of at least onewhoseemail is not known. Users with a quantity score less than 1 have little to no activity on your site and are not great candidates for engagement with an email mailing list. You also don't want to present a message to users who already receiving your newsletter. You are not limited to this criteria. You can target any audience who you think would be valuable to engage with via email. ",
        "Create Collect Lead Campaign": "Once you have an audience of users you would like to target use it when building yourcollect lead campaign. The key step is where, when, and how often you want your messagedisplayedto your users. You may only want to solict email addresses from a particular section of your site and only display the message after the user has spend at certain amount of time on your site. ",
        "Export Leads to Email Service Provider": "Once your new campaign collects email addresses you canuse your campaign to build a new audience. A user who enters an email address will be considered converted, these users can be exported to to your ESP and added to your email list. The specific export instructions for your ESP will vary, see ourIntegrationssection for instructions. ",
        "Creating Templates": "Clicking the+ Create Newbutton will open a wizard that will guide you through creating a new Template. The first step involves selecting a Name, Description, Data-Type, and Template-Type. The Data-Type allows you to select the type of data used by the Template,  eitherUserorEvent(the default option isUser). The Template-Type configures the type of templating language used by the Template, eitherJsonnetorHandlebars.js. After clickingNext, you will be guided to the Template configuration page where you must first select the type ofSample Dataused by the Template. If theUserdata-type was selected, you will see a dummy user comprised of all of the fields in theuserschema, or the option to search for a user. If theEventdata-type was selected in the previous step, theSample Datawill be populated by a recent event from the selected stream. Once theSample Datahas been selected, Step 2 involves specifying your desired JSON structure for your destination in theTemplate Co-Pilottext box. Consider the example below where we specify the structure of our JSON payload, with fields such as_uid,last_active_ts,last_name, andpillowcase_affinity(derived from thelytics_contentfield). We are also using theFind a Userfeature to populate the Sample Data. Note: this example is using theAdditional Prompttext box to assist the Co-Pilot in generating the template. Here we are instructing the Co-Pilot to use thelytics_contentfield to derive thepillowcase_affinitytemplate field. OnceGenerate Templatehas been clicked, the text box in(3)will populate with the template. With the previous example, the following template has been generated: The Template contains all of the data specified in Step 2, and theOutputto the right shows the Template applied to the Sample Data. We can see that thepillowcase_affinitywas derived from thelytics_contentfield, as well as the other fields specified in the Template.",
        "Testing Templates": "Once templates have been created, the/v2/template/:id/testendpoint makes it easy to test the template's behavior given a particular job configuration and outgoing user profile. A configuration can be included in your request, as well as a custom entity profile. If no entity is included in the request, the API will auto-generate one from your Lytics user table schema:",
        "Import Activity": "Import contact activity data from Cordial to build targeted audiences in Lytics.",
        "Import Recipients": "Import Recipients from your Episerver Lists into Lytics to create enriched user profiles that can then be exported as audiences for refined targeting.",
        "Klaviyo API Key": "You would need Klaviyo private api key in order to use this authorization. Please refer toKlaviyo documentationon how to generate a private api key. Follow the steps below to add this authorization in Lytics: SelectKlaviyofrom the list of providers.Select theKlaviyo API Keymethod for authorization.In theLabeltext box, enter a name for the authorization(optional) In theDescriptiontext box, enter a description for this authorizationIn thePrivate API Keypassword box, enter your Private API Key credential.ClickSave Authorization. ",
        "Klaviyo OAuth Sign-In": "When you select this authorization type, a Klaviyo login pop-up window will be visible. Once you log in, you will be redirected to the consent dashboard. Please allow the Lytics app access to the various scopes required for the import and export jobs. Please refer toKlaviyo OAuth documentationfor more information on how OAuth works with Klaviyo. Follow the steps below to add this authorization in Lytics: SelectKlaviyofrom the list of providers.Select theKlaviyo Sign-Inmethod for authorization.Enter your Klaviyo login credentials in the login popup and confirm the authorization.In theLabeltext box, enter a name for the authorization.(optional) In theDescriptiontext box, enter a description for this authorization.ClickSave Authorization. ",
        "Import Users & Activity": " In the Import scenario, you\u2019ll click on the \u201cImport Audiences and Activity Data\u201d button. Next, you\u2019ll select the individual Brevo List(s) that you want to import by clicking on them. As you click on a list it moves from the left column to the right column. You can also click on the blue + and - buttons to do this.  By default, campaign activity data and transactional data will be synced. In the advanced options tab, you can also specify a certain time of day and timezone for the sync.  Finally, click on the \u201cComplete\u201d button located below the advanced options section  Once import has started a new data stream will be created calledbrevo_users",
        "What are Authorizations?": "Channel tool authorizations refer to the permissions granted to various integrations that allow our platform to access and use data from third-party channels. These integrations can include email service providers, social media platforms, and advertising networks. Once permission has been granted, Lytics surfaces \"Authorizations,\" which allow you to manage and maintain the connection to your channel tools over time. This document outlines how to create and manage auths to ensure the long-term health of your data pipeline.",
        "Creating Authorizations": "Authorizations are currently accessed from your account navigation menu by selectingAccount > Security > Authorizations. Within the Authorizations section of the interface, you will see a list of existing Authorizations and a button to \"+ Create New,\" clicking this button will take you to the Authorization wizard, where you will: Choose the provider.Select the desired authorization method.Add a name (label), description, and complete the configuration options. You will need authorization for most jobs when going through the job creation wizard in Lytics. At the \"Choose Authorization\" step, you can opt to create a new authorization which will direct you to the authorizations wizard in a new tab. You will be prompted to choose the method and configure the authorization before configuring your job.",
        "Authorization Methods": "Note that some integration providers only have one authorization method, but others offer multiple methods such as API keys, OAuth, etc. Specific methods may enable different jobs, such as real-time vs. bulk audience exports. If you are unsure which method to use, you can find more information in your provider's integration documentation. For example, if you are looking to import event data from Google Cloud Pub/Sub into Lytics, there is only one method from the list of Google Cloud authorization methods that supports theImport Data (Cloud Pub/Sub)job in Lytics.",
        "Managing Authorizations": "The Authorization Summary provides essential details about each authorization to help ensure an integration is appropriately set up to enable data to flow in and out of Lytics. At the top of the summary page, you\u2019ll see the following information: Provider: The third-party tool that this authorization connects to.Method: Indicates how the authorization was made, such as API keys, OAuth, personal or business users, etc.Health: Indicates whether the Authorization is valid and active.Owner: Lytics user who created the authorization.Created At: The date the authorization was initially created.Last Updated: The date the authorization was most recently edited.",
        "What determines authorization health?": "Lytics checks the status of authorizations daily to help you proactively resolve authorization issues. Our API will return one of three possible statuses, depending on the data available for your provider: Healthy: The authorization is valid and active.Unhealthy: The authorization is invalid and inactive.Unknown: The authorization status is unknown. If your authorization is marked \"Unhealthy,\" Lytics cannot verify that it has the requisite access and permissions. Visit the Authorization page for your provider and confirm that the authorization credentials you entered have the necessary permissions in the external tool. If adjusting permissions in the external tool does not apply or is not an option, you may need to create a new authorization to use with your jobs in Lytics. You can optionally delete the unhealthy authorization.",
        "Edit or Delete Authorizations": "You can edit or delete authorizations directly in the Lytics UI using the buttons at the top right of each authorization summary page. The edit option allows you to rename the authorization and revise the description, but the configuration options cannot be changed. If you need a different configuration, you must create a new Authorization, and you can delete the previous one.",
        "Activity Metrics": "Once metrics are available for each authorization, this chart will become populated with the number of requests made using the authorization. You can adjust the date window and toggle to view the metrics hourly, daily, weekly, or monthly, depending on the available data.",
        "Associated Jobs": "This table provides a helpful organization of all active jobs using the current authorization. Jobs can be sorted by name, type, status, and creation date. Click on a job to view more details on the job summary page.",
        "Authorizations Dashboard": "Authorizing the connection between Lytics and other providers allows you to create jobs to import, export, and enrich your first-party data. Navigate toAccount > Security > Authorizationsto find your Authorizations Dashboard, which gives an overview of all authorizations in your account and their usage. All authorizations are displayed in a table and are sortable by the following fields: Label: The name of the authorization as defined by your input for the \"label\" field.Description: Optional text field to differentiate authorizations. This is particularly useful when you have multiple Authorizations for a given provider.Provider: The third-party tool that the authorization is connecting to Lytics.Status: Indicates whether the authorization is valid and active.Created By: Lytics user who created the authorization.Last Modified: Date the authorization was last edited. Click on a specific authorization to view its summary page, which will provide more details and allow you to edit or delete the authorization.",
        "Interpreting Model Messages": "Building effective Lookalike Models can be an iterative process. To help with these iterations, Lytics provides diagnostic messages that identify ways to improve your model performance. In general, these model messages can help you both select the best source and target audiences for your model and create the best predictive Audience from your model.",
        "Selecting the Right Source and Target Audience": "Most of the time, Lookalike Models are built to drive users toward conversion. This flow should be reflected in how you consider selecting the right source and target audiences to build your model. Source Audience: the group of users to reach with your marketing messages, such as \u201cunknown users.\u201dTarget Audience: the group of users that represents the desired outcome for users in the source audience, such as \u201cusers with email addresses.\u201d When defining your source and target audiences, try selecting audiences that are at adjacent stages in your customer lifecycle, rather than at divergent ends of your funnel (like Brand New Visitors and Multi-purchase Premium Customers). The diagram below represents audience similarity on a spectrum of model performance.  Consider the following examples of audiences with different levels of similarity. Overlapping Audiences(Bad)audience multi-year subscriber\u2192audience with high loyaltyunknown audience\u2192audience with no purchasesDivergent Audiences(Bad)unknown audience\u2192audience with newsletter signup and single purchaseanonymous audience\u2192audience with multiple purchasesAdjacent Audiences(Good)unknown audience\u2192audience with emailknown audience of single purchases\u2192known audience with multiple purchases If you\u2019ve selected divergent or overlapping audiences in your model, you will see diagnostic messages to guide you to select audiences that are more adjacent.",
        "Adjusting the Decision Threshold": "Most Predictive Audiences are built by identifying users in the source audience who have a high model score. This \"high score\" is called theDecision Threshold, and in most cases is around 0.5. To adjust the reach of the audience you\u2019re building, you might consider using a different decision threshold. Use a lower decision threshold to reach more users in the source audience.Use a higher decision threshold to be more accurate but reach less users in the source audience. Diagnostic messages may suggest creating audiences using different decision thresholds, but you are always free to make the decision threshold whatever makes the most sense to generate a Predictive Audience of the right size.",
        "Improving Unhealthy Models": "Most of the time, diagnostic messages for unhealthy models will suggest building a model with different source or target audiences. Occasionally, you might see a diagnostic message suggesting that a model should be built with different features. In these cases, the underlying model didn't have enough of a signal in the data provided to be able to predict target behavior. Imagine that you're an online retailer trying to build Lookalike Models for multi-purchasers, but you build a model that doesn't have any purchase data.  Or you want to build an email churn model, but you don't have any email data in your model.  In these cases, regardless of how your source and target audiences are constructed, your model would be missing the underlying signal required to make a successful model. You could either manually select additional fields to include in the Model Builder, or you can use theAuto Tuneoption on the model to have the machine attempt to automatically identify the best field candidates to include in the model.",
        "Accuracy vs. Reach": "All Lookalike Models try to balance a trade-off between accuracy and reach, which are two of the most important indicators of how your model will perform. Accuracy: the precision of a Lookalike Model\u2019s predictions.Reach: the relative size of a Lookalike Model\u2019s addressable audience. As a general principle, you cannot optimize for both accuracy and reach. Deciding which one to focus on will depend on your marketing use case.",
        "Optimize for Accuracy": "Optimizing your Lookalike Model for accuracy is typically used for targeting later stages of your funnel. This enables you to be more precise, with the trade-off of reaching fewer users. By identifying users who are most likely to convert, you can optimize their high-touch experiences to drive engagement, improve conversion rates, and increase customer lifetime value.  In the example above, the model has a high accuracy score of 9 and a low reach score of 1. The shape of the model predictions graph has little overlap between the source and target audience, which indicates less similarity between the users of those audiences. However, for the select users that fall into the area of overlap, they have a higher likelihood of converting.",
        "Optimize for Reach": "Optimizing your Lookalike Model for reach is most applicable for targeting users in earlier stages of your funnel. This will allow you to reach more users, with the trade-off of being less precise. You can think of this as \"casting an intelligently wide net\". By identifying users who are least likely to convert, you can focus your marketing resources on the users whoarelikely to convert, improving conversion rates and maximizing your budget spend.  In the example above, the model has a low accuracy score of 2 and a high reach score of 8. The shape of the model predictions graph has a good amount of overlap between the source and target audience, which indicates more similarity between the users of those audiences. Therefore, you will be able to reach more users in the source audience, but they have a lower likelihood of converting compared to a model with higher accuracy.",
        "Balancing the trade-off": "When balancing the trade-off between accuracy and reach, consider the sum of accuracy and reach to determine a model\u2019s fitness to be used.  See the table below for a quick estimation of your model's fitness to be used. In the first two screenshots shared, each model had a sum score of 10 for accuracy and reach (9 and 1, 2 and 8 respectively). Therefore, both models would be considered \"good\" but they are optimized for different use cases. For a comparison, see the model below that has a moderate accuracy score of 5 and a moderate reach score of 5.",
        "Initiate cookie exchange": "To activate cookie exchange you need to: Install Lytics JavaScript Tag.Install Lotame Tag.ContactLytics Supportto initiate the exchange.",
        "Export Audiences to Lotame": "Sending audience membership to Lotame requires some coordination between Lytics and Lotame. Lotame can receive audience membership data in various services. This integration chooses to send audience membership data to a Lytics-managed SFTP server. Lotame will be able to access the SFTP folder.",
        "The framework": "There are two important concepts within retention marketing as it pertains to Lytics. While these concepts apply across industries, we will focus on the subscription vertical for this playbook.  For our purposes, we will define these concepts as follows: Renewals: customers who have an upcoming renewal date.Winbacks: recovery efforts for subscribers whose renewal date has lapsed.",
        "How Lytics helps": "Lytics helps you renew and winback customers in multiple ways: Identifying individual users who have upcoming renewals.Scoring the likelihood of renewal or churn.Triggering cross-channel messaging to encourage renewal.Executing targeted promotions and discounts.Recognizing a user whose renewal date has past or subscription has expired.Driving personalized winback communications across channels. The big breakthrough for marketers is that retention teams now have the opportunity to automate consistent messaging across multiple channels. In their research,The Aberdeen Groupfound \u201cCompanies with extremely strong omnichannel customer engagement see a 9.5% year-over-year increase in annual revenue, compared to 3.4% for weak omnichannel companies. Similarly, strong omnichannel companies see a 7.5% year-over-year decrease in cost per contact, compared to a 0.2% year-over-year decrease for weak companies.\u201d",
        "Step 1: Send all customers to ad platforms for suppression": "Why would you want to throw valuable acquisition dollars on a wasteful audience of existing customers? The answer, you don\u2019t. But your teams are probably doing this today (almost everyone is) because they don\u2019t have a good source for this audience.  With Lytics, you can send all customers to Facebook, Google, Verizon (Yahoo) and other platforms for suppression from lead generating advertising. Building the audience The key to building the right audience is to include all the customers and/or free subscribers you don't want to spend money on. If you are new to using Lytics, see ourAudiences documentationto get started. The following examples skip the first few steps and jump right into building custom audiences. For most companies, an audience to suppress would resemble something like this: Be sure to suppress active users on all your lists and products. This will give the most comprehensive set of customers to limit from targeting. Export a continuously updated file Export these audiences to the key ad platforms:Google Ads,Facebookand,Yahoo Ads. Build suppression in your ad campaigns And then create suppression lists in your ad campaigns. See the respective help docs for more information onFacebookorGoogle Ads.",
        "Step 2: Build lookalikes of your best customers": "If you\u2019re using lookalikes of your customer file as a targeting audience, it\u2019s likely already the best performing audience in your ad campaigns.  Have you heard of thePareto principleor the 80/20 Rule?  80% of your revenue comes from 20% of your customers.  For most companies, it\u2019s more like 90/10. If you can build lookalikes off your absolute best customers, you\u2019ll help Google, Facebook, and Verizon Media to identify the absolute best audiences for your advertising. Building the audience In order to help train the big ad platforms as to who your best users are, send the audience over that most resembles your goal audience.  For many companies, this is likely to be people with a high lifetime value (LTV) or multiple purchases.  For example, your audiences might look like this:  Note that there are several ways to determine high LTV, so feel free to customize to your specific requirements.  The important concept is that you should be targeting lookalikes of your best customers, not lookalikes of all customers. Export a continuously updated file Export these audiences to the key ad platforms:Google Ads,Facebookand,Yahoo Ads. Build lookalikes in your campaigns Build a lookalike inFacebookor a \u201csimilar audience\u201d (lookalike) inGoogle Ads.",
        "Step 3: Setup data science based remarketing audiences": "The next step is to route audiences to the most impactful remarketing campaigns.  For example, you may have certain creative stored in Google and Facebook that better address one content affinity over another.  Additionally, you probably want to consider engagement-based suppressions for remarketing over carpet bombing all of your site visitors. Building the audience Think through which content affinities best align to your creatives. For example, break out your core audiences based on content affinity.  At Lytics, our customers tend to look like the following:  ...or the user might belong to this group:  You can and should use \u201cAny Affinity\u201d to start.  You\u2019ll note that users with any affinity for a topic show above-average interest in that affinity.  To narrow your focus, you might try limiting to \u201cHigh Affinity.\u201d Also, be thinking about the likelihood of a user to re-engage.  Typically, your time window of attention is very short.  Consider filtering for users with highRecency.Descriptive & Predictive Modelingof interactions may also play a role.  Your audience might look something like the following: Exporting the audience Anonymous audiences are continuously exported from the LyticsJavaScript TagtoFacebookandGoogle Analyticstags. Continue with step-by-step instructions topromote relevant content to users based on their interests.",
        "Step 4: Collect emails": "As we mentioned at the start of this playbook, acquisition marketers seldom have the ability to personalize web landing page experiences.  With Lytics, they can.  The next step in upping your acquisition marketing game is to extend the lead generating efforts to your site with targeted email collection. Building the audience Don\u2019t taint the user experience with untargeted email collection.  For known users, suppress your email collection experiences: Building email collection campaigns See theExperience editor docfor how to set up this type of campaign in Lytics. ThePathfora SDKallows easy customization of the look and feel such as the example below. Continue with step-by-step instructions on how togrow your email marketing list.",
        "Step 5: Targeted offers": "Many prospective customers will need an additional nudge to make a first purchase.  The big advantage of using Lytics is the ability to target an offer consistently between site, email, and ads.  This consistent approach between channels frequently provides up to a 10% lift in conversion rates. Building the audience Your audience can be limited to non-customers for special incentives, and may include engagement filters or price sensitivity (e.g. if you\u2019re trying to sell to a known cheapskate). This is a common starting audience:  As a final note, keep in mind there are many other areas to cover including copy choices, optimizations, and common pitfalls to avoid. These are beyond the scope of this playbook, but should be taken into consideration while developing your marketing strategy to acquire new customers.",
        "The Basics": "DataDestinationintegrations are managed primarily by Decision Engine but may also be available in Conductor to streamline the delivery of raw events and profiles to your warehouse. Creating aDestinationis simple:",
        "Adding a new Job": "New data import Jobs are added from the Conductor interface by first navigating to theJobssection under thePipelinesection in the main navigation.  From there, you'll click \"+ Create New\" at the top of the list and enter the wizard to guide you through the creation process. ",
        "Select a Provider": "EachDestinationis first categorized by the provider, making it easy to narrow down the channel you'd like to integrate with. To select a provider, click the tile representing your desired provider, such as \"Google.\" ",
        "Select Job Type": "With the provider selected, we'll surface the various ways you can integrate with that particular provider. This will vary significantly by the provider. ",
        "Select Authorization Method": "Each provider and job type may require additional authorization to finalize the connection. On the \"select authorization\" step, you can either select an existing authorization or create a new one. When creating a new authorization, you will be asked to provide the required credentials, such as key and secret, to proceed. ",
        "Create New Authorization": "You can create it during source configuration if you lack valid authorization for your desired provider/source combination. Click \"Create new Authorization\" above the list, follow thewizard in Vault, and return to where you left off.",
        "Configure Job": "The final step lets you provide the specific configuration details for your chosen provider and job type. Again, the options supported by each provider will vary greatly, and provider-specificintegration detailsshould be leveraged to determine the optimal approach. ",
        "Monitoring a Job": "Once you have one or more Source jobs running, they will be accessible from the Source list view, as pictured below. This view provides quick access to essential details: Name: Name of job, such as \"Export of High-Value Users to Facebook.\"Authorization: Name of the associated authorization.Provider: Third-party tool that you are connecting with Lytics.Type: Indicates whether the job is an import, export, or enrichment.Status: Current state of a job such as running, paused, completed, etc.Created: Date the job was initially created. ",
        "Job Status": "Detailed states are provided to understand better what is happening in the background during a job's lifecycle. These states will vary by job but include: For more information on job states or troubleshooting failed jobs, seejob processing.",
        "Job Summary": "Clicking on any of the items in the Source list will navigate to its dedicated summary view for greater detail. This summary provides all the relevant information about each job you've created in Lytics and an entry point to alter the configuration or status. At the top of the Job Summary page, you\u2019ll find the following quick-access information: Status: Indicates the current state of a job. See the table below for descriptions of each status.Provider: Third-party tool that you are connecting with Lytics, such as Facebook, Google, Mailchimp, etc.Type: Indicates whether the job is an import, export, or enrichment.Job Name: Name of the job, such as \u201cImport Users & Activity\u201d or \u201cExport Audiences.\u201dAuthorization: Name of the authorization, such as \u201cMain Salesforce auth.\u201dCreated By: Lytics user who created the authorization.Last Updated: Date the job was most recently edited.",
        "Logs": "The Logs section records the history of events for this Connection, which are helpful to ensure your connection is working as expected. Below are the connection event types you may see.",
        "Backfilling Data": "Some use cases involve having historical data available for segmentation.  This data might be demographic in nature, or describe how customers prefer to be contacted. This document offers guidelines for cases where large amounts of this data must be available in your Lytics account.",
        "Separate Backfill from Real-time Streams": "A real-timedata streamcontains messages sent in response to the activity they describe. This is distinguished from batched data streams, where messages are sent in groups on a given schedule or according to another trigger. For attributes that will be kept updated by a real-time stream, there is the additional requirement to populate that attribute with a substantial amount of pre-existing information, separating that backfilling from now-forward messages. Backfill messages can be sent using multiple means. API loads should be sent via the bulk CSV or bulk JSON endpoints. It is also possible to use integration workflows to import this data, such asAmazon Web Services (AWS) S3. The benefit of separating this data from real-time message streams is that the processing of backfill messages does not impact the processing time of messages received from real-time streams. The bulk imports are processed in parallel to real-time messages. This means that marketing activations reliant on real-time updates are not affected.",
        "Utilize Timestamps": "Whenever possible,allmessages should have an explicit timestamp. While all messages are additionally time stamped by Lytics at the time of ingestion, specifying a message timestamp is helpful in all circumstances, particularly in cases when messages are received out of order so that Lytics knows which one is the most up-to-date.  It is essential when a backfill occurs concurrently with a real-time stream of the same attribute. All means of loading data permit specifying timestamps. Via API, this is via atimestamp_fieldURL parameter. In the Admin UI, data import configuration options feature a menu to pick among the file schema for a timestamp field.",
        "Evaluate Necessity": "All messages imported into your Lytics account are stored in their raw form and represented as profile attributes in the graph. The purpose of storing all messages is to enable the reprocessing of those messages, a process calledrebuilding. Rebuilding enables all received messages to be represented differently with different attributes, identity resolution rules, etc. All data ingested into Lytics incrementally increase the overhead of rebuilding, making it a longer and more processing-intensive operation. Therefore, before importing large amounts of data, consider the value/benefit of that data. If there is no clear use case for backfilling, consider skipping it.",
        "What is lead capture?": "Traditionally, lead capture involves gathering information from individuals interested in your products or services, such as names, email addresses, and phone numbers. This occurs when people visit your website, subscribe to your newsletter, download resources, or interact with your content. However, in recent years, the role of lead capture within your global consumer engagement strategy has undergone significant transformation.",
        "Activate Required Attributes": "As an admin user of Lytics, you canenable or disable any attributesfrom being surfaced to the web via our JavaScript SDK. Depending on how your account has been configured, you may need to allow some of the required attributes for this particular use case. Required Attributes:",
        "Bonus Idea": "You can extend this use case to progressively improve your visitor profiles by surfacing data capture forms specific to gaps in their profile data. For instance, when they are unknown, only ask for email. As soon as you have an email, perhaps you want to know more about their preferences or learn how to improve a particular portion of your product/offering.",
        "Configure Reporting": "The Lytics platform has a pre-defined schema to facilitate this use case. As such, we recommend building acustom reportfrom within our UI to showcase your progress in converting unknown or anonymous visitors to known visitors with a strong identifier. There are many ways to configure this type of reporting, but here we'll guide you through a basic example to measure impact. ",
        "Add Anonymous vs. Known \"Size\" Component": "Step 1:Once you have created your new report, add a component using the \"+ Add New Component\" button at the top of the report. Step 2:Select \"Size\" as your component type. Step 3:Name your component. We recommend something simple like \"Anonymous vs. Known\". Step 4:Add an optional description to explain the goal of this report to ease the consumption for others later. Step 5:Select the audiences you'd like to highlight. In this use case, we'll select \"Anonymous Profiles\" and \"Known Profiles\", which are both available by default. Step 6:Save your component. Please note it may take some time for the report to populate once you have configured it, but as long as you see the empty \"Not enough data\" state, you are good to go.",
        "Add Impression \"Composition\" Component": "Step 1:Following steps similar to the previous component, we'll add another component, but this time, we use the \"Composition\" component type. Step 2:Name your component \"Campaign Impressions,\" as this component will focus on showcasing impression information to validate that the campaign is being shown. Step 3:Select the \"All\" audience to ensure all visitors, regardless if they are known or unknown, are included in the report. Step 4:Select the field \"ly_impressions.\" This field represents an array of all campaigns each visitor has interacted with and is available by default. The values returned for custom campaigns will match the \"ID\" you set above in the campaign configuration. Step 5:Finally, replace \"my_campaign_id\" in the example below with the ID of your campaign above to highlight only interactions for that campaign.",
        "Add Conversion \"Composition\" Component": "Following the same steps as above, configure another \"Composition\" component, but this time, use the field \"ly_conversions\" as it will be an array of all campaigns your users have converted on instead of the impressions used previously. Thats it. Your report has been configured. As you begin to gain impressions and conversions, this report will populate. Generally, every hour or two, to highlight the impact.",
        "Bonus!": "Get additional guidance on how to pass interaction information related to your campaign to the following common sources for additional reporting and insight discovery: Google Analytics [coming soon]Custom BI via Data Warehouse [coming soon]",
        "Execute Your Experience": "Once you have configured and tested your Experience, activation is as simple as publishing the JavaScript code to your site. This may be done via a tag manager like Google Tag Manager or using another mechanism such as Drupal blocks or WordPress.",
        "Import Files": "Import user activity and profile files from Google Cloud Storage into Lytics.",
        "CSV Export": "Lytics allows you to export your audiences to a CSV file for importing into other tools that Lytics does not natively integration with. Export CSV for DownloadExport the CSV to an SFTP Server",
        "Auths": "Some jobs, like specific Webhook flows, don't require explicit credentials to access external systems.  However, most jobs require some form of authorization to retrieve data from or send data to external systems.  In Lytics, these credentials are generically referred to as Authorizations. Authorizations can be managed from theAuth API, or directly from the Lytics interface.  All authorizations are either configuration-based or OAuth-based. Configuration: The authorization is based on credentials provided to Lytics.  These usually take the form of API keys/secrets and usernames/passwords.OAuth: The authorization is given access delegation from an OAuth flow.. Commonly, authorizations will change \u2013 some systems require periodic password changes, tokens get revoked, users who complete OAuth flows leave an organization, etc.  Consequently, a job that is running successfully with authorization today may fail tomorrow if the authorization is no longer valid.  If you know in advance that credentials will update or change, it is recommended that you create a new authorization and update the job to use the new one. To stay alerted on the health status of a job, you can set up alerts as described in theMonitoringsection below.",
        "Configuring and Submitting Jobs": "Jobs can be managed from theJobs API, or directly from the Lytics interface. Each job has custom configuration options that tell the job when to run, what type of data to pull, which lists to pull data from in external systems, how to send the data to downstream sources, etc.  Each job's API endpoint will validate the presence of any required parameters, or other conditions that must be satisfied for the job to run correctly.  In the event of a validation error in the job configuration, the API will return a 400 error with a message indicating what is misconfigured. If the configuration for the job request is valid, the job will be immediately submitted to Lytics internal job running system.",
        "Job State Machine": "Jobs running on Lytics' internal job running system may be in any of the following states: Runnable: The job is currently running.Completed: The job has run successfully and is not scheduled to re-run.Paused: The job has been manually paused, and will remain so until it is unpaused or terminated.Sleeping: The job is in between runs and is scheduled to run again at a later point in time.Failed: The job has exceeded its retry limit and will not attempt to run again.Terminated: The job has been manually terminated and will not attempt to run again. Jobs will usually first enter into a Runnable state.  If the job is fed input from real-time triggers, it will stay running unless it encounters any errors.  If the job runs on a batched interval, it will usually switch between Runnable and Sleeping.  While most transitions happen automatically in the system (Runnable \u2192 Completed, for example), some are triggered manually, namely:",
        "Errors": "There are a variety of reasons a job will enter a Failed state, including: Authorization issues: This is the most common reason for job failures.  Some authorizations can become invalid if the external system requires periodic password changes and the authorization isn't updated, if tokens get revoked, or if users who complete OAuth flows leave an organization and the token no longer has access to the resources it is requesting.External failures: Sometimes external systems can produce intermittent failures, like HTTP 503 errors if the system is temporarily unavailable.  Because of their intermittency, it's less likely that one of these errors will result in a job ending in a Failed state.Internal Lytics failures: While rare, sometimes the availability of internal Lytics resources can cause a job to fail.  Because of the high availability of Lytics systems (>99.9%), it's unlikely that a job will be in a Failed state from Lytics internals. When an error occurs, the job will enter a Sleeping state, and retry a minute later.  If an error occurs on the second attempted run, the job will sleep double the amount of time as the previous run.  If errors still occur after two hours, the job will enter a Failed state. Any failed work can be manually restarted, through the Job API or through the Lytics interface.  If, for example, a job is in a failed state because of an authorization issue, you can create a new authorization for the job and restart it.",
        "Contributing": "Many jobs are maintained by Lytics, and even more workflows can be enabled by using our generic tooling for Webhook and SFTP-based integration flows. Some jobs are open-sourced and run on Conductor's Opus framework for running jobs. Check out ourConductor Opus Template on GiHubto get started creating your own jobs.",
        "Import Enrichment Data": "Enrich your Lytics audiences withVersium REACH APIdata to improve your identity resolution and reach.",
        "How to Use Collaborative Filter Engines": "The primary way to leverage Collaborative Filter Engines is through Recommendations. While Lytics' Recommendation API, and Experience toolkit allow you to deploy Recommendation campaigns onsite or via email, the Collaborative Filter UI allows you to experiment and test out Lytics' Recommendations. In theRecommendtab, you can fetch Recommendations for any user. Simply select the identifier field (_uidsin this case) and click on theRecommendbutton. This will request Recommendations for the selected user using the Lytics Recommendation API. The Recommended items are displayed below. TheItem Recommendationinput allows you to fetchsimilaritems for any given item. In the example below, we are finding items similar to the item withshopify_product_id = 631195017394. This can be used to find similar products based on user behavior.",
        "Creating a New Collaborative Filter Engine": "To create a new Collaborative Filter Engine, click on theNew Interest Enginebutton and click onCollaborative Filter. This will open a wizard with the following options. When creating a Collaborative Filter Engine, there are 4 fields: Name(required): the name of the new Collaborative Filter.Description(optional): a description for your Collaborative Filter.Inventory ID(required): the identifier associated with the items in your inventory (ie: an identifier from thecontenttable).Inventory Field(required): the field on theusertable that contains a user's activity (ie purchase history, browsing history, etc). This field must be a set or map type and must contain IDs referenced in theInventory IDfield. Once you create your new Collaborative Filter wizard, Lytics will train a Collaborative Filter model within minutes. Once the model has finished training, the UI will be available to use, as well as Lytics' Recommendation API.",
        "User Profile Health": "Because Lytics user profiles are created by stitching data across sources the stability of a given profile depends on the quality and quantity of data used to create it. Lytics is now surfacing the \"health\" of user profiles to improve your account performance and reliability. Healthy profilesare stable and actionable, with strong identifiers connecting data fragments across sources to the right user.Unhealthy profilesare unstable and not actionable, as they are either missing entirely or missing a subset of data. Lytics is providing visibility into user profile health to facilitate the resolution of various issues caused by unhealthy profiles, which can include missing data, inconsistent audience counts between Lytics and other tools, and duplicate profiles in audience exports/triggers.",
        "Unhealthy Profiles": "A user profile is considered unhealthy if the traversal through all data fragments fails for any reason. Unhealthy profiles are mostly commonly hitting one of the following limitations: Max NeighborsMax TraversalsProfile size is too large Unhealthy profiles in Lytics are no longer processed for audience evaluations, audience exports or triggers, and enrichment processes, including behavioral scoring and content affinity calculations. Because unhealthy profiles are unusable in Lytics or any of your connected marketing tools, this change will reduce latencies and ensure that only valid user profiles are surfaced for activations.",
        "What information is contained for unhealthy profiles?": "Unhealthy profiles only contain unique identifiers (\u201cby fields\u201d) and meta fields such as profile size, an indication of why the profile is unhealthy, the data stream names, etc. _num_aliases_num_events_num_streams_num_days_num_conflicts_conflicts_num_max_neighbors_max_traversals_num_nested_values_total_sz_cust_sz_internal_sz_elasticsearch_size_streamnames_profile_processing_failure_broken_profile_broke_max_size_broken_profile_broke_max_fragment_size_broken_profile_max_neighbors_broken_profile_nested_count Lytics is not destroying or losing any data about your users. Rather, unhealthy profiles simply no longer surface the portion of a profile that can be retrieved as it may result in inaccurate audience counts and activations.",
        "How can I identify unhealthy profiles?": "A new, predefined audience called \"Unhealthy Profiles'' will be made available in all accounts. If user profiles become unhealthy, then we should see an increase in this audience at the same rate that we see a decrease in other audiences.",
        "Will other audiences be affected by this change?": "The only fields that we drop from the profiles when they become unhealthy are the non-\"by fields.\" So an audience that only has conditions on those \"by fields\" will not be affected. For example,Filter Exists emailwouldn't be affected, but the audienceFilter Exists emailANDState=WAwould be affected. The latter would be affected because Lytics will drop the state field for the unhealthy profiles.",
        "How can I resolve unhealthy profiles?": "Contact your Lytics Account Manager to discuss a strategy to resolve or reduce the number of unhealthy profiles. The approach will vary depending on your identity resolution strategy and the requirements of your marketing use cases.",
        "Can I export unhealthy profiles?": "Yes, the \"Unhealthy Profiles\" audience can be exported via BigQuery, SFTP or S3. However, unhealthy profiles cannot be exported to other provider tools for activations since they are excluded from the audience evaluation process.",
        "Profile Limits": "User profiles become unhealthy by hitting one or more of the following limits.",
        "Max Neighbors": "\u201cMax neighbors\u201d refers to the number of edges associated with a node. A node is created in Lytics' graph when an identifier is introduced to a user profile. Audience Impact: From an activation perspective, this means that what would have been one user profile in the audience is now increased to how many fragmented nodes there were. Above is an example of when max neighbors could impact a profile. The profile on the left has one email and many web cookies. Each time a user gets a new web cookie and they are logged into the site, Lytics will update the graph with a new node. Over time, the node with an email will have more and more cookies associated with it until it hits the max neighbors limit. Once the profile hits max traversals, the email node will drop off. Then, instead of having one user profile in the Lytics audience builder, there will be as many profiles as there are fragmented nodes off the original (four in this example).",
        "Max Traversals": "\u201cMax Traversals\u201d refers to the number of edges that need to be traversed in order to retrieve the user profile for the audience. Once this hits the limit specified on the account, the profile will no longer update. Audience Impact: If a user hits max traversals, their profile will no longer be updated for audience building and will fall into a \u201cstale state.\u201d See the following examples: If a user has opted out for emails and they hit max traversals, they will not be opted out even if the event is received to opt them out. They will be stuck in that audience unless the criteria changes and they no longer qualify.If a user has opted out for an email and they do not qualify for the audience originally but eventually opt in, that will not be reflected on the profile in the audience builder.",
        "Profile Size": "When a profile becomes too large, it cannot be pulled in for audience building at all. At this point the profile is unusable.  There are a few different points in our pipeline which can trigger a profile to be marked unhealthy due to size: If a single node in the graph becomes too large to write to the underlying data structure for our graph, we mark that node as unhealthy (subsequently marking any profile built from that node in the graph as unhealthy).  This particular size limit leads to data loss, as we could not write updated attribution to the node in the graph, and cannot guarantee profile accuracy.  This particular instance of size check can be resolved by addressing the root source contributing to the size of data attempting to be written (i.e. place a cap on the number of values retained in some set / map fields), and replaying the account.If the traversal of the graph builds a complete profile that is too large to write to the underlying data structure for our entity storage, we mark the profile as unhealthy. This can typically be resolved without requiring a replay, as there has been no potential data loss in the graph, and often involves addressing the root source contributing and simply re-evaluating the profile impacted (this re-evaluation can occur by receiving new events for that profile, or during our natural cadence of re-scoring profiles).If a full profile is not too large for our key indexed profile storage, but is too large to write to our fully indexed (used for bulk exports / ad-hoc audience scans) profile storage, this is treated equally to (2) and the profile is marked unhealthy with equal resolution strategy.",
        "Google Optimize Overview": "Google Optimizeis part of the Google Marketing Platform and is natively integrated with Google Analytics. Google Optimize allows you to easily run tests on your website such as A/B and multivariate tests, to determine how to best engage your visitors. Integrating Lytics and Google Optimize opens up personalization use cases to create more engaging online experiences.  TheLytics JavaScript tagis automatically available inGoogle Tag Manager(GTM) to start collecting data from your website and writing it to Google'sdata layer. Once you have the Lytics JavaScript tag installed and connected to GTM, you can access the audiences from your Lytics account in Google Optimize following Google'sfirst-party cookie targetingdocumentation. By default, the Lytics cookiely_segswill be passed with all of your audiences that haveAPI accessenabled.",
        "Published Campaigns": "Published campaigns can be thought of as your \"active\" campaigns. In order for a campaign widget to show up on your website, the campaign must be published. To edit a published campaign, it needs to be paused. This will temporarily hide it from your website while you make changes.",
        "Drafted Campaigns": "If you want to save a partially completed campaign, or save a finished campaign that you\u2019re not ready to put on your website, you can save it as a draft. A campaign draft can be published or edited at any time.",
        "Archived Campaigns": "Published campaigns automatically become archived when they reach their end date. In order to keep their reporting historically accurate, archived campaigns cannot be edited.",
        "Enrich Users": "Enrich Lytics users to addUnified ID 2.0to their user profile. Use this UID to identify consumers to target with more relevant ad campaigns.",
        "Navigating Vault": "Vault is comprised of the following sections: Account Usage- an overview of your general account quota usage related to event consumption.Account Settings- view and change necessary administrative settings and details of your account. These are sectioned into the following categories.Account DetailsJavaScript TagLytics APIContentSecuritySchemaUsers- view a list of users accessing your Lytics account, change their details or permissions, remove a user, or invite a new user to the account.SecurityAccess Tokens- view and manage a list of Lytics API access tokens. New tokens can be provisioned with specific permissions.Authorizations- create, edit, view, and delete authorizations within Lytics. Authorizations are credentials to third parties that enable the necessary scopes and permissions for data import and export jobs to run.Account SetupJavaScript Tag- learn how to install the Lytics JavaScript tag and validate the installation.",
        "Who can access Vault?": "Vault is focused primarily on account admins or those that have permission to administer settings, user access, etc. Based upon these permissions, your experience in Vault may vary, and all sections outlined above may not be available. In general, Admins will be able to access all areas. Data Managers, Campaign Managers, Experience Managers, and Goal Managers can access the Authorizations section to manage the credentials for their jobs and experiences. Finally, all Lytics users, regardless of role, should have access to view the JavaScript tag installation page under Account Setup, and they should be able to view their user profile where they can change details such as their Name, Email Address, Phone Number (for two-factor authentication) and change their password. Your user profile will be accessible under the main Lytics navigation under \"Manage My Profile.\"",
        "Notable Changes": "For existing customers, slight changes will impact your day-to-day management activities.",
        "Product Switcher": "Vault will be available directly from the primary product switcher at the top left of your Lytics interface. This will be your primary access point for account management from now on.",
        "Account Usage": "Our account usage data and quota meters received a much-needed facelift. The usage metrics act as the Vault \"dashboard\" for admin users. ",
        "Account Settings": "Account setting sections are now accessible through the main navigation. These settings have received a minor facelift update. The form controls for multi-text fields have been slightly updated for a more standard user experience. In addition, users are now prompted to save or discard their changes when navigating away from these pages with unsaved settings.",
        "Users": "The user list is now sortable and filterable based on name, email, who invited them, and how long they've been a Lytics user. Each user has a page to view their details and roles. In Vault, in addition to assigning new roles, an administrator can edit any user's name and email address.  The user invite form is now on its page to improve the flow and experience of inviting users. Roles are now sorted into two categories - Admin or Custom Roles. An admin inherently has access to everything, while custom roles give a finer-grain definition of what the user can access. The details on each role are shown in a tooltip when you hover over the role name. ",
        "Access Tokens": "Creating and managing your access tokens gives you fine-grained control over how your Lytics account and data is accessed. To get started, navigate toSecurity > Access Tokensfrom the menu in Vault. Note that just like all other account settings, you must have administrator privileges to view and make changes.",
        "Authorizations": "Authorizations are now to be created and updated only in Vault. You will still be able to utilize the auths you create in Vault in other products, but for example, when creating a job in Conductor or Decision Engine, if you don't see an authorization you want to use for that job, you will be linked into the authorizations wizard within Vault to create the auth. ",
        "JavaScript Tag Installation": "This page remains unchanged, with simple styling changes and updated links to our documentation for troubleshooting.",
        "Editing and Deleting Reports": "Users can quicklyEditandDeletereports at the top of the Report page. TheEditoption allows you to rename the Reportnameanddescription.",
        "Sharing Your Reports": "Creating Audience Reports in Lytics can be a collaborative process. By default, Reports within Lytics are shared across all users with the.Reportingrole. To prohibit users from accessing Reports globally the.Reportingrole can be disabled for specific users. In addition to the global role, a report creator can further restrict access to their reports by setting the report asprivateand adding the specific list of users who have access. The screenshot below outlines the simple setup process to fine-tune collaboration and access to individual reports. ",
        "Change Logs": "Lytics records a log of all changes made to a Report in the \u201cLogs\u201d tab. This record lets users keep track of who updated a report's components.",
        "Downloading Reports": "Each component in Lytics provides the option to download the associated data in a CSV format. TheDownloadicon is located in the bottom right of each component:",
        "Creating Audiences": "Building new audiences allows you to target specific groups of your users. Why send the same email to your entire audience? Instead, send emails to distinct audiences with content that you know those users enjoy. Lytics gives you the ability to create audiences based on cross channel user data, behaviors and interests. Audiences can be built using existing audiences, user content affinity, your personalization campaigns, or by custom rules utilizing any user fields in your Lytics account. Create rulesets by combining multiple audiences and boolean logic to target users with as much or as little granularity as you require. Lytics updates your audiences automatically. Once an audience is built, users will join or leave whenever they match (or fail to match) the rules you have set. Export your audiences, often in real-time, to your integrated marketing tools giving you the power of Lytics however you engage with your users.",
        "Building Audiences: Existing Audiences": "TheExisting Audiencestab in the audience builder contains a list of all existing audiences and their size categorized by audience type such as characteristic. Characteristics are pre-built audiences pertaining to single user attributes such as location or behavior. Keep in mind that the audience sizes you see in this list arenotreal-time. They are updated every few hours for performance reasons. When creating or editing an audience, however, you will always see real-time numbers so that you can make precise decisions based on accurate data. You can use theFilter by Audience Typedrop-down to filter the list or use theSearchbox to quickly find an audience by name.  Select an audience or characteristic to add it to the definition of the audience you are building or editing. ",
        "Building Audiences: Content Affinity": "TheContent Affinitytab in the audience builder is used to add Affinity-based rules to your audience. SinceAffinitiesare agroup of related Topics, this allows you to effectively target users interested in several subjects or products.  In the example above, we created a \"Lavender\" Affinity for an online beauty store, which includes 6 Topics such as \"lavender\", \"lavender lotion\", and \"lavender luxury.\" Instead of creating an audience with 6 rules (one per Topic), we just select our Lavender Affinity that contains those 6 Topics. Once you choose an Affinity for your audience, you will be able to select the range of affinity values. The histogram displays the distribution of user interest for the selected Affinity and is helpful to build an audience that has the desired size. By default, users withAny Affinityare selected. You can choose a built-in range by clicking any of theAffinitybuttons or, to set a custom range, use the slider to refine the range of users to target. As you change the range, the total number of users that fit the range is shown in the right column. As soon as you create an Affinity, Lytics start scoring users against it as they interact with your content or products. For active users, you can expect this number to start populating within a few days. For inactive users, it can take up to 2 weeks for them to be scored against an Affinity. So it's recommended that you create Affinities in advance of the audience needed for an Affinity-based campaign.",
        "Topic-Based Audiences": "Under theCustom Ruletab in the Audience Builder, you can find the existing audiences based on Topics in your content taxonomy. If you want to create new audiences based on a single Topic, you can do so as follows: Search for \"Topics\" in the Custom Rule tab of the Audience BuilderSelect the Topic of interest.Set the threshold according to the desired users' level of interest in that Topic. Once you hit \"Add Condition\", the custom rule for a Topic will look like this: Although Topics are still supported in the Audience Builder, we strongly recommend you use Affinities to build audiences for a more efficient workflow and more effective targeting. Some users may find a field namedInferred Content Affinitywhen trying to build affinity or topic based audiences. This field contains topics that a user could be interested in even though they may not have interacted with the topics directly. To learn more about how these inferrences are made, you can read about ourcontent taxonomy process. Note that topic affinity already includes this information, but this field contains only inferred affinity. We would also recommend not using this field to build audiences as it is not a wholistic view of a user's affinity.",
        "Building Audiences: Custom Rules": "TheCustom Rulestab in the audience builder is used to add any rule based on auser fieldto the audience being built or edited. Custom rules allow for ultimate access of all data that is aggregated in user fields. Custom rules are simple statements that can be combined to create very precise audiences. Using them correctly requires a thorough understanding of the data being used, but there is no replacement for this level of segmentation.  The user field list displays all the fields in your account. TheSourcedrop-down can be used to filter on the data stream the user field came from. TheIncludesdrop-down can be used to filter user fields on the percentage of users that have a field. Use theSearchinput to search for a user field by name. Generally, to add a Custom Rule to an audience definition: Choose a User FieldChoose an operator (e.g., text contains, value is less than)Choose a value There are variations to this general procedure based on thetypeof the user field and the operator chosen.",
        "\ud83d\udea7Audiences Field Value Limits": "Audiences can have a maximum of 1,000 field values. \"Values\" in this context apply to a single evaluation against a user field. When exceeded, the error message \"Audience too large! Remove values to save this audience\" will be displayed. For example, the following scenarios would trigger the alert: An audience with a single rule that supports multiple values ( e.g. contains one of) with 1001+ values.An audience with 1001+ user field rules each with a single evaluation (e.g. exists).Any variation in between such as two rules each containing over 500 evaluations, etc.",
        "User Field Types": "How you work with user fields in the Custom Rules tab will depend on way the field is mapped in on the user profile.",
        "Out of the Box Custom Rules": "Some user fields are mapped by default and populated through the use of the Lytics Javascript tag and Experiences. These out of the box fields can be incredibly useful when building audiences using Custom Rules.",
        "Building Audiences: Multiple Rules and Rule Sets": "All audience definitions are comprised of rules based onexisting audiences,content affinity,campaigns, andcustom rules. You may have noticed that after you add your first rule in the audience builder you have the option toAdd New RuleorAdd New Rule Set. Adding new rules and rulesets, and setting the conditions for their interaction, gives you the ability to refine which users your audiences target.",
        "Working With Multiple Rules": " This audience has a single rule: included all users with the characteristicBehavior: Perusers. This will select all users with anintensity scoreof 20 or less but with afrequency scoreof 20 or greater. These are users who interact often but not on a deep level. You can refine this rule further by clickingAdd New Rule.    This audience now also includes a rule for users who have a low content affinity forTechnologycontent. Now that it has been added, there are three numbers of interest. The first number, 87,650, is the number of users with the characteristicBehavior: Perusers. The second number, 466,730, is the number of users who have low affinity forTechnologycontent. The third number, 11,514, are the users who meetbothof these conditions.   Clicking theAnd/Ortoggle, will switch the way the two rules intereact. The audience now includes all 542,921 users whoeitherhave the characteristicBehavior: Perusersor have low afffinity forTechnologycontent.",
        "Excluding Rules": "In addition to theAnd/Ortoggle, there is theIncluded/Excludedtoggle. This toggleinvertsa condition. This audience contains a rule that targets the 159,084 users with the characteristicBrowser / OS: Mobileor users who have used a mobile devices.  Clicking theInclude/Excludetoggle the audience will now target the 705,166 users whodo nothave the characteristicBrowser / OS: Mobile.  The sum of the two numbers will be the total audience size that meets those rule(s).   This is especially apparent when using a rule such asCreated Timestamp Exists. Every user in Lytics will have a timestamp of when they were created. Therefore, the number of users whodo notmeet the condition of having a created timestamp will always be 0.  ",
        "Working With Rule Sets": "When building elaborate audiences, having a single set of rules that are evaluated using either anANDor anORis not enough. A ruleset is a set of rules that itself can be used as a rule for an even larger set of rules that is again evaluated using anANDor anOR.  This audience has two rulesets. The first ruleset has one rule: all users who have the characteristicEmail Capture Status: Known Email. The second ruleset has two rules: all users have some affinity forMarketingORAdvertisingcontent. Combining these two rulesets with theANDoperator produces an audience that targets users with an email address that have some interest in either marketing or advertising. This audience could be used to email a special newsletter or promotion about a product or trending topic that is related to marketing or advertising.",
        "Excluding Rule Sets": " Rulesets, just like rules, can be excluded. Clicking theInclude/Excludetoogle on the second ruleset will result in the audience targeting all users with an email address who do not have some interest in either marketing or advertising. It could be used to send a different email to the remainder of your users with email addresses.",
        "Building Audiences: Configuration Options": "Configure your Lytics audiences with the following options to execute your use cases. ",
        "Enabling API Access": "By default, newly created Lytics audiences are not enabled for API access and thus will not be available to public APIs. Enabling API access is necessary when integrating with other client-side tags/ pixels,Pathfora, or the Lyticspersonalization APIendpoints. Select theAPI Accessiblecheckbox to enable access.",
        "Generate Insights": "Lytics Insightsprovide visibility into the performance of your audiences and campaigns as users interact with your brand. Insights are generated out-of-the-box for Lytics Behavioral and Engagement audiences. You can also request up to10 custom audiencesto be prioritized as candidates for generating Insights. When creating a new audience or editing an existing one, select theGenerate Insightscheckbox to make that audience a candidate for Insights (shown in the screenshot above). Lytics prioritizes Insights based on the statistical significance of the data and the most frequently used audiences for your account. Therefore, even if you select the \"Generate Insights\" option for a particular audience, it is not guaranteed to show up in your Insights drawer.",
        "Managing Audiences": "Managing audiences becomes important as an account develops and your user base grows. Audiences are critical to both targeted campaigns as well as discovering insights about your users. TheAudiencessection of Lytics houses all of your account\u2019s audiences. You can view, create, and manage audiences from this section.  Audiences are listed alphabetically by default. Click any column header to change the sort property. Common sort options areLast ModifiedandSize(the number of users in the audience).",
        "Naming Your Audiences": "For better targeting, Lytics encourages the creation of many audiences. They\u2019re easy and free to make. When working with a large library of audiences, naming conventions become important. Name your audiences by purpose and include any categorization in the name. Giving them a common prefix (e.g., Holiday Promo: Tech lovers, Holiday Promo: Loyal Shoppers) will help you to keep everything related next to each other in the user interface.  Audiences appear throughout the Lytics platform. Make sure your naming convention works in any context to avoid confusion.",
        "Finding Audiences": "The easiest way to find an audience is to use theSearchbox. As you enter your search term the audience list will display any matching audiences.",
        "Editing Audiences": "Occasionally you will want to return to previously created audiences and update the logic used to create the segment. Find the audienceyou would like to editClick on the audience to navigate to the overview of the audienceClick theEditbutton to enter the audience builderMake your desired changes within the audience builderClickSave",
        "Duplicating Audiences": "Find the audienceyou would like to duplicateClick on the audience to navigate to the overview of the audienceClick the...button to enter the audience builder. The audience builder will be pre-populated with all of the criteria of the audience you duplicated.Make any desired changesClickSave",
        "Deleting Audiences": "Deleting audiences is an integral part to keeping a tidy workspace in Lytics. Audiences can be deleted in one of two ways: individually or in batch. Note: Deleting an audience does not delete the users associated with that audience.",
        "Deleting an Individual Audience": "From theBrowse Audienceslist, select the name of the audience you wish to delete.From the options menu, selectDelete.ClickDelete.ClickAcceptto confirm.",
        "Deleting Multiple Audiences": "From theBrowse Audienceslist, select the checkboxes of the audiences you wish to delete.NOTE:Audience selection persists when changing pages allowing you to select audiences from multiple pages.ClickDelete Selected Audiences.ClickDelete.NOTE:Lytics will check to see if the audiences you selected are safe to delete. Audiences are considered safe if they are not used in an ongoing export, campaign, or another audience's definition. You will be prompted to delete dependant audiences, clickYesto delete the selected audience and any dependants listed orSkipcancel deletion of the selected audience.ClickAcceptto confirm.",
        "Maintaining Audiences": "An audience's landing page contains a lot of valuable information about your users and the audience itself. At the top of the audience page, you will find the following information... Audience nameIDSlugCreated byCreated on dateLast updated dateAPI access configuration Below this information you will find multiple tabs where you can access additional information about the audience.",
        "Audience Summary": "The audience summary page is designed to provide insights into the current state of an audience as well as how the audience has changed over time. When first navigating to an audience landing page, this tab will be selected by default. The Summary tab includes the following sections: General MetricsThe total number of users in this audienceThe total percentage of this audience as compared to the total number of profiles in your accountThe percentage change in audience size over the last 7 and last 30 daysActivity trend (graph of audience size over the defined date range)",
        "Activity Graph": "After creating a new audience, the audience size graph will begin to populate within a day. Depending on the date range you specify, the activity graph can be viewed using following intervals: HourlyDailyWeeklyMonthly Audience Size Trends Keep in mind that audiences are real-time. This makes them quite different from the user lists in other marketing tools which are frozen in time and only change when you update the list. Audiences are filters applied to the total audience and will change whenever a user's behavior changes in a way that will add or remove them from the audience definition. In this way, tracking audience size over time is a powerful reporting tool. For example, you create an audience that contains users who have visited the international news section of your website. Upon creation the audience contains 120,000 users. You then run a marketing campaign promoting your international news section. After one week, the audience has grown from 120,000 to 400,000 users with most of the growth occurring in the 24 hours after the campaign was launched. This is clear evidence that the campaign was effective, and most effective in the first 24 hours, at driving users to your international news section.s",
        "Audience Details": "Select theDetailstab to view the following sections: Exports: This section outlines what exports (if any) are using this audienceRecent Users: This section allows you to view a sample of users in this audience who have been active recently (by clicking theViewbutton)",
        "Audience Characteristics": "Select theCharacteristicstab to view the characteristics of users in this audience. Characteristics include: BehaviorWeb ActivityContent AffinityCampaign Referral Interactions",
        "Audience Logs": "Select theLogstab to view recent events relating to this audience. Activity such as creation, updating and syncing can all be found here.",
        "Activating Audiences in Experiences": "Creating a Lytics Experience is one of the easiest and most common ways to activate Lytics Audiences. Learn more aboutLytics Experiencesto find out how you can engage your audiences on site.",
        "Accessing Audiences Using the Public Entity API": "The Lytics Experience Editor is a great way to get started with website personalization, but through the use of the Lytics API, there is much more that can be done. The Lytics Experience editor is powered by the Pathfora SDK which is in turn powered by the Lytics Entity API. The Lytics Entity API is how to find out what audiences the current user belongs to as well as what traits the user has. Using the Lytics Entity API it's possible to create completely custom personalized experiences on your site. To read more about the Entity API,check out the API documentation.",
        "Exporting Audiences to Other Tools": "A core benefit of Lytics is exporting your smart audiences to the other tools in your marketing tech stack. That way, the campaigns run on your channels are enriched with thebehavioral dataandcontent affinitiespowered by data science on Lytics. From the audience overview page, click theExportbutton to create a new export job . TheExportmenu lists built in exports such asEmail CSVand any out of the box Integrations. Specific instructions for exporting can be found in the Integrations section (under the desired tool) of our documentation.",
        "How to Import Data from Mixpanel into Lytics:": "Navigate to the Mixpanel integration in the integrations section of Lytics.Connect Mixpanel to Lytics by entering your API Key and Secret. (You will only do this once.)These can be found in your Mixpanel admin underAccount->Projects.After you have connected your Mixpanel account to Lytics, you can start importing data. SelectImport Dataunder Actions.After you have selected your account, you can begin your import by simply selectingStart Import.  When you start the import, we will bring in all of your user profiles and events.  We will import new events and profiles once a day going forward. NOTE:Since the majority of your data will be custom fields, our team will need to do a little bit of work to process the data.  In order to speed this up, please contact ourCustomer Successteam and let them know that you would like to import your custom Mixpanel data.  Advanced Options: This field is not required Filter: If you don't want to pull in all events, you can specify events to import.  To learn more about this parameter, you can see theMixpanel documentation.",
        "How to Export Data from Lytics into Mixpanel": "To export an audience, leverage the LyticsJavaScripttag. With the JavaScript callback function, you can pass audience membership from Lytics to the Mixpanel tag using the appropriate field set up to accept Lytics audience data.",
        "Building a Content Collection": "You can build a content collection in Lytics by navigating toContent > Collectionsfrom the dashboard. Then click theNew Collectionbutton above the list of current collections in your account. Collections can either be dynamic or locked. In the collection builder you can toggle this between these two options.",
        "Dynamic content collections": "Dynamic collections are rule-based, so individual pieces of content may enter or exit the collection over time. Dynamic collections are built by selecting and setting content filters which determine what content is present in the collection at any given time. Preview content currently in the collection by hovering over a content card and clickingPreview this content. For example, you can create a collection that includes all articles published in the last 7 days. This setting is ideal for making sure your content collection remains fresh.",
        "Locked content collections": "A locked content collection is a static collection of documents that you hand-select using the collection builder. Content filters can be used as a searching mechanism to help find the specific documents you would like to add to your custom collection. You can add content items to the collection by hovering over a content card and clickingSelect this content. Selected content items are indicted by cards with a blue border. Once you save the collection, documents are not added or removed until you edit and save the collection again.",
        "Content Filters": "There are a number of filters available when navigating the content collection builder. These filters can be combined in such a way that narrow down the documents in the collection using \"AND\" logic.",
        "Filter content by title, description, or URL": "This filter acts as a catch-all text search for the title, description, and URL of the document. It can be a full or partial match to any of these fields, and the input is case sensitive. You can enter multiple values for this filter type, and the results are not mutually exclusive. That is, if you have entered the search terms \"Dog\" and \"Cat\", each of the documents returned should contain \"Dog\" or \"Cat\", but not necessarily both (\"OR\" logic). Perhaps the most common use of this field is to filter by a URL path. For example, say you're looking to build a dynamic collection of all blog posts from your website, and the URL of your blog posts match the following pattern:http://yourdomain.com/blog/name-of-your-post. To build this collection you can enter/blog/into this filter.",
        "Content type": "The type of a document is derived by the Lytics Content Affinity Engine while analyzing and indexing the document. A document maybe be classified as any of the following: ArticleDiscussionEmailFAQImageJobOtherPersonPostProductProfileVideo These checkbox filters allow you to view content of only the selected type(s). For example, if you want to build a dynamic collection of videos to recommend to users, you might select theVideocontent type.",
        "Published date": "This filter allows you to limit documents based on their date of publication. You may filter documents published after a date relative to the current time or after a static date in time. Relative dates may be set by increments of days, weeks, months, or years. Using a relative date filter allows you ensure that the content in your collection is evergreen. For example you may want to only recommend content that was published in the last week: If you launched a new product recently, you may want to build a collection of content relevant to that launch. Using the static date filter set to the day before the launch, combined with other filters could help achieve this.",
        "Features": "The features filter allows you to select whether or not the content should have a description and/or a thumbnail image. Both the image and description fields are extracted from HTML meta tags during the scraping process of the content affinity engine. Enabling these settings may be especially useful if you are looking to implement content recommendations into a space on your website and you wish to include more than just a title and link to the content.",
        "Author": "The author filter allows you to include only articles written by the author(s) selected. If you have a long list of authors, you can use the search bar to find the author(s) you are looking for.",
        "Advanced Collection Editor": "The filters above cover many common use cases. However, in some cases a more advanced set of filtering criteria is required. This is achievable via theAdvanced Editorwhich is accessible from the triple dot menu on a collection summary or from the collection editor view. From theAdvanced Editoryou can leverage the inclusion of other content collections or a custom rule just as you would with custom rules in the core Audience Builder. These custom rules surface the full set of fields associated with your content during the classification process. Word count, language in addition to allfiltersreferenced above are accessible through this advanced editor.",
        "What are fields?": "Fields can include demographic data, behavioral data, or any other data point that is relevant to understanding and engaging with customers. In addition, fields themselves are defined by one of many common data types, such as strings, integers, sets, and maps. ",
        "What are mappings?": "Sometimes, you may want to transform or cleanse messy data before it hits a profile.  Other times, you may want to perform an operation or aggregation to reflect its usefulness for business decisions or segmentation \u2014 like aggregating multiple purchases into a combined lifetime spend metric. In either case, mappings provide the necessary translation layer to ensure each data point collected is clean and consistent. Most importantly, mappings guide the relationship between disparate data points and the final materialized user profile. ",
        "Managing User Fields": "Lytics' profiles are composed of a collection of user fields.  These fields have standard metadata (name, description) and typical properties you'd expect from a traditional database column (kind/type, capacity).  In addition, each field has amerge operatorthat describes how we want to combine data from different sources at different times into a single field.  For example, a field describing a customer's first purchase date would use aminimumoperator to ensure that, regardless of the order in which Lytics sees purchase data, the first purchase date for the field would represent theearliestpossible value.",
        "Creating a User Field": "To create a user field, visitFieldsunder theSchematab in the main navigation at the left of your window. Here you will find a list of all current field definitions. Clicking on+ Create Newwill enter the field creation wizard.  The field creation wizard has a single step where you will define a few parameters related to your new user field. ID: An alphanumeric key that defines how this user field will be stored on the profile.Short Description: A user-friendly description that will be used throughout the ID to provide additional context to the field.Data Type: The type of data that will be stored in this field.Long Description: An optional long description that provides additional context in a few areas throughout the UI.Categories: An optional categorization for the field in question. This helps further inform the intent of the field and enhances our ability to measure the comprehensiveness of your user profile and overall C360 readiness index (coming soon).Identity:A field to identify a user within Lytics or in a downstream channel tool.Governance:Any context related to consent, governance, etc.Interests:Information supporting surfacing and understanding a consumer's interests, such as a product purchase.Behavior:Interactions to be used in understanding behavioral patterns such as page views and non-conversion-related clicks.First Party:Any standard first-party data.Intelligence:ML/AI-related attributes such as LTV ingested from BigQuery or scores related to Lytics models.Activation:Data points related to supporting or monitoring activation in downstream channels.Merge Operator: How the merge between two different data points mapped to the same field will be handled.Identity Key: A true or false statement identifying if the field should be used as a key to link two or more independent events together.Keep Days: The number of days to keep values for this field.Capacity: The number of values to store in the set.PII (Personally Identifiable Information) Key: A true or false statement identifying if the field is to be considered PII (Personally Identifiable Information).",
        "Data Types": "Standard: Advanced:",
        "Merge Operators": "Supported merge operators are as follows: Minimum: When comparing two values, take the smallest one.Maximum: When comparing two values, take the largest one.Oldest: When comparing two values, take the one with the oldest timestamp.Latest: When comparing two values, take the one with the most recent timestamp.Merge: When comparing two sets, take the union of the two.",
        "Capacity and Keep Days": "Thekeep_daysandcapacityproperties specify the amount of time to keep values, and the number of values to retain in a set field, respectively. Any values in the set older thankeep_daysin the past will be discarded when the profile is evaluated, and no more thancapacityvalues will be stored in the set. When values are removed from a set due to exceeding the capacity limit, they are removed in FIFO (first in, first out) order. Ifkeep_daysorcapacityare set to 0 or omitted when creating a field, they are ignored.",
        "Maintaining a User Field": "As your schema grows in complexity, keeping an eye on the defined relationships is essential. The field summary provides a window into how a single user field is being populated from various channels. This view is excellent for diagnosing what may be causing a conflict or that things are operating healthily. ",
        "Deleting User Fields": "Sometimes you may want to remove a user field from your schema altogether. This can be done from the field summary pictured above. Click the menu with three docs to the right ofEdit Fieldand you will be presented with a sub-menu. This menu contains aDeleteoption. ",
        "What happens when I delete a field?": "As shown in the screenshot above, selecting and confirming deletion will not have an immediate impact. The delete will request will be referenced in the unpublished version of your schema. You must publish a new version of your schema for the changes to take effect. At that point, it is essential to understand the following: Data previously collected and mapped to that field will no longer be accessible on the user profile.Any existing mappings related to that field will no longer be implemented as new events occur.Any mappings that are associated with this field will need to be independently deleted. Failing to delete associated mappings manually will result in validation errors when attempting to publish the schema version.",
        "Managing Mappings": "Mappings are expressions that provide instructions for how to perform transformations (if any) on raw data.  They can range from simple to expressive or even include conditional logic. Mappings occur when data is processed or observed within Lytics' real-time profile pipeline.  Multiple mappings can map data into a single field, where the field's merge operator controls updates. Let's say we have a user currently visiting the lytics.com website, and we observe the following data sent from JSTag: Based on the data from the event, we might consider some of the following mappings: email_address: no transformation performed, and takes the value of the email as-is.email(email_address): use the email function to validate that the email address is syntactically valid.email(oneof(email_address, emailAddress)): coalesce either email_address or emailAddress to extract the value of the email address.count(event_type) IF event_type == \"page-view\": sets up a counter to track page views, only if the event is of type \"page-view\".",
        "Creating a Mapping": "To create a mapping visitMappingsunder theSchematab in the main navigation at the left of your window. Here you will find a list of all current mapping definitions. Clicking on+ Create Newwill enter the mapping creation wizard.  The mapping creation wizard has a single step where you will define a few parameters related to your new mapping. Stream: The name of the stream for which to apply the mapping to.Expression: The rules to apply to the mapping.Condition: An optional condition statement of when to apply the mapping. ",
        "Maintaining Mappings": "Since mappings depend on the data available to a stream, monitoring that things are working as desired is important. For instance, if a key changes or stops streaming entirely, it may negatively impact your profiles and, ultimately, your marketing efforts. As such, the mapping summary view is available to review the health of mappings and provide a path for editing mappings in the future if necessary.",
        "Deleting Mappings": "Sometimes you may want to remove a mapping from your schema altogether. This can be done from the mapping summary pictured above. Simply click the menu with three docs to the right ofEdit Mappingand you will be presented with a sub-menu. This menu contains aDeleteoption.  As shown in the screenshot above, selecting and confirming deletion will not have an immediate impact. The delete will request will be referenced in the unpublished version of your schema. You must publish a new version of your schema for the changes to take effect. At that point, it is crucial to understand the following: Data previously collected via the mapping being deleted will remain on the profile as mapping changes only impact event processing from when the change has been published.Fields associated with the mapping will not be impacted or deleted.",
        "Stackdriver Metrics Service Account JSON Key": "During the configuration process for authorizations of typeJSON keyyou will need tocreate a service account. InGoogle Cloud consoleunderIAM & AdminselectService Accounts.Click+ Create Service AccountEnter the details for your new service account and clickCreate.From theRoledropdown menu, search for and selectMonitoring Editorand clickContinueto grant these account permissions.Optionally grant users access to this service account.You will be directed back to the mainService Accountspage and you should now see your newly created service account.Select this service account, clickKeystab, clickAdd Key, and selectCreate new keyfrom the dropdown menu.SelectJSONfor key type and clickCreateto download your new key as a JSON file.The entire contents of this file represent your Google Service Account JSON key and will be used to authorize Google Stackdriver in Lytics. ",
        "Export Metrics": "Export Lytics custom metrics to an existing Google Cloud project. These metrics include a monitoring heartbeat, an API heartbeat, collection count, and stream count. These metrics allow you to monitor, alert, and visualize important metrics from Lytics. Instead of building alerting into Lytics, Google Stackdriver Export Metrics allows metrics to be written into your own monitoring tool. These metrics can be used to: Allow alerting, oncall distribution lists, quiet-hours to be managed within a tool where you are already doing that for other metrics.Allow correlation of metrics you have (possibly website performance, etc) to be shown in context of other metrics.Prevent operational users from having to create a Lytics admin user-account.",
        "Using the Metrics": "Once you have the metrics imported, you can now set updashboardsandalerting.",
        "Content Enrichment": "To better understand how users are engaging with content, Lytics first needs to understand that content itself. One way Lytics does this is by analyzing the URLs that are passed to Lytics to determine the topics that best describe the URL. When Lytics receives data about actions taken by a customer, it is called an event. Each event has fields that store pieces of information describing the event, including the URL. By associating topics with URLs, Lytics is also able to understand which topics a user has engaged with. In doing so, the Lytics Content Affinity Engine can find relevant content for users, as well as find relevant users for content. When Lytics receives an event with a URL in it - specifically when an event with a field namedurlcomes in on any data stream - Lytics determines whether the URL is new or not. A new URL is one that Lytics has not previously handled. Lytics then creates a new event and writes that event to the data streamlytics_content_enrich, called thecontent enrichment stream. An LQL query namedlytics_contenthandles events written to the content enrichment stream. This results in a new entity being created in thecontenttable. Lytics listens for events with new URLs on the content enrichment stream. When a new event is available, Lytics runs the URL enrichment process. Data enrichment is a common practice in Lytics. It refers to the ability to add data onto inbound data to improve its quality. This process is also used in user profile enrichment.",
        "Enrichers": "Enrichment is handled by components calledenrichers. Each enricher performs a specific task. A common task for an enricher is to associate topics with a URL, but there are other tasks that enrichers can perform. Whatever its specific purpose, the result of an enricher running is that additional data may be added to the inbound data (event). After the enrichers run, another new event is written to the content enrichment process. This time, the new event is not enriched because the URL is not new. But the event includes all of the data that was previously added during the enrichment process, so when the querylytics_contentruns, it is able to map that new data to the corresponding entity in the content table. The specific enrichers that Lytics uses depends on how your account is configured. The account settingenrich_content_sourcescontrols which enrichers are used. Your Lytics representative can help you change the enrichers that are enabled on your account.",
        "Meta Enricher": "The meta enricher is always used by Lytics for content enrichment. The meta-enrichment process begins with Lytics sending a request for the URL. The response allows Lytics to collect some information to improve the efficiency of the overall enrichment process. Examples of information collected are: Status code- This is data returned from the web server that handled the request. It tells Lytics whether the URL is valid and accessible on the server. This is important because Lytics is able to generate content recommendations, and you don't want Lytics to include URLs that will result in a 404 or other errors.Meta tags- Lytics can read data from certain meta tags to associate topics with a URL. This logic runs during the meta-enrichment process.Canonical URL- The content on a web page may be accessible using multiple URLs. For example, a product online may appear in multiple categories. The canonical URL is used to associate the multiple product pages with one another. This is an important value to ensure Lytics doesn't process the same content multiple times, just because the URL is different.",
        "Natural Language Processing": "The following Natural Language Processing (NLP) services are available in Lytics for content enrichment. Each link takes you to the Language support page for that service, if applicable. The Setting column denotes the account setting change needed to enable the service, which must be enabled byLytics Support.",
        "Topic Extraction": "Since Lytics collects and stores every event without any aggregation, automatic topic extraction becomes a possibility. For every URL seen, Lytics uses a bot (calledlyticsbot) to fetch the web page at that URL. The content, metadata and images of the URL is analyzed and boiled down to a set of Topics.",
        "Lytics Content Authorization": "If some of your content is premium and requires a login to access, then you'll need to create a new authorization so Lytics can access this content. To do this: AccessVaultby using the product switcher at the top left or following thislink.From the left hand menu selectSecurityand thenAuthorizationsChoose+ Create Newto begin the creation of anew authorization method.When prompted to select a provider selectLytics Content.Next, you'll need to choose an authentication method. Lytics currently supportsbasic authenticationorcookies.Follow the configuration guidance for your selected authorization and then save.Once you have saved your authorization Lytics will automatically recognize that it is available and leverag eit during subsequent enrichment runs.",
        "lyticsbot Directive Configurations": "Whenlyticsbotscrapes your content, you can identify it with some HTTP headers that will be present on every request, namely: User-Agent:lyticsbotLytics-Id:<YOUR_ACCOUNT_ID> This will allow you to identify requests from Lytics to scrape that content to enhance your topic graph. For some websites it is desirable to allowlyticsbotto crawl everything as fast as possible. However, some web administrators would like more flexibility and control over how fast and where the bot attempts to pull content from. The bot will follow a set of directives that would be located at the root of the website, for instancehttps://www.lytics.com/robots.txt. Below you can see three commonrobots.txtconfigurations. Disallowlyticsbotfrom attempting to crawl any links that reside in the/admindirectory. Add a \"crawl delay\" to set the amount of time (in seconds) in between crawl attempts (effectively allow the bot to only crawl 8,640 pages a day) Combine the disallow and crawl delay settings.",
        "Providing Custom Topics": "Lytics will automatically extract topics from the main content at a URL, but sometimes domain specific topics are also desired to track. In this case, Lytics supports a special meta tag for annotating custom topics. Provide a comma-separated list of topics in alytics:topicsmeta element in your HTML source. Here is an example from a Lytics blog post: Additionally, your Lytics account can be configured to also scrape other meta tags to feed into your topic graph by setting the account'scontent_custompropssetting to the names of the meta tags you'd also like to include. For example, if you wanted your Lytics topic graph to include topics from yourarticle:tagmeta tags, you could update your account settings with the following API request. Now, after adding thearticle:tagtopic, any values fromarticle:tagmeta tags will also appear in the topic graph \u2014 which means they'll be eligible for content affinities, targeting and personalization, and inform content recommendations.",
        "Viewing Topics Assigned to a Document": "Each document is assigned a URL as a unique identifier. You can use the Lytics Content API to retrieve a document and view the topics assigned to it. This will return a JSON object of the requested document: You can see the topics assigned to the requested content and the relevancy range of those topics from 0 to 1.",
        "Manually Assigning Topics": "In most cases topic extraction automatically assigns the expected topic to your content. If, however, you find that to not be the case or you would like to expand the topics assigned to content, Lytics allows you to manually assign topics to your content. Content is stored in an entity called a document. Each document is a collection of fields each storing a specific piece of information about that content. Each document may have multiple fields that are used to store the topics for that particular entity. The process of manually assigning topics involves updating one of those fields.",
        "Assigning Topics Manually": "Manually assigning topics can be done in several ways:",
        "Removing Topics Manually": "When a topic is associated with a document, a new field is created on the entity. The field stores a value from zero (no relevance) to one (highest relevance). In Lytics, you cannot delete fields from documents. So, technically, there is no way to remove a topic from being associated with a content entity. Instead, what you do is set the relevance to zero. Since zero indicates no relevance, it effectively removes the topic from the document. Removing a topic is not the same as blocking a topic. Blocking a topic acknowledges that a topic may be relevant but is too generic to be useful. For example, at Lytics we block the topic \"data\" because that topic is relevant on almost all of our content, and for that reason it is not useful at all. Topics can be removed from content using one of the following approaches.",
        "Cloud Connect Data Models": "A Data Model is used by Cloud Connect to link or \"connect\" a user's external data warehouse to Lytics profiles. Each Data Model represents a set of records defined by a SQL query. Each Data Model will also configure a join key which defines how the Data Model is joined to Lytics Profiles. Once a Connection to your data warehouse has been created, Data Models can be created via theData Modelstab within the navigation bar:",
        "Creating a Data Model": "Click+ Create Newfrom the Data Models dashboard and complete the following steps. Select the Connection.Write the SQL query.Query Editor: Write and test standard SQL queries directly in Lytics. Alternatively, copy and paste queries you've tested in your BigQuery or Snowflake instance. When you clickTest Query, Lytics will return 10 sample records.Test the query and validate the results. Configure the Data Model.Name: Data Model nameDescription: The Description of the Data Model.Slug: The Data Model name that will be used in the membership and activation fields in the audience builder. It is important that the name chosen here makes sense for those building audiences in the tool. If no slug is selected, it will auto-populate using the model name.Primary Key: Select the data warehouse ID that will be used to match against profiles in Lytics.External Lytics Key: Select a Lytics user profile field that is a unique identifier to map to an identifier in your incoming data source.Activated Fields:Select the fields you want to bring into Lytics profiles for activation. Note that each data model can only include 25 columns as activated fields.Sync Frequency: Select a time interval to run the query on - this is also the frequency that your Lytics profiles will be updated. The beta options are currently every 1 hour to every 3 weeks.Create new Lytics profile: Check this box if you want to create new profiles in Lytics if a profile with a matching ID does not already exist in Lytics. If you do not select this check box, only users with matching IDs to the Data Model IDs will be updated.Activate the Model. Until the Data Model is activated, its SQL query will not be run against your data warehouse and Lytics profiles will not be updated with the selected fields.",
        "Example Use Case": "Consider this scenario to demonstrate why you'd want to create a Cloud Connect Data Model driven audience instead of a standard Lytics audience. Your company sells e-bikes and wants to run a holiday campaign that sends a promotion to any customer who purchased an e-bike in November or December last year. Perhaps you also want to refine your audience to those who are interested in particular bike brands. All of this purchase and product data already exists in your Google BigQuery instance (as shown below).  Instead of directly importing all that purchase history data into Lytics, you can write a SQL query to find which customers meet this audience criteria. By copying this query directly into the Lytics Model Builder (as shown in the generate a query screenshot above), you'll create a new audience in Lytics that will be updated on the frequency interval you decide. Once the Data Model is activated, profiles will be updated with a membership field that can be used to create an audience. Activated fields can be added to the Data Model that can then be used for more dynamic audiences.",
        "SQL Translator": "If you would like to skip having to write a SQL query, simply describe the Data Model of what you wish to fetch and using GenAI, Lytics will translate the description to a SQL query for you. Lytics makes queries to your database to ensure that the data is up to date and accurate when creating the query.",
        "Provide authorization as request header or parameter": "To provide authorization as a request header or parameter you will need to enter the following: In theLabeltext box, enter a name for the authorization(optional) In theDescriptiontext box, enter a description for this authorization(optional) In theWebhook Request Headerstext box, enter headers for the webhook POST request. Add a new line for each header as key:value e.g.Authorization: some-token.(optional) In theWebhook Request Parameterstext box, enter URL parameters for the webhook POST request. e.g.param-key1=value1&param-key2=value2. ",
        "OAuth 2.0 Client Credentials Grant": "Before attempting to create an Oauth 2.0 Client Gredentials Grant authorization, make sure the service you are connecting with supports authenticating via theclient_credentialsgrant type. For authorizing with an OAuth 2.0 Client Credentials Grant you will need the following credentials. Once these are submitted, they will be used to retrieve an access token from theToken URLand stored securely in order to retrieve a new token when the existing one expires. When a job is configured using this authorization, the access token will be included in calls from the export as a header in the formAuthorization: Bearer YourAccessToken. In theLabeltext box, enter a name for the authorization(optional) In theDescriptiontext box, enter a description for this authorizationIn theToken URLtext box, enter the Oauth2 token URL for the service you wish to connect to. ex:https://www.example.com:9031/as/token.oauth2.In theClient IDtext box, enter the Client ID for the client you wish to use to connect to the service.In theClient Secretpassword box, enter the Client Secret for the client you wish to use to connect to the service.(optional) In theScopestext box, enter the authorization scopes to request from the service.(optional) In theWebhook Request Headerstext box, enter Enter headers for the webhook POST request. Add a new line for each header as key:value e.g.Header1: values.  If your webhook api server accepts authorization in a different format other than the header asAuthorization: Bearer {token}, please specify it here with the format. Use{token}as placeholder for access token. We will replace{token}with actual token when sending the request.(optional) In theWebhook Request Parameterstext box, enter Enter URL parameters for the webhook POST request. e.g.param-key1=value1&param-key2=value2. If your webhook api server accepts authorization in a different format other than the header asAuthorization: Bearer {token}, please specify it here with the format. Use{token}as placeholder for access token. We will replace{token}with actual token when sending the request.(optional) In thePing Federate Access Token Manager ID, enter your Ping Federate Access Token Manager ID. This is the instance ID of your desired Ping Federate ATM instance. ",
        "Audience Triggers": "Audience triggers are used in conjunction with an API implementation for export or enrichment job types. Audience triggers are powered by an internal system called subscriptions. Subscriptions listen to specific events, such as when a user has entered or exited an audience. A subscription can then send a trigger to any job listening to the subscription. Once received, the job can push the updated user data to the third-party tool via APIs. This process enables real-time exports versus a scheduled batch export.",
        "Webhook User Enrichment": "Improve the performance of your ads campaign by enriching Lytics user profiles with the help of responses from the webhook server.",
        "Error Handling": "Webooks jobs are set up with a basic retrying mechanism on top of thestandard job retryingfor Lytics jobs. When a webhook request encounters a non-200 status code, the request will be retried unless the HTTP status code is in the do not retry list explained below. The job will retry errored requests until the job encounters 10 failed requests. Once 10 failed requests are encountered, the job will return an error, and enter into the normal Lytics job retry schedule as describedhere. If the error persists, then the job will go into a failed state. The following http request status codes are not retried since retrying will most likely result in the same status: 400 - Bad Request404 - Not found413 - Request Entity Too Large When setting up a webhook job, more status codes can be added to the never retry list by entering status codes intoDo not retry for HTTP Statusconfiguration. Requests that encounter a status code that is not retried, will be marked as omitted with the cause logged in the job logs. These request willnotcause the job to error. If the server returns aRetry-Afterdirective in the header, the job will go into sleeping state for the duration or until the time specified. The job will not error in this case, and the request will be retried when the job runs next.",
        "Managing Your User Profile": "To access your profile, click \"Manage My User\" from the account menu at the bottom of the primary navigation. You'll have access to personal information and usage statistics from the resulting profile page. It doubles as an ID card and a stats card. You can edit your name and email address. Both fields are required. A phone number is only required for 2FA. Click the \"Change Password\" button to change your password.",
        "Resetting Passwords": "The password must be reset by the user whose password is being reset.Passwords cannot be reset on behalf of others. To reset your password, go toManage My Profilein the account menu and click theChange Passwordbutton. A modal will open, prompting for a new password can be entered.",
        "Managing Account Users": "Click \"Manage Users\" in the account menu to manage account users.",
        "Modifying User Permissions": "User roles define the amount of access and permissions a Lytics user has when logged in and through theLytics APIs. A user may have any number of these roles, and the permissions for multiple roles are additive. For example, a user with the Campaign Manager and Content Manager roles will have access to all permissions granted by both roles. Users with the Admin role can control other users' roles within your organization via the \"Manage Users\" option from the account dropdown menu.",
        "Managing Personally Identifiable Information": "You can indicate any user fields in your account that contain Personally Identifiable Information (PII) via the private fields account setting. These fields will be hidden for anyone who does not have Admin, Data Manager, or User Search roles. You should verify withLytics Supportthat the field hiding in the segment scan is also enabled for your account to ensure these fields are hidden there.",
        "What does a user of each role have access to?": "A user's role determines which parts of the Lytics app they can access. Here is a breakdown of what is shown in the navigation for each role.",
        "What tasks can a user of each role perform?": "Roles define a set of permissions the user has, which also dictates what actions they can take in the app and through the APIs. Here is a breakdown of the permissions for each role by feature.",
        "Personally Identifiable Information (PII)": "To shield PI from users who should not have access, you will need to use the private fields account setting to mark the profile fields you want to hide from anyone who does not have Admin, Data Manager, or User Search roles.",
        "Jobs (imports & exports)": "The former \"Integrations\" tab is now comprised of the \"Jobs\" and \"Authorizations\" sections, which allow you to manage your import and exports.",
        "Additional API-Only Features": "All roles have API read access to topic rollups, segment collections, and SegmentML. Marked below are the roles with full CRUD access to these features.",
        "Inviting Users": "Click theCreate Newbutton from the user list to invite a new user to the account. Inviting a new user will prompt for an email address and roles to select the appropriate level of access the user will have within your Lytics account. Upon completing this form, the new user will get an email with a link that will take them to the Lytics account login screen.",
        "Removing Users": "From the user list, select the user you wish to delete. You can remove this user from Lytics by clicking theDelete Userbutton from their profile page.",
        "Building Audiences with Campaign Data": "Once you have created and published a campaign, you can begin to create audiences based on the results of the campaign. You can then use those audiences to target additional campaigns. By doing this, you can string campaigns together into user journeys. To quickly create audiences based on campaign results, look for thelittle blue plus icon on your campaign report:  Clicking this will take you to the audience builder with either reached or converted users selected (depending on what you clicked). You can further edit the rule based on your needs. Here is an example of an audience condition using campaign data:  This example shows a condition that will select users in Variation A who have been reached but didn\u2019t convert, and users who have been converted and reached in Variation B.",
        "Secret Key": "This method supports theExport Audiencesjob type only. To use this authorization, you will be required to provide a Secret Key that Lytics uses to send data to your account in The Trade Desk. To obtain your Secret Key, please contact your account representative at The Trade Desk. Enter yourSecret Key.",
        "Username and Password": "This method supports theExport AudiencesandImport Experiencesjob type. To use this authorization, you will be required to provide a Partner ID and Secret Key that Lytics uses to send the data to your account in The Trade Desk, in addition to your API username and password. To obtain your Partner ID and Secret Key, please contact your account representative at The Trade Desk or refer toThe Trade Desk API documentation Enter your The Trade DeskPartner ID. This Partner ID is used during experience import and is different than The Trade Desk Advertiser ID.Enter your The Trade DeskPartner Secret Key.Enter yourAPI Username.Enter yourAPI Password.",
        "Cookie Sync": "Match identifiers from theLytics JavaScript tagwith Taboola IDs to expand user profile data and enable the Taboola audience export within Lytics.",
        "S3 CSV Authorization": "Follow the steps below if you want to drop the file in a S3 bucket before importing into LiveRamp. SelectEnter AWS Keys.In theAccess Keybox, enter your AWS S3 access key.In theSecret Keybox, enter your AWS S3 secret key.",
        "SFTP CSV Authorization": "Follow the steps below if you want to drop the file on an SFTP server before importing into LiveRamp. ClickLiveRamp SFTP Server with Username/Password.In theHostbox, enter the host name or IP address of the SFTP server you want to upload to.In thePortbox, enter the port number for the SFTP server.In theUsernamebox, enter the username for the SFTP server.In thePasswordbox, enter the password for the SFTP server.In theFolderbox, enter the relative path to the folder to place the CSV. Contact your technical point of contact at LiveRamp to ensure this directory is in place.",
        "Export Audiences (S3)": "Exporting user profiles and audience membership to LiveRamp allows you use behavioral data, content affinities, and insights from Lytics combined with LiveRamp's identity resolution to fine-tune your targeting. Export your audience to S3 in a LiveRamp CSV format. Use S3 to stage your files before importing into LiveRamp.",
        "Export Audiences (SFTP)": "The SFTP Export allows you to export your Lytics audiences to an SFTP server in a CSV format, which can then be consumed by Mapp and used to create lists.",
        "Export Audiences (EU Format)": "Exporting user profiles and audience membership to LiveRamp allows you to use behavioral data, content affinities, and insights from Lytics combined with LiveRamp's identity resolution to fine-tune your targeting.Export your audience to an SFTP server in a LiveRamp CSV EU format. Use SFTP to stage your files before importing into LiveRamp.",
        "Export Identifier": "Upload user identifier like RampID, Cookie ID or Mobile ID to leverage LiveRamp's Identity Resolution to improve the ads campaign.",
        "RampID Enrichment": "Enrich Lytics users to addRampIDto their user profile. Use this RampID to identify consumers to target with more relevant ad campaigns.",
        "Collect LiveRamp Envelope via ATS": "Associate a Lytics profile with a LiveRamp encrypted envelope as a means to power theLiveRamp RampID Enrichmentintegration. This approach should be leveraged only when a known identifier is present such as email.",
        "Option 1": "Option 1 for implementation is recommended. This option leverages an event listener to send ATS envelope information to Lytics as soon as it is received from ATS. Additional information about this approach is available inLiveRamps documentation.",
        "Option 2": "Option 2 for implementation is focused on sending data to Lytics if you are confident that an ATS-managed envelope will already be in place when the page loads. It is always best to consult with your web development team to ensure that suggested implementation patterns are compatible with your environment(s).",
        "Adding Additional Context": "Regardless of the implementation option, adding additional context may be beneficial. For instance, you can incorporate a variable into the payload to track a specific LiveRamp envelope originating from ATS. As illustrated below, for any envelope sourced from ATS, the event will include a variable named _e holding the value \"ATS_AUTHENTICATED.\" This variable can subsequently be utilized for mapping profiles, reporting, data analysis, and more.",
        "Collect LiveRamp Envelope via RTIS": "Associate a Lytics profile with a LiveRamp encrypted envelope as a means to power theLiveRamp RampID Enrichmentintegration. This approach should be leveraged when there is no known identifier present.",
        "Implementation Steps": "Configure Endpoint for Receiving Pixel & UID LinkConfigure Client Side Pixel",
        "Configure Endpoint for Receiving Pixel & UID Link": "Upon completion, a tracking pixel will be initialized to a LiveRamp domain (we'll cover this in step 2). LiveRamp will associate the anonymous _uid from Lytics with their Ramp Envelope and return that association to an API endpoint at Lytics. To get started, we must ensure that LiveRamp sends the link data to the proper Lytics endpoint and your account is configured to receive and associate that data with user profiles. (Required)Provide proper URL to the LiveRamp team for forwarding RTIS pixel call to (Manged in LiveRamp).https://c.lytics.io/c/provider/liveramp(Optional)Stream configurationBy default, all link data will be delivered to theliveramp_idldata stream. If you would like this data to be delivered to an alternate stream, you can configure the account settingliveramp_idl_streamto point to your desired stream.(Required)Map link data to your user profiles.For each identity that is linked, the configured stream will receive the following key/value pairs:origin_acct: the full ID of the account for which the initial request was made._uid: the user in question anonymous Lytics web cookie.liveramp_idl: the LiveRamp Envelope associated with the _uid.Though you may map this data any way you see fit, we highly recommend following this pattern:SELECT\n      epochms()               AS lr_idl_timestamp             SHORTDESC \"Date of Resolution\"          KIND DATE\n      , origin_acct           AS lr_origin_acct               SHORTDESC \"Account of Origin\"\n      ,liveramp_idl           AS last_ramp_envelope           SHORTDESC \"Last LiveRamp Envelope\"           \n \n--BY FIELDS\n      , set(_uid)             AS _uids                        SHORTDESC \"Web Cookie Ids(all)\"                     KIND []string\n      ,set(liveramp_idl)      AS ramp_envelopes               SHORTDESC \"All LiveRamp Envelopes\"        KIND []string\n \nFROM liveramp_idl\nINTO user\nBY _uids OR ramp_envelopes\nALIAS liveramp_idl",
        "Configure Client Side Pixel": "RTIS leverages acookie syncintegration between Lytics and LiveRamp. Below is an example implementation. It is always best to consult your web development team to ensure the proper checks and balances are in place to prevent unplanned errors and impacts on site traffic based on your unique environment.",
        "Label": "Create a label for your authorization. This will only be surfaced in the Lytics UI for easy navigation when selecting the authorization in additional workflows or reviewing the details and logs. ",
        "Description": "Add an optional description to provide context for where the auth came from and how it will be used. ",
        "Token URL": "Ansira will provide the token URL. This is the endpoint used to get a token. In most cases, your specific instance information will replace the XXXX in the example. ",
        "Client ID": "Enter the provided Client ID from Ansira. ",
        "Client Secret": "Enter the provided Client Secret from Ansira. ",
        "Additional Request Parameters": "Lastly, since the Ansira API expects an access token as part of the request, we must configure the additional parameters to include the token generated via OAuth, as described below. ",
        "Audience": "This integration works by ensuring every member of an audience has been enriched with Ansira data. In most cases, the desired outcome is matching a UUID or Ansira-specific identifier with an email address. Though the source audience is ultimately up to each customer's discretion, we recommend keeping the following in mind: Every user entering the audience will receive an API call to Ansira. This may impact limits and cost, so one general rule of thumb is to define your source audience by highlighting that the desired data is missing and should only be enriched when the prerequisites are met.For example:If I am collecting email addresses from a web form and I want to be sure that every known user with an email address also has an Ansira UUID, I would define my source audience as all users with an email address and no UUID. This ensures we only make calls for users with an email that has not been enriched.",
        "Schema": "Enforcement begins by ensuring consent wishes are accurately materialized on each user's profile. This is done by first defining consent-related attributes to be used in segmentation.",
        "Create the new Ansira UUID Field": "",
        "Create New Mapping for the Ansira UUID Field": "",
        "Create New Mapping for the Existing Email Field": "",
        "Configure Enrichment Webhook": "In the final step, we will bring it all together by configuring our webhook destination to use the auth, template, and audience to enrich a Lytics profile.",
        "Enrich Audiences": "The FullContact enrich audiences job provides additional demographic and social data for enriching user profiles. This can be done across all profiles or for specific audiences, which is often referred to as \"selective\" enrichment.",
        "Yahoo Sign In": "Yahoo Sign In is an Oauth2 based authorization for the Yahoo Gemini platform. Enter your Yahoo login credentials in the login popup.In theLabeltext box, enter a name for the authorization(optional) In theDescriptiontext box, enter a description for this authorization ",
        "DataX": "For the DataX exports, you will need the following information: In theLabeltext box, enter a name for the authorization(optional) In theDescriptiontext box, enter a description for this authorizationIn theMDM IDtext box, enter your Master Data Management (MDM) ID obtained from your Yahoo account.(optional) Select theGDPRcheckbox, if GDPR protected users will be in the dataset. You are responsible for managing consent.",
        "Export Audience": "Exporting user data to Cloud Pub/Sub allows you to receive real-time audience updates from Lytics on a Pub/Sub Topic in your Google Cloud account. The integration is similar to Webhooks Audience Triggers. You can either create a topic for Lytics to push to, or the integration can be configured to create one automatically.",
        "Using Custom Audiences in campaigns": "Within a few minutes the exported audience from Lytics should be visible in the Yahoo Gemini dashboard. The full synchronizing time depends on the size of the exported audience, and will usually take 24 hours to see the audience count updated to reflect the full export. ",
        "DataX Export Audience": "Export users to Yahoo DataX for targeting across Yahoo's platform.",
        "Partner Match": "Add a Partner Identifier to your users (PXID) to use across the yahoo platform.",
        "Define: Relationships Between Identifiers": " Profile definition is the first step in surfacing unified user profiles that enable your brand to create the best consumer engagements while meeting evolving compliance requirements. To build a strong identity resolution strategy, three key questions must be answered: How important is accuracy?How do you define and manage the strength of each identifier to ensure profiles are materialized properly?How can the materialized profiles be analyzed and delivered to other tools?",
        "Construct: Complex Profiles": " Data is messy. Lytics' Profile Materialization provides a necessary cleansing & polishing layer atop the defined resolution strategy. This ensures what is surfaced is accurate and accessible and enables your team to accelerate impact. Answering the following two key questions guarantee profiles represent the ideal marketable entity: What attributes should be surfaced on profiles for segmentation, and does the data need to be normalized at all?What rules should be used to maintain profile integrity?",
        "Maintain: Accurate & Compliant Profiles": " Over time profiles will bloat, attributes will become stale, and use cases become more complex. If unmanaged, a quality identity strategy will begin breaking down the moment it is implemented. Lytics prevents your identity strategy from breakdown through a set of powerful tools focused on ensuring the following profile health-related questions have concrete answers: How can you validate the health of your ID resolution strategy over time?What is the process for managing consent?How long is a profile relevant if it no longer has a means of being updated?What is the life expectancy of expirable IDs, such as browser cookies?",
        "The Lytics Identity Graph": "Behind each Lytics profile is an identity graph.  This graph represents connections between pieces of data observed across multiple sources or even within a single source. To create (or update) profiles from a data stream, the stream must contain one or moreidentity keysthat identify distinct users with which to associate the data.  When data is observed for a given identity key, it stores the relevant profile metadata in an object called anidentity fragment.  When there's evidence on a data stream  that two keys or fragments should be connected When evidence on a data stream shows that two keys or fragments identify the same real-world entity, those fragments become connected in the same identity graph.  Some identity graphs are significant and represent complex relationships in the data. In contrast, other identity graphs are small and describe a small interaction, like an anonymous cookie from a single-visit, incognito browser. ",
        "Graph Mechanics": "As you learned from the identity resolution overview, a profile comprises one or more identity fragments.  Many profiles start as singletons \u2013 new data is observed on a data stream. That event's identifier keys create any necessary identity fragments and store the event's associated data on that fragment. However, we're not satisfied with several singletons \u2013 our objective is to stitch data sources together by linking the appropriate underlying fragments.  Stitching occurs when we observe two identifiers in a single event.  A common stitching event is a newsletter signup, where the email address from the newsletter form is linked to the cookie from their web activity and creates a link between activity from the browser on the device and any activity associated with the email address, which could eventually encompass purchases, support tickets, CRM data, etc. In graph terms, stitching creates an edge between two nodes representing two identity fragments.  If we wanted to retrieve a profile associated with an email address, we would retrieve all of the fragments with edges or connections to other fragments.  From there, we'd want to find all of the connections to those other fragments, and so on, until there are no more connections to follow.  Following one fragment to another is calledtraversal.  The full set of fragments that are found to have connections to the initial fragment are calledneighbors. In Lytics, default graph limits cap the number of traversals allowed for an individual profile at 50 and cap the number of neighbors allowed at 50.  Changing these values can create different sets of user profiles over the same data set and should not be adjusted lightly.  To change these values for your data, please contactLytics support.",
        "Identity Keys": "As we traverse identity graphs, we'll quickly find that identity fragments and their corresponding identity keys are not all created equal. An identifier's strength must contribute proportionally to its influence on identity resolution. For example, you have email addresses and cookies as identity keys.  Generally, a user identified with an email address can have multiple cookie values (from different devices, browsers, periods, etc.).  Imagine hosting a promotional, in-person event and having multiple tablets collecting participants' email addresses.  Depending on how those email addresses are collected (most likely through an online form), you'll likely have one cookie associated withmanyemail addresses. ",
        "Field Types": "Field types for Identity Keys can be either a string or a string set.  String sets are a common field type for cookies since one profile is expected to have many cookie values over time.  Email addresses are not so cut-and-dry.  Some organizations will constrain profiles to have one email address, while others will allow profiles to have multiple (personal, work, etc.).  In our example of email collection via physical tablet, if the email address is a single-valued string type, we won't end up with an over-merged hairball. Using an identity key that allows for a set of values is usually a good idea to have a sensical capacity cap on the field type.  A set of cookies, for example, might have a capacity limit of 50 values.  On the other hand, a set of emails might have a capacity limit of 5 values.",
        "Identity Key Rankings": "The ranking of your identity keys should reflect their reliability and their relative importance in the strategy.  In the event of a conflict in stitching and merging, higher-ranked identity keys will win.  Typically, most Lytics users configure email identifiers to be ranked higher than cookie identifiers. Imagine a scenario where email A is connected to cookies X and Y, while email B is connected to cookie Z.  If new data is observed that connects email B with cookie Y, we have aconflict, meaning that a resulting stitching between the two fragments would yield a profile with twodifferentemail fragments and violates our merge rules. The ranks of identity keys would dictate that, for the new event, the email address it contains is of a higher priority than the cookie value that it contains and would consequently update the profile for Email A andnotthe profile for Email B.",
        "Graph Compaction": "We mentioned that a critical tenet of bulletproof identity resolution is that profile complexity remains stable over time.  That is, we need a way to ensure that a relatively greedy algorithm doesn't result in profiles becoming more fragile and susceptible to conflicts. In Lytics, that is accomplished via graph compaction, a process by which data from multiple fragments is combined into a single fragment.  Doing so allows well-established relationships in the graph to be solidified while making room for new relationships within the profile.  It functions more as a type of profile housekeeping to keep profile fragments tidy. Compaction in identity graphs can take on a few forms.",
        "Rank-based Compaction": "Let's go back to our example with Email A and Email B.  The point of identity resolution within a customer data platform is to enable long and rich relationships with customers.  The longer that Email A and Email B represent profiles in the platform, the more cookies with which they'll eventually become connected.  Each identity key's ranking allows an identity graph to compact by size or time. Size compaction: Identity key sets can be compacted after they reach a configurable size.  If size compaction is enabled to compact a set after 30 values, then the data from the oldest 30 fragments would be combined into a single fragment and would be further compacted with new data as new values are observed.Time compaction: These sets would be compacted after a configurable time threshold.  If time compaction is enabled to compact a set after 14 days, then the data from fragments older than 14 days would be compacted into a single fragment.",
        "Import Leads": "Import your Leads from LeadSquared to Lytics for a richer full-picture profile of your Leads. You can utilize this data to power your communications with your Leads in different channels, or to gain powerful insights on how your Leads are likely to behave.",
        "Additional Fields": "Based on yourconfigurationof theLead Fieldsoption, you may want additional fields mapped beyond the defaults listed above, contactcustomer supportfor assistance.",
        "Export Leads": "Enrich your Leads in LeadSquared with cross-channel data from your Lytics user profiles. Data from Lytics is updated in real-time, and can help power uniquelead automationsto personalize interactions with your Leads based on their activity with your brand. Note:You are required to run theimport leadsjob before you can update existing Leads via this job.",
        "Selligent": "The Selligent integration provides the ability to export files to Selligent via sftp. Instructions are based on Message Studio 10.",
        "Authentication": "First, you need to authorize Lytics to use your AdRoll account. Log into yourLytics account.Open AdRoll integration or ClickData>Integrationsand selectAdRollfrom the integrations list.ClickAuthorizations.ClickAdd new authorization.In theAdmin Usernamebox, enter your AdRoll admin username.In theAdmin Passwordbox, enter your AdRoll admin password.In theAccount Pixelbox, enter your AdRoll SmartPixel ID.- Your ID can be found within your SmartPixel. Access that pixel by followingAdRoll's documentation. Once you have the pixel pulled up your ID can be found followingadroll_pix_id=in your pixel tag.In theDescriptionbox, enter any description.ClickAuthorizeto save the authorization.",
        "Export to Selligent": "Initiating this workflow exports a Lytics Audience to Selligent via SFTP Connect Lytics to Selligent if you have not previously done so. SeeAuthenticationabove.Select SFTP ExportSelect authorization to use.Choose an authorization:If you have multiple Selligent accounts authorized in Lytics, then select the one that you would like to use.Configure the work:Segment Name:Select the segment that you would like to export to Selligent.Show Advanced Options:expands more configuration options.Keep UpdatedRun this export daily.Fields to Export: Use this to choose only specific fields to export. If it is left blank, then all user fields will be exported.Append Segment Name:Adds a column with the name of the segment to each row.Append Segment Slug:Adds a column with the slug of the segment to each row.Time of Day:Set when this export will run each day if Keep Updated is set.Time Zone:Set what timezone to use for Time of Day.Click \u201cStart Export\u201d to begin the export.The file should appear in the sftp folder in a few minutes.",
        "Setup Reoccurring FTP Import in Message Studio": "You can configure Internal Profile Data Sources and Profile, Supplemental and Local Interaction Extensions with recurring imports of data via FTP. To set up a recurring import from an FTP source or an External Data Source into your Internal Data Source, or to an Extension that is of type Profile, Supplemental, or Interactions > Internal (also known as Local), perform the following actions: In the Message Studio navigation panel, click Data Management, then expand Data Sources.Click either Internal Profile or Extensions.On the summary tab, choose the data source you want to modify and click Edit.Click the Recurring Imports tab or click Next to go to the Recurring Imports step.Click Add Import to open the configuration screen.On the Step 1. Information screen, specify the following parameters:Import type\u2014Choose FTP:Recurring updates will come from the FTP server at the IP address shown when you mouse-over the Information icon. If you choose this option, enter a common File name prefix (export-lytics-segment_slug) for the file, and enter the File name suffix (.csv). Other characters within the name are wild-carded.Import mode\u2014Choose one of the following import modes:Merge\u2014Adds new records that don't have a matching primary key; updates records that do have a matching primary key.Update\u2014Updates existing records that have a matching primary key; ignores new records that don't have a matching primary key.Replace\u2014Drops all records and imports all new data from the file.Append\u2014Adds new records that don't have a matching primary key; ignores records that do have a matching primary key.Refresh Mode\u2014Select one of the following:Partial refresh using \"last modified\" column\u2014Only imports the data records that indicate they have been modified since the last import.Complete refresh, get all records\u2014Imports all data source records.Click Next. The Schedule screen appears.On the Schedule screen, specify the following parameters:Choose when you want the import run (Manually, Hourly, Daily, Weekly), and the Start Time.the lytics export runs daily, so select Daily, Weekly, or Manuallybe sure to set the start time after the lytics export is set to run.Click the information icon to see the current date and time.Note: If you select Manually , you must go and edit the database then return to this step, highlight the import in the Summary Page and click Run Now.Click Next. The Column Mapping screen should appear, specify the following parameter:Primary Key\u2014Select the column in the data source to use as the Primary Key.Last Modified\u2014Select the column that contains the last modification date of the records.Map Columns\u2014Use the drop-down lists to select the columns in the Internal Data Source (on the left) that map to the External Data Source (on the right) that is providing the updated data.Click Add to complete the setup.",
        "Prerequisites": "httpie- (optional) a more user friendly version of cURL that we'll be using in this guide for readability.Lytics Command Line Tool- (optional) we'll show you how to execute commands with our CLI for each endpoint that is supported.A valid API token for your Lytics account (learn about managing API tokens). For ease of use we suggest adding as an environment variable in your command line:",
        "Step 1: Create a JavaScript Tag in GTM for Lytics Embedder": "Go to your Google Tag Manager Dashboard and click on \"New Tag.\"Name the tag \"Lytics Embedder.\"Choose the tag type as \"Custom HTML.\"Paste your Lytics Embedder JavaScript code into the HTML field. For example: Choose the triggering event for your tag. For instance, you can set it to trigger when a custom event \"embedder variable = true\" occurs.Save and publish the changes.",
        "Step 2: Create a CSS Tag in GTM for Lytics CSS Master": "Go back to the GTM Dashboard and create another new tag.Name the tag \"Lytics CSS Master.\"Choose the tag type as \"Custom HTML.\"Insert the CSS code wrapped in style tags into the HTML field. For example: Set the trigger to \"All Pages Page View\" so the CSS will be applied universally.Save and publish the changes.",
        "Step 3: Testing and Validation": "Open your website and inspect the Lytics Pathfora widgets to make sure they are behaving and appearing as intended.Use browser developer tools to troubleshoot any issues, such as incorrect application of styles or non-functioning JavaScript.",
        "Step 4: Customization and Advanced Uses": "You can further customize the Lytics Embedder JavaScript to manipulate widgets based on user behavior or other events.Similarly, modify the Lytics CSS Default to change the look and feel of the widgets to better align with your brand.",
        "Conclusion": "Utilizing Google Tag Manager along with Lytics' Pathfora framework can greatly enhance your ability to deliver personalized and targeted content to your website visitors. By following these steps, you can modify widget behavior and appearance directly from GTM, making your marketing efforts more efficient and effective.",
        "A javascript to dynamically alter pathfora behaviour:": "Use this code in gtm and associate to a custom variable",
        "Publishing & Reverting Schema Changes": "Publishing changes is easy. You can first determine if there are any unpublished changes to your schema by visiting theVersionsunder theSchemamenu in the main navigation within Conductor. There you will see a summary of your version history and the number of unpublished changes.",
        "Publishing Changes": "If you have unpublished changes, click on thePublish Changesbutton to review those changes and ultimately publish when ready. Once you have opted to publish the changes, you will be asked to complete a final review. This review will outline the changes tofields,mappings, andrankings. On the final step, you'll have an opportunity to provide a description to associate with the version and ultimately publish. This ensures any changes have a clear history and reduces any change of accidental schema changes.",
        "Discarding Unpublished Changes": "If you determine that you would like to discard all the current unpublished changes to the schema, click on theDiscard Changesbutton. This will open a dialog box that will confirm that you wish to reset the schema to the last published version. Select theDiscard Changesbutton to discard all changes.",
        "Reverting Published Changes": "If you would like to remove the changes you published in your most recent schema version(s), navigate to the version you would like to return your schema to and click theRevert to Schemabutton. This will create a new unpublished version that contains all the changes needed to return all the fields, mappings and ranks back to the state of the desired schema version. Your will then need to publish these changes in order to complete the reversion.",
        "Zuora: Import Users & Subscription Activity": "Zuora provides cloud-based software on a subscription basis that helps companies launch, manage, and transform into a subscription business. Enrich your user profiles in Lytics with user profile and subscription data from Zuora. Integration DetailsFieldsConfiguration",
        "Impact of Browser Tracking Changes": "Announcements about browsers changing their cookie tracking policies have created tremendous concern and confusion within marketing and advertising industries, and rightfully so. These changes represent dramatic shifts in how users and their activity are tracked across the web. Browsers are removing third-party cookies and altering the default behavior of first-party cookies, which makes first-party data mission-critical for marketers more than ever before. The following explains how current and upcoming browser changes will impact your web tracking involving Lytics.",
        "Overall impact": "The blocking of third-party cookies, whether by browser default or user choice, will significantly impact the effectiveness, if not viability, of ad targeting, retargeting, attribution, and measurement efforts dependent on the use of third-party cookies. In addition, Apple\u2019s ITP 2.2 impacts certain marketing use cases. For Safari users, cookies get cleared after seven days if there is no activity. So if a user visits a site once and returns eight days later, their cookie will be gone. However, if that user returns every day for eight days, their cookie will remain intact. This can skew analytics, impact personalization efforts for anonymous users, and affect conversion attribution. But put it in perspective, while Safari accounts for over half of mobile web traffic in the US for non-mobile and worldwide browser usage, Safari\u2019s market share is around 20 percent as of January 2020. More importantly, these browser changes continue the trend of giving individuals more control over what brands/entities they want to engage with and transparency in that regard. They point to the need for brands to maximize relevant content or offerings for a site visitor in real-time, which can lead to registration or other means of establishing a known, first-party relationship. These browser changes reflect the change from a model that exploits third-party data outside an established relationship to transparently establishing and cultivating a relationship with the consumer for mutual benefit.",
        "Impact on Lytics": "The good news is Lytics was designed to help companies establish first-party relationships with their customers and implement intelligent identity resolution strategies. Lytics enables you to collect and leverage first-party data in various ways, such as: acquiring new customers, serving ads on-site with highly targeted audiences, and promoting relevant content to users based on their interests. Onto the technical details. To understand how these browser changes will impact the Lytics JavaScript tag (aka Tracking Tag) you must first understand the core mechanics of how Lytics builds profiles for your anonymous visitors. It all begins with Lytics\u2019 unique client-side identifier, which is referred to as the UID or_uid. When a user visits your web property for the first time, the Lytics tag generates a unique value for this UID and stores it as a browser cookie. This UID is then appended to any behavioral data collected from their visit. Typically when a user returns after a subsequent visit, this UID persists since it has been stored as a cookie. The impact of cookie changes on this process will vary by browser. In the case of Safari, for example, it means that if a user does not return within seven days, that particular identifier will be deleted. As such, it will become imperative to double down on your identity resolution strategies and ensure that efforts are being made to associate first-party anonymous cookies with other known identifiers that can persist, such as user IDs upon login. In addition, some customers leverage theloadidparameter as part of their tag configuration. Historically, this has enabled a simple method for resolving identities across web properties. Thisloadidmethod relies on third-party cookies, which are impacted heavily by these cookie changes. Like the first-party implications above, cross-domain identity resolution strategies become more important. To date, efforts have been made to rely less on browser cookies, given their uncertain nature. Version 3 of the Lytics JavaScript tag was built to be more secure and flexible**, allowing Lytics to respond to these types of changes more efficiently. At the end of the day, however, restricting or removing cookies, in general, will affect the way that you, the marketer, use and monitor the web. Focusing on safe, secure, and transparent resolution strategies is at the core of the Lytics platform, and we believe will allow any marketer to overcome these changes in the future. Q. What cookies are issued in connection with the Lytics tag? By default, Lytics sets a cookieseeridthat is used to identify a user. This is then surfaced in a users profile as_uid. Out of the box, this is what our JavaScript tag uses for web-based identity resolution. Q. Are Lytics\u2019 cookies first-party cookies or third-party cookies? When issued by the client website domain on which they run, Lytics\u2019 cookies are treated by browsers as first-party cookies. This is the case for all of our clients. In some cases, clients have established separate top-level domains for two or more of their brands. In these cases, many have leveraged an optional identity resolution aid that allows Lytics to store the same identifier as a third-party cookie. This allows for a hands-off approach to resolve identities across domains but in no way impacts the nature of our first-party cookies described above.",
        "Stale cookie removal feature": "Lytics has enabled a \u201ccookie culling\u201d feature that prevents \u201cstale\u201d (old) cookies from being used as identifiers to stitch the user profile fragments. Given the browser cookie limits mentioned above, this ensures only valid identifiers will be used to build your Lytics user profiles, which helps keep them clean and consolidated. For example, if a Safari user visits your site once and doesn\u2019t return within two months, Lytics can remove this cookie from their first visit as it has become \u201cstale.\u201d There is no reason to keep this cookie as an identifier because it can\u2019t be used for personalization. Lytics recommends opting for a maximum of 60 days to keep cookies, but this setting can be customized according to your needs. Don't hesitate to contact your Lytics Account Manager if you are interested in enabling this cookie-culling feature. Once Lytics removes cookies after your specified time frame, not only the \u201cstale\u201d cookies are removed, but any other profile fragments attached to those cookies will be omitted from the stitching process. As a result, any associated data with the old cookies will not be included on user profilesunlessthat data can be matched to some other identifier (e.g., an email or user ID). However, the data lost from turning on cookie culling can be found again if the feature is turned off or if the time range is shortened so that the cookie associated with the missing data is no longer skipped.",
        "Cookie changes per browser": "The following information has been updated as of January 31, 2020. Cookies are used for persistent login, preference storage, and tracking across websites. Google, Apple, and other browser providers have deployed functionality that can block the tracking of users across websites. This functionality is promoted as privacy-enhancing by giving a user greater control to prevent website tracking. The technology generally focuses on third-party cookies, but Apple\u2019s technology also focuses on certain first-party cookies.",
        "Apple": "Apple introduced its Intelligent Tracking Protection (ITP) functionality in 2017 to block the use of cookies deployed on sites not belonging to the party issuing the cookie. You can track Safari ITP changes on Apple\u2019sWebKitwebsite. Generally, Safari has set a 7-day expiration period on first-party cookies but a shorter 1-day setting on certain cookies used with link decoration. This means for users that don\u2019t revisit a site within seven days to extend their expiration, the cookie will be deleted, and these users will get new identifiers the next time they visit the site.",
        "Firefox": "Like Safari, Firefox blocks third-party cookies by default throughEnhanced Tracking Protection.",
        "Google": "In 2019 Googleannouncedit was changing its approach to third-party cookies by not blocking them by default and recommending cookie developers useSameSite attributes. Chromegives users optionsto block the cookies they want to block and to delete cookies after a session (fingerprinting out as well). In mid-January of 2020,Google announcedits intent to phase out support for third-party cookies in Chrome by 2022.",
        "Thelytemplates.libsonnetLibrary": "For convenience, Jsonnet templates can import Lytics' customlytemplates.libsonnetlibrary, which contains utilities for common operations on profiles in exports. To import the library, simply include the following line at the top of your template: The following methods oneventcan be used to access data on the profile and job configuration: has(field)get(field, default=null)jobConfGet(key, default=null)inSeg(id)isEnter()isExit()segSlug()sha256(obj)isValidSha256(obj)sha1(obj)isValidSha1(obj)profileFieldFromConfig(configField, default=null)validatedEnum(field, enumOptions, default)",
        "has(field)": "Reports whether the entity on the event contains a field namedfield.",
        "get(field, def=null)": "Retrieves the field on the profile namedfield. Iffielddoes not exist on the profile, the optionaldefaultwill be returned, or null ifdefaultis not supplied.",
        "jobConfGet(key, def=null)": "Retrieves the job configuration field namedkey. Ifkeydoes not exist in the job configuration, the optionaldefaultwill be returned, or null ifdefaultis not supplied.",
        "inSeg(id)": "Reports whether the user is a member of the segment with IDid.",
        "isEnter()": "Reports whether the trigger event is an enter event.",
        "isExit()": "Reports whether the trigger event is an exit event.",
        "segSlug()": "Returns the segment slug for the trigger event.",
        "sha256(obj)": "Returns the sha256 hash ofobj",
        "isValidSha256(obj)": "Returns whether obj matches the formatting of a sha256 hash",
        "sha1(obj)": "Returns the sha1 hash ofobj",
        "isValidSha1(obj)": "Returns whether obj matches the formatting of a sha1 hash",
        "profileFieldFromConfig(configField, default=null)": "If a field callconfigFieldexists on the job configuration, retrieves the field corresponding to the field's value from the user profile. For example, if the job configuration contains{ \"foo\": \"bar\" },profileFieldFromCOnfig('foo')is equivalent toget('bar'). If'foo'does not exist on the job configuration, the optionaldefaultwill be returned, or null isdefaultis not supplied.",
        "validatedEnum(field, enumOptions, default)": "Returns the value forfieldon the job config, verifying that the resulting value is contained in theenumOptionsarray. If the value is not contained inenumOptions,defaultis returned. For example, if the job configuration contains{\"num\": \"Three\" },validatedEnum('num',['One', 'Two', 'Three'], 'Zero')will return'Three'. If the job configuration contains{ \"num\": \"bar\" },validatedEnum('num',['One', 'Two', 'Three'], 'Zero')will return'Zero'.",
        "IDP-initiated SSO (Legacy)": "Lytics supports enterprise Single Sign-On (SSO) by usingAuth0as a service provider usingSAML protocol. Lytics integrates with Identity Providers (IdPs) in such a way that the IdP initiates SSO. That is, when your end user logs in to their IdP, they will use a global portal for your organization. Users can then click a link or button that will log them into Lytics seamlessly. Behind the scenes, the IdP will be contacting the Lytics Auth0 service provider to verify the user and redirect them to a logged-in instance of the Lytics app. This document describes the process for integrating with a new IdP that uses SAML.",
        "Service Provider Configuration": "To configure SAML for the Lytics service provider, some information is required about your IdP. If you have a metadata file that contains SAML provider information, this may be appropriate, but please ensure that the following information is provided toLytics Support: Entity IDSign In URLX509 Signing Certificate Further configuration details, such as mappings, may need to be provided, but the Lytics implementation only requires the email address field to be mapped. Once this information has been received, Lytics can configure the SAML connection in the Google Cloud Identity Platform.",
        "IdP Configuration": "After Lytics configures the SAML connection on the service provider,Lytics Supportwill provide the following key fields of information to the customer to complete the configuration in their IdP. Assertion Consumer Service (ACS) URL (aka postback or callback URL)Entity ID of the Service ProviderSign-in URL With this information, your IdP connection can be configured to complete the SSO integration.",
        "Testing SSO": "Once all the information has been configured in both the IdP and the Lytics service provider, you can test and verify that the SSO implementation works as expected. If you are using SSO as your only sign-in method, please disable any password restriction or expiration settings that may have been enabled in the UI. During the testing process, Lytics can be configured to allow both SSO login and regular username and password (or Google OAuth) login through the app. This allows users to test SSO without disrupting the day-to-day usage of the app. If requested, once the SSO implementation has been tested and verified, Lytics can disable the use of other login types for an account.",
        "Troubleshooting SSO": "If it's known that SSO will be added to an account, the user email addresses added to the account should match the email address present within the IdP. If the email address doesn\u2019t match, the login will fail as Lytics will not be able to verify that there is a user with that email address. For instance, if the email listed in the IdP isabc@123and within Lytics, it isdef@456, then there will be potentially multiple error points for a user trying to log in via the SSO form on Lytics. If the user entersabc@123into the IdP pop-up, it will immediately fail as we use the Lytics account user to determine which IdP to redirect to. You would see an error message like the following:  However, should the user enterdef@456(their Lytics account email), they will be redirected to their IdP, but the verification of the login will fail once information is sent back to Lytics. To remedy this situation, you would need to create a new user within Lytics with the email addressabc@123for the SSO login to be successful.  You can check, add, and remove users for an account using theManaging Usersguide as a reference. Lytics Supportcan assist in the troubleshooting process. When testing for the first time, Lytics can enable logging to help troubleshoot any issues you encounter. With this, the team can help debug if you provide information on the login attempt, such as the login time, user, and account. If you\u2019re encountering a verification error, but you've checked that your emails from the IdP and Lytics match, this may be an issue with the SAML configuration either on the IdP or SP side. ContactLytics Supportwith details of the issue, and our team can coordinate a fix.",
        "Basic Configuration": "For most use cases, building a model by setting the basic configuration parameters is sufficient. The only required parameters are the selection of a source and target audience, which arevery important for building a usable model. If you select audiences that are too dissimilar, the model may be unable to find lookalikes in the source audience.Learn more about selecting the right audiences for your use case. The size of each audience is also important. When building a Lookalike Model, your source and target audience must have a minimum of 25 users and a maximum of 20 million users. If your selected audience exceeds the maximum size, you can add filters to refine it. For example, if the source audience is \u201cUnknown users\u201d you could add a filter for \u201cActive in the last 30 days\u201d to ensure you aren\u2019t targeting unknown users with stale cookie identifiers. The basic model parameters are defined below.",
        "Advanced Configuration": "The advanced configuration is intended to give your developer(s) access to visitors' profile and/or current audience membership via the Lytics JavaScript tag. This ensures your developer(s) retain complete control on how the Google Ad Manager integration is performed to ensure it is configured optimally based on your unique source code. Please seeTechnical Overviewsection above for a full understanding of how the Ad Manager integration should work at a base level. Ensure ads will not be initialized before the visitors profile has been retrieved.Configure Lytics Profile CallbackThe Lytics JavaScript tag makes adding a callback easy. Our fullJavaScriptdocumentation walks you through the process of creating and managing callbacks. Said callback will be responsible for passing the visitors profile data, including audience membership, back to your desired JavaScript function. The following is an example of the callback used in the default Ad Manager integration. You will see it receives thedataobject, verifies that it has audiences and handles the result by either passing them to Google or logging a warning to the console.JavaScriptvarhandleGoogleAdManagerSync=function(profile){if(typeofwindow.googletag==='undefined'){console.warn('google ad manager expected but not found');return;\n    }if(profile&&profile.data&&profile.data.user&&profile.data.user.segments){window.googletag.cmd.push(function() {window.googletag.pubads().setTargeting(\"LyticsSegments\",profile.data.user.segments);window.googletag.pubads().refresh();\n        });\n    }else{console.warn('unable to load Lytics audiences for Google Ad Manager');window.googletag.pubads().refresh();\n    }\n}jstag.call('entityReady',handleGoogleAdManagerSync);Push audience membership to Google Ad ManagerAs shown above, within the callback you will process thedataobject, full details on what is included can befound here, and ultimately passing a list of audience IDs to Ad Manager using their tags APIs.JavaScriptwindow.googletag.pubads().setTargeting(\"LyticsSegments\",data.segments);Initialize AdsLastly, once the audience IDs have been pushed, we leverage thegoogletag.pubads().refresh()call to show the proper ads. This refresh may be unnecessary in your application if you are ensuring the audiences have been pushed before the ads tag has been initialized in the first place.JavaScriptwindow.googletag.pubads().refresh();",
        "Build a Predictive Audience in Lytics": "There are several ways to build your audience in Lytics that you can utilize in your ad platform for targeting. To begin testing this strategy, Lytics recommends defining and building the deterministic rule for who you\u2019d like to retarget in your ad strategy. This most commonly looks like: \u201cHas done event X but not Y.\u201d\u201cHas web activity but is not a paying subscriber.\u201d It\u2019s likely that you have rules like this already set up in your ad tools. To improve the performance metrics of these campaigns, you can use Lytics to decrease the overall audience size available for targeting but increase the likelihood of conversion against your business goal. This will impact your ad KPIs by potentially decreasing the total number of unique impressions but increasing conversion rate and/or decreasing cost per acquisition. To do this, layer in one of the Lytics out-of-the-boxbehavioral audiencesthat use data science, such asEngagement: Deeply Engaged Users. *This rule set includes all deeply engaged users who haven\u2019t made a purchase.  By passing this audience directly into an ad channel, your remarketing efforts will be more refined to only include users who are deeply engaged and more likely to convert than your total user base.",
        "Leverage Content Affinity in Audiences": "Lytics can also create data-science based audiences specific to a topic of interest for an individual user. This logic is more robust than retargeting users around a site category that they have shown past interest in because it evaluates changes in topics in real-time and assesses a user\u2019s interest in a topic in relation to all existing users within your account and their actions. Read more aboutuser-level topic affinities here. You can utilize acontent-affinity audiencefor remarketing as a single rule for an audience, or as part of a rule set as demonstrated below. This rule set includes users with a high affinity for \u2018Chicken\u2019 and who have their membership on pause. By using content affinity-based audiences for remarketing in ad channels, your ad copy and creative can be strategically paired to be more relevant to your customers based on what they have previously shown an interest in.",
        "Custom Predictive Audiences": "The last strategy for predictive audiences within Lytics is to rely on our proprietarySegmentMLproduct to do lookalike modeling across your own first-party data. Most advertisers are familiar with lookalike modeling within ad channels to extend your reach. You can also mine your own first-party data for more relevant lookalikes to offer your products and services to within an ad channel. Your Lytics Account Manager or Services team can help you create predictive audiences within your account, which can then be exported into downstream ad tools the same way as any other audience built within Lytics. See theIntegrations documentationfor provider-specific information.  This summary report shows the growth of a custom predictive audience over a week. While these audiences may be smaller than the lookalikes you\u2019ll get back fromFacebookorGoogle Adsdirectly, because these users have interacted with your brand previously, and are showing signals towards behaving like your seed audience, they will have a higher conversion rate that can be influenced by additional ad campaigns with the right strategy and messaging.",
        "What is it?": "Subresource Integrity (SRI) is a security feature that enables browsers to verify that the resources they fetch (for example, from a CDN) are delivered without unexpected manipulation. It allows you to provide a cryptographic hash that a fetched resource must match. This definition is sourced from MDN Web Docs, where full documentation on SRI is found. The following demonstrates a simple, non-lytics-specific example, as provided by MDM, of how the cryptographic hash can be added to a standard script element to validate that what is loaded is, in fact, what is expected:",
        "Is SRI for me?": "An SRI requirement often stems from strict security compliance for enterprise-level policies. Though it adds a layer of certainty that the file you expect matches precisely what is delivered, the overhead for managing SRI is often not worth the investment.",
        "What will it impact (drawbacks)?": "With this approach comes a few significant implications: Account settingsaltered in the Lytics UI will only be reflected after the tag has been synced and the SRI hash updated. Most settings updates result in alterations to variables in the core tag file.If leveraged, LyticsPersonalization Campaigns & Experienceswill not be automatically started, ended, or deployed. Personalization campaigns depend on a dynamic configuration file that changes each time a configuration update is made. As such, it is impossible to prevent SRI from blocking the script as it should. This means that the config must be cached and served from a customer-managed resource, as outlined below. Each time a Campaign/Experience is altered in any way, this configuration file will need to be altered. In addition, the \"Preview\" functionality also relies on a dynamic configuration file to be loaded, which means there will be no ability to \"Preview\" an experience before it is deployed if SRI is in place.Lytics Supportcan offerlimited assistanceon implementing and managing SRI other than tag configuration guidance. In the case of bugs and technical issues, our support team cannot utilize existing methods to assist you. Since the file is custom and hosted outside of Lytics, you must do so at your own risk.Bug fixes or feature availabilitywill depend on your team's ability to update the external files and associated SRI hashes, as none of the updates pushed directly by the Lytics team will be automatically deployed to your assets.",
        "Recommended Implementation": "The following outlines a recommended but not exclusive approach to implementing SRI. The following also outlines several options that may or may not be relevant based on your account's features in use and configuration specifics.",
        "What JS files does Lytic use/load?": "//c.lytics.io/api/tag/{{ACCOUNT_ID}}/latest.min.jsThis is the core JavaScript web SDK. It is used for identification, data collection, and personalization via the standard profile callbacks or our out-of-the-box web personalization campaigns/experiences. //c.lytics.io/static/pathfora.min.jsThis is the core web personalization SDK managed by Lytics. This is used, and only used, to test and deploy the various Lytics web personalization Experiences and/or Campaigns. Anyone who is leveraging our modals or inline personalization/recommendations will need to ensure Pathfora is included in the SRI strategy. For those that aren't leveraging this directly, they may opt to alternatively turn Pathfora SDK off, which will prevent both this JS file and the following CSS file from being loaded. //c.lytics.io/static/pathfora.min.cssThis default CSS file informs the look and feel of our web personalization Experiences. This file, though required, can also be hosted and customized to your liking as outlined below in the tag configuration section. //c.lytics.io/api/experience/candidate/{{ACCOUNT_ID}}/config.jsFinally, this file loads the configuration for web personalization Experiences and Campaigns. If you leverage any Lytics-managed web Experience, this file must be included in the SRI strategy and updated each time an Experience changes.NO EXPERIENCE WILL BE UPDATED, DEPLOYED, OR SHUT OFFautomatically with SRI in place.",
        "File hosting/caching": "Due to SRI requiring the static asset, which is also loaded to match the fixed hash, all assets outlined above must be copied and hosted at an alternative location outside of Lytics' purview. This means that no bug fixes, updates, configuration changes, etc., will be managed directly by Lytics. As part of this asset management, an independent update process is recommended to be in place. Outside of the technical lift required to provide a web-accessible hosting solution, which will not be covered here, accessing, downloading, and surfacing the necessary files outlined above is relatively easy. You should be able to download the above files via your preferred method and surface that same file on a managed domain. As an example,//c.lytics.io/api/tag/{{ACCOUNT_ID}}/latest.min.jsbecomes//acme.co/lyticsjstag/latest.min.jsor whatever naming convention you'd prefer to leverage. From there, you will need to configure the asynchronous tag wrapper responsible for initializing Lytics to point to this new asset as outlined below:",
        "Additional Configuration Options": "The Lytics JavaScript tag is highly customizable. For additional configuration options visit ourJavaScriptdocumentation.",
        "Settings Management": "Finally, since JavaScript tag-related settings will not take effect immediately, it is possible to manually configure many of those directly in the tag as well, if preferred over an SRI update. For any other settings within theJavaScripttag section of Vault settings management consult with support for the best path forward. ",
        "Providing your AWS keys": "Follow the steps below to authorize AWS with Lytics using your AWS keys. For more information on obtaining your keys, see Amazon's documentation onsecret and access keys. If you are new to creating authorizations in Lytics, see theAuthorizationsdocumentation for more information. SelectAmazon Web Servicesfrom the list of providers.Select theAWS Keysmethod for authorization.Enter aLabelto identify your authorization.(Optional) Enter aDescriptionfor further context on your authorization.Enter yourAccess KeyandSecret Key.ClickSave Authorization.",
        "AWS Keys with PGP Encryption": "To create an authorization with AWS keys and PGP encryption, follow the steps as described above, and then select either the Private or Public PGP Keys option. For imports of PGP encrypted files, enter yourPGP Private KeyTo encrypt the resulting export file using PGP, enter yourPGP Public Key.",
        "Delegating Access via AWS IAM": "You can also authorize using AWS Identity and Access Management (IAM). For more information see Amazon's documentation onIAM. You will need to enter the following policy in your S3 bucket: Before you start a job, please letLytics Supportknow the name of the bucket you are going to use. This will allow delegated access to the given bucket. When you start the job, just selectUse Delegated Access To Lytics with AWS IAMinstead of an AWS keys authorization.",
        "Delegated Access with PGP Encryption": "To create an authorization with Delegated AWS access and PGP encryption, follow the steps as described above to set up delegated access, and then select either the Private or Public PGP Keys option. For imports of PGP encrypted files, enter yourPGP Private Key.To encrypt the resulting export file using PGP, enter yourPGP Public Key.",
        "Import Custom Data": "Many applications let you write JSON files to Amazon S3, you can easily import this custom data to Lytics. Once imported you can leverage powerful insights on this custom data provided by Lytics data science to drive your marketing efforts.",
        "Export Activity Data": "Export triggers to AWS SQS when users enter or exit selected audiences.",
        "Leveraging user behavior": "Lytics usesdata science scoresto move users into preconfigured audiences based on their behavior. This use case relies on a user\u2019sfrequency,recency, andintensityscores to identify those who are highly engaged with your content generally and, through further refining, individual content items, topics, or collections.",
        "Building your target audience": "Users can be members of preconfigured audiences determined by  a combination of**characteristics.",
        "Anonymous Deeply Engaged": "This audience combines the characteristicEmail Capture Status: Unknown emailwith the behavioral audienceLytics Deeply Engagedusing the \u201cAND\u201d rule. The deeply engaged characteristic uses theintensityscore, and is a great way to qualify anonymous traffic for targeting. ",
        "Activating Your Audience": "YourAnonymous Deeply Engagedaudience is a great target audience for site personalizations, content recommendations, and well-timed email capture forms.",
        "Delivering targeted content": "To deliver the content most relevant to a user\u2019s interest you can build additional audiences based onAnonymous Deeply Engagedwith added content affinity filters. SeeBuilding Audiences: Content Affinityto learn how to add content affinity rules to new and existing audiences.",
        "Driving email capture": "YourAnonymous Deeply Engagedaudience is an excellent target to capture email address and convert users to a known status. See our buildCollect Leads Campaigndocumentation to learn how.",
        "Individualized content recommendations": "Lytics offers a Content API that provides recommendations based on user content affinity. Our Developer Academy articleInline Content Recommendationsand ourContent API documentationprovide guidance on delivering content recommendations programmatically. These documents are targeted for a more technical audience, so please contactLytics supportif you need some guidance.",
        "Creating a New Custom Interest Engine": "To create a new Custom Interest Engine, click on theNew Interest Enginebutton on theInterest Enginespage. Once the modal opens up, clicking on theAffinity Customizationtile will open a wizard. The first step requires selecting the unique identifier and features (or Topics) from your inventory (ie thecontenttable). In the screenshot below, we are using the Shopify example from earlier, and using theshopify_product_idandshopify_product_tagsfields. Once the Inventory and Features have been identified, the next step requires mapping the data to your user profiles. The Inventory Field refers to the field on theusertable that contains a set ofshopify_product_ids(ie a set of unique identifiers from thecontenttable). In this case, theshopify_product_idsfield is an array of IDs that a user has purchased. Next, configure the name of the output field - this field will contain theshopify_product_tagsthat a user has expressed interest in based on theirshopify_product_ids.",
        "Import Audiences & Activity": "Import Acoustic users and their activity information into Lytics so you can leverage that data to build behavioral audiences and gain Insights powered by Lytics data science.",
        "Export Audiences with Recipient ID": "Send Lytics user profiles and audience membership to your Acoustic Database to refine your targeting and deliver personalized messaging across channels. All existing users and new users of the selected Lytics audiences are exported.",
        "Export Audiences to Relational Tables": "Send Lytics user profile data and audience membership to your Acoustic Relational tables.  All existing and new users of the selected audience are exported in real-time.",
        "Build an Effective Remarketing Audience": "Remarketing to users most likely to transact starts by building an audience that includes the users who show the greatest depth of engagement and have the most recent activity. Advertisers have had success measuring engagement with a semi-arbitrary metrics such as 3 or more pageviews but Lytics has found greater success (up to 60% more efficient) by usingLytics data science scores, in this casequantity. Similarly, usingLytics recency scorehas been more effective than a fixed date range in targeting the most recent users. Using theLytics audience builder, you can build an audience based on twocustom rules. The goal is to produce an audience that represents the top 10% of your total audience. First, add a custom rule based onscore_quantity, setting the value that includes the top ~15% of your total audience. Second, add a custom rule based onscore_recency, setting a value that leaves ~10% of your total audience. You should play with these numbers to find the audience that makes the most sense for your campaign. A higher quantity score will produce an audience of users who are more deeply engaged while a higher recency score will target users who have been active more recently.  Finally, if you\u2019re really looking to get the extra mile, you can avoid spending on remarketing to users that you can already reach through other channels such as email. Consider removing users for whom you have an email address. To filter users with known email addresses, add aexisting audience rulebased on theEmail Capture Status: Known Emailcharacteristic and toggleExcluded. ",
        "Export Remarketing Audience to Ad Networks": "Your remarketing audience can be exported toFacebook,Google Ads, or other ad networks for use in your advertising campaign.",
        "Installing the Tag": "In the Web SDK, Lytics provides a tag that can be placed on your site to collect behavioral data and surface the materialized profile back to your browser in real time. Manual installation instructions are available from within the app atData Pipeline>SDK>Web SDK. If your site is aDrupalorWordpresssite, or you useGoogle Tag Managerwe recommend using one of these turnkey methods to install the Lytics Tag:",
        "Testing the Lytics Tag Installation": "Once the tag has been installed, validate a successful installation via one of the three following methods:",
        "Testing the Current Visitor's Profile via JavaScript SDK": "The final step to verify installation is ensuring you can access your visitor profile. This profile is built and delivered in real-time as you engage with content.",
        "Testing & Verification": "Follow these steps to verify the successful setup of the integration in your browser. In a new tab or window of your browser, open your developer tools. These instructions pertain to Google Chrome, but other browsers with Network tracking developer tools will work as well.Visit the website where you have the Lytics tag installed after following theconfiguration instructions.In the network tab of your developer tools, look for an API request containing the URLhttps://trc.taboola.com/sg/lytics. If found, this means that Lytics is successfully initiating the cookie sync.Also in the network tab, look for an API request containinghttps://c.lytics.io/c/provider/taboola. This is the URL that Taboola will redirect to containing the proper data as query params to complete the cookie sync.If both API calls are appearing you should be able to see data populated in thetabooladata stream within Lytics shortly. For additional assistance in testing or troubleshooting the integration, feel free to reach out toLytics support.",
        "Taboola: S3 CSV Export": "Export Lytics audiences to Taboola to reach the most relevant users with your Taboola ads. To export audiences to Taboola, an active cookie sync using the Lytics JavaScript Tag is required. Additionally, only audiences that have been manually configured in Taboola can be exported to using this workflow. See theRequired Setupsection below.",
        "Required Setup": "This export requires additional account setup before you can begin exporting audiences. Follow these steps to prepare your account for exporting audiences to Taboola: Make sure the Taboola LQL has been added to your account. Contact your account manager to ensure this step is completed.Enable the client-side cookie sync, ensuring thatVersion 3 of the Lytics JavaScript tagis installed on your website. Enable theTaboola sync in your account settingsand ensure thattaboolais not included on yourintegration blocklist.The cookie sync needs time to sync users with Taboola and populate profiles in your Lytics account with Taboola IDs. The S3 export will only send users that have been captured by the cookie sync and that have a valid Taboola ID on their profile. It is recommended to turn on thecookie syncat least a week prior to sending audiences via the S3 export.Contact your Taboola account manager to configure segment taxonomies for the audiences you wish to sync to Taboola. This is a step required by Taboola. Only audiences that have been manually allow-listed and configured in Taboola can be synced.",
        "Configure Your Sync:": "Now that the account is authorized, you can begin exporting or importing your users.",
        "Exporting Users": " In the export scenario, you\u2019ll click on the \u201cExport Audience\u201d button. From there, select the Brevo List that you want to import and the Lytics audience segment that you want to sync them into from the dropdowns.  You should also select the Email field. Finally, you'll want to map each Lytics field to a Brevo field.  By default, the advanced options are set to continuously import and to add users who already exist in the selected Lytics segment.  Finally, click on the \u201cComplete\u201d button located below the advanced options section.  Note: You can export segments by clicking on the Brevo tile within the Integration section of the Data menu tab (as demonstrated above). Alternatively, if you are within the Audience tab, you can also click on a specific segment and then click on the export button in the upper righthand corner to begin the export workflow.",
        "\ud83d\udcd8TikTok Custom Audience requirements": "It might take a few hours for the audience matching to be completed in TikTok. After the initial file upload in a TikTok custom audience, a new file will not be appended until the custom audience is available to do so. The resulting TikTok custom audience must have a minimum of 1,000 matched and opted-in users in order to be available for use. If this minimum has not been met the custom audience will be marked asUnavailablewithin TikTok. We recommend sending at least 5,000 phone numbers or 10,000 emails, as phone numbers tend to have a higher match rate in TikTok. This suggestions is based on TikTok's requirement for 1000 matched trackable users within their platform.",
        "What is an \"event?\"": "In Lytics, an \"event\" is an action or activity performed by a user or a customer, such as visiting a website, purchasing, or subscribing to a newsletter. As pictured below, events are showcased as independent key/value pairs on a single data stream. Thesekeysare then translated intoFields & Mappingsin order to materialize to user profile for segmentation. ",
        "Exploring Data Streams": "Lytics will ingest data that is sent to one of Lytics' Collection APIs: the collect API or the bulk API. The bulk API is generally intended for larger imports of offline data, while the collect API is generally intended for more real-time sources and usage.",
        "Viewing Your Data Streams": "You can view information about your data streams in your Lytics dashboard by navigating toConductor>Pipeline>Streams. The primary purpose of this section is to verify that data is successfully being received by Lytics. If your account has multiple data streams, you can view a different stream from the dropdown menu above the graph. Many integrations have multiple streams. For instance, it is common for email integrations to have an activity stream and a user stream. Integration streams should be prefixed to help identify the source. You can find the streams for integration under the documentation for that integration.",
        "Event Ingress Graph": "The event ingress graph shows the number of events collected on a stream for the selected time period (past day, week, month, 3 months, and year)  and interval (hourly, daily, weekly, and monthly). Above the graph, you will find the time the last message from this stream was received, the source of the data stream, and the number of fields in the stream. ",
        "Raw Keys Table": "Below the event ingress graph is the raw keys table. An event may contain any number of key-value pairs. Each record in this table represents a unique raw key seen on the stream in at least one event.  The table has the following information on keys: In addition to these seven columns, each record in the table can be clicked to open up a set of sample values. This can be used to verify that values are being collected and they match the expected data. The table can be filtered in three ways: used vs. unused, common vs. uncommon, and text search. Raw event keys can be hidden, but it is important to note that keys cannot be made visible again through the user interface - only through the API. It is recommended that a list is kept of hidden keys in the event one needs to be resurfaced at a later date. If you need assistance, please contactLytics Supportwith your key name and account ID. To hide a key: Select the checkbox next to the name of the key or keys.ClickRemove selected key.",
        "Final Thoughts": "Having logically differentiated data streams for different data sources also helps to facilitate a more straightforward process for mapping data from data sources to user profiles. However, integrating data from some data providers can make this distinction difficult.  For example, data routers like Segment or Rudderstack can route data from multiple sources into a single destination, like Lytics.",
        "Drive Traffic Campaign": " The Drive Traffic campaign type is used to reroute visitors on your site. If you have an ongoing promotion, a Drive Traffic campaign can lead visitors on your home page or elsewhere to a landing page for the promotion. Furthermore, the campaign can be configured to only a set of pages or a subset of your total audience. This allows you to limit promotions to high-value visitors, send the right message at the right time to users with a high propensity to convert, only show the campaign on articles, or any number of other tactics to optimize conversion rates.",
        "Technical Overview": "The following overview is meant to cover the basics, from a technical JavaScript standpoint. If you are not a developer or are not interested in understanding in detail how the integration will work, please skip toAccount Configurationbelow. In order to properly target ads we must ensure that Lytics has loaded the audiences the current visitor is a member of. As such, Lytics will temporarily disable the initial loading of ads usinggoogletag.pubads().disableInitialLoad(). This ensures ads will not load before targeting has completed.Acallbackwill be initialized using the Lytics JavaScript tag. This callback will be notified as soon as the visitors profile has been loaded. This will serve as the mechanism that completes the Ad Manager integration.When thecallbackis fired Lytics will pass the audiences into Ad Manager using thegoogletag.pubads().setTargeting()function and then complete the process by triggering a refresh usinggoogletag.pubads().refresh(). This will result in the ads loading properly with the desired targeting criteria. NOTE:Lytics targeting works for all Google Ad Manager tag typesexceptpassback tags.",
        "Ad Manager Account Configuration": "The first step in targeting Lytics audiences is to configure Ad Manager to recognize them.  This is done by defining custom \"key-values\". Log into Ad Manager.Navigate toInventoryin the menu at the left.SelectKey-valuesin the sub-menu.ClickNew Key.For a Name, useLyticsSegmentsexactly as shown here (no extra characters, spaces, or different capitalization).For a Values type, select theUsers will enter targeting values when creating line items or checking inventoryoption.ClickSaveat the bottom. ",
        "Site Setup & Targeting": "Now that Ad Manager is configured to recognize Lytics audiences, configure your site to pass that Lytics audience information to Ad Manager using the custom \"Key-values\" for each visitor. Install the Lytics JavaScript Tag as described in ourJavaScriptdocumentation.For best performance when working with Ad Manager, place the standard Lytics JavaScript tag above the tags that Google Ad Manager generates. Incorrect placement may cause delays in displaying the proper ad(s) or undesired behavior.Install the additional Google Ad Manager integration tag as outlined below.It is required that his loads AFTER the Lytics JavaScript tag to prevent errors.HTML<!-- lytics for google ad manager --><scripttype=\"text/javascript\">varhandleGoogleAdManagerSync=function(profile){if(typeofwindow.googletag==='undefined'){console.warn('google ad manager expected but not found');return;\n        }if(profile&&profile.data&&profile.data.user&&profile.data.user.segments){window.googletag.cmd.push(function() {window.googletag.pubads().setTargeting(\"LyticsSegments\",profile.data.user.segments);window.googletag.pubads().refresh();\n            });\n        }else{console.warn('unable to load Lytics audiences for Google Ad Manager');window.googletag.pubads().refresh();\n        }\n    }jstag.call('entityReady',handleGoogleAdManagerSync);</script>Complete the site setup by installing the Google Ad Manager generated tags per their instructions. Remember, for the optimal experience all additional Google generated code should come after the Lytics code above. For reference, please find a full example of a working installation below: NOTE:In many cases the above code snippets/examples will work out of the box. However, client side JavaScript can vary greatly and the Google Ad Manager's tag can be configured inconsistently.  Due to this, it may be necessary to configure the integration using theadvanced configurationsection below. This commonly resolves flickering, ads not loading, ads loading twice or any other undesired behavior.",
        "Lytics Audience Configuration": "Next, create your audiences in Lytics. These will be the audiences you will be connecting to \"Line Items\" within Ad Manager in the final step. In Lytics, create an audience using the criteria with which you'd like to target.Ensure the \"Enable API Access\" box is checked, this is what allows it to be shared to your site.Write down the \"ID\" exactly as it appears to the right of the checkbox, you will need this inside of Google Ad Manager.Be precise. Capitalization, symbols, and spacing will need to match exactly. For instance, if you generated an audience called \"Cat-aholics\", later, you won't be successful if you target Ad Manager at \"cataholics\", \"cat-aholics\", or \"Cat- Aholics\".",
        "Google Ad Manager Line Item Configuration": "Finally, configure Ad Manager to map Lytics Audiences to Line items. Log into Ad Manager.Navigate toDeliveryin the menu at the left.Navigate toLine itemsin the sub-menu.Select a line item you'd like to target at your Lytics Audience.On the Settings tab of the line item you're editing, scroll to the bottom and in theAdd Targetingsection, selectKey-values.Where it saysSelect a key, select \"LyticsSegments\".Choose theisoperator.Enter the ID of the Lytics Audience you created. Again, precision is important, capitalization, symbols, and spacing need to match exactly.ClickSaveat the bottom.Verify that your selection has been properly saved by navigating away from the line item, then going back and checking that your key-value is there when you look.  That's it! Your ads will be targeted to that audience as often as you've set up within Google DFP, which allows ads to be prioritized and rotated.",
        "Account Usage Metrics": "The Account Usage section shows how many inbound and outbound events Lytics has received for your account. If your account has an inbound event quota, you will find a meter displaying how much of your inbound event quota you have used for each quota period. In the Lytics UI, selectVaultfrom the product switcher. Or, if you are already in Vault, selectUsagefrom the main menu. This view offers a quick way to gauge the quantity of data Lytics has received from your various Sources. Of course, if you would like to explore details for incoming data further you can view more granular details of all connected Sources from theConductorinterface.",
        "Inbound Events": "Aninbound eventis a record of activity collected or uploaded to Lytics from a customer data source, such as a record of user web activity, email opening, or an update to a subscriber record. The inbound events graph shows your total inbound events, which can be displayed daily, weekly, or monthly. The inbound quota meter displays the number of events recorded in the quota period, your quota limit, and the meter to gauge your usage versus your quota limit quickly. For monthly term accounts, quota periods begin on the first of each month and end on the last day of the month. For annual term accounts, quota periods begin at the start of your Lytics subscription. Quotas (unless otherwise stated in your contract) only count for production accounts.",
        "Monitoring the Inbound Event Quota": "Automated alerts for Inbound Event Quotas can be configured to sent toMicrosoft Teams,Slack, or directly to anemail address.",
        "Outbound Events": "Anoutbound eventis any record of data exported from Lytics, whether manually or automatically. The outbound events graph shows your total outbound events, which can be displayed daily, weekly, or monthly.",
        "Deep Dive: Migrating from Queries to Conductor Schema": "TL;DR:Many things you liked about LQL are still here, and many things you didn't like are out.  Migrating your schema management to Lytics' Schema API within Conductor can simplify your workflows.",
        "Intro": "Lytics' profile pipeline processes your customer data in real-time to perform data cleansing, data transformation, data deduplication, and identity resolution. Historically, these functions were performed in an expressive SQL-like scripting language calledLQL.  After listening to how our customers used LQL and the operational scaffolding they developed to ensure reliability while enabling process scalability, we introduced a new Schema API. Conductor provides a UI-based flow for interacting with Schema AP. Longtime Lytics users who are comfortable with LQL have appreciated: Brevity: LQL combined both datadefinition(DDL) and datamanipulation(DML).Readability: LQL reads similarly to SQL, which makes sense to a lot of database practitioners. Longtime Lytics users who were uncomfortable with LQL didn't appreciate: DRY violations: A common development mantra statesDon't Repeat Yourself(DRY).  Repeating your code in multiple places introduces opportunities for inconsistency, and many LQL statements mapping data into the same field could yield inconsistent or unexpected results.Lack of versioning: When you post LQL, you're doing it live!  This can be problematic when there are large changesets that you want to stage together, or if you want to understand what changed at a given time.Learning curve: Not every update needs to be complicated, and requiring users to learn LQL to make even small changes became prohibitive for many of them. Lytics' newSchemaAPI allows you to use LQL expressions within a JSON framework with native version control.",
        "Analogs": "An LQL statement combines features of datadefinitionand datamanipulation.  In the Schema API, data definition is expressed through fields, and data manipulation is expressed through mappings. ",
        "How to Get Started": "Getting started is simple.  All of your queries have been automatically translated into appropriate fields and mapping objects. It is recommended to immediately publish your default schemabeforemaking any changes to it.  After it has been published, any changes can be published in batches to ensure better visibility on what changes go out. While you're not required to immediately start using Lytics' new Schema API, the classic Query API will be deprecated starting February 1, 2024",
        "Onboarding Web Data": "Lytics provides two ways to onboard data from your website, email marketing, and online advertising. The Lytics JavaScript tag, once installed on your website, will send user activity data. The Lytics pixel can be embedded in email or ads and configured to send user data back to Lytics on load.",
        "Lytics JavaScript Tag": "The Lytics JavaScript tag sends page views and any custom events you have configured to thedefaultstream (unless configured otherwise). This event data will be translated into user fields in the Lytics schema with predefined or custom mappings. For example, the following request would send a user's email address, first name, and last name to Lytics:",
        "Using a tag manager": "Our tag can be installed manually or with a tag manager such as Google Tag Manager. If the Lytics JavaScript tag is installed using a tag manager, thejstag.sendcommand may not be the correct function to call.",
        "Lytics Image Pixel": "When the Lytics JavaScript tag cannot be installed, the Lytics Image Pixel can be used to onboard user data from any source that allows an image to be loaded, such as email and online ads.",
        "Building the pixel": "There are three key variables that need to be defined when sending information to Lytics using the Lytics Image Pixel. The account ID, the name of the receiving data stream, and the event data in the form of query parameters. For example, this pixel will send the user\u2019s email address and Google Universal Tag Manager (UTM) medium and source variables to thedefaultstream of account24546133b65465413w9. This image tag can be inserted into any HTML document including advertisements and emails and will send the event data on load.",
        "Predefined user fields": "Lytics does not have a fixed schema, any key-value pair can be passed using the JavaScript tag or pixel. However, only data which has been mapped to user fields will be available for use in audiences. Out of the box, the Lytics JavaScript Tag and Image Pixel have the ability to onboard the following information when using the default data stream:",
        "Event Data": "Events regarding specific user actions such as a CTA click can also be sent to Lytics. In addition to any of the parameters identified above you can append event specific data to the send request resulting in a few more fields on the profile:",
        "Campaign Data (Google UTM)": "Google UTM campaign parameters are automatically pulled in and added to profiles as well. For more details on Google UTM parameters and how to use them visit theGoogle help docs. The following fields will be added to the profile when UTM parameters are received:",
        "Form Data": "In order to collect data entered on a form, you will need to use jstag.send() when a user submits that form. The fields below will be available as user fields if the form name is sent asform_name, and any fields in the form need to start withformdata_, for exampleformdata_country.",
        "Conversion Data": "As a convenience, a standard conversion event has been defined in order to surface some useful conversion related aggregates such as most recent conversion value or the oldest conversion time. When passing conversion events, the event name must be \"conversion\" for the following mapping to occur:",
        "Build a Content Collection": "Before you proceed, you will need to create a content collection. See the documentation oncontent collectionsto learn how to build a collection. And the documentation onusing content collections for recommendationsfor instructions on how to get the ID of the collection, which you will need in the next step. The collection in this example contains product pages, so that the recommendation algorithm can recommend products to users based on their interests.",
        "Constructing The Recommendations": "Start by thinking about how you want your content recommendations to look on the page. Check thePathfora documentationfor the list of available fields that you may include in the presentation of your content. This example will include an image, title, description and a link to the product page.  Next determine how many recommendations you want to show on the page. Similar tocontent modularization, Pathfora uses the concept of groups and blocks, but this time in a slightly different context: Ablockis a single unit of content (a single article, product, etc).Agroupdefines one or multiple blocks of content recommendations that pull recommendations from the same Lytics content collection. That is, the same content will not be repeated more than once in a group. This example has four blocks in the group. These concepts map to the followdata-HTML attributes: data-pfblock- A unique string name for a single recommendation block.data-pfrecommend- The ID of the content collection. Blocks with the samedata-pfrecommendvalue are considered part of the same group as they use the same content collection. There is an additional attributedata-pftypewhich is applied to multiple elements within the block. This identifies what field Pathfora should use fill in the value of the element. See thedocumentationfor in in-depth breakdown of what fields are available and how they should be named. Now consider this HTML example for a single recommendation block: These appear to be empty elements, however once Pathfora makes the call to the Lytics content recommendation API it will fill in the elements accordingly. Setting thesrcattribute for theimg, thehrefvalue for theaelement, and the inner text of the title and description elements. That's really all there is to it. You can repeat this code to include multiple blocks for multiple recommendations, and then style your recommendations accordingly.  Remember that the recommendation API will return different content for different users based on their affinities in Lytics. Be sure to test the style of your recommendation block with different pieces of content in your collection to ensure that it will work for content with or without an image, descriptions of varying size, etc. Or, you may choose to adjust your content collection accordingly.",
        "Additional Notes": "You may notice some flickering of elements when the page loads. This can happen if Pathfora library and is loaded after the DOM itself has loaded. To get around this, you can add the following line of CSS to your website: This ensures that all blocks are hidden upon page load, and once Pathfora has loaded it will set the proper display settings to show only the appropriate block for each group.",
        "Custom Image Approach": "There are three ways you might approach adding an image to a web Experience or Pathfora widget: Setting a background image with custom CSS.Using thenatively supported image featurewith some custom CSS to adjust the size and placement of the image relative to the text of the widget.Adding an image via inline HTML to the message or headline field. This guide will discuss the implementation of options 2 and 3. Note that option 3 is only available for widgets built directly through the SDK. Inlineimgelements can not be added in theLytics Experience editor.",
        "Using the Default Image Feature": "To use the native image capability in Pathfora, simply add thevariantandimagesettings to a basic widget configuration to get started. This config will generate a simple slideout, with a headline, message and CTA, and a small, circular image above the text.  Adding an image in this way is natively supported for web Experiences in thedesign stepof the editor.",
        "Using an Inline Image": "You could also present an image within the text content of the widget by adding animgHTML element in the headline or message of the widget: You may want to add a custom class name to this element to help style it with CSS later. Note:As mentioned above using HTML inline images in this manor is not compatible withweb Experiencescreated through the Lytics UI, not even withAPI overrides. This approach only works if you are implementing widgets with the SDK directly. The complete code example in this guide will contain both a large feature image using the native Pathfora setting, and a small inline image. ",
        "Styling the Image Slideout": "With some additional CSS work, you can style images of both types to fit the slideout.",
        "Styling the Default Image Setting": "The image generated by theimagesetting in the config has the class namepf-widget-img. In the CSS, to select this element you will also need an additional level of specificity to ensure that your styles will override the default Pathfora styles for images. The example below specifies thepf-widget-variant-2class name in addition the the standard custom class name of your widget. These styles make the image span the full width of the widget, and position it at the top. Next you can adjust the text to appear below the image, by applying a margin to the headline. Remember that CSS specificity may be important here as well. Lastly, we'll need to adjust the \"x\" button in the corner of the modal to display prominently above the image. ",
        "Styling an Inline Image": "Now to style the inline image, select the custom class name you defined. The example in this guide floats the image left of the text of the widget:  Remember you candownload the code for this exampleto get the completed JavaScript and CSS. This can act as a starting point for your own custom image widget.",
        "Defining the Lytics Audience": "The example widget in this guide will be targeted at an audience of anonymous users with ahigh intensity. Remember toupdate your config with the audience slugyou wish to serve the widget to. Alternatively, you may build your widget as aweb personalize Experiencewith theCapture Leadstactic in the Lytics UI and apply theformElementssetting with an API override.",
        "Lytics System Events: Audit Logs and Alerts": "In Lytics, audit logs and system alerts originate from a shared source:Lytics System Events. A System Event is simply a log of an action taken within the platform, whether it\u2019s a user creating a job, segment, or role, or an error message indicating a job failure.",
        "Export vs. Alert": "Log Export: This is a job that either streams logs in real time or performs a batch export to an external source, such as a webhook or a data warehouse table.Log Alert: Alerts are triggered based on specified conditions and notify users when certain actions occur within the system. In both cases, the content of exports or alerts depends on the filters you set up. Filtering is typically done using theSubject(such as User, Segment, or Job) and theVerb(the action taken on the subject, like created, updated, deleted, or failed).",
        "Creating an Audit Log export (aka System Alert) Job via the UI": "Creating a job to export your Audit and System Event logs is like creating any other job; for more information, seeData Pipeline-> Jobs.    In theFilterspanel, simply selectAudit Logsto create an export of your Audit logs or System Events. Depending on yourProvider, you can then select to exportSystem Events. For example, if you selectedGoogle Cloudand you want to export the events toBigQuery. Then, you'd want to select theBigQuery: Export System EventsJob-Type tile.   After choosing the Job-Type, it's like configuring any other export job.  In the case of BigQuery, you can do a one-time or continuous export.",
        "Creating an Alert based on System Events": "Creating a job to alert on an Audit and System Event logs is like creating any other job; for more information, seeData Pipeline-> Jobs.    In theFilterspanel, simply selectAudit Logsto create an export of your Audit logs or System Events. Depending on yourProvider, you can then select to exportSystem EventsorEmail Alerts. For example, if you select to get an email when an Alert is triggered, then useLyticsas your provider and selectEmail Alerts. Then configure whatSubjectyou wanted to listen too and whatEvent Types (aka Verbs). ",
        "Filtering Audit logs": "Audit logs can be filtered by Subject Type: what the event is about, such aswork,workflow,user,campaign. See thelist of subject types below.Subject ID: identifier of a subject, such as work ID, workflow ID, campaign ID, etc.Verb: action described by the event performed on a subject. See the list of available verbs below.",
        "Job Status Monitoring via Webhooks": "Job-status events can be observed by creating a webhook subscription that POSTs data (or JSON) to a specific URL. These updates, like email alerting and reporting, can be consumed downstream for your monitoring use cases.  Some common examples include listening for audience exports created/updated/deleted or being notified whenever a batch import or export for a given integration fails.",
        "Work related filters": "For events related to the subject typework, the followingverbsmay be emitted: synccomplete- Emitted when a sync operation is completed.updated- Emitted when the job configuration is modified.created- Emitted when a new job is created.deleted- Emitted when a job is terminated.synced- Emitted when the job sync operation completes.completed- Emitted when a job has finished successfully.started- Emitted when a job begins execution.failed- Emitted when a job encounters an error.syncing- Emitted while a job is actively syncing.",
        "Configuration Examples": "The following example shows how to subscribe to multiple event types and send them to a webhook endpoint: You can integrate Lytics system events with Slack using a custom JSON template. This example shows how to send failure notifications to a Slack channel: You can filter events by source type and ID using thesystem_event_sourcesconfiguration: To receive events for all works in your account, use an empty array.",
        "Benefits": "Events are delivered to Lytics in real-time.Reduces the number of SDKs installed.Leverages Google-managed SDKs to streamline app approval.Easy access to advanced testing and debugging tools.Automatically passes events to Google Analytics or analysis or further delivery to BigQuery.",
        "Install Firebase SDK": "This solution leveragesFirebase Analyticswhich Google Analytics powers under the hood. This means that in addition to getting data delivered to Lytics, you\u2019ll also be able to leverage Google Analytics and its direct integrations to other Google products such as BigQuery as an automatic value add. Google providesthorough documentationfor getting a Firebase project set up and installed in your platform of choice. Once you have completed the setup and made it to theStart logging eventssection of the documentation, you are ready to move to the next step. Feel free to log some sample events per the documentation, either way, we\u2019ll come back here further down in this tutorial.",
        "Install Google Tag Manager SDK": "Next, we\u2019ll leverage Google Tag Manager, which offers seamless integration with Firebase Analytics to forward events collected by Firebase to Lytics. Like Firebase, Google hasexcellent documentationfor getting a Google Tag Manager account set up and the iOS or Android SDKs configured.",
        "Sending User Properties & Events": "Now that we have configured Firebase and Google Tag Manager, we are ready to start configuring and sending our events. Firebase makes both setting user properties as well as passing interaction-based events easy. For more details on the power of Firebase, you can revisit theStart Logging Eventssection of the getting started documentation. The above represents an example of a custom event nameddid_something. For that event, we also include two parameters_uidandlytics_test_key. These parameters are entirely up to you and should be in support of your use case. Keep a record of the parameters used, as you will need to explicitly define those in Google Tag Manager to pass them to Lytics. That said, including an identifier of some for every event is essential to your identity resolution strategy. In the above example, we use_uid, which is one of the default identifiers. Consult your account manager or solutions team for the optimal approach here. Regardless of the approach, it is essential that the ID(s) passed persist. There are various ways to generate and store a value on the device. For this step, it is best to consult your app development team. One example would be to use Swift'sUUIDto generate a unique user identifier and store it usingUserDefaults. The method used to generate, store, and access this value has no impact on the results. It is only essential to include that value in all events and have it persist for as long as possible. On that note, whenever possible, it is a best practice to include multiple identifiers such as email, or IDFA, to maximize the match rate.",
        "Configuring Google Tag Manager for Event Forwarding": "The final step in getting user details from your app to Lytics is configuring your Google Tag Manager Container. We\u2019ll start by definingVariablesfor each of the parameters you pass in your events from your app. Based on our example, we\u2019ll define two variables: _uid and lytics_test_key.",
        "Step One": "From the Google Tag Manager dashboard, navigate toVariablesin the left-hand menu.",
        "Step Two": "Create a new variable, one for each parameter, using theNewbutton. In the new window that loads, clickChoose a variabletype and selectEvent Parameter. This is just an example, you can use any variable type based on your needs.Event Namemay also prove helpful in adding more context to the event data delivered to Lytics.",
        "Step Three": "Finish your variable configuration by selectingCustom Parameter, followed by inputting one of the parameter names we defined above. In our case, we\u2019ll use_uidwith a default value of an empty string. Save your settings and move to the next parameter until you have created a variable for all parameters being passed to Lytics.",
        "Step Four": "With your variables defined, we\u2019ll move on to creating a newTagwithin our container. SelectTagsfrom the left-hand menu andNewtag once the list of tags has loaded. On the following window, clickChoose a tag typeand selectCustom Image.",
        "Step Five": "With your tag type selected, we\u2019ll configure theImage URLas well as theTriggeringrules. The Image URL will leverage theLytics Image Pixelcollection method. To configure, you\u2019ll need to know yourAccount ID. For simplicity, we\u2019ll break the full URL into two parts that will be joined together in actual execution. Focusing first on the path, everything fromhttpsup to the?you will need to replace theYOUR-AIDportion of the path with your actualaccount IDand theSTREAMportion of the path to your desired ingest stream. The query parameters will contain the field name you\u2019d like to pass to Lytics, as well as a value that is populated by referencing the GTM variables. Putting those two parts together results in the final value of theImage URLin our config. Last we\u2019ll configure the event that triggers our forward by selectingAll Events.",
        "Step Six": "With our tag defined, the final step is publishing the Google Tag Manager container and storing the config that is generated in the proper directory of our App. Once again,Google provides instructionsfor doing so.",
        "Test & Debug Implementation": "With our implementation all configured, the final step is to test and debug. Each of the steps defined above supports testing and debugging. We\u2019ve added quick links to those resources below. Firebase Event DebuggerGoogle Tag Manager DebuggerLytics Data Streams: Once successfully tested, you can do the final validation by reviewing your incoming data as part of thestreamsUI within your Lytics account.",
        "ML Powered Meta Fields": "These meta fields are the result of Lytics machine learning algorithms.",
        "Orchestration Meta Fields": "These fields will only be available when orchestration has been turned on in an account.",
        "Javascript Configuration": "Begin with a generic widget configuration. This config will generate a simple modal, with a headline, message and call to action.  To add a video to the modal, you can define anonLoad callback function. This function is called just before the modal appears on the user's screen, and the variables passed to the function as arguments include the widget's DOM element, which you will append the video element to. Start by creating adivelement which will later contain your video element. You may want to add a class name such aspf-widget-videoto style this div later. Next set theinnerHTMLof the div with the code for your video. Note that you'll need to escape quotes within the HTML when transforming it to a JavaScript string. Now that you have the video element within adiv, simply insert thedivinto the widget. You can access the DOM node for the widget using thewidgetfield on the second argument, in this casemodule.widget. From there you will want to select two  elements with the class names: pf-widget-contentthe main div that contains content of the widget.pf-widget-texta child div ofpf-widget-content, which contains the text content of a widget. Using theinsertBeforefunction, you can insert the div containing your video into the main content div, before the text of the widget. With this new config you should now have a modal with a video! But it could probably use CSS to make it look cohesive. ",
        "Testing the Personalized Widget": "Testing widgets that contain user profile fields can be tricky. If you are using the Pathfora SDK, it may help to use thepathfora.customDataobjectto simulate the user field data during the development process. Alternatively, add theoffer_codefield to your current profile by sending an API request to the data stream which contains the raw data for the field with the appropriate identifier (usually_uid). You can use thejstag.getid()function in the developer console of your website which has the Lytics JStag loaded to get your_uid. Here is an example API call to add theoffer_codefield via thedefaultstream. This example makes a request to the Lytics collector API for your account. Ifoffer_codeis mapped in the LQL for the streamdefault, and_uidis a unique identifier for the stream, it should add the valueFREE324234234to the user profile. Once you see it show up in your user profile in the Lytics UI, refresh your website to ensure that the widget is populated with the new field value. ",
        "Define the Lytics Audience": "The example widget in this guide will be targeted at audience of users with a highcontent affinity scorefor iPhones, the product that the modal is promoting. Remember toupdate your config with the audience slugyou wish to serve the widget to. Alternatively, you may build your widget as aweb personalize Experiencein the Lytics UI and apply theonLoadcallback with anAPI override.",
        "Building the Experience in Lytics": "If you haven't already, you will need to build the base Experience through the LyticsExperience Editor. The goal is to create in the experience in the UI as close as possible the desired end state of the widget, and rely on the API overrides to fill in the gaps that are not available for configuration in the UI.",
        "Selecting a Tactic": "Selecting the correct tactic is key, as it determines whattypeof Pathfora widget the Experience outputs. The tactics map to types as follows: Drive Traffic- produces amessage widgetwith CTA (okShowset totrue).Capture Leads- produces aform widget.Present a Message- produces amessage widgetwith no buttons (okShowset tofalse).Recommend Content- produces amessage widgetwithcontent recommendationsandvariantset to3. The example in this guide uses aform widget, thus you will select theCapture Leadstactic.",
        "Experience Editor Steps": "You will be dropped into thecustomize form stepof the editor. Since the example form has custom checkboxes set to select which feeds to subscribe to, you can't configure this in the UI. Simply click theNext Stepbutton.In thedesign stepenter the headline, body, and call to action text. Select theSlideoutlayout, and under theme you can check theI have my own CSSbox and enter the custom class namecustom-tracking-widget.In thetarget step, select the audience you wish to show this slideout to. This example uses an audience of high intensity anonymous users.In thedisplay stepconfigure when and where you want the form slideout to display on your website. This step controls to thedisplayConditionssettingsin the Pathfora SDK. Once you've completed all the steps, you may want to preview your Experience. Though it may not look or function in the way you would like, this starting experience will generate a foundation configuration which you will augment in the next step.  You will also need to save the Experience to make API override requests. Be sure to clickSave as Draft.",
        "Creating the Override Request": "To make the request for the override, you will need the ID of the Experience you just created. You can get this from the URL of the summary or review page of the experience after saving it. In the URL example below, the ID of the Experience is2d3e345b2ac24acd9d6d3d33f93516fd:  You will also need a LyticsAPI tokenwith theAdminrole. It may help to save the token as an environment variable for future use. You will make aPATCHrequest to the Experience endpoint with the id of the Experience. This command example usescurlandjq: Your config changes will need to be nested in a field calleddetail_overridethis field itself is nested in theexperience.vehiclemodel: Next you will need to convert the JavaScript configuration into a JSON payload containing the settings which you were unable to apply through the UI. In this example that is: formElements- which customizes the form.formStates- which sets the success and error states on form submission.confirmAction- which handles the callback to send the data to a third party. For the most part, you can translate a config to a JSON override in the same way that theJSON.stringify()function would convert a JS object to JSON. In this example, that works perfectly forformElementsandformStates: But forconfirmAction, it gets tricky because it contains a JavaScript function: However, the API can accept a JavaScript function as a string. It may help to minify your function first. There are a number of online tools or command line tools which will minify JavaScript for you. This helps for the API override because it creates a shorter, single line version of the function. This is the example callback function run through a minifier: To include this function as a string, you will need to escape any double quotes within this before pasting it into your config. Putting the whole thing together, your curl command will look like this: Once you run the command, check that the response from the API includes your changes. You may make subsequentGETrequests to the same endpoint to view the entire payload of the experience.",
        "Testing and Validating the Override": "Once you've made the API request, you should be able to view the changes as part of the regular preview process for your Experience. In the Lytics UI go the the summary or editor for your Experience. ClickPreviewand enter the URL you wish to preview the Experience on. Note:If you are using the same browser session to preview the Experience that you did to create it, you may need to hard refresh the Lytics UI before clicking preview. This will ensure that Lytics is serving the most up to date version of the configuration. You should be able to see the updated Experience in the preview. If this is not the case there are two scenarios to troubleshoot: If your widget is not displaying at all, check the JavaScript console for errors. If you find a formatting error in the translated JavaScript config you may investigate and adjust your override command accordingly. This scenario can happen if you failed to escape quotes properly, for example.If your widget is not displaying the changes, double check the response from the API and cross-reference your settings with the appropriate settings in thePathfora SDK Documentationto ensure everything is named and formatted correctly. If the issue persists, you can always contactLytics Supportfor additional assistance with debugging.",
        "Import Users": "Import GIGYA contacts and email subscriptions into Lytics to build a more complete view of your customers. You can import from full accounts and lite GIGYA (SAP Customer Data Cloud) accounts.",
        "Get Started with Lytics Segments": "Lets take an in-depth look at Lytics segments. We'll cover how to define a segment with SegmentQL or SegmentAST, as well as the Segment API endpoints to create, read, update, delete, scan, and list the segments in your account. Note that throughout this guide we use the term segment and audience interchangably. Specifically, the term audience refers to a segment on the user table. In your Lytics account there is also a separate table and schema for content. A segment on this table is often called a content collection. DifficultyBeginner TLDR:Checkout the Segment API documentation. Other resources: Lytics Go Client- Lytics API library for the go programming language.QLBridge- a go SQL Runtime Engine.Lytics Segment Scan with SQL.",
        "1. Defining A Segment": "If you haven't yet, take a minute to check out theaudience builderin your account. This can be accessed from the Audience tab in the Lytics UI and then by selecting tocreate a new audience. The audience builder is a visual interface for what we will cover in the first section of this guide - defining a segment. A segment definition at its core is really just a conditional statement. If the data we have collected on a user meets the conditions of the segment than they are considered a member. We have two different syntaxes for these conditional segment definitions: SegmentQL, and SegmentAST.",
        "SegmentQL": "SegmentQL is simple Query Language using filter statements. Before we dive too deep into the syntax, lets look at a segment built in the segment builder UI side by side with its SegmentQL definition.  In this examplecm_statusrefers to the subscriber status in Campaign Monitor, an email provider. So a marketer might build this audience as a target for all users who are active across their email and web channels in the last 7 days. For us developers, this simple query language is probably quite intuitive! Our UI exists for marketers to build audiences, but you might find it quicker to write SegmentQL, and create a segment through the API. Just to cover all our bases lets take an in depth look at the anatomy of a filter statement: [identifier]= alphanumeric string (often the name of a data field, table, or segment).[literal]= a string, int, float, boolean, or timestamp literal value. [from]is optional if creating a filter on the user table. [alias]is optional if you do not wish to save the segment.",
        "Segment AST": "SegmentAST expressions look very similar to SegmentQL but instead of having its own filter syntax it is formatted in JSON. It's perhaps less readable for a human, but if you're writing a program which creates or updates segment definitions it might be easier to format the bodies of your requests using SegmentAST. Lets look at the same example from above in SegmentAST.  Definitely not as easy to look at as SegmentQL, but if you spend a second scanning the JSON that you'll see it's generally the same expressions. Here is a breakdown of the key components:",
        "2. Segment Validate API": "The segment validate endpoint accepts a plain text SegmentQL statement and ensures that the definition is valid. This can be especially useful when creating a segment. Before sending a request to build the segment via the API you might want to validate it first. Note that this endpoint does not validate field names in your account. It simply checks the syntax of the SegmentQL statement.",
        "Segment Create": "The segment create endpoint accepts plain text (containing SegmentQL) or a JSON body (SegmentQL or AST).",
        "Segment Read": "You must know the segment ID or slug name to fetch the segment via the read endpoint.",
        "Segment Update": "You must know the ID or the slug of the segment to update.",
        "Segment Delete": "You must know the ID or the slug of the segment to delete.",
        "4. Segment List API": "For many of the CRUD operations we just talked about you need to know the ID or slug name of the segment. It may help to have access to a list of all existing segments in your account to use as a reference. The list endpoint will return a list of all segments for your account including id, slug, and the QL and/or AST definition of the segment.",
        "5. Segment Scan API": "The scan endpoint provides a list of all entities in a segment. For audiences (segments on the user table) the entities returned are user profiles. The scan endpoint can accept a number of different query parameters, we won't cover all of them here, but as always you can check outour docsfor the full details on this API. You can use this endpoint to scan an existing, saved segment in which case you'll need the segment ID or slug. You can also scan an ad-hoc segment by passing SegmentQL to the endpoint. If the total number of entities in the segment exceeds the limit provided in the request (the default value is 20) then we will paginate the results. In the JSON body, there will be a token value with the keynext. To get the next page of results in the scan, make a subsequent request with the query paramstartequal to thenexttoken from the prior request. The API will continue to returnnexttokens in each response until you've fully scaned the segment.",
        "What's next?": "We've covered a basic example of getting custom data into Lytics. This is very much just the beginning of how LQL can help improve your marketing teams efficiency. We invite you to explore all of our technical docs and reach out to our services team for more in-depth training on LQL and data management!",
        "Import User and Survey Data": "Import your SurveyMonkey contacts and their survey responses to add this information to your cross-channel user profiles in Lytics.",
        "Collecting Identifiers": "There are three ways to collect identifiers from SurveyMonkey surveys in order of complexity: Use SurveyMonkey's built in contacts and mailing service and theemail invitationcollector.Addcustom variablesto your survey and use aweb linkcollector in SurveyMonkey.Add a question directly asking for an identifier.",
        "Method 1": "This is the easiest to setup. Just followSurveyMonkey's instructionsto send out your survey by email to a list of contacts. The recipient's email address will be included in their responses and will allow them to be merged with their Lytics profile when imported.",
        "Method 2": "This method takes a little more setup, but is much more flexible in delivery options. Your surveys will need to havecustom variablesadded to themand will need to have the user's information inserted into the web link collector's URL. Depending on your delivery method, specific instructions on how to insert identifiers into the web link collector's URL will vary. TheSurveyMonkey Importwill automatically use thelytics_emailandlytics_uidcustom variables as identifiers.lytics_uidshould be a Lyticsuid. You can export theuidto a tool that sends the survey link, or retrieve the uid from theLytics JavaScript tagto embed or link to the survey from a webpage.lytics_emailshould be the user's email address that is stored in Lytics. Additionalcustom variablescan be used, but will need to bemapped in LQLto be used in Lytics.",
        "Method 3": "This method is the most complicated to configure. It will require specific wording of questions on all surveys you want to track and custom LQL to implement. To implement this method you will need to create a question in your survey to collect an identifier. For example: Please enter your email:Please enter your username from our website: You will then need to createcustom LQLto recognize the responses to these questions as identifiers so the responses can be merged into the user's profile. Using the example questions above, the LQL would look something like: Only questions with the exact wording used in the LQL will be recognized as an identifier, this includes capitalization and punctuation at the end of the question. Using the example LQL above, the following questions would not be recognized as containing an identifier: please enter your email:Please enter your emailenter your username:Please enter your Username from our website: This is the only method that works with all SurveyMonkey collector types, but due to the added complexity, we recommend using one of the other methods if possible.",
        "Using Survey Data": "By default, all questions and answers are stored in a map field,question_answer, on the user profile. This can be difficult to use as the map keys will be a truncated version of the question text. We recommend creatingcustom LQL mappingsfor questions that are relevant to your marketing audiences. Questions and their answers are placed into thesurveymonkey_responsesstream in the fieldquestions_answers. They are stored as a map with the question text as the map key, and the answer as the value.Long questions will be truncated. Questions should be kept as short as possible. For example, if you wanted to create an audience around users that were planning to retire soon you might have a true/false survey question like:Are you planning to retire in the next year? This will appear in the Lytics stream as:questions_answers.Are you planning to retire in the next year? This can be difficult to use in an audience definition, so we suggest remapping it using LQL like: This would remap the response to the user fieldretire_next_yearfor use in Lytics audiences.",
        "Identify the Visitor": "Identifying the visitor is simple and comes out of the box as part of the installation of ourJavascript SDK. By installing this tag, we'll automatically place a first-party cookie on the visitor's device specific to Lytics. This cookie contains the Lytics_uid, which enables associating interactions across your site with a single C360 profile. In our handoff to Jebbit below, we'll leverage one of the many available utilities to access and deliver this value per Jebbits preferred method.",
        "Hand off UID to Jebbit": "There are several ways that Jebbit can be leveraged. This document outlines the recommended path for using an iFrame embed for your Jebbit Experiences. If you are using an alternate method, consult the Jebbit documentation. However, a similar approach can be used across all Jebbit launch methods. From the Jebbit UI, navigate to your desired experience. Along the top, you'll see a navigation option for \"launch.\" This is where you'll access the source code and configuration options for the iFrame launch method. To the right of the code snippet, you'll find a toggle for \"User ID,\" as shown below. Ensure that this is enabled, as it allows the Lytics _uid to be passed to Jebbit and associated with all tracked interactions.  Finally, place the Jebbit snippet on your website as you usually would. No additional actions are necessary specific to Lytics. Further details on Jebbit's iFrame Experience launch configuration can be foundhere.",
        "Configure Jebbit to Lytics Connection": "Feeding interaction data back to Lytics in near real-time is always recommended. This is made possible via the Jebbit to Lytics integration. This integration leverages a webhook that passes data to a unique data stream for mapping into the profile. Both webhooks and the Lytics integration require assistance from the Jebbit team to configure and will require a validaccess tokenalong with the name of your desired stream. Stream definition is entirely up to you, but we recommendjebbit_activityto take advantage of a set of pre-defined mappings.",
        "Access Token Creation": "Though any role with the ability to create and manage data sources will work without an issue, we recommend creating a token with theData Managerprivilege exclusively for this integration. This reduces any risk of granting unnecessary access. As a result of a properly configured connection, you'll have access to all interaction data collected via Jebbit, which in most cases looks similar to the following:",
        "Connect Lytics Identity with Jebbit": "The last step in the process is to dynamically update the iFrame url to include the visitors Lytics _uid. There are several ways to do this, depending on the tools and resources at your disposal. Below you'll find one example and a description of what is happening. Please review this code in its entirety with your development partners to ensure there is no chance of an unintended conflict with your source code. Out of the box, the Lytics JavaScript SDK provides an asynchronous\"getid\" function. This is used to retrieve the visitor's current_uid. To leverage this function, the core Lytics SDK must already be present. Otherwise, a JavaScript error will occur. In the example below, we leverage this function to find the current \"src\" definition for the Jebbit iFrame.An assumption is made that only 1 Jebbit iFrame is present on the page and that it leverages a class of \"jebbit-iFrame.\"If this is false, the querySelector on line 4 must be altered to match your implementation. We then leverage a simple query add or update function to determine if the src already contains a uid. If it does, we'll update and if not we'll append a new query param to the src, which includes the uid, as per Jebbit's preferred method and documentation. Example In most cases, the above example can be dropped into your preferred tag manager alongside the core Lytics JavaScript SDK.",
        "Testing": "Upon completing the above configuration a test can be performed for a published Jebbit Experience. Simply visit the test site where the Lytics tag and Jebbit iFrame are present. Complete the survey and verify data is being updated in the defined Lytics stream. In our testing, it can take several minutes for the form data to populate in Jebbit and be subsequently passed to the Lytics platform.",
        "How Insights Work": "Insights surface meaningful data and recommended action, enabling you to make better, data-driven decisions during your campaign planning and execution. The core idea behind Insights is simple. Insights are a combination of facts and actions: Fact: Observable, data-driven information providing demonstrable value. This is a unique set of raw metrics, such as the number of users or the amount of lift.Action: Suggested behavior based on user role, data source, activation channels, industry, etc. This is the contextualization of a fact into something to be done. Insights are made possible thanks to the data science capabilities built into Lytics. The heavy lift of processing and interpreting data is automatically handled by machine learning models under-the-hood. This is a prime example of how machine learning can augment (not replace) a marketer\u2019s role. Machines efficiently and accurately sift through lots of data, allowing people to then make use of the important information in the most relevant context. For example, a model can predict which users are most likely to churn, and a marketer can then target those at-risk users with a win-back campaign on Facebook.",
        "Insight Cards": "Insights are presented asInsight Cardsthat are available throughout the entire app.Simply navigate to the left-hand side of the Lytics UI to open theInsights Drawer, which contains a list ofInsight Cardsunique to your account. Below is an example of an Insight card and what it's comprised of:  Creation date: When the Insight was generated.Expiration date: When the Insight will be removed from the drawer. Insights expire after 2 weeks.Insight statement: The explanation or comparison of noteworthy data will depend on the type of Insight. You can click on an audience name to view its summary report.Recommended action: Lytics will suggest how you can apply this Insight to your targeting along with a related use case.Expanded definition: View more details by clicking the arrow. This example explains the Lytics score or other data field being calculated. Insights are generated and refreshed on aweekly basisto ensure you are viewing the most recent user data. Insights expire after 2 weeks, so you only target and activate an audience and campaigns during a relevant time frame. The Insights Drawer will display a maximum of 25 cards at a time. Insights will be created for up to10 custom audiences, in addition to Lytic's out-of-the-box engagement audiences. Lytics will prioritize Insights based on the statistical significance of the data and the most frequently used audiences.",
        "Compositional Insights": "Compositional Insights help you understand the makeup of your audiences. This information can answer, \u201cwhat makes an audience unique?\u201d and \u201cHow should I target these users differently?\u201d Compositional Insights make it easier for you to create effective segmentation strategies. They compare data fields within audiences to surface which attributes drive user behavior. Compositional Insights currently include the following:",
        "Audience Pairs": "This Insight compares high-value audiences on their levels of engagement. This can take the form of comparing Lytic's out-of-the-box audiences on a particular behavioral score or a custom audience.  As a bite-size piece of information, this Insight Card has provided: An easy way to leverage Lytics audiences and scores that are derived from data science.A suggested next step of what to do with this information. For example, you may target these at-risk users with a win-back campaign.",
        "Field Candidates": "This Insight compares the prevalence of specific data fields within two audiences. Candidates included arefields used in audience definitionsand fields from thePromoted Schema Fieldsin yourAccount Settings.  Having a granular understanding of your audiences allows you to make more strategic targeting decisions. For example, there may be a data field that your organization is paying to collect. If you discover through an Insight Card that this field is not performing as expected in your campaigns, you could save marketing spend by eliminating that field and using others that are shown to be more predictive of user behavior.",
        "Experience Management": "This Insight will surface when an experience is missing UTM parameters, which enable Lytics to track conversions from downstream tools such as Facebook.  While this Insight is more straightforward than the other types Lytics offers, it helps you quickly identify when an Experience configuration is incomplete, affecting your ability to track performance and target users who have previously engaged in future campaigns.",
        "Experience Performance": "The Experience Performance module gives a summary view of all the campaigns you imported into Lytics including the conversion rate of each campaign, and how much this conversion is impacting your marketing goal. Use these metrics to prompt discovery into your campaign performance. Select an Experience from the list to view its specific Intelligence Report to surface additional actions and recommendations.",
        "Activating Insights": "Insights for this Component can be accessed by hovering over or clicking directly on any line in the diagram. Hovering over a line will prompt the Dataflow Tooltip, which contains information about the Job - such as when it was created and the number of users exported. Clicking directly on the line will take you to the Job's page for the corresponding job. For instance, clicking on the yellow line between \"All\" and \"Google Cloud\" will redirect you to this page. ",
        "Activating against Lytics Behavioral Scores": "User engagement is a crucial component of personalization. Each Lytics Score (quantity, frequency, recency, intensity, momentum, propensity) indicates a different aspect of user behavior,  but overall, higher scores indicate high engagement, while lower scores indicate low engagement. Below are some everyday use cases you might consider, depending on whether the target audience hashigherbehavioral scores (anything above 50) orlowerbehavioral scores (anything below 50).",
        "Targeting Users with Low Engagement": "There are two main approaches to reaching unengaged users. The first is to drive engagement using a variety of tactics, such as messaging on different channels, targeting based on content affinities, etc. The second approach is to increase marketing efficiency by suppressing these users, thus improving your conversions.",
        "Targeting Users with High Engagement": "When creating strategies around your most active users, you will want to keep them engaged by delivering relevant content and establishing a first-party relationship that increases their Lifetime Value (LTV). You can find more users who are similar to your best customers using lookalike audiences. You can also learn more about what makes these engaged users different, which can inform your overall targeting strategies. View the Audience summary by clicking the audience name on your Insight Card to see the history of this audience and its characteristics.",
        "Activating against Lytics Content Affinity": "Giving your users more of what they love is a great tactic to consider to either drive engagement for less active users or to keep currently active users engaged.",
        "Why Use Lookalike Models?": "To demonstrate how Lookalike Models can bring your marketing team's segmentation strategy to the next level, we've outlined how you could segment users by hand compared to how a Lookalike Model would accomplish this task. For our example use case, the goal is to identify users who are likely to buy a travel package for the summer. How a marketer may create segmentation rules by hand: Users buy travel packages months in advance, so identify users who are visiting in winter and spring.Users who receive promotions are more likely to buy expensive items.Users who have already bought a package are likely to buy it again. Targeting users that match these three rules will certainly improve the efficacy of a campaign, but why stop at these? Are there more factors that can be used to refine this group of users? How Lytics Lookalike Models would do it instead: What do users who have bought travel packages in the past \"look like\" (i.e., what features do they have in common)?Analyze all the information known about these users who have bought travel packages.Determine which features are significant and which values of these features are significant (e.g., visiting the website is important, specifically three to five times).Analyze all the information known about users who have not bought travel packages.Determine which users share common features with past summer travel package purchasers.Take the most similar users to use in targeting. The key difference here is that a marketer may use a handful of criteria using logical heuristics to define a segment of users while Lookalike Models will look athundredsof factors including potential non-obvious, impactful criteria to define an audience of users for the same purpose.",
        "Choosing Your Source & Target Audiences": "In traditional supervised machine learning, we collect samples of data with one field represented as the Target. This is what we look to model or \"classify\". Some examples of real-world classifiers are predicting whether an email is spam or not, predicting if the weather tomorrow will be cloudy or sunny, classifying a news article as happy or sad, or classifying an image as a dog or not a dog. We do this same thing in Lytics' Lookalike Modeling but you get to choose what the target we want to predict is. This works by providing two audiences: aTargetandSource.",
        "Target": "The target audience is the group of users you want to model and predict, i.e. the users you want to find more of. Most of the time this audience will represent users that have done something favorable that is worth repeating by finding more users highly likely to repeat this favorable event. For example, users who provided their email on a newsletter signup, high LTV users, users who purchased a product, etc. Getting the target audience in order is step #1 before creating the model. This audience defines the basis of the model or the model's objective. Aside from \"positive\" events, you can also model users you might want to suppress like users who have churned. If the data is in Lytics, you can model it.",
        "Source": "The source audience is the group of users to find lookalikes for, i.e. the users you want to target in campaigns after creating the model. For example, if we chose users with email as the target, then targeting unknown users would make sense as the source. You could also make the source audience users who have viewed a certain line of products, users who have been active within the last 90 days, etc. The source should be adjacent to the target. More information on choosing the right source and target audiences can be foundhere.",
        "Automated Machine Learning": "Once a Source and Target audience are chosen, Lytics handles all the heavy lifting of building, optimizing, and deploying the model. For each model, Lytics automates the (1) feature selection, (2) model training and validation, and (3) scalable real-time prediction, which allows you to spend less time wrangling data to build models, and more time to create better experiences for your customers.",
        "Feature Selection": "Feature selection is the process of identifying which features (or fields) to include in a machine-learning model. While all user profile data is accessible for use in building Lookalike Models, manually identifying which features to include can be a tedious and time-consuming process if there are hundreds or thousands of fields. Lytics provides an option calledAuto-Tunewhich uses intelligent feature engineering to select the most predictive features across all the available features.",
        "Model Training": "When training models, Lytics utilizes three model types:Random Forests,Gradient Boosting Machines, andLogistic Regression. Under the hood, Lytics trains dozens of models using different parameters and hyper-parameters to optimize the model-building process.",
        "Real-Time Prediction": "Lytics Lookalike Models update user scores in real-time, so you can start targeting users immediately once the model is building, but also as their behavior changes or new users are added. Rather than using a static list, Predictive Audiences built from Lookalike Models provide a dynamic pool of users that will respond best when they are ready for ads or other marketing messages. You can also adjust your targeting criteria to make the best tradeoffs between reach and accuracy to maximize your marketing budgets.",
        "Getting Started": "Once the extension is installed, you can visit any website where Lytics is or should be installed and begin debugging. Enable the extension: To activate the Lytics Dev Tools extension, open the extension and toggle the slider at the top right of the extension to either \"Enable\" or \"Disable.\"Verify Installation:The extension will verify that the Lytics tag has been installed successfully before further debugging. This is confirmed by the \"Lytics JavaScript SDK Installed\" alert displayed on the dashboard of the extension.Navigation:Once enabled, you'll access three key sections: Debugger, Profile, and Personalization, each offering specific functionalities tailored to streamline your debugging and exploration process.",
        "Inline Content Recommendations": "Learn how to add seamless content recommendations to your website fueled by Lytics' Content Affinity Engine; all it takes is an API request and a little HTML. Difficulty:Beginner TLDR:Here's some docs instead. Other resources: A blog post - covering basically the same thing - with a live example!Lytics Segment API Docs(for segment creation API and SegmentQL)",
        "1. Build a Content Collection": "Before we go crazy adding recommendations to your website we should think about what kind of content we want to allow for recommendation. Lets create a dynamic set of content that will be the target pool of documents for this content recommendation block. To do this, we can use theLytics Segment APIcreate endpoint. In the command above, we made a POST request to create a new segment on the content table, or as we call it - a content collection. We usedSegment QLto define the filter on the content in your account. In the example above, we're only accepting urls that contain the pathfroyo/flavors. Simple enough, right? If you've used theAudience builderyou may notice that the logic for building a segment is essentially the same format. Here's a more complex example: You're probably wondering where these field names are coming from! These are the standard fields used in our content table. Here's a list of relevant fields you might use in building a content collection: Also if you have access to your queries in the Data tab of the Lytics UI you can see how the raw data from thelytics_content_enrichstream is parsed.",
        "2. Set up an inline recommendation": "To add inline content recommendations to your website all we have to do is draft some simple HTML. Pathfora will look for appropriately named data attributes, and replace the contents of these elements with recommendations from our APIs. First lets look at a simple, quick example, then we'll break down what these attributes mean: Here's a quick rundown of the relevant data attributes: imagedata-pftype attributes will set thesrcvalue if applied to animgelement (as with the example above) or if used on adivor some other type of element it will set thebackground-imageto be the primary image from the document. Similarly, theurldata-pftype attribute will set thehrefvalue to the url of the document if applied to anatag, but otheriwse will set the innerHTML of the element.",
        "Multiple Recommendations": "You may have seen a section on blogs  \"If you liked this, you may also like...\" with a list of three or more related articles. This is a common use case for content recommendations. We repeat the same HTML pattern to create multiple recommendations in a set, just remember to change thedata-pfblockvalue. If we have multiple recommendations using the same content collection then we will ensure that the same piece of content is not shown twice on one page. Here's a more in depth example, with a little CSS this can be easily styled to match your website:",
        "Before you begin": " You should have a generic HTML email template set up in your ESP. This guide uses an email template promoting new products of a shoes retailer and personalizes it based on the types of shoes the user is interested in. Think about what parts of the email you would like to change based on audience membership.",
        "Export Metrics to New Relic Insights": "You can choose to export incoming events metrics, audience change metrics, or user metrics to Insights for alerting and monitoring. Example: In Lytics, navigate to the New Relic integration in integrations.ClickExport Monitoring Metrics.Select the authorization you created in theauthorization step.In theNew Relic Account IDtextbox, enter the ID of your account. You can get this ID from the URL of \"Account Settings\" in New Relic.ClickStart Export. You should see metrics coming in as custom events into your Insights account after setting up this integration.",
        "Alerting Examples": "Once the export has started and some data has been collected on the New Relic side, you can use the data tocreate alerts within New Relic. For instance, using the LyticsMonitoring Heartbeat metric, you can create an alert if the service happens to be interrupted. The following alert(s) are just examples, Lytics recommend building in some buffers so alerts don't begin to go off for simple network issues. In this case we'll issue a warning if the query returns less than 1 for 5 minutes and a full alert if we see less than 1 for 15 minutes.  We could also create an alert to watch traffic collection which may tell us if there is an issue with the JavaScript tag on a production website. In this case if the value of collection counts falls below one, we know no data is coming into any stream so there is likely an issue. ",
        "BlueKai SFTP Server with Username and Password": "For this authorization method, you will need the following credentials: SFTP host name or IP address, port number, username and password for the SFTP server. You may also want to have your partner name (the name that Bluekai recognizes your account with), and the site ID of your Bluekai container. If you have a specific folder you want data loaded into in BlueKai, you should have the full path to it from the root of your SFTP login. In theLabeltext box, enter a name for the authorization(optional) In theDescriptiontext box, enter a description for this authorizationIn theHostbox, enter the host name or IP address of the SFTP server you want to upload to.In thePortbox, enter the port number for the SFTP server.In theUsernamebox, enter the username for the SFTP server.In thePasswordbox, enter the password for the SFTP server.(optional) In theFolderbox, enter the relative path to the folder to place the file.(optional) In thePartner Namebox, enter the name that BlueKai recognizes you with.(optional) In theSite IDbox, enter the Site ID of the container of BlueKaiClickSave Authorization.",
        "BlueKai SFTP Server with Username and Private Key": "For this authorization method, you will need the following credentials: SFTP host name or IP address, port number, username and private key for the SFTP server. You may also want to have your partner name (the name that Bluekai recognizes your account with), and the site ID of your Bluekai container. If you have a specific folder you want data loaded into in BlueKai, you should have the full path to it from the root of your SFTP login. In theLabeltext box, enter a name for the authorization(optional) In theDescriptiontext box, enter a description for this authorizationIn theHostbox, enter the host name or IP address of the SFTP server you want to upload to.In thePortbox, enter the port number for the SFTP server.In theUsernamebox, enter the username for the SFTP server.In thePrivate Keybox, enter the ssh private key of your account for the SFTP server. This key is usually generated by you. Important to mention that Lytics is compatible with keys in the OpenSSH format. If you used a tool like PuTTY to generate your key make sure you export it to the OpenSSH format.In theFolderbox, enter the relative path to the folder to place the file.In thePartner Namebox, enter the name that BlueKai recognizes you with.In theSite IDbox, enter the Site ID of the container of BlueKai.ClickSave Authorization.",
        "SFTP Export": "Connect Omeda and Lytics to personalize the onsite experience and drive anonymous visitors to become known, increase event registrations and deliver truly relevant content to the right visitors at the right time. Lytics combines Omeda subscriber data (including user audience membership) with online behavior data to deliver 1:1 experiences to each visitor. To export audiences to Omeda follow these steps: Log into yourLytics account.ClickData>Integrationsand selectOmedafrom the integrations list.SelectOmeda SFTP Export.In theAudiencedropdown, select the audience of users you would like to export.InOmeda User ID Field, enter the Lytics field that corresponds to Omeda\u2019s user ID field.Click> Show Advanced Optionsto reveal additional settings.Keep Updated: If checked the export will reoccur dailyTime of Day: Set the time the export begins each dayTimezone: Set the timezone of the export  ClickStart Export. Log in to Omedaand your file should appear undersftp://sftp.omeda.com/\"DATE AND TIME EXPORTED\".csv",
        "Expressions": "Mapping Keys vs. Literals To map the data value passed in an event you will want to use the associated key within the Expression. When creating mappings it is important to remember that keys need to be wrapped in single backticks (``). Literals (which can be a string, int, float, bool, or timestamp) are used to map a static value to the field. These are often used with conditionals.",
        "Conditions": "Conditions allow you perform logical evaluations to determine when to store a value on a field based on any of the data passed within the same incoming event. Merge OperatorsMerge operators allow you to determine how a value will be written to a field in relation to the currently stored value. Data TypesKind = \"int\" | \"number\" | \"string\" | \"date\" | \"[]string\" |\"ts[]string\" | \"map[string]int\" | \"map[string]number\" | \"map[string]string*",
        "Screenshot Example of supplying expression / condition to a mapping": "This example showcases the default mapping for themo_email_unsubfield from the Marketo Activity import.  Here we see we are writing count over time in which we seeactivityTypeIdas a key in the incoming event data.   However, we are only writing this expressions results when that incoming event data evaluates the conditioneq(activityTypeId, \"9\")astrue.",
        "String Functions": "These functions are used to manipulate string fields. joinJoin together multiple values, coerce them into strings.  Last argument is which string to use to join (may be empty string).join(\"apples\",\"oranges\",\",\")   => \"apples,oranges\"join(\"apples\",\"oranges\",\"\")   => \"applesoranges\"lenLength (of array, string) find the length of a string, return integer value of length.oneofChoose value from the first field that has a non nil value.oneof(fielda,fieldb,fieldc)replaceReplace a matching part of a string with another string or an empty string.  Converts to string first.replace(url,\"/search/apachesolr_search/\")- Removes/search/apachesolr_search/from URL (in this case, leaving the search term)replace(url, \"%20\", \" \")- replaces%20from URL with a space.string.splitBreaks a variable into smaller fragments given a specific delimitersplit(cc,\",\")- Splits the variableccat each comma it containsstring.strip(field)Strips leading and trailing whitespace (spaces, tabs, newline, carriage-return) from string, or arrays of strings.string.lowercaseConvert strings to lower casestring.uppercaseConvert strings to upper casestring.titlecaseConvert strings to title casestring.indexFind position of substring within a string, return ordinal starting position.string.index(\"apple\",\"p\") => 1find starting index of the first \"p\".string.substrExtract a string from a string using positional start/end.string.substr(\"android\",0,3)  => \"and\"string.substr(\"android\",2)  => \"droid\"containsDoes this value contain this string?  Is a sub-string match, not full match (eq)IF contains(total_price, \"$\")- Check to see iftotal_pricehas a$in itIF not(contains(subscriber_key,\"-\")) AND not(contains(subscriber_key,\"@\"))check to make sure-or@is not in it.hasprefixDoes this value start with this string?hasprefix(event, \"created\")- Check to see ifeventstarts with \"created\"hassuffixDoes this value start with this string?hassuffix(subscriber_key, \"user\")- Check to see ifsubscriber_keyends with \"user\"",
        "join": "Join together multiple values, coerce them into strings. Last argument is which string to use to join (may be empty string).",
        "join Example": "Breaking down the Example Thejoin()function takes at minimum 2 arguments (utilizing the LAST argument as the \"joining\" string), and concatenates each argument in order utilizing the \"joining\" string.  In this example we produce a single string consisting ofstreet,city,statevalues concatenated utilizing the \"joining\" string,(comma followed by a space).",
        "len": "Length (of array, string) find the length of a string, return integer value of length.",
        "len Example": "Breaking down the Example Thelen()function here is operating on a field whose value is an array, so the output is the number of items in the array. Applyinglen()to a string results in the character length of the string.",
        "oneof": "Choose value from the first field that has a non nil (empty string\"\"inclusive) value.",
        "oneof Example": "Breaking down the Example Thisoneof()function first checks and sees that theaccount_nofield does not exist in the event data, so then moves to theaccount_idwhich is found to be a non nil value and is returned. ",
        "replace": "Converts the value to a string and then replaces the matching part of a string with another string. If no replacement string is passed or an empty string is passed the matched string is simply removed.",
        "replace Example": "Breaking down the Example Usingpathfinds the path of the url which is \"/parks-and-recreation\". The innerreplacefinds all parts of the string that matches \"/\" and since there is no specified string replacement \"/\" is replaced with nothing. This results in a value of \"parks-and-recreation\" The outerreplace()takes the resulting \"parks-and-recreation\" and finds all of the parts of the string that matches \"-\" and replaces with a space (\" \"). Leading to the final value of \"parks and recreation\"",
        "split": "Breaks a variable into smaller fragments given a specific delimiter",
        "split Example": "Breaking down the Example Thesplit()splits the incoming value associated with thefavorite-genreskey at each of the \"|\" characters the value contains. Becausesplit()will split the value into fragments that are stored as an array, you will need to map it to a field that is a set data type ([]stringor[]time) and you will also want to wrapset()(for strings) ortotimeset()(for datetimes) around the function. The order of a set of strings resulting from thesplitfunction will be alphabetical. The order of a set of datetimes resulting from thesplitfunction will be oldest to latest. However, if you use an array.index function to pull out a value based on position, the originally passed order will be referenced.",
        "strip": "Strip removes leading and trailing whitespace (spaces, tabs, newline, carriage-return) from strings, or arrays of strings.",
        "strip Example": "Breaking down the Example Thesplit()function splits the incoming value associated with thefavorite-sub-genreskey at each of the \"|\" characters the value contain. In this example it will create 4 strings that are stored in an array output alphabetical order. Thestrip()function then removes the leading and trailing spaces around each of the strings within the array.",
        "string.lowercase": "Convert strings to lower case",
        "string.lowercase Example": "Breaking down the Example Thestring.lowercase()function looks at each character in the string value and converts to the lowercase equivalent.",
        "string.uppercase": "Convert strings to upper case",
        "string.uppercase Example": "Breaking down the Example Thestring.uppercase()function looks at each character in the string value and converts to the uppercase equivalent.",
        "string.titlecase": "Convert strings to title case",
        "string.titlecase Example": "Breaking down the Example Using thestring.lowercase()will first normalize the incoming string by converting it to lowercase. Wrapping thestring.titlecase()will then convert the string into title case.",
        "string.index": "Find position of substring within a string, return ordinal starting position.",
        "string.index Example": "Breaking down the Example string.index()utilizes a '0' based index, so the first character in the string is at index0, and the \"1\" being the second character in the string is at index1",
        "string.substr": "Extract a string from a string using positional start/end.",
        "string.substr Example": "Breaking down the Example string.substr()utilizes a '0' based index and returns the substring of the provided string, starting at the provided start index up to but not including the end index.  The end index is optional here, and will return from the start index through the remainder of the string. string.substr(\"android\",0,3)  => \"and\"string.substr(\"android\",2)  => \"droid\"",
        "contains": "Does this value contain this string?  Is a sub-string match, not full match (eq)",
        "contains Example": "Breaking down the Example Thecontains()function takes 2 arguments, the field (or raw string) to be searched, and the substring to search for.  In this example the value for thestringExamplefield is searched for the substring \"string\", and being found returns the boolean valuetrue",
        "hasprefix": "Does this value start with this string?",
        "hasprefix Example": "Breaking down the Example Thehasprefix()function takes 2 arguments, the field (or raw string) to be searched, and the substring to check for a match starting at index 0 of the 1st argument.  In this example the value for thestringExamplefield is checked for the substring \"This is a string\", and being found to match as the prefix returns the boolean valuetrue",
        "hassuffix": "Does this value end with this string?",
        "hassuffix Example": "Breaking down the Example Thehassuffix()function takes 2 arguments, the field (or raw string) to be searched, and the substring to check for a match to end the 1st argument.  In this example the value for thestringExamplefield is checked for the substring \"as an example\", and being found to match as the suffix returns the boolean valuetrue",
        "Hash & Encoding Functions": "You can apply hash functions to encode incoming data. hash.siphash.sip(email)Hash the given value using sip hash to integer output.hash.md5hash.md5(email)Hash the given value using md5.hash.sha1hash.sha1(email)Hash the given value using sha1.hash.sha256hash.sha256(email)Hash the given value using sha256.hash.sha512hash.sha512(email)Hash the given value using sha512.encoding.b64encode(field)base64 encode.encoding.b64decode(field)base64 decode.",
        "hash.sip": "Hash the given value using sip hash to integer output.",
        "hash.md5": "Hash the given value using md5.",
        "hash.sha1": "Hash the given value using sha1.",
        "hash.sha256": "Hash the given value using sha256.",
        "hash.sha512": "Hash the given value using sha512.",
        "encoding.b64encode": "base64 encode.",
        "encoding.b64decode(field)": "base64 decode.",
        "Casting & Conversion": "These functions allow you to cast and convert data into different types. tointConverts strings to integers.  Useful for converting a string to a number before applying a number-based expression.toint(order_total)- Convertsorder_totalto an int.set(toint(split(cc,\",\")))- Takes the fieldccand splits it at commas, and converts the results to integers.  Then adds them to a set.tonumberConvert to NumbertodateConverts strings to dates, see full doc in Date/Time section below.tobool(field)Cast to Boolean.",
        "toint": "Converts strings to integers. Useful for converting a string to a number before applying a number-based expression",
        "toint Example": "Breaking down the Example Here we have a summation function applying to the result of converting two different string fields toint(s) and wrapping them in thesum()function. We first convert the value forstringOne=\"1.7\"into it'sintequivalent1, then convert the value forstringTwo=\"2\"into it'sintequivalent2... then the summation result of the twointvalues is3",
        "tonumber": "Converts string to numbers. Useful for converting a string to a number while needing to keep decimal precision",
        "tonumber Example": "Breaking down the Example Here we have a summation function applying to the result of converting two different string fields tonumber(s) and wrapping them in thesum()function. We first convert the value forstringOne=\"1.7\"into it'snumberequivalent1.7(keeping decimal precision), then convert the value forstringTwo=\"2\"into it'snumberequivalent2... then the summation result of the twonumbervalues is3.7",
        "tobool": "Casts to boolean:Numeric Values of 0/1 convert tofalse/truerespectivelyString Values will convert as: \"0\", \"f\", \"F\", \"false\", \"FALSE\", \"False\" ->false\"1\", \"t\", \"T\", \"true\", \"TRUE\", \"True\" ->true",
        "Map & Set/Array Functions": "These functions manipulate map or set fields. filterFilter out Values that match specified list of match filter criteriafilter(split(\"apples,oranges\",\",\"),\"ora*\")  => [\"apples\"]lenLength (of array, string)mapCreate an object/map of key-value pairs.   Often used to keep map of key (event-name?) to value  (last occurrence date?).  Or other user level key-value pair data.map(key1, todate(date_field))map(key1, todate(date_field))  KIND map[string]timeBy default themapis generic map, cast to map[string]time withmatchType:  Map (generic map, use KIND to cast) Match a key, and then keep a map of key/values with the match value removed., match(\"topic_\")           AS global       KIND map[string]numbermapkeysType:  Map input, []string{} output. Given a map, return a list of string of each of the keys.mapvaluesType:  Map input, []string{} output. Given a map, return a list of string values of each of the values.mapinvertType:  Map input, MapString output. Given a map, return a map[string]string inverting keys/values.array.indexCherry pick a single item out of an array:array.index(split(\"apples,oranges,peaches\",\",\"),1)  => [\"oranges\"]array.sliceSlice an array of items selecting some sub-set of them.array.slice(split(\"apples,oranges,peaches,pineapple\",\",\"),2)  => [\"peaches\",\"pineapple\"]array.slice(split(\"apples,oranges,peaches,pineapple\",\",\"),1,3)  => [\"oranges\",\"peaches\"]",
        "filter": "Filter out values that match the specified criteria",
        "filter Example": "Breaking Down the Example Thisfilter()operates on top of the array built bysplit()'ing andstrip()'ing thesliceField.  Once we have our base array, we then apply the filter\"bl*\"(in which*is a wildcard) to match the \"blue\" value and filter it OUT of the array (returning an array including all the unmatched values)",
        "map": "Create an object/map of key-value pairs.   Often used to keep map of key (event-name?) to value  (last occurrence date?).  Or other user level key-value pair data.",
        "match": "Match a key, and then keep a map of key/values with the match value removed.",
        "match Example": "Breaking Down the Example Thematch()function here looks for all keys prefixed with \"example_\", and finding 3 results, creates a map of key (key suffix) -> value pairs from those results utilizing the suffix of the original keys  to write to the map.",
        "mapkeys": "Given a map, return a list of strings consisting of the keys from the map",
        "mapkeys Example": "Breaking Down the Example Themapkeys()function here utilizing the result of thematch()statement as it's input map, and returns a list consisting of the keys from that map.",
        "mapvalues": "Given a map, return a list of the values (as strings)",
        "mapvalues Example": "Breaking Down the Example Themapvalues()function here utilizes the result of thematch()statement as it's input map, and returns a string list consisting of the values from that map.",
        "mapinvert": "Given a map, return a map of typemap[string]string, inverting the keys/values from the initial map Breaking Down the Example Themapinvert()function here utilizes the result of thematch()statement as it's input map, and returns a map (map[string]string) with each key-> value pair inverted (value->key).",
        "array.index": "Cherry pick a single item out of an array",
        "array.index Example": "Breaking Down the Example Thearray.index()function utilizes a 0 indexed array to cherry pick the requested indexes value, in this case 0 being the first indexed item returns \"apples\".",
        "array.slice": "Slice an array of items selecting some subset of them",
        "array.slice Example": "Breaking Down the Example For this example thearray.slice()function takes 3 parameters (the 3rd being optional), and returns an array consisting of the subset of the initial array (1st parameter) beginning at the start index (2nd parameter) up to, but not including, the end index (3rd parameter, if not provided then it defaults to the end of the input array). Reminder that these functions utilize 0 indexed arrays, so the index of1is actually the 2nd item in the array.",
        "URL/HTTP & Email Functions": "These functions manipulate strings which are URLs or email addresses. emailExtract email address from \"Bob <emailaddress@lytics.io>\" format, note that email addresses are converted to lowercaseemailnameExtractBobfrom \"Bob <emailaddress@gmail.com>\" oremail@gmail.comemaildomainExtractgmail.comfrom \"Bob <emailaddress@gmail.com>\" oremail@gmail.comdomainExtract domainlytics.iofrom URLhttp://www.lytics.io/index.htmlhostExtract hostwww.lytics.iofrom URLhttp://www.lytics.io/index.htmlpathExtract the URL path from URL (no query string or domain), must be valid URL parseable string.qsExtract the query string parameter from URLqs(urlfield, \"nameOfParam\")qs(url, \"mc_eid\")- Extracts the MailChimp user IDset(qs(url, \"video_id\")- Creates a set ofvideo_idqs(tolower(url), \"riid\")- Converts the complete URL to lowercase before attempting to matchemail(oneof(email, qs(url, \"email\")))- Attempts to get the email address from the URL and from the regular fields, chooses whichever is populated and treats it like an email fieldqs2Extract a querystring parameter without lowercasing before checking for the parameter.qs2is the same asqsabove except that it does not lowercase before checking for a querystring.urlChecks if URL string is valid and returns URL if true.urldecodePerform URL decode on a field.urldecode(field)Iffieldcontains \"my%20value\",urldecode(field)will return \"my value\"urlmainRemoves the querystring and scheme from the urlurlmain(\"http://www.lytics.com/?utm_source=google\")will return\"www.lytics.com/\"urlminusqsRemoves a specific query parameter and its value from a urlurlminusqs(\"http://www.lytics.com/?q1=google&q2=123\", \"q1\")will return\"http://www.lytics.com/?q2=123\"useragentExtract info from user-agent string.   Below examples based onMozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.11 (KHTML, like Gecko) Chrome/23.0.1271.97 Safari/537.11useragent(user_agent, \"bot\")- Extracts True/False is this a bot?useragent(user_agent, \"mobile\")- Extracts True/False is this mobile?useragent(user_agent, \"mozilla\")- Extracts \"5.0\"useragent(user_agent, \"platform\")- Extracts \"X11\"useragent(user_agent, \"os\")- Extracts \"Linux x86_64\"useragent(user_agent, \"engine\")- Extracts \"Linux x86_64\"useragent(user_agent, \"engine_version\")- Extracts \"AppleWebKit\"useragent(user_agent, \"browser\")- Extracts \"Chrome\"useragent(user_agent, \"browser_version\")- Extracts \"23.0.1271.97\"useragent.map(field)Extract map of all of above.",
        "email Example": "Breaking Down the Example In this example theemail()function first extracts the email address of \"TEST@test.com\" from the input (this input can be in the the simplified input already, and will then just apply the second part of the function). Once we have the email address, the function then performs a lowercase operation and checks to ensure the presence of an@symbol. This check is fairly rudimentary and may change to provide stricter validation in the future.",
        "emailname": "ExtractBobfromBob <test@test.com>",
        "emaildomain": "Extractgmail.comfrom \"Bob <emailaddress@gmail.com>\" oremail@gmail.com",
        "url": "Checks if URL string is valid and returns URL if true",
        "urlmain": "Removes the query string and scheme from the url",
        "urlmain Example": "Breaking Down the Example Theurlmain()function here strips the leading scheme (\"https://\") and query string parameters (\"?qsparam=p1&qsp2=param2\") from the provided url.",
        "domain": "Extract domain from URL",
        "host": "Extract host from URL",
        "path": "Extract the URL path from the URL",
        "urldecode": "Perform URL decoding on a field. (my%20value->my value)",
        "urldecode Example": "Breaking Down the Example With this example ofurldecode()we want to decode the \"spaced\" query parameter value.  To do this we first extract the parameters' value utilizing theqs()function, returning \"this%20is%20spaced\".  We then proceed to use theurldecode()function to convert the \"%20\" to \" \", and have our decoded string of \"this is spaced\".",
        "urlminusqs": "Removes a specific query parameter and it's value from a URL",
        "qs": "Extract the query string parameter from URL",
        "qs Example": "Breaking Down the Example Here theqs()function attempts to find the requested query string parameter in the URL and returns it if found.  It's important to note here that this function inherently lowercases the URL provided BEFORE attempting to match the query string, whereqs2()will search the raw URL provided without lowercasing.  Effectivelyqs(url, param)==qs2(tolower(url), param)",
        "qs2": "Extract a querystring parameter without lowercasing the URL before checking for the parameter",
        "useragent": "Extract info from user-agent string. Below examples based onMozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.11 (KHTML, like Gecko) Chrome/23.0.1271.97 Safari/537.11 useragent(_ua, \"bot\")- Extracts True/False is this a bot?useragent(_ua, \"mobile\")- Extracts True/False is this mobile?useragent(_ua, \"mozilla\")- Extracts \"5.0\"useragent(_ua, \"platform\")- Extracts \"X11\"useragent(_ua, \"os\")- Extracts \"Linux x86_64\"useragent(_ua, \"engine\")- Extracts \"Linux x86_64\"useragent(_ua, \"engine_version\")- Extracts \"AppleWebKit\"useragent(_ua, \"browser\")- Extracts \"Chrome\"useragent(_ua, \"browser_version\")- Extracts \"23.0.1271.97\"useragent.map(_ua)- Extract map of all of above.",
        "Date & Time Functions": "These functions manipulate date fields. Our core date parser recognizes about 50 date formats, so in general these will operate onanyformat.If you are using EU dates, you will need to specify the parser format. dayofweekType: Integer.  0-6 integer of day of week.Examples:dayofweek() => 4ORdayofweek(mydatefield)epochmsType: Integer.  Unix MS of the date stamp on the current message being processedextractCan be used to extract parts of date and time.  Example usage on thestrftimesiteextract(reg_date, \"%B\")Returns name of monthextract(reg_date, \"%d\")Returns day of monthhourofdayType: Integer.  Hour of day (in 24 hour utc time).hourofday()ORhourofday(field)hourofweek0-167 integer for hour of weekmmType:  Integer.  0-11 month  (alias for monthofyear)mm()=> current month, 6 for june,mm(my_date_field)monthofyearType:  Integer  Output the 0-11 month valuenowType:  Date   The current message/event times.secondsType: Integer.  Seconds, extracts things likeseconds(\"00:30\") => 30andseconds(\"10:30\") => 630todateConverts strings to dates.Datemath:todate(\"now-3m\")Date math relative to message timestamp.Parser:todate(\"02/01/2006\")More than 30 formats supported.Date ParserExamples with 2 arguments: todate(date_field_format,date_field_name) wheredate_field_formatrepresents howdate_field_nameshould be parsed and usesgolang's time packageformattingtodate(\"02/01/2006 15:04:05 PM\",date_field_name)outputsdate_field_nameas European format (where 01 is a placeholder for month, 02 is a placeholder for day, and 2006 is a placeholder for year, 15 is a placeholder for hour, 04 is a placeholder for minute, and 05 is a placeholder for second. If the timestamp is in AM/PM format, use PM. For 24Hr format, do not add am/pm after second field.)for e.g.,todate(\"02/01/2006 15:04:05 PM\",\"30/04/2014 12:25:30 PM\")parses the date\"30/04/2014 12:25:30 PM\"as European format. Please refer togolang's time packagedocumentation for more information about date time formats.todateinConverts strings to dates and parses the given datetime for the given location. For e.g.,todatein(date_field_name, \"America/Los_Angeles\")parses thedate_field_namefor Los Angeles location. If no location info is provided in date string such as \"2017-09-30 17:00:00\" this will allow you to apply a timezone.  We still convert back to UTC for storage.totimestampConvert to Integer Unix Seconds (UTC).yyType: int   Date conversion to YY format, so May 1 2014 is expressed as 14.   yy(dob), or yy() for record time stampyymmString The YYMM date format, so May 1 2014 is expressed as 1405.     yy(dob), or yy() for record time stamptimebucketCreates a tabulation of timestamps which can be used to segment based on timewindows.timebucket(now())for collect time, ortimebucket(todate(field))to bucket on the value of a field",
        "now": "The current message/event times.",
        "now Example": "Breaking Down the Example The event in question contains no attribution defining the date/time of the event, and therefore is processed utilizing the current time as the datetime for the event (The time of this example being \"2024-11-18T17:51:21.483775672Z\").",
        "epochms": "Unix MS of the date stamp on the current message being processed Breaking Down the Example Similar to thenow()function, theepochms()function uses the current event's timestamp to produce a result.  This time however, we return it's representation as the integer unix seconds (UTC) format.",
        "todate": "todateConverts strings to dates.Datemath:todate(\"now-3m\")Date math relative to message timestamp.Parser:todate(\"02/01/2006\")SeeDate Parserfor supported formats.Examples with 2 arguments:todate(\"02/01/2006\",\"07/04/2014\")usegolang's time packageformattingtodate(\"02/01/2006\",\"07/04/2014\")Reformats the date07/04/2014from US formatting to UK formatting, with the resulting output being04/07/2014todate(\"02/01/2006\",date_field_name)Outputsdate_field_nameas European format (where01is a placeholder for month,02is a placeholder for day, and2006is a placeholder for year)",
        "todatein": "Converts strings to dates whiles specifying a location / timezone.We still convert back to UTC for storage",
        "totimestamp": "Convert date(time) to Integer Unix Seconds (UTC)",
        "extract": "Used to extract parts of a date(time). Example usage on thestrftimesite",
        "yy": "Date conversion to YY format, so May 1 2014 is expressed as 14.   yy(dob), or yy() for record time stamp",
        "yymm": "String The YYMM date format, so May 1 2014 is expressed as 1405.",
        "mm": "Date conversion to Integer month value (alias formonthofyear)",
        "monthofyear": "Seemm",
        "dayofweek": "Integer representation of the day of the week. (0-6, 0=Sunday, 1=Monday, etc.)",
        "hourofday": "Hour of day (12:00-12:59am || 00:00:00-00:59:59 = 0)",
        "seconds": "The Integer number of seconds in a give time (works with datetime as well as (MM:SS) things likeseconds(\"00:30\") => 30andseconds(\"10:30\") => 630",
        "Aggregate Functions": "There are a variety of expressions for building document type structures (maps, lists, sets). These are functional expressions but can only be used in Columns. countCount of this key.  For instance, count occurrences of sessions that have started (ie, visited website).setCreate a unique list/array of each value we have seen from this field.min,maxMinimum or Maximum value (for numerics).sumSum values (keep track of total video play time, etc).",
        "count": "Count of # of occurrences of the specified key",
        "valuect": "Count of # of occurences of each value for a specific key (stored as a map[value]count)",
        "sum": "Summation of values for a specified key over time",
        "min": "Minimum numeric value seen for the specified key over time",
        "max": "Maximum numeric value seen for the specified key over time",
        "set": "Create a unique list/array of each value seen for this key over time",
        "Logical Functions": "These functions are used for local evaluation, and return boolean values (true/false). allCheck for the existence of n keys. Returns true of false.all(key1,key2,key3,...)anyCheck for the existence of at least one of the given n keys. Returns true or false.any(fieldname, value1,value2,value3)existsCheck for the existence of a single key. Returns true or false.exists(purchase_total)checks to see ifpurchase_totalis defined for the current messagevaluect(yymm())        AS visits_by_yymm    IF exists(_sesstart)Only firesvaluect(yymm())if_sesstartexistsinCheck if a field value is in a set of values. Returns true or false.\"t\" AS is_student IF role_type IN (\"student\",\"other\")dailyContact          AS dailyContact   IF dailyContact IN (\"student\",\"other\")eqCheck if the two values are equal. Returns true or false.eq(domain,\"google.com\")neCheck if the two values are not equal. Returns true or falsene(domain,\"google.com\")ltCheck if the first value is less than the second value. Returns true or false.lt(seconds(video_time), 30)leCheck if the first value is less than or equal to the second value. Returns true or false.le(seconds(video_time), 30)gtCheck if the first value is greater than the second value. Returns true or false.gt(seconds(video_time), 30)geCheck if the first value is greater than or equal to the second value. Returns true or false.ge(seconds(video_time), 30)notReturns true if the inner value resolves as false and returns false if the inner value resolves as true.not(exists(domain))orReturns true if at least one of the inner statements resolves to true.or(exists(domain), contains(domain,\"google.com\")) AS from_googleifCheck if the first IF clause is true. If not, use the value in the else clause.\"Planet Earth\" IF CONTAINS(Planet, \"Earth\") ELSE \"Some Other Planet\"Format:{IF_TRUE_VALUE} IF {CONDITION} ELSE {ELSE_VALUE}ifcaseCheck for multiple IF conditions. If none are true, default to the last value.ifcase(\"Planet Mars\" IF CONTAINS(Planet, \"Mars\"), \"Planet Venus\" IF CONTAINS(Planet, \"Venus\"), \"Planet Earth\")Format:ifcase({IF_TRUE_VALUE_1} IF {CONDITION_1}, {IF_TRUE_VALUE_2} IF {CONDITION_2}, {ELSE_VALUE})",
        "all": "Checks a list of values (or keys' values), and returnstrueif ALL those values (or keys' values) are non zero-values (keys that are missing from incoming data will result in a false return here).",
        "any": "Similar toall(), except only requires one of the given parameters to evaluate to true'ish (not a zero-value) for the result to betrue",
        "exists": "Checks for the existence of a key (or a passed value), and confirms the value (if exists) is not the empty string.",
        "eq": "Checks for equality",
        "ne": "Inverse ofeq()",
        "lt": "Checks if the first argument isless thanthe second",
        "le": "Checks if the first argument isless than OR equal tothe second.",
        "gt": "Checks if the first argument isgreater thanthe second.",
        "ge": "Checks if the first argument isgreater than OR equal tothe second.",
        "not": "Returns the inverse of a boolean valuation (not(false)==true)",
        "in": "Checks ifis in the supplied value list",
        "Setup Drift App and Webhooks": "Note this will only need to be done once per account. Go to Drift'sdeveloper siteand click onBuild your apporCreate new appafter logging in.Click onOAuth & ScopesUnder the scopes heading, from theadd permissions by scopesDropdown, select the following permissions:contact_readconversation_readUnder theDisplay Informationtab, set the app name to \"Lytics Integration\".Under theInstall to Drifttab click theInstall App to Driftbutton to generate an OAuth Access Token. This will be used to authenticate with Drift.Copy theOauth Access tokenthat replaces theInstall App to Driftbutton. If you are new to creating authorizations in Lytics, see theAuthorizationsdocumentation for more information. Select Drift from the list of providers.Select theDrift App Keymethod for authorization.In theLabeltext box, enter a name for the authorization(optional) In theDescriptiontext box, enter a description for this authorizationIn theApp Keytext box, enter your OAuth Access Token.ClickSave Authorization.",
        "Streamlined Data Integration Process": "Integrating custom data into Lytics is simplified into two straightforward steps: Schema Definition: Outline the schema to dictate how your new data source should update existing customer profiles.Data Stream Creation: Establish a new data stream tailored to your source to ensure seamless data flow.",
        "Features of Schema Copilot": "Schema Copilot leverages advanced generative artificial intelligence to comprehensively scrutinize your new data source. It offers intuitive suggestions for fields and mappings, enabling seamless integration of your new data source with the existing schema. This intelligent assistance ensures that your data aligns perfectly with the Lytics platform, enhancing your customer data integration's overall efficiency and accuracy.",
        "How to use Interest Engines": "Interest Engines are a rather foreign concept and you may be confused about how this can help improve your marketing initiatives. Keep in mind that the end goal is to gain abetter understanding of your users. With Lytics' Interest Engine, there are two commonly used techniques to best utilize this powerful data: Topics & Affinities: With theDefaultengine andCustom Affinity Engines, you can use the Lytics UI to view theTopic Taxonomy. This visualization displays how your Topics co-occur and relate to each other. TheTopic Taxonomyview can be useful to visualize similar Topics, which can be useful in creatingAffinities.User Segmentation: Lytics Interest Engines output a new field on the user profile that captures the interests that a user has exhibited. Using this data, you can create high-powered audiences to target users with specific interests using Lytics Audience Builder.Recommendations: Using Lytics' Recommendation API, you can enable personalized recommendations for your users based on their activities. For instance, you can recommend URLs based on a user's browsing history, or recommend products based on a user's purchase history. To get started with Recommendations, check out the Recommendation documentation, or navigate to the Experiences tab.",
        "How to Create Interest Engines": "To create a new Interest Engine, navigate to theEnginespage in theContentnavigation dropdown, and click on theNew Interest Enginebutton. When clicked, this displays a modal that allows you to select between: Custom Affinity EngineShopify EngineCollaborative Filter Each option will present you with a wizard to configure the Engine. Please contact Lytics Support if you experience any issues creating your new Interest Engine.",
        "Build a Content Affinity Audience": "You can use theContent Affinitytab of the audience builder to build an audience of users who show interest in a particular group of Topics (what Lytics calls an Affinity). Then, that audience can be used to target users with the desired Affinity in a personalized Experience. From yourLytics dashboardselectAudience.ClickCreate New Audience.Select theContent Affinitytab.Select the Affinity you would like to target.Any Affinityis selected by default. You can adjust the affinity score range to refine the users who will be targeted or select one of the predefined ranges.In theName your audience...field, give your audience adescriptive name.SelectEnable API Access.In theIDfield, enter an audience ID.ClickSave. Audience with any affinity for articles about \"data science\" This audience can now be used in aRecommend Content(or any other) campaign to target users who have the desired level of interest for your chosen Affinity. You can combinemultiple rules and rulesetsto create highly refined audiences. You are also not limited to audiences based on content affinity, see theAudiencessection for other options.",
        "Content Collections": "Content collections give you the ability to group content together that you can then recommend in your campaigns. Collections can be a group of articles related by Affinities, Topics, recently published, author, hand picked by you, or a combination of any of these criteria. In addition, these collections can be dynamic. Content in a dynamic collection will be added to or removed automatically when they match or fail to match your criteria. For more information, seeContent Collections. Content collection grouping article related to \"data science\" on learn.lytics.com.",
        "Build Your Campaign": "Next, you\u2019ll need to navigate to the Experiences tab and pick the \"Recommend Content\" tactic for Lytics. The first step will be to choose the content collection you want to feature: Next, select whether you want this to be based on a user's interests (\"Highest Affinity\"), content or product freshness (\"Freshness\"), or what a user has engaged with most recently (\"Last Interaction\"): Don\u2019t forget to give your Experience a name. See ourExperience editordocumentation for step-by-step instructions.",
        "Lytics Integration Options": "If you don\u2019t see your ads or marketing tool in our integrations list we offer the following options to connect with other platforms. With our Integration Options we enable flexibility across the various channels where your data exists and tools to build customized integrations. Lytics Integration Options include: WebhooksLytics File Service integrations for server-to-server file transferJavascript TagMobile SDKData Warehouse Integrations Read on for details on each option. If you have questions about any of these options or would like to further explore an integration with a specific platform reach out to your Lytics contact.",
        "Webhooks": "Many third-party vendors offer webhooks or API callbacks, which send requests containing relevant user-activity data to another service after an event has occurred in the system. The Lytics API has a collection endpoint that can be used as a destination for third-party webhooks. Providers that support webhooks will have documentation detailing the preferred setup method for Lytics to receive webhook data. In addition, Lytics supports webhooks that can be configured to send user-level events, such as real-time audience membership updates.",
        "Server-to-Server File Transfer": "Server-to-server file transfer is a very common mechanism for data sharing. Lytics File Service options enable Lytics to pull or push user data from an SFTP (secure file transfer protocol) using a CSV (comma-separated values) or JSON file. If you have another platform that also enables integrations via SFTP, this integration path helps to automate the process of passing data between your system and Lytics. Check out theLytics File Service documentationto learn about setting up the file transfer job. In addition to SFTP, Amazon S3 is a very commonly used medium for server-to-server file transfer. Refer to ourAmazon S3 documentationfor data import and export options.",
        "Javascript Layer/JS Tag": "The Lytics JS Tag is the default option for client-side (website) data sharing with Lytics. The JS Tag supports the delivery of user behavior attributes from your website into Lytics and can also return back to the website user profile data such as audience members or content recommendations from Lytics for real time personalization. Refer to our JS Tag Documentation for full details.",
        "Mobile SDK": "The Lytics Mobile SDK facilitates in-app data sharing with iOS and Android. Similar to the JS Tag, the Mobile SDK supports user behavior-based data delivery to Lytics and profile data to the app for real-time personalization. See ourMobile SDK Documentationfor more details.",
        "Using Your Data Warehouse": "If you already use a data warehouse to manage your data this is a great option for data import and export with Lytics. Lytics integrates with Google Big QueryAmazon RedshiftMicrosoft AzureSnowflakeDatabricks Each of these providers offers options for importing data into databases which can then be imported into Lytics using Cloud Connect or our Data Import integrations. Lytics can also export data into each of these data warehouses.",
        "SFTP Server Username and Password": "Enter the following:Host: address of SFTP server for external access.Username: user name used to log into server.Password: password used to log into server.Folder: path in the SFTP server where files will be placed. You are now ready to start aSFTP Exportjob.",
        "API Username/Password": "Enter the following:REST API URL: REST API URL of the Mapp instance to which you want to export. This is thehost componentof the Mapp instance URL and the path to your Mapp instance (i.e.https://staging11.shortest-route.com/qatest).Username: user name used to log into your Mapp instance.Password: password used to log into your Mapp instance.  You are now ready to start anExperiencejob. Use this authorization to only import Experience metrics.",
        "API Integration ID and Secret": "Enter the following:Async API URL: Async API URL of the Mapp instance to which you want to export. This is thehost componentof the Mapp instance URL and the path to your Mapp instance  (i.e.https://charon-test.shortest-route.com).Integration ID: your Mapp Connect Integration ID.Secret: your Mapp Connect Integration Secret.  You are now ready to start anAudience Exportjob.",
        "API Username/Password, Integration ID, and Secret": "Enter the following:Async API URL: Async API URL of the Mapp instance to which you want to export. This is thehost componentof the Mapp instance URL and the path to your Mapp instance (i.e.https://charon-test.shortest-route.com).Integration ID: your Mapp Connect Integration ID.Secret: your Mapp Connect Integration Secret.REST API URL: REST API URL of the Mapp instance to which you want to export. This is thehost componentof the Mapp instance URL and the path to your Mapp instance (i.e.https://staging11.shortest-route.com/qatest).Username: user name used to log into your Mapp instance.Password: password used to log into your Mapp instance.  You are now ready to start anExperiencejob. Use this authorization to import Experience metrics and activate your Experiences.",
        "Single Send Tactics": "When you activate this Experience, users will be pushed to a SendGrid contact list, and a segment will be created for the Experience in SendGrid. Ensure that a new segment was created and populated in your SendGrid account. Navigate toMarketing > Contacts. (If you have the legacy marketing campaigns enabled use the Marketing tab that is marked asNEW).You should see a new segment under the list you selected during configuration. This segment should have the same name as your Experience in Lytics and will have a single contains rule.Next you will assign this segment to your single send campaign. Navigate toMarketing > Single SendsAnd click on the single send you imported.Configure your single send to use the segmentby selecting the segment under theSend Toin theRecipientsof the email editor.Once your email is ready to send, clickReview Details and Sendto proceed with the sending process.",
        "Adding a Destination": "NewSourceintegrations are added from the Decision Engine interface by first navigating to theJobsoption under theDatasection in the main navigation.  From there, you'll click \"+ Create New Job\" at the top of the list and enter the wizard to guide you through the creation process. ",
        "Configure Destination": "The final step lets you provide the specific configuration details for your chosen provider and job type. Again, the options supported by each provider will vary greatly, and provider-specificintegration detailsshould be leveraged to determine the optimal approach. ",
        "Monitoring a Destination": "Once you have one or more Destination jobs running, they will be accessible from the Jobs list view, as pictured below. This view provides quick access to essential details: Name: Name of job, such as \"Export of High-Value Users to Facebook.\"Authorization: Name of the associated authorization.Provider: Third-party tool that you are connecting with Lytics.Type: Indicates whether the job is an import, export, or enrichment.Status: Current state of a job such as running, paused, completed, etc.Created: Date the job was initially created. ",
        "JavaScript Configuration": "Begin with a configuration for aform widget. Without defining any field customizations, Pathfora will default to using legacy form elements. However, you will be usingcustom form elementsin this guide. This config will generate a simple slideout with a form containing some default fields.  Once you have an idea of what form elements you want to include in your widget, you may use theform builder drag and drop UIprovided in the Pathfora documentation to construct the form you wish to display. Each field can be marked as required and may have a label and name (used for as the primary key for tracking the field in Lytics and other external systems). Text fields may also include placeholder text, and for fields with multiple options you may define the display text and value of each option.  Once you clickSave, the builder will output theformElementsfield that you can simply copy and paste into your widget config. This UI is intended to help you save time when constructing configurations with custom forms. ",
        "Styling the Recommendation Modal": "In this example, we will make some small adjustments to soften the look of the content recommendations. You can download the styles for this example fromGitHub.  When writing your own custom styles, remember that different users will see a different pieces of content based on interests. Be careful when adjusting the size of things like the image in relation to the text. Images from different articles may be of a different size, and text descriptions may vary in length if not controlled by thedescriptionLimitsetting. Be sure to test your modal with multiple different content items. You can do this by adding theshufflesettingto your config, which will change the content every time you see the modal. You may also want to ensure that all content in your collection has similar image sizes if you do want to adjust its styles in the modal. ",
        "Content Curation": "Content curation on Lytics involves scanning your website and other content to ingest topics and build content affinities. Properly setting up the curation process is key to enabling use cases such as promoting relevant ads and delivering targeted web content. This document gives an overview of important concepts and considerations to make while curating your content during the early stages of implementation. It will also help determine if any custom content curation should be planned for. Ensure that any content customization is performed to support your campaign execution and audience building, not for the sake of customization itself.",
        "What domains should Lytics scan?": "By default, Lytics will scan all content on your domains. You can specify which domains Lytics will scan by adding domains to the \"Domain Allowlist\", which can be modified by navigating toAccount Settings > Content. If there are only certain sections of your website Lytics should be scanning, you can customize by allowing or blocking specific paths as well. For example, if you have a blog section and other pages won\u2019t be relevant to gauge what users are interested in, you can add the path/blog/to the list of allowed paths. You can see which domains have been improved in the domain and path settings on the Content Classification page. Visit the content account settings documentation for detailed instructions on how to add more domains or paths.",
        "Are there any paths that should be avoided?": "A website may have sections that should not be scanned for topics such as password reset pages or any pages hidden behind a log-in (e.g./password-reset/or/admin/). These paths can be blocked in your Account Settings under Content.",
        "Which NLP service should I be using?": "Lytics usesGoogle NLPwhich pulls from their knowledge graph/taxonomy. If you determine you're not getting enough topics from your content, Lytics can useDiffbotin addition to Google NLP, which has more loose associations between topics and content. You'll bring in more topics, but may be slightly surprised by what you see! If your brand is international, you may need to consider which languages are supported by each NLP service. Please consult the provider documentation for a list of languages supported byGoogle NLPandDiffbot. Another option is to turn NLP completely off and use only custom topics. For more details on each service Lytics uses, seeNLP services.",
        "Backlog": "If you aren\u2019t seeing the content you expected to, note it may take some time for Lytics to crawl all of your content. By default, monthly limits exist for scanning new content (seeContent Enrichment limitsbelow). If Lytics scans all new content without having reached the limit, Lytics will move on to older content until it is all scanned. Please allow time for this. Also, consider how far back should Lytics be scanning. For example, it is likely unnecessary to scan content from 2 years ago. If people are no longer interacting with that content, use Account Settings to set a date to start the scan from.",
        "Content Enrichment limits": "Lytics will scan and enrich up to 20,000 URLs per month by default. This limit is designed to act as a guard rail to ensure good filter hygiene is in place. Most accounts do not publish close to 20,000 pieces of distinct content per month. If you believe your account is hitting this limit, please check with your Lytics Implementation team. Once confirmed, you can consider the following options: Are there any domains or content paths that you can block? This will likely be part of the solution. SeeAre there any paths that should be avoided?above for more information.Do you need Lytics to increase the limit?",
        "Robot directives": "Your domain likely has a robot directive (e.g.domain.com/robots.txt) that provides instructions to crawlers on how to crawl or index your content. Lytics will follow these directives. While typically not an issue, it\u2019s worth turning to your directives when troubleshooting any missing content or information.",
        "Metadata": "You may want to build a collection of content based on publish date or author. Consider the following if this is the case: If you have metadata on your website, is it usingOpenGraph? Open Graph tags will populate the following default values in Lytics:title,image,published_time,description, andlytics:topics. You can check thelytics_contentqueryin your account to check if all the Open Graph tags you need are being picked up. If you are not using Open Graph, Lytics may not be picking up any meta tags automatically. The quickest way to check that Lytics is bringing is to use our find a document tool to view the data Lytics has for a specific piece of content. Navigate toContent>Find a Documentand search for the URL of the piece of content you would like to view. If you make changes to your content's metadata and would like to preview those changes you can use our manual classification tool located at the bottom of theContent>Classificationdashboard. Enter the URL with the updated metadata and clickGet Details. Lytics will scan the document and display a preview of the updated metadata. If you are satisfied with the changes clickComplete Classification. If not, you can make additional edits and preview again. Alternatively, you can check by navigating toContent > Collections. Try to build content collections by author or publish date as these are the most commonly used filters. If the content doesn\u2019t come up as expected, you may need to curate your tags.",
        "Custom topics": "For many users outside of publishing who may not have rich content, NLP derived topics aren\u2019t enough. To accommodate this, Lytics can add custom topics via the metadata. The easiest way to do this is by adding thelytics:topicsmeta tag. Read more on providing custom topics. If you already have topics in your metadata using a different meta tag than the above, it\u2019s possible Lytics may be able to bring those in as well by making a change to your account settings. Speak with your implementation team about this. Once this setting is changed and content is rescanned, you will be able to build content collections with this topic, and users' affinities will be generated for these new topics.",
        "Total number of topics": "In the Lytics UI, you will see a max of 500 topics. Lytics keeps all of your topics, but only the top 500 are surfaced in the UI. As you block topics, Lytics will backfill to show 500.You can allow specific topics to ensure that they make the top 500. If you choose to allow topics, make sure that the topics actually exist - either as custom topics or are being generated by NLP. Case sensitivity is important when adding topics to the allow list. For example, allowingABBAis different than allowingAbba.",
        "Other content": "If you have other content on your site outside of web pages or images (e.g. PDFs) that you\u2019d like to derive topics from and have them generate affinities, Lytics will need to develop a plan to bring those in using our APIs.",
        "Document properties": "All topics - NLP derived or custom topics - will allow for two things: Building a content collection with the topics.Assign affinities to users for those topics. There are instances where you may want to build collections based on a topic, but not have them generate affinities. For example, a collection of featured sale items, SKUs, genres, etc. You can set these asdocument propertiesin your metadata to allow for this.",
        "Activating Data Models": "Because the queries that power your Cloud Connect Data Models can have costs associated with your database usage, Lytics doesn\u2019t run the scheduled sync automatically. After you've created a new Cloud Connect Data Model, you'll notice that the Status is listed as Inactive: In order to activate, click theActivatebutton on the Data Model page: Once you click theActivatebutton, your query will complete its first run within an hour of theNext Sync Ontimestamp listed under the Details tab and then continue to run at the interval you selected. During each sync, the SQL query will be run against your database, and your Lytics profiles will be updated according to the configuration defined on the Data Model. Once synced, theTotal Recordsmetric on the Data Model summary tab may take up to 2 hours to display. This number represents the number of rows returned by the SQL query.",
        "Data Model Field Creation": "Upon activation of a Data Model, Lytics will automatically create one customer profile field which represents the membership of a user for the Data Model. Additional fields will be created if the Data Model was configured to include Activated Fields. Below is a guide for how to locate these generated fields within a customer profile or within the Audience Builder (on theCustom Fieldstab)",
        "Deactivate Data Model": "If you need to disable a Cloud Connect Data Model from syncing at any point, you can do so by navigating to the Data Model and clicking theDeactivatebutton. Deactivating the Data Model will not remove its corresponding fields from schema or from Lytics profiles; however, it will cause the values for these fields to become out of date. In order to completely remove the fields from schema/profiles, the Data Model must be deleted.",
        "Total Records vs. Count of Users": "Throughout the app, you may see different metrics for Cloud Connect Data Models.",
        "Total Records within the Data Model Summary": "On the summary page for your Cloud Connect Data Model, under theTotal Recordsstatistic (shown in the graph below), you will see the number of records that are selected by the query based on the unique number of primary keys in the origin database. This may be equal to or less than the number of total rows imported and may be significantly higher than the number you see in the audience builder when utilizing this Data Model.",
        "Count of Users within the Audience Builder": "When adding Cloud Connect Data Model membership or activated fields to audience logic in the Audience Builder, you may see a smaller size than theTotal Recordson the summary. The count of users within the Audience Builder is the number of users in your Lytics account who met the qualifications for the query based on the matching Lytics user field configured on the Data Model. This number reflects the number of records that would be included if you were to run an export of the audience. Most common reasons for why this number may be smaller than theTotal Recordsmetric are the following: If the \"Create New Profiles\" option isn't selected on the Data Model, the count of users in the Audience Builder will only include profiles that already existed in your Lytics accountsAudience Builder will exclude any profiles which are marked as being Unhealthy. Learn more about unhealthy profiles here -https://docs.lytics.com/docs/user-profile-health#unhealthy-profiles",
        "Exporting Data Models": "Once activating a Data Model, its associated fields will be stored as user fields on the customer profile (to see the naming conventionhere) . Therefore, in order to export the members of the Data Model, you will first need tocreate an audiencein the Audience Builder. You can create an audience of only members or layer in additional audience rules to get the most out of your customer data. For example, you could build a Cloud Connect audience using account-level data for a B2B use case and add one of Lytics behavioral scores to refine your targeting based on your desired user engagement level. To learn how to export audiences, visit our Activating Audiences documentationhere.",
        "Enriching Users": "Using email addresses, Lytics will consume Clearbit data to provide more information about audience members. To get an example of the enrichment data returned by Clearbit, visithttps://clearbit.com/enrichment.",
        "Configuring enrichment of an audience": "To start enriching your audience select the audience you would like to enrich. Some audiences might havesetof emails addresses, so it's important to define which of their fields contains single email address.  Audience: The Lytics audience with the users to enrich.Email Field: Field name with email data. In almost all cases the default email field is already selected.Enrich Existing Users: SelectEnrich Existing Usersto enrich users who are already members of the audience. By default users will only be enriched as they enter the audience. In theAdvanced Optionssection: Max Calls per Month: Defines how many API calls this integration should use. ClickStart Importto start. IfEnrich Existing Usersis chosen, enrichment of those users will happen immediately. As new users are added to an audience, new enrichments will be triggered until the max calls limit is reached or the enrichment workflow is canceled.",
        "Build a content collection": "To make recommendations, you will need a content collection. You can see our documentation onbuilding content collectionsfor more information. Once saved, you will also need theid of the content collectionas you will use it in the next step.",
        "Add content recommendation API as a data feed": "To populate recommendations in the email, you will use the Iterable feature called a Data Feed to hook into the Lytics API before sending. Login to yourIterableaccount.Navigate toIntegrations > Data Feedsin the side bar.Click+ Create New Feed.Enter \"Lytics Content Recommendations\" or some other descriptive title in theNametext input.Make sure theJSONoption is selected from theFormatdropdown.Copy and paste the following URL into theURLtext input:https://api.lytics.io/api/content/recommend/user/email/{{#urlEncode}}{{email}}{{/urlEncode}}?contentsegment=At the end of that URL, paste the ID of the content collection you created in the previous step.  This ID is the hash in the URL, not the name or slug of the content collection.In theAuthorization Tokenfield paste a Lytics API token with read permission for content and user search.Verify that your settings look correct and then clickSave Feed. Enabling this data feed in a template allows Iterable to call the Lytics content recommendation API before each email send. Iterable will replace{{#urlEncode}}{{email}}{{/urlEncode}}with the URL encoded version of the recipient's email address. The next step will cover how to access the content returned and format it to be surfaced in the email template.",
        "Modify your email template": "In this example the header of the email will change based on audience membership. For every element we want to personalize we need to have three versions of the content: Users in theInterested in Dress Shoesaudience.Users in theInterested in Athletic Shoesaudience.Users in thePromotional Listwho have not shown interest in either shoe type (generic promotion). If a user is interested in both dress shoes and athletic shoes we can prioritize which to show based on the order of the conditionals we will set up in the template. We will opt to promote dress shoes in this example for that case.  Follow these steps to personalize your email template in this manor: Open your template in an HTML editor, locate the text, image or HTML element you want to change based on audience membership.Use the customLyticsAudiencefield and conditional logic to bind your HTML content in if/else statements. Take a look at the following example, which the example template uses to change the text on the banner of the example email:HTML<!-- custom headline -->\n[if:LyticsAudiences=Interested in Dress Shoes]Dress shoes for any occasion\n[elseif:LyticsAudiences=Interested in Athletic Shoes]Kick start your summer with new shoes\n[else]Shoes of all shapes and sizes you'll love[endif]Continue to edit your template to modularize the content of your email. In our example, we also switch out the image in the banner as well as the CTA link and text based on audience membership:HTML<!-- custom image -->\n[if:LyticsAudiences=Interested in Dress Shoes]<img src=\"dress-shoes.png\" alt=\"dress shoes\" />\n[elseif:LyticsAudiences=Interested in Athletic Shoes]<img src=\"athletic-shoes.png\" alt=\"athletic shoes\" />\n[else]<img src=\"default-shoes.png\" alt=\"shoes\" />[endif]\n\n...\n\n<!-- custom CTA -->\n[if:LyticsAudiences=Interested in Dress Shoes]<a href=\"https://shoepalace.com/products/dress-shoes\">Shop Dress Shoes</a>\n[elseif:LyticsAudiences=Interested in Athletic Shoes]<a href=\"https://shoepalace.com/products/athletic-shoes\">Shop Athletic Shoes</a>\n[else]<a href=\"https://shoepalace.com/products/shoes\">Shop New Shoes</a>[endif] In the next step you will be able to see the rendering of the templates in each of the three contexts.",
        "Previewing and verifying the recommendations": "Iterable allows you to preview a rendered version of your email in the browser with live data from your data feed. Follow the instructions inthis tutorialto preview your email with content recommendations.  Once you're happy with the preview, you are ready to send your campaign. You can implement your template into aworkflowor send it as ablast campaign.",
        "Rate Limiting": "For best results, please talk to your Iterable representative about setting up aMessage Send Rate Limit. The Lytics' Content Recommendation API can handle up to 250 requests per second, or 15,000 requests per minute. If the rate-limit is not enabled with Iterable, Lytics may return a 429 HTTP Status Code, preventing emails from being delivered.",
        "Available Segments (Audiences)": "The following audience segments are all available out of the box, with no customization necessary in all Lytics pricing tiers.",
        "Meta User": "Authorizing as a Meta user will guide you through an Oauth process to authorize access to ad accounts tied to that particular Facebook user. Using Oauth is fast and simple, but can be prone to failure in the long run as authorizations can be canceled if the user changes their password, leaves the organization, and some other authorization invalidating event. You will be prompted to login to Facebook and grant permissions. You will then need to enter a description for your authorization.Enter aLabelto identify your authorization.(Optional) Enter aDescriptionfor further context on your authorization.Complete the configuration steps needed for your authorization.ClickSave Authorization.",
        "Business Manager System User": "Authorizing with a Business Manager System is not tied to a specific user, so it is more reliable in the long-term than the User Oauth, and is valid until revoked in the Business Manager settings. This is therecommended authenticationmethod if possible. Before creating a system user, your Meta Business Manager must: Have an admin user.Own a Meta app. You should claim the app and associate it with a business via API or in Business Manager. To see your apps, visit theMeta Apps page. Please note that the app must not be listed as \"in development\". In development apps are not meant forproduction or live advertising. Your app must also be associated with the Ads Account ID you want to send audiences to. This can be configured underAdvanced Settingsfor your App. Once you satisfy these requirements, complete the following steps to set up your authorization: Go toBusiness Settingsin Meta. UnderUsersclickSystem Users, then clickAddto create a new System User.Give a name to the system user and selectEmployeeas the role, then clickCreate System User.Assign Ad Accounts to the System User by selectingAdd Assets. Then select the Ad Accounts, choose the Ad Account that you would like to push Lytics Audiences to, and then assign itAdmin Access - Manage Ad AccountNext you'll need to generate a token that will be entered into Lytics to make API calls with. To start this, selectGenerate New Token.Selectyourcompany's Meta developer appand make sure thatads_managementis checked.Then click onGenerate TokenThe generated token will be only shown once. Copy it, and keep it handy for authorizing in Lytics.Enter aLabelto identify your authorization.(Optional) Enter aDescriptionfor further context on your authorization.Enter theAccess Tokenyou created in step six.ClickSave Authorization.",
        "Audience Sizes in Meta": "Immediately after export, in the Meta UI you will see a size ofBelow 1000and under this you may see the wordPopulating. Even if the audience is much larger than 1000 users, Meta will always sayBelow 1000if it isPopulating. Meta says about this state:  This process may take a number of days to complete. If the exported audience has a lot of activity, such as an audience that has users enter/exit every hour, then the Meta status mayalwayssay \"Populating\" since the job is continually sending new data to Meta. If you would like to see the current side of the audience in the Meta UI, the only way is to pause the job in Lytics on theJob Summarypage for that job. Once you've seen the audience size and want to continue adding users, you can unpause the job on the same page by clickingResume. After resuming, the audience size may go back into the \"Populating\" and \"Below 1000\" state. In addition to the size shown, in the Meta UI, you can click through to the audience and see theCustom audience updatedmessages on the History tab to see the batches of users that were sent to Meta.",
        "Export Audiences (Web Traffic)": "Target people who visit your website using the Meta Pixel. Create audience definitions powered by Lytics that can be used as custom website audiences or lookalike audiences in Meta Ads.",
        "Setting up the JavaScript Tags": "If you haven't done so already, add the Meta Pixel to your website. See Meta documentation for assistanceadding the Meta Pixelandadding the Meta Pixel using Google Tag Manager.Also ensure that theLytics JavaScript taghas been installed. This integration needs both tags present to work properly.Once you have added the tag and pixel to your site,enable API accessfor the Lytics audience(s) you would like to target.You can verify that Meta is receiving Lytics audience membership via events. Use theMeta Pixel HelperChrome extension to help verify this:Proceed with the job steps below to make this user data available for targeting in Meta.",
        "Setting up the Job": "SelectMetafrom the list of providers.Select theExport Audiences (Web Traffic)job type from the list.Select the Authorization you would like to use orcreate a new one.Enter aLabelto identify this job you are creating in Lytics.(Optional) Enter aDescriptionfor further context on your job.From theMeta Ads Account IDinput, select the account ID for this custom audience.Note:If it says you \"must accept Custom Audiences Terms\" next to the Ads Account ID, copy and paste the following URL into your browser:https://business.facebook.com/ads/manage/customaudiences/tos/?act=YOUR_ADS_ACCOUNT_ID. ReplaceYOUR_ADS_ACCOUNT_IDwith the account id you wish to use. There you can accept the Meta Ads Custom Audience terms and conditions, and then refresh this list in the Lytics App.From thePixelinput, add your Pixel to automatically create aMeta Custom Web Traffic Audiencefor all public Lytics audiences.From theAudiencesinput, select a set of audiences to do a one-time export to Meta. If none are selected, a Meta Web Audience will be created for every public audience in Lytics.ClickStart Export. You should now see the website custom audiences in Meta Ads. They may already be populated if your audience was previously API enabled, or they may need time to populate as users visit your website. You can also build a lookalike audience with Lytics and Meta based on these users.",
        "Export Audiences (Conversion API)": "Send Lytics Audience events to theMeta Conversion APIto improve your ad campaign performance.",
        "Conversion Tracking": "As mentioned above, Lytics brings in the conversion count for a Meta Experience directly from the Meta API. You can also begin to track click-throughs byadding custom UTM parameters. Follow these steps to configure these custom parameters using Lytics, and you may also refer to theMeta documentationfor URL parameters. While editing your ad in Meta, locate theTrackingsection, and underURL Parametersclick onBuild a URL Parameter.It is recommended you fill out the default campaign UTM tracking variables, as Lytics will automatically capture these query parameters when users visit your website.UnderCustom Parameterscreate the following new parameters:Parameter Name:utm_lytics_experienceValue:{{adset.id}}This dynamic value will be populated with the ID of the ad set so that Lytics will recognize which Experience the user clicked on.Parameter Name:utm_lytics_sourceValue:metaClickApplyto save the changes to your URL parameters.ClickPublishto save the changes to your ad. After you've added these parameters and users begin clicking on your ad, you should see values populating theUTM conversions user fieldin Lytics.",
        "Viewing Templates": "Templates are available in the Lytics UI under theData Pipelinesection. The Templates Overview page shows all the templates in an account, as well as an action button to create a new template.",
        "Activating Templates": "To activate a template, create aWebhook Export job. See theLytics Webhooks documentationfor full instructions on configuring outgoing webhooks. From theLytics Templatedropdown menu, select the template you would like to activate against each outgoing request.",
        "Using Templates": "Once you have created your first Template, you can find it in the Template List view, and click on it to access the individual Template. From this view, you can create Webhooks to export users or events using your Template by clicking on the+ Create Newbutton. This view will show all of the Webhooks associated with your Template.",
        "Import Users & Activity Data": "Importing your BigCommerce customers and their order activity data into Lytics enables you to run personalized marketing campaigns for your BigCommerce customers.",
        "Styling the Video Modal": "With some additional CSS work, you can style the video to fit your modal.Download the code for this exampleto see how you can style the video to be side by side with the text of your modal.  Once you've tested and are happy with the look and feel of your video modal, it's time for the most important step: defining the Lytics audience that should receive this modal.",
        "Let Lytics Build Your Audiences": "Using the LyticsJavaScript Tag, Lytics will catalog your website content in real-time and group individuals based on their interest levels for particular topics or products. Additionally, using behavioral algorithms, the platform will analyze and score your customers by their overall engagement level across your communication channels, for example 'highly engaged'.  As your audiences change over time, use Lytics as a centralized hub for automatically syncing them with your connected advertising tools such as, Facebook Ads, AdRoll, Google Ads, or DoubleClick for Publishers, for an up-to-date view of your target customers.",
        "Refine Your Target Audience": "TheLytics Query Language (LQL)allows you to refine your target audience and remove customers with certain profile attributes or behaviors, enabling further efficiencies in your ad spend.",
        "Targeted Ad Campaigns": "Examples of how Lytics audiences can help with your most common targeted advertising. Smarter 'lookalike' prospecting with Facebook\u201cLookalike\u201d audiencesbased on your most valued customers\u2019 behaviors, engagement scores, and lifetime value.Target your users with content they are interested inusing Lytics topics and content affinity scores for building your audiences.",
        "Salesforce Data Management Platform (Krux)": "Connecting Lytics and Salesforce Data Management Platform (DMP) allows you to use Lytics audiences and behavioral data within the Salesforce DMP ecosystem. This is achieved through a two-step process. First, Lytics uses the Salesforce DMP pixel to send our_uid, or unique web indentifier, to Salesforce DMP linking the user. Next, a CSV with selected Lytics user fields and_uidis exported from Lytics and imported into Salesforce DMP addding additional user attributes and enhancing targeting capabilities. This article covers: Supported FeaturesEnable the Salesforce DMP Pixel Sync for your AccountShare Audience Membership and Profile Attributes to Salesforce DMP",
        "Enable the Salesforce DMP Pixel Sync for your Account": "Log into yourLytics account.Click your account name at the top right corner of your Lytics dashboard.SelectAccount Settings.SelectTagto view the Lytics JavaScript Tag setttings.To enable Salesforce DMP ensure the checkbox next toAutomatically place the Krux sync img tag onto your site?has been checked.ClickSave. When a user visits your site the Lytics JavaScript tag will automatically pass the unique_uidto Salesforce DMP. You can confirm that this is happening by searching for the Krux pixel on your site:https://beacon.krxd.net/1x1_usermatch.gif?partner=lytics&partner_uid=<your _uid>",
        "Share audience membership and user fields to Salesforce DMP": "This feature is currently in beta and requires a CSV with a particular format to be generated in order to import data to Salesforce DMP. Though it is possible to export a standard CSV from Lytics and then reformat the CSV using alternative tools we recommend contacting our success team for assistance and further information on this process. Additional information related to the CSV format.",
        "Viewing Affinities": "To view yourAffinities, selectInterest EnginesunderContentin the Lytics navigation bar. At the bottom of the page, you\u2019ll find a table displaying theAffinitiesin your account. You can filter the list byInterest Engineor search for a specific Affinity. This table presents theName,Description,Topics,Last Modified Date, andInterest Engineassociated with each Affinity. To view the Affinities tied to a particular Interest Engine, simply navigate to the Interest Engine page.",
        "Creating Affinities": "Affinities allow you to group related Topics into a targetable unit, making it easier to reach your customers based on their interest groups. This document will walk you through how to use the Affinity Builder step-by-step. Once an Affinity is created, you can use it to build Audiences and content collections. To create an Affinity, navigate to theInterest Enginespage underContent, select theStandardtile, and click on+ Create New. Add aNamefor your Affinity. This will be displayed in the Affinity tab of the audience builder.Add aDescriptionfor more context. The name and description fields are required.Assign Topicsto your Affinity. There are a few ways to explore and add Topics.Use the filter to quickly findCommon Topics. Lytics surfaces the Topics that are most prevalent in your taxonomy, based on the number of documents with that Topic. The default limit for Common Topics is 500.Click on a row to see related Topics.Select the checkbox next to a Topic to add it to your Affinity.Use the Search to find any Topic in your account. The search option gives youaccess to any Topic in your taxonomy, even if it's not included in your list of Common Topics. This means you don't need to allow or block certain Topics to make the ones you care about available in the UI. As you select Topics, they will be added to the Assigned Topics list on the right. In this example, we created an Affinity named \"Concert Affinity\" that included Topics such as \"Concerts\", \"Live Music\" and \"Performances\". There are two ways you can remove selected Topics from your Affinity. Hover over the Topic in the Assigned Topics list and clickRemove Topic.Unselect the checkbox next to the Topic within the table view. Once an Affinity has been created, you can add or remove Topics at a later time, and edit the name or description.",
        "Affinity Summary": "EachAffinityhas a summary page that provides an overview of how many users are interested in this group of Topics and how much of your brand's content or product inventory is represented. After creating an Affinity, or when clicking any row in the Affinity list, you'll be directed to this summary page. At the top of the Affinity Summary page, you\u2019ll find the following details: Owner: The Lytics user who created the Affinity.Created: The date the Affinity was first created.Updated: The date the Affinity was last modified.",
        "Chart": "After creating an Affinity, it may take a few days for the chart to fully populate as Lytics evaluates users' interest in the selected Topics. To the left of the chart, you\u2019ll find key metrics:: Total Documents:The number of documents that include one or more Topics from this Affinity.Users with Affinity:The number of users who have shown interest in one or more Topics in this Affinity. On the chart, the X axis shows affinity scores on a scale from 0-100, where 0 represents \"no interest\" and 100 represents the \"highest interest.\" The Y axis displays the number of users. You can click on individual bars to see how many users fall within specific affinity ranges. This data helps your marketing team identify which users to target based on their interest levels. For example, you can focus on users with medium-to-high affinity for certain Topics in an ad campaign, increasing the likelihood of relevance and conversions.",
        "Sample Documents": "At the bottom of the summary page, you will see a paginated list of sample documents for this Affinity. Your content must beclassifiedbefore it will appear in this list.",
        "Downloading a Customer Profile": "GDPRrequires the right of access and data portability. You can use the Lytics platform to download a JSON file of a customer's profile. You can also use the Lytics API to download a customer profile. Log into yourLytics account.ClickAudience>Find a user.Select the field you would like to search on from the drop-down menu. By default,Emailis selected.In theSearchbox, please enter the email address (or selected search term) of the customer who has requested their profile.Select the desired customer from the results list.ClickDownload profile. You will receive a success message andprofile.jsonwill begin to download.",
        "Deleting a Customer Profile": "TheGDPRandCCPArequirements grant the user in scope to request the removal of their personal data without undue delay. This guide provides instructions on how to delete a profile from Lytics via the Lytics UI. In addition, you can use the Lytics API to delete a customer profile. A successful deletion request of a profile using the Lytics APIwill notupdate the status of a customer in the Lytics UI until the profile has been removed from Lytics.  The removal is handled in background batch processes that will be fulfilled within 14 days. Log into yourLytics account.ClickAudience>Find a user.Select the field you would like to search from the drop-down menu. By default,Emailis selected.In theSearchbox, enter the customer requesting removal's email address (or selected search term).Select the desired customer from the results list.ClickDelete user.A confirmation message will pop up; clickDelete this userto confirm.There will be a Request ID in this message totrack or audit the success of the GDPR deletion request.The customer will be marked for deletion and removed from Lytics within the GDPR-required timeframe.",
        "Import Table": "Import user data directly from your AWS Redshift database into Lytics, resulting in new user profiles or updates to fields on existing profiles.",
        "Import User and Activity Data": "Importing your iContact contacts into Lytics is the first step to using Lytics powerful insights to market to your users smarter. Pull in your user's activity and profile data from iContact to get a complete picture of how your users engauge with your email campaigns.",
        "Export Audience to List": "Export a Lytics audience to an iContact list to make use of your Lytics powered audiences in iContact.",
        "Google Sheets OAuth": "This authorization allows us to access the Google Sheet service for your Google drive account. SelectGoogle Sheets Oauthauthorization method.From the user selection window, select the Google account you want to use from the list of accounts.ClickAllow.",
        "Import Data": "Importing custom data stored in Azure Blob Storage to Lytics allows you to leverage Insights provided by Lytics data science to drive your marketing efforts.",
        "Leveraging Affinities": "The Affinity Engine enriches user profiles based on behavior, but its true value lies in how you act on that data. In Lytics, Affinity Engine insights can be applied in three key ways: segmentation, recommendation, and Lookalike Models.",
        "Segmentation": "Businesses can better understand their customers and create more effective campaigns by breaking down audiences into smaller, more targeted segments. Here we will discuss best practices for constructing \"building block\" audiences related to consent that can then be leveraged in campaign audiences. This progressive approach ensures consent rules are enforced while minimizing ongoing maintenance overhead.",
        "Recommendation and Personalization": "Lytics' Recommendation system blends AI with marketing strategy to help you easily suggest relevant content and products to your users. By setting up personalized Recommendations, you can deliver more engaging user experiences, which in turn boosts time on-site and overall engagement. Lytics makes it easy to get up and running with Content Recommendations. Upon configuration, our Recommendation system: Leverages over 500 behavioral signals to deliver highly relevant recommendations.Operates autonomously, with models that retrain and optimize on a weekly basis to keep recommendations fresh.Works effectively right out-of-the-box for new users, requiring little to no prior data to provide valuable content suggestions.Scales rapidly to meet the demands of large volumes and high user traffic. To get started, navigate to thePersonalizepage underUsing Profiles, or consult theContent Recommendation API documentation",
        "Lookalike Models": "Affinity Engine scores are also highly valuable when building Lookalike Models. These scores offer a standardized way to compare users, creating data consistency across profiles, which is crucial for machine learning models. This consistency allows affinity scores to often emerge as key factors in Lookalike Models. If your team is working on predictive audiences, leveraging Affinities can significantly enhance the accuracy and effectiveness of your models.",
        "Import Products": "Importing Shopify products allows you to enrich your use ofAffinitieswith product data from your Shopify store. Serve your customers with relevant messaging and product recommendations based on how they have engaged with your brand.",
        "1. Targeting": "Though it may be obvious, considering who you're targeting with modals is extremely important and often overlooked. Put yourselves in the visitor's shoes. If, for example, you are visiting a site for the very first time, it is unlikely your experience will be positive if the vast majority of the site that you are visiting is covered with a giant offer modal as soon as the page loads. Example of full page takeover on the second click of a user's first visit before they had a chance to explore the brand. Example of half page takeover on user's first visit. Added for comedic value. First page view popup on article talking about discussion one of Google's many announcements to penalize such experiences. Because Lytics allows you to target users based onprofile details, audience membership and behavior in real-time, perhaps you take a more subtle approach for new customers while being a bit more bold with visitors showing strong signs of conversion or history with your brand. It is all too common on the web today to see companies take a \"spray and pray\" marketing approach where all visitors get blasted by intrusive data collection or offer modals at the beginning of their experience. This is seen as a negative for nearly all search and ad providers, many of which addressed this particular topic publicly. Google Blog Post Addressing Mobile Popups",
        "2. Modal Size & Position": "To build onTargeting, there may be cases where delivering a message at the beginning of a visit is important and actually improves the experience. Consider the size and position of where that message is delivered. Does it cover prime real estate requiring the user to alter their patterns in order to see the content they want? Does it leverage a small slide-out and offer a more efficient path to value while not restricting consumption? There is no silver bullet here but testing various applications to determine the right balance can yield great results.",
        "3. Device": "The device being used is often not considered when attempting to locate the root cause of a negative change. Keep in mind that modals and the site experience in general can be drastically different from device to device. A modal that is not intrusive on desktop may cover the entire canvas on mobile. Be sure to leverage your analytics tools to determine what devices are suffering from a drop off. Follow up by altering the targeting and presentation based on device or consider shutting web personalization modals off entirely in the case of mobile. Example of cookie acceptance modal on desktop - small and not disruptive. Example of cookie acceptance modal on mobile - much larger and more disruptive.",
        "4. Time on Site & Session Depth": "As we mentioned in our first example where the visitor received an offer immediately upon their first page view, one thing to consider and test is when a modal is shown. Lytics offers a variety ofdisplay optionswhich allow you to only show the modal after a set amount of seconds, after some amount of scrolling, etc. All of these levers can ensure a message is delivered after a visitor has made a connection and consumed enough content in order to make the modal experience positive.",
        "5. Inline Personalization": "The Lyticsweb personalization SDKis one method for delivering personalized communications on site. It was designed to offer a light lift and quick time to value but may not meet the needs of every use case. As such, theLytics JavaScript tagand variousPersonalizationAPIs can be leveraged in more custom applications to offer hyper targeting and personalized content in virtually any situation. At the end of the day, the user experience is everything and this should be a core focus of all marketing applications when looking to get the most out of ad and search related tools.",
        "Lytics Engagement Audiences": "Engagement: First-time Visitors:Users who are visiting for the first time. Definition: visitct = 1.Engagement: Casual Visitors:Users who show little activity when they do interact with your brand. Definition: score_intensity < 25.Engagement: Moderately Engaged Visitors:Users who show average activity when they do interact with your brand. Definition: score_intensity > 24 and score_intensity < 76.Engagement: Deeply Engaged Users:Users who show a lot of activity when they do interact with your brand. Definition: score_intensity > 75.Engagement: Repeat Visitors:Users who have visited multiple times. Definition: visitct > 1. To learn more about Lytics' prebuild audiences and for more detailed information about each prebuilt audience, please refer to the documentation in theBehavioral Audiencessection.",
        "Access": "Cloud Connect tabs are found underData Pipeline>Cloud Connect. Connections configure the access to your data warehouses, and Data Models configure the SQL queries that are run to connect audience membership and profile attributes.",
        "Creating a Connection": "Click+ Create New Connectionfrom the Connections Dashboard at the top right and complete the following steps. Choose the provider.Choose the Connection type.Select an existing Authorization or create a new one by following theAuthorization instructions.Add a name (label), description, and complete the configuration options. These will vary slightly between providers.",
        "Authorization & Security": "The authorization selected for your Connection will control your Lytics account users' access to your data warehouse. You can control whether a user has read access to the entire dataset or individual tables, maintaining your security and governance practices within your data warehouse.",
        "Supported Data Warehouses": "Cloud Connect currently supports a number of popular data warehouses: Amazon RedshiftDatabricksGoogle BigQueryMicrosoft Azure SQL DatabaseSnowflake",
        "Managing Connections": "Once you have created a Connection, you can access a summary page showing how data from your data warehouse is being leveraged in Lytics. At the top of the page, you\u2019ll see the following information: Provider: Data warehouse that you are connecting with Lytics.Authorization: Name of the authorization, such as \u201cCloud Connect JWT.\u201d Note: Lytics users can access any data tables that the Connection Authorization has read access to.Type: Indicates the type based on your provider.Created By: Lytics user who created the Connection.Created On: Date the Connection was initially created.Last Updated: Date the Connection was most recently edited. The rest of the Summary tab shows how many active and inactive data models are built using this Connection as a data source and how many tables are accessible from this dataset. The Activity chart displays how many rows are being queried, which can have cost implications based on your data warehouse usage.",
        "Explore": "The Explore tab provides a simple Schema Explorer to validate that the data shown is as you would expect to see in your data warehouse. In the example below, we only connected an individual table, but here you will see as many tables as the authorization has read access to.",
        "Identify Key Topics": "The Lyticscontent affinity enginebuilds atopic taxonomyfrom content on your website. Lytics will use this taxonomy and user activity data to determine a user's affinity for a topic. These content affinity scores are available in the audience builder, and will be core to building your target audience. Select the piece of content you wish to promote. Then take some time to familiarize yourself with the topics in your Lytics account and try to identify the Lytics topics that are relevant to your content. You can sort by the average level of affinity on the topic list view to ensure you are using a popular topic for targeting criteria. You may choose one or more of these topics to include in anAffinity, which you can then use to create audiences. Clicking on a topic displays a summary page which shows a full distribution of user interest. ",
        "Build an Audience Using Content Affinity": "For most marketing use cases, it's useful to target users based on their interests in a group of related topics. For this reason, Lytics enables you to create your own, brand-specific Affinities. You can also target users based on their interest in an individual topic. See ourAudience Builder documentationfor more information. Once you have your Affinity rules included, verify that the user count of your audience looks correct. If you plan to use this as a source audience for a lookalike, Facebook expects anywhere from1,000 to 50,000 users. For a simple retargeting audience, you'll want to reach a broad number of users. You may adjust the sliders on your affinity rules accordingly.",
        "Using your Lytics Audience in an Ad Campaign": "To use your Lytics content affinity audience in Facebook lookalike ads campaign, Follow through with the steps below: Export to FacebookCreate a Facebook Lookalike AudienceAssign your Lookalike Audience to an Ad To run a retargeting ad to the users in this audience: Export to Facebookas a custom audience.Create your ad following your normal process. If you are building an ad for the first time, follow theFacebook Ad creation guidelines.Select the custom audience created by the export during the audience set up process of ad creation. ",
        "Topics & Affinities": "Topics & Affinitiescan be viewed under their respective tabs. The Topics view shows all of the Topics associated with the Default Interest Engine. Clicking on a Topic will present more information a specific Topic. Similarly, the Affinities tab displays all the Affinities associated with the Default Interest Engine.",
        "Using the Default Interest Engine": "Since the Default Interest Engine is designed to work quickly out-of-the-box for new accounts, you can quickly take advantage of all of the offerings provided. Interest-based user segmentation. Leverage Topics (via thelytics_contentfield) and Affinities (via thelytics_rollupfield) to create powerful interest-based audiences.Recommendations. As Lytics gathers all information about your webpages through the Javascript tag, Lytics can power dynamic Recommendations based on user interests.Export to other tools! The NLP-based Topics and user interests (saved in thelytics_contentfield) provide powerful insights that can be useful in downstream dashboard and analysis tools. This data can also be exported to email tools for powerful email personalization.",
        "Find a user": "There are a number of circumstances where you may need to search for a user. For example, you built a \u201ctest\u201d user to validate a campaign is working correctly, you want to see how your marketing efforts are influencing individual user's behavior, or you need to view/delete a profile for GDPR compliance. Follow these steps to search for a user profile in Lytics: In the app, selectAudiencesfrom the navigation menuSelectFind a userfrom the expanded menuSelect the field you would like to search with from the drop-down menu. By default,Emailis selected.In theSearchbox, enter the email address (or selected search term) of interest. If you find multiple profiles for a single email (or any other identifier), this may indicate data fragments haven't been merged together yet or that your identity resolution strategy is not configured correctly. Please contactLytics Supportfor assistance.",
        "Account Details": "Configuration options related to your primary account. Here, you can adjust your account name and primary contact and access your account ID or AID.",
        "JavaScript Tag": "All default client-side integrations somehow utilize the Lytics JavaScript tag. The tag implements functions for sending data to a data stream and receiving user profile data which has been surfaced. Behind the scenes, these functions utilize some of the public Lytics API endpoints.",
        "Lytics API": "Configuration options related to the Lytics Personalization API. Here, you'll be able to manage detailed access settings, what data is surfaced, and more.",
        "Content": "The Lytics Content Affinity Engine uses data science and Natural Language Processing to analyze content, extract topics, and calculate user-level topic affinities.",
        "Security": "Configuration options related to account security, such as password complexity, session duration, and two-factor authentication.",
        "AI & Modeling Controls": "Configuration options related to Lytics' suite of AI and modeling capabilities such as private fields.",
        "Schema Controls": "Configuration options related to your core profile schema",
        "Custom Data Sources": "Lytics can onboard and process data from custom data sources, helping to build out furtherUser Profileswith data specific to your company. Custom data is often the most valuable for your marketing efforts as itallows you to personalize web experiences and build meaningful audiences using data unique to your users.",
        "Onboarding process": "Unlike data collected from web activity and integrated marketing tools which automatically map data to Lytics user fields, custom data sources require an evaluation, planning, and implementation process. The Lytics team will assist in identifying use cases and the data fields needed to achieve them. Our CE team will then produce a data dictionary that describes the structure, format, location and relationship of your data. The data dictionary is used to writequerieswhich map custom data fields to Lytics user fields as it is ingested. Finally your custom data is uploaded to the Lytics and made available for use to build audiences, personalize web experiences, or export to marketing tools.",
        "Uploading custom data": "Data can be uploaded to Lytics via CSV SFTP upload or ourcollection APIs. Lytics can ingest both large, bulk uploads of user data and smaller, real-time uploads of event data. Before uploading data to Lytics, ensure all necessary LQL queries are in place or it will not be mapped to Lytics user fields, preventing identity resolution. When considering large uploads to the Lytics platform, be aware that this may cause an event backlog on your account. This occurs as our platform queues incoming information as it processes the content of the upload. Processing begins when the upload completes and can last considerably longer than the initial upload. This can result in a delay in seeing information from other sources in your account as well as delays in any outbound, trigger-based workflows.",
        "Custom Data Ingestion": "Getting your custom data ready to be properly ingested is crucial during your onboarding process with Lytics. This document gives an overview of the formatting requirements for custom data sources sent to Lytics via batch CSV or JSON, utilizingS3orSFTPimport workflows, as well as batch or real-time imports using ourcollection APIs. File namingFile compressionField formattingHeadersTimestampsJSON formatting",
        "File naming": "When you have a recurring bulk import, fromS3orSFTPfor example, you must consistently follow naming conventions to ensure your data is ingested and displayed correctly. Keep file naming consistent by determining casing and spacing.E.g. all lower-case, use underscores for spaces:file_source_1_date.Name each successive file with an identical 'root' along with a time-based suffix such asYYYYMMDDinfiletitle-20191119.csvor a UNIX timestampfiletitle-1678297720.tab.Lytics will import files that match the 'root' filename and will use the modified timestamp of the file to determine the order to import files and to determine which new files to import for continuous imports.",
        "File compression": "If needed, files may be compressed using the zip format prior to ingestion. The zip file will be decompressed and deleted after the content has been ingestied.",
        "Field formatting": "Phone numbers should be standardized. Lytics suggests normalizing phone numbers in a format such as12223334444.Omit double quotes or escape quotes.Omit newlines.Keep all free form text in quotes if possible.Avoid page breaks or special characters. For more, reference thebasic rulesfor CSV files.",
        "Headers": "Keep headers consistent across your organization and your vendors by determining casing and spacing.Column headers have to match the sample file exactly.When adding a new source, review current mappings and headers to determine if any headers need to be mapped or consolidated into the same field in Lytics.E.g. if the fieldmobilecomes in from source A, and the fieldcellcomes in from source B, it\u2019s likely these should be mapped to the same Lytics field, which will require anLQLmodification.",
        "Timestamps": "Lytics is able to ingest data in a different order than the events described in that data transpired. For this to work, individual JSON or CSV records must have a timestamp associated with them. For workflow-based imports, select the key/column which contains the event timestamp when import workflows are configured. For Collection API imports, use the timestamp field URL parameter. When Lytics looks for new files, it will choose first based on the file'slast modified date. If multiple files have the same last modified date, the date stamp in the file name is used to select the next file to import.",
        "JSON formatting": "JSON file formatting varies slightly depending on the method and nature of the data to import. See examples for bulk and real-time imports below.",
        "Bulk": "Files imported via S3, SFTP, or bulk collection should benewline delimited, meaning each object represents a single record/event, and there is a newline separating them.",
        "Real-time": "A number of different integration types are considered real-time (or \"near\" real-time). In that, user data is sent or received downstream as soon as the data is available in the native system. The following types of integrations are considered real-time: Client-Side Integrations- All client-side integrations are considered real-time unless otherwise stated in the documentation. Data collected via a pull integration is captured in the browser while the user performs actions, and push integrations always contain the most up-to-date data from the user profile.Webhooks- As stated above, webhooks are API calls that happen after an event has occurred, often used to notify downstream tools.Jobs Using Triggers- As stated above, jobs utilizing triggers receive and may push events downstream in real-time. Depending on rate limits or other such limitations of the third-party in question, the job may queue a small batch of events over a short amount of time before sending them, hence \"near\" real-time.",
        "LinkedIn: Export Audiences to New Conversion Rule": "Connect to LinkedIn's Conversion API using a new conversion rule to measure the performance of your LinkedIn campaigns. Integration DetailsFieldsConfiguration",
        "LinkedIn: Export Audiences to Existing Conversion Rule": "Connect to LinkedIn's Conversion API using an existing conversion rule to measure the performance of your LinkedIn campaigns. Integration DetailsFieldsConfiguration",
        "Advantages of This Approach": "This strategy leverages the Zero Copy approach, utilizing Lytics' identification and data science capabilities. Hence, it transcends the limitations of data stored in the customer warehouse. Cloud Connect audiences can be enriched with Lytics' behavioral scores, profile affinities, content recommendations, and predictive models alongside Lytics' top-tier identity solution. Consequently, Lytics' customers can implement a dynamic, maintenance-light activation strategy over time.",
        "Better targeting criteria": "Predictive Audiences simplify your targeting strategies by eliminating the need to manually define parameters to sort your customers. Lytics gives you direct insight to help answer the question, \"how do I know what to target users on?\" Each Lookalike Model in Lytics has a Feature Importance chart that shows the mostimportant attributes that predict user conversionsfor your source and target audiences. These attributes include Lyticsbehavioral scores,content affinities, and otheruser fields.  In the example above, Lytics created a Lookalike Model to understand which of our documentation site users are mostly likely to move from \"Casual Visitors\" to \"Deeply Engaged Users\". As you can see in the chart, our model determined that \"Total Pageview Count\", \"Quantity\", and an affinity for \"marketing\" are the top three attributes of users mostly likely to become deeply engaged. Your Lookalike Models may simply confirm assumptions, but in others cases, they can offer a new understanding of your customers' behaviors and which factors really influence desired actions.",
        "Optimize based on goals": "Once a Lookalike Model is built, you can easily create different Predictive Audiences to optimize the balance between reaching more people and targeting your best customers for each campaign. When reach is a priority, you can build Predictive Audiences with higher reach but lower accuracy to target more users. When Return on Ad Spend (ROAS) is a priority, you can build Predictive Audiences with higher accuracy but lower reach to target only the users mostly likely to convert.  For example, if you're targeting single purchasers to encourage them to become repeat purchasers, accuracy may be more important than reach. Learn more about how to optimize the trade-off betweenaccuracy vs. reach.",
        "Use cases across the customer lifecycle": "Predictive Audiences help you engage and convert customers across their lifecycle, from anonymous users visiting your website for the first time, to high lifetime value (LTV) customers that are brand advocates. Below are common examples of how to build Lookalike Models that power Predictive Audiences through different stages of your marketing funnel. Earlier stages: Which of my anonymous users are most likely to convert to known users?Which of my users with free trials are most likely to convert to subscribers? Mid stages: Which of my single purchasers are most likely to become multi-purchasers?Which of my highly engaged users are most likely to become subscribers? Later stages: Which of my subscribers are most likely to churn?Which of my engaged users are likely to make a purchase soon and do not require additional marketing?",
        "Setup the audience": "In this example, you\u2019re going to send a personalized text message to a user when they become highly engaged determined byLytics behavioral scores. Additionally, you only want to send messages to non-subscribed users who have a phone number. Therefore, you are going to need to create an audience with three criteria. First, only users who are (or become) deeply engaged based on Lytics data-science scores. Second, their profile has a phone number. And third, they are not yet a subscriber as the text message will include a sign up link. In the audience builder, you're going to use anexisting audiencefor highly engaged users and twocustom rulesfor subscribers and phone numbers. For additional instructions on building audiences, check outthe audience builder docs. NOTE:Twilio's API requires phone numbers with a country code (e.g. +11234567890). If phone numbers are not stored in this format in the user's Lytics profile the cloud function can be used to correctly format the number. ",
        "Create the cloud function": "This example uses Google Cloud Functions to power this custom workflow, however this will work just as well with other serverless tools like AWS Lambda or Azure Functions. For a quickstart on Google Cloud Functions seeQuickstart: Using the gcloud Command-Line Tool. The Lytics webhook sends the full profile of a user when they enter or exit the audience. The code below receives the profile in the body of the request, then uses the Twilio API to send a custom text message to the user. Note:You'll also need aTwilio account. When using a Twilio free trial, you can only send text messages to the numbers in yourVerified Phone Number list. Next, deploy the function. You can get your account_sid and auth_token from the Twilio dashboard. After deploying the cloud function, you will be provided with an endpoint that triggers the function. You can also find this in the Cloud Functions console: ",
        "Create the Webhook": "Next, you need to export the audience previously created. Return toLyticsand navigate toData>Integrationsand selectWebhooks. Next, clickNew WorkflowthenSegment Triggers Webhook, select the audience created previously moving it to the right column, enter the url trigger for your cloud function into theWebhook URLfield, and clickStart Export. You should received a message that your webhook was successfully created. When a user enters or exits the selected audience, the Lytics webhook will send a POST request to the provided URL, triggering the cloud function.  That\u2019s it! You\u2019ve successfully created a webhook trigger that will automatically send a text message to unsubscribed users when they enter the highly engaged audience.",
        "About the Lytics SDK for iOS": "Approximate Installation Size:3.2 MB (includes Swift Concurrency)iOS Version Support:14.0+Xcode Version Required:v14.2 or Higher",
        "IDFA (Identifier for Advertisers)": "IDFAoradvertisingIdentifieris an alphanumeric string that is unique to each device. Collection of this identifier requires explicit confirmation from the app user, which the Lytics SDK for iOS can trigger. In addition, upon confirmation, the IDFA will be treated as a strong identifier and included with all outbound events emitted to Lytics streams for improved resolution. The Lytics SDK for iOS offers two methods to collect IDFA and handle the opt-out of IDFA. To present a required modal asking for tracking access and storage/inclusion of the device-specific IDFA upon confirmation, call the following method: To disable the storing and emitting of the user's device specific IDFA, call the following method:",
        "Defining a Global Stylesheet": "If you are planning to implement many web Experiences on your site, you will want to establish a stylesheet that will lay out the foundation for common CSS rules among them. It may help to generate an Experience or Pathfora config of each layout, and think about what you want to augment for each. The example JavaScript configuration in this guide includes a slideout, bar, gate, and modal widget to test against.    The stylesheet in this guide will be kept light, by simply adding a background image to all widgets and tweaking some colors and font styles and sizes of common widget elements. However as you can imagine, you can go even further to change any aspect of the widget including the size or positioning. For reference, thePathfora SDK documentationprovides a robust list of all of the class names of elements used in Pathfora widgets. Start by adding some general styles you would like to apply to all widgets.pf-widgetis the class name applied to the  highest leveldivelement of all Pathfora widgets.pf-widget-contentis the first child element. For slideouts, thepf-widget-contentis essentially wraps the same area aspf-widget. However for modal and gate style widgets,pf-widgetwraps the entire page with a background overlay whilepf-widget-contentcontains the modal content itself.  There's a problem though, widgets with the bar layout don't have apf-widget-contentdiv. So you can adjust the selector for the above styles as follows:  Next apply some additional styles to the call to action buttons.pf-widget-btnis the class name used on all Pathfora buttons, howeverpf-widget-okselects the first, primary CTA button. Lastly add some styles to match the color of the headline with the widget text, and minimize the size of the thewidget footer textwhen it is not being used.  These styles will cover future widgets you wish to build on the website. As you are building your own stylesheet, if you run into issues, while testing your CSS please refer to thetroubleshooting section.",
        "Widget Specific Styles": "Using the Experience Editor or the Pathfora SDK, you can define a custom class name for your widget. With this class name, you can style specific elements that are unique to that widget. Always preface your selector with thepf-widgetclass name to ensure that you don't accidentally effect other elements on your website: For a full in-depth example of specific widget styles, see theimage customization guide.",
        "Troubleshooting and Selector Specificity": "A common problem when writing CSS for Pathfora widgets is providing the correct level of specificity when selecting the element to style. If you've ever written a line of CSS and expected to see the change applied but didn't - this section is for you. The default CSS that the Pathfora library ships with is deliberately specific so that it is careful to not effect any other part of your website. The main side effect of this is that developers wishing to override the default styles must pay special attention to their selectors or employ the use of!importantto override the default styles of the library. Consider the following example using!important: The selector here is short and simple. However, because the the boldfont-weightis embedded in a deeper selector in the Pathfora base styles, you have to override it with!important. For reference, these is the relevant snippet of styles in the Pathfora CSS: Now consider how you would need to write this CSS without using!important: This is a ridiculously long selector! However the advantage is you no longer need to know which styles need to be overwritten with!important. Both techniques are valid and will result in the same changes to your widget. Feel free to use whatever method is easiest for you. Keep in mind if you are using SASS, LESS, or some other CSS pre-processor that allows for nested selectors, what would longer selectors may be easier to interpret. If you're not a fan of!important, continue reading for an in-depth look at how to easily derive the required level of specificity for your selector.",
        "Utilizing Chrome Developer Tools": "The best way to determine how to override a default style is to look the conflicting style from the CSS of the library. You can access the style sheet in thePathfora GitHub repositorywhich is open source, but it may be easier to debug your CSS using theInspect Elementfeature in Google Chrome developer tools.  Locate the relevant element in your widget by clicking on it while in the element selection mode in developer tools. You may also find it by navigating through the HTML source under theElementstab. Examine theStylestab for that element to see where the conflicting style is. Once you've located it, copy the selector and paste it in to your style sheet for modification. Then simply add the custom class of your widget accordingly to the selector: Add your rules, save your stylesheet, and test that the changes in the browser. Remember, you can always consult theLytics Support teamfor assistance when debugging CSS for your Web Personalize Experiences.",
        "Stitching User Profiles": "This guide gives context on Lytics user profiles and helps illustrate the most common ways to combine orstitchprofiles based on shared identifiers. While it is geared for marketers, you will likely need a developer on your team to implement some of these tactics.",
        "Common identifiers": "A common identifier is used by Lytics to create a user profile, but it\u2019s one that also exists in another source. Some examples include: All of these users will merge into one profile in Lytics because they share identifiers. This will happen automatically as soon as these data points are imported. The import is themerge event.",
        "Merging events": "But how would you merge an anonymous profile that ONLY has a cookie ID with a profile that has a known identifier like email? This is one of the most common use cases for Lytics users, and one that will set you up for success. To do this, you\u2019ll need to identify or create potential merge events. Again, more details can be found in the Profiles and Identity Resolution document, but here we\u2019ll provide examples of common scenarios that you may consider when planning your strategy.",
        "Lytics Modal Experiences": "You can build Email Capture Modal Experiences directly in Lytics that prompt users to sign up for an e-newsletter, or receive a coupon, for example. As soon as a user fills in their email, Lytics will tie the anonymous cookie profile to the email they submitted. If their email was already in our system under a different profile, Lytics will merge the two profiles into one. For information on building email capture Experiences, see the Experience Editor documentation.",
        "On-site events: forms": "Any of your website forms where a user submits an email are great places to create a merge event of a Lytics cookie to email. By using thejstag.send()event or setting up a Google Tag Manager Trigger, Lytics can attach an email to an anonymous profile. Some types of commonly used forms include: User log-inAny sign-ups (events, newsletters)Registrations / warranties (products)Gated content (webinars, whitepapers) Note that you can submit other data using this method. For example, if users fill in their name, profession, address, etc. in the form, you can send this as well and Lytics will append it to their profile. Keep in mind this will only work for forms living on your website. For forms populated in an iframe or hosted in a different way, you may need to use the Lytics pixel or an API call to capture this information. See the documentSending Data to Lyticsfor more information on using the JavaScript tag and other methods.",
        "Email click-through identifier": "Many email service providers (ESPs) have a unique user ID within their system, separate from email. It's encouraged to pass this identifier, hashed email, or any other non-PII identifier as a query string parameter within links you include in any email communication. If you are familiar with UTM parameters, this is similar. Once you set this up in your ESP, Lytics can associate this additional identifier as another merge rule. So moving forward, whenever a user clicks on an email link that drives them to your website with the Lytics tag, Lytics will be able to stitch the email to a cookie. Examples: domain.com/news/article_name?<your_esp_identifier_or_hashed_email_name>=<value>domain.com/news/article_name?esp_id=2ncihj894y2jnjknfjsd",
        "Tracking growth": "Next, you can create an audience in Lytics to track the growth of profiles that have stitched a cookie to an email. An example of what this could look like in the Lytics audience builder is shown below: On the Audience Summary page, you can view how your audience of unknown to known users has grown over time. ",
        "Content Recommendations": "Content Recommendations are a powerful tool that can be used to increase user engagement by presenting users with relevant content that resonates with them. By utilizing the Lytics topic taxonomy and powerful Machine-Learning algorithms, Lytics can make data-driven Content Recommendations within milliseconds. Powered by over 500 features, and the flexibility to customize your recommendations, Lytics' Content Recommendation API removes the heavy lifting when it comes to content personalization via web, email, etc. Out of the box, Lytics' Content Recommendations can be leveraged in 3 ways:",
        "Content Recommendations in the Lytics UI": "The Lytics UI offers multiple ways to test, validate, and get started with Content Recommendations. Both theInterest Enginepage and theContent Collectionpage provide options for fetching recommendations for individual users.",
        "Recommendations via the Interest Engine Page": "To access Recommendations from the Interest Engine page, navigate to theInterest Enginepage under theContentnavigation menu, select an Interest Engine, and click on theRecommendtab.  After entering a user ID and clicking Recommend, Lytics will generate recommendations tailored to that specific user.",
        "Recommendations via the Collections Page": "To access Collection-specific recommendations, navigate to a Collection under the Collections page in the Content menu and select the Recommendations tab. This page provides options for creating RecommendationsExperiences, links to theContent Recommendation API, code snippets, and a search bar to fetch Collection-specific Recommendations for a user.",
        "Recommend content with Lytics Personalization": "You can create personalized, customizable recommendation experiences on any site where the Lytics JavaScript tag is installed.",
        "Recommend content with Pathfora JS": "Pathfora JS is capable of taking a content collection and fetching and displaying the recommended content for the current user. Read more about it in thePathfora JS Docs.",
        "Recommend content with Lytics APIs": "The Lytics content recommendation API takes a handful of parameters, including the user the recommendation is for and an optional content collection to recommend from.",
        "Using content collections in recommendations": "Content collections are commonly used in content recommendations. By selecting a content collection as the source of your content recommendation, you're limiting which content the recommendation engine can select from. This is desirable for most use cases, because by default Lytics will scrape any and all pages of your website that users are visiting. Without a content collection defined, the recommendation engine will return what is most relevant to the user regardless of the context. For example, in a marketing context you wouldn't want to promote your careers page, even if it is the most relevant piece of content to the user in question. Size of the collection is also an important factor when using a collection for recommendation. The more content the recommendations engine has access to, the better, more personalized recommendations it can serve. If you can, try to make sure your content collection is of a decent size, and at the minimum fits the following requirements: Documents are valid (return a 200-status from an HTTP get request)Documents have been enriched by LyticsDocuments have topics associated with them If none of the documents in your collection fit these conditions, the recommendation engine may have issues returning recommendations as it doesn't have enough information to make a recommendation. If you would like assistance setting up these filters in your collection or to set up a collection with content published less than 1 week in the past, contactLytics Support.",
        "Content collections in Lytics Experiences": "When setting up a recommended content Experience through the Lytics UI, you will be asked to select an existing content collection or build a new collection to use in the experience.",
        "Using collections in custom content recommendations": "If you're using the Lytics API to recommend content, you can use thecontentsegmentquery parameter to define the content collection you want to use. The value of this parameter should be the ID of the content collection. You can locate the ID of the collection from the URL on the summary page. The ID will come after/collections/in the URL, and will end before the beginning of the query parameters.",
        "Affinity Alignment Score": "When building a recommendation-based Experience, you have access to theaffinity alignment scoreautomatically calculated by Lytics.  The \u201calignment\u201d score measures the similarity between the selected Content Collection and the Audience. This similarity is based upon the \u201cvector similarity\u201d of the most prevalent topics in the Collection, and the most prevalent topics in the Audience. If the Collection and the Audience share many of the same topics, the \u201calignment\u201d score will be high. Conversely, if the Collection and the Audience share few or no topics, then the \u201calignment\u201d score will be low. For successful Content Recommendations, we recommend pairing Collections and Audiences with moderate to high alignment. If there is low alignment, Content Recommendations may have low conversion rates and may be irrelevant to users in the selected Audience.",
        "Installing the Latest Lytics Tag": "Sign into your GTM account.Navigate to theworkspacethat you would like to install the Lytics tag into.ClickNew Tagfrom the dashboard orNewat the top right from the *Tagslist.Name your tagLytics JavaScript Tagor something descriptive.ClickTag Configuration.SelectCustom HTMLfrom the resulting menu.In a new tab/window log in to your Lytics account.From the Lytics dashboard access the administration menu at the top right.SelectInstall Javascript Snippet.SelectI'll Do it myself.Select theGoogle Tag Managertile.Copy the full Lytics tag to your clipboard.Navigate back to the GTM tab and paste your full Lytics Tag into theHTMLtext area.ClickTriggeringSelectAll PagesNote: This will load the Lytics tag on all pages. Feel free to customize the triggering to your liking but it is recommended to place our tag on all pages in order gather the best behavioral data for your visitors.At the top right clickSaveto save your changes.Once you have saved you should be taken back to the list of tags.In order to complete your changes you must publish a new version of your container.At the top right clickSubmit.Fill in the resulting form with as much detail about the changes as possible. This is useful when tracking changes to your container over time.ClickPublishat the top right to complete the installation.",
        "Verify the Installation": "Once you have published the container your changes should take effect within a few minutes. Navigate to the web property where the GTM container was installed. You may need to refresh your browser window to ensure you have loaded the latest GTM container configuration.Most browsers haveDeveloper Tools. In Google Chrome if you right-click and go toInspectthe Developer Tools menus will open.ClickConsolefrom the top menu on the panel that opened up.In the console typejstagand press enter.The result of that command should be something along the lines of{_q: Array(0), _c: {\u2026}, ts: 1549050811024, ver: \"2.0.0\", init: \u0192,\u00a0\u2026}. The numbers and version may differ but you should not receive a message related to beingNot Found.If you do see not found please refer to the GTM documentation on further debugging their tag to ensure the current version is being shown.You are done. Withing a few minutes you should begin to see some default web data flowing into theStreamssection of your Lytics account.",
        "Sending Additional Data to Lytics from within GTM": "Since GTM is a tag manager there are various approaches to sending data related to a users behavior, such as filling out a form or clicking a button, to any number oftagsmanaged by GTM. In the following section we'll cover the basics and give a few examples of common use cases but this is no way covers the gamut of what can be done to collect custom data. It is always best to refer to your account manager and the support team to get recommendations on solving specific data collection challenges. All of the following examples will assume you understand the basics of creating a newtagof typeCustom HTMLwithin GTM (steps 1 through 6 underInstalling the Latest Lytics Tag), just be sure to name the new tag(s) something descriptive and relative to the action being taken such as \"Pass form data to Lytics.\" In addition, for more information on collecting data with Lytics view our detailedJavaScriptdocumentation.",
        "Click Actions": "In order to track clicks with GTM we'll need to configure a newTrigger.From the side menu within GTM selectTriggers.ClickNewat the top right of the list of existing triggers.Name your trigger something descriptive like \"Clicked Picture of John Snow\".ClickTrigger Configuration.GTM has many pre-built triggers. For this example we'll use theAll Elementsoption underClick.Two additional options will appear. SelectSome Clicksso that we can trigger based on the click of a specific element on the page.In the firstselect menuthat appears we'll selectChoose Built-In Variable.This contains several pre-built helpers such as listening for clicks on an element with a particular class, ID, etc.For our example we'll assume our image of John Snow is implemented with the an id ofgameOfThronesas shown here:<img src=\"/john.png\" id=\"gameOfThrones\">SelectClick ID, which will allow us to trigger our tag only when a visitor clicks on elements with a specific ID.Change thecontainsmenu toequalsas we'll be expecting an exact match.In the empty space to the right of theequalsmenu typegameOfThronesor the ID that you are targeting.Save the trigger at the top right. Now that we have configured a trigger to listen for our specific action we can now tie that to a tag that sends Lytics information every time that action takes place. After you have created a newtagand selected type ofCustom HTMLwe'll configure the data sent and our trigger.In our example we are going to pass a simple event that sends context of what the user clicked but this could be expanded to be full featured and dynamic. Paste the following snippet into theHTMLtext area:HTML// the following script sends and event for a user stating they have clicked a particular ID\n<script>\n    jstag.send({\n        _e: \"clicked: gameOfThrones\"\n    })\n</script>Under the *Triggeringmenu we'll then select our custom trigger we created in the initial steps.Save the tag and publish.",
        "Form Actions": "Note: When it comes to collecting form data using the built-in GTM helpers is a great place to start. However, it is always best to refer to your Engineering resources to ensure what is being collected is correct, secure, etc. This is merely an example of what can be done rather than the recommended approach. That very much will change based on your unique architecture and use case. Warning: Never collect/send secure data such as passwords, credit card numbers, etc. via this method. In order to collect form data with GTM we'll need to configure a newTrigger.From the side menu within GTM selectTriggers.ClickNewat the top right of the list of existing triggers.Name your trigger something descriptive like \"Submitted Fancy Form\".ClickTrigger Configuration.GTM has many pre-built triggers. For this example we'll use theForm Submissionoption underUser Engagement.Several additional options such as validation will appear. We will ignore those for this example but GTM offers a variety of helpers to ensure data collected is of high quality.SelectSome Formsso that we can only collect data from a specific form.In the firstselect menuthat appears we'll selectChoose Built-In Variable. This contains several pre-built helpers such as listening for clicks on an element with a particular class, ID, etc. For our example we'll assume our form has an id ofmyFancyForm.SelectForm ID, which will allow us to trigger our tag only when a visitor clicks on elements with a specific ID.Change thecontainsmenu toequalsas we'll be expecting an exact match.In the empty space to the right of theequalsmenu typemyFancyFormor the ID that you are targeting.Save the trigger at the top right. Now that we have configured a trigger to listen for our specific form submission we can now tie that to a tag that sends Lytics information every time that action takes place. After you have created a newtagand selected type ofCustom HTMLwe'll configure the data sent and our trigger.In our example we are going to pass along one of the inputs within the form with the namefirstName. Keep in mind that for your collected data to make it to user profiles the sent data must match what is mapped with LQL. Paste the following snippet into theHTMLtext area:HTML// the following script uses jquery to pull the specific value of an input with the name **firstName** within the **myFancyForm** form\n// this is a basic example and there are countless more detailed examples online of how to convert form inputs into JSON\n<script>\n    jstag.send({\n        firstName: $('#myFancyForm').find('input[name=\"firstName\"]').val()\n    })\n</script>Under theTriggeringmenu we'll then select our custom trigger we created in the initial steps.Save the tag and publish.",
        "Leveraging GTM Variables": "GTM variables are very useful and make the actual collection of data far simpler though they take a bit of work to configure. In our form example above, instead of using jQuery to get the actual value of inputs those values are already passed to the GTM data layer upon submit. By creating aVariablemapping of that input you can refer to the variable in your tag as opposed to writing more custom JavaScript. Google has somegreat documentationto get you started. Likewise, by searching on Google for things like \"collecting form data with GTM\" you will get some great step by step walk throughs on how to configure these variables.",
        "Schema Visualization": "The Lytics schema visualization tool lets users visualize the relationships between data fields and data sources within the Lytics platform. The visualization provides a graphical representation of the data schema, making it easier for users to understand the structure of their data and how it is connected. With the schema visualization, users can: Explore data relationships:The visualization lets users see how different data fields and sources are connected, providing a clear picture of the overall data schema.Identify data gaps:Users can quickly identify gaps in their data by looking for missing fields or disconnected data sources.Plan data integrations:The visualization can help users plan data integrations by providing a visual representation of how data from different sources can be connected.Troubleshoot data issues:The visualization can be used to troubleshoot data issues by identifying inconsistencies or errors in the data schema. Overall, the Lytics schema visualization provides a powerful tool for understanding the data structure within the Lytics platform, enabling users to gain deeper insights and make more informed decisions about their data strategy. ",
        "Schema Audit": "The Schema Audit provides an overview of how your user fields are being used, the data they contain, and where they are getting that data. It is a great way to see if you have user fields that don't have any data or are not being used for any audience. On your Lytics dashboard, navigate toData>Schema Audit. This section offers a snapshot of your data to verify your data mapping techniques.",
        "Filtering the Schema Audit": "By default, the Schema Audit includes all user fields in your account. You can filter the report in three different ways.  Users with fieldallows you to filter fields based the percentage of users in your account who have a user field. For example, if you chooseAt least 50%, the Schema Audit will be filtered to include only user fields found on 50% or more of the profiles in your account.Data streamallows you to filter the Schema Audit based on the data source of a user field. Remember that a user field may receive data from multiple data streams.Search boxallows you to search for a specific user field by name.",
        "Interpreting Data Visualizations": "The visualizations near the top of the report tell you how user fields are used in your Lytics account. You can click on one of the visualizations to filter the table below.  The first pie chart shows the percentage of user fields that are providing data to build profiles from. In the example above, only 40% of user fields provide data to profiles. That means the other 60% of user fields have no data at all. This number should be as close to 100% as possible. The second pie chart shows the percentage of user fields that are being used to define audiences. In this example, 94% of user fields are not being used to define any audience.  This account probably mapped too much data, and could trim down their LQL quite a bit. The third pie chart shows how many user fields are merging data from multiple streams. A common example of merging data from multiple streams is anemailuser field. Typically, this may read data from your website as well as an email tool. One of Lytics' core competencies is merging data from different sources, so be sure to take advantage of this when creating your user fields. The distribution bar along the bottom of this section shows you what type of user fields are in your account. Certain types of user fields, especially any containingmap, are more expensive and should be used carefully. Mouse over the slices of the distribution bar to see how many user fields of each type are in your account.",
        "User Fields Table": " Browse the table to find detailed information about a specific user field. Select theList audiences each field usescheckbox to expand the table and see any audience that contains a user field in its definition. If you have questions about the user fields on your Schema Audit, contact your Lytics customer success representative for more information.",
        "Export Audiences (Triggered Emails)": "You can use Lytics to trigger a SendGrid transactional email to your customers when they enter a Lytics audience. For example, send a welcome email when a customer moves from unknown to known, or a retention email when a customer becomes disengaged.",
        "Personalizing the Email": "In this integration, you can send Lytics user fields to SendGrid for use in personalizing the email. It's important to design and test your template carefully before setting up this job to ensure your emails will look the way you want them to. SendGrid dynamic templates support Handlebars syntax for substitution (see SendGrid's docs about using Handlebars here), so you just need to make sure that your template has the right variable name for the Lytics user field that are being sent. Let's say you want to personalize our email to show a known user's first name in the greeting line. There is a user field, called \"First Name\" (the slug for which isfirst_name). You can see the slug for the field in parentheses in the \"Fields To Include\" section when you are configuring the job. In SendGrid, the template HTML should look something like this (such that the variable name matches the Lytics user field slug exactly):",
        "Import Audiences (Legacy)": "Importing user and activity data from SendGrid results in new users and/or existing user profiles supplemented with SendGrid campaign activity data. You can use this data to build and refine your existing Lytics audiences to power better, cross-channel campaigns. For more information, see how tomigrate from legacy marketing campaigns. If you are using the current Marketing API in SendGrid, please use the standardImport Usersjob.",
        "Export Audiences (Legacy)": "Exporting a Lytics audience to SendGrid allows you to send a Marketing Campaign email to your users based on your own, relevant targeting criteria, such as: cross-channel behavior, content affinities, and more. For more information, see how tomigrate from legacy marketing campaigns. If you are using the current Marketing API in SendGrid, please use the standardExport Usersjob.",
        "Legacy Marketing Campaign Tactics": "When you activate this Experience, users will be pushed to a legacy contact list in SendGrid, and a segment will be created for the Experience. Ensure that a new segment was created and populated in your SendGrid account. Navigate toMarketing > Contacts.  (If you have new marketing campaigns enabled use the Marketing tab that does not sayNEW).You should see a new segment under the list you selected during configuration. This segment should have the same name as your Experience in Lytics and will have a single contains rule.Next you will assign this segment to your single send campaign. Navigate toMarketing > CampaignsAnd click on the marketing campaign you imported.Configure your marketing campaign to use the segment by selecting the segment under theSend Toin theRecipientsof the campaign editor.Once your email is ready to send, clickReview Details and Sendto proceed with the sending process.",
        "Transactional Email Tactics": "After you activate this Experience, emails will automatically begin sending when users enter the audience in Lytics. No further configuration is necessary assuming your HTML template is ready to go. Confirm that emails are sending, you can check yourSendGrid email activity feedonce new users have entered the audience in Lytics.",
        "Configuring Webhooks": "Lytics highly recommends setting upSendGrid event notificationsto post event data to the Lytics. This will allow Lytics to get real-time activity data such as clicks, opens, etc. on your marketing campaigns and transactional emails. If you are running theSendGrid Import Audiences, the job will configure webhooks for you. However if you want to set them up manually, or you had previously configured webhooks to send to another system, you may need to update your settings. Lytics requires that webhooks are enabled to use SendGrid Experiences as part of the Lytics Canvas because it allows for the collection of reporting metrics for your SendGrid Experiences. To set up event notifications to go to Lytics, you will need to have a Lytics API token ready, you can read how togenerate a new API tokenif you do not already have one. Then follow these steps: Login to yourSendGridaccount.Navigate toMail Settingsunder the Settings menu.Click onEvent Notificationto open your webhook settings.Clickeditin the top right corner to change your event notification settings.In theHTTP POST URLtextbox, enter the following URL:https://c.lytics.io/collect/json/sendgrid_webhooks?access_token={API_TOKEN}Replace{API_TOKEN}with your Lytics API token.UnderSelect Actionscheck the boxes for the events you would like to recieve in Lytics. You may selectAll, or the following event types are recommended for collection:DroppedDeliveredBouncedOpenedClickedUnsubscribed FromMark as SpamASM Group UnsubscribeASM Group ResubscribeOnce you've selected event types, click the check mark button in the top right corner. Then make sure you toggle the button in the left-hand corner toOn. Lytics will then receive real-time event data to thesendgrid_webhooksstream. If you've run theSendGrid Import Audiences job, default mapping is automatically provided. Otherwise, contactLytics supportto add the mappings for SendGrid activity in your account.",
        "Microsoft Teams: Alerts": "Monitor jobs, authorizations, audiences, and queries in your Lytics instance with real-time alerts to a Microsoft Teams channel.",
        "Microsoft Teams: Event Quota Alerts": "Monitor the event quota in your Lytics instance with real-time alerts to a Microsoft Teams channel.",
        "Google Ads Customer Match": "When setting up an export to Google Ads Customer Match, there are two dropdowns where you can set the given consent for the audience being exported:Ad User Data ConsentandPersonalization Data Consent. ForAd User Data Consent, there are three separate options. Setting toGrantednotifies Google Ads that everyone in the audience has provided consent to send user data to Google for advertising purposes. ForPersonalization Consentthere are the same three options. Setting toGrantednotifies Google Ads that everyone in the audience has provided consent for personalized advertising. For more about these two fields and how they are interrpruted by Google Ads, visit theirFAQs. Find the complete docs on the setting up a Google Ads Customer Match exporthere.",
        "Google DV360": "When setting up an audience export to Google DV360, you must check theUser Consent Confirmedcheckbox to confirm that you have collected all required consent for the exported audience. If you have an ongoing audience export to Google DV360, those that haven't confirmed that user consent was granted will go into a failed state. For these jobs, if in fact you have confirmed consent, then you can edit the job, check theUser Consent Confirmedcheckbox and hit Complete. Once saved, you can select to retry the job and the export will update DV360 list with the consent confirmation, fulfilling the user consent requirements. Learn more about Google's User Consent policyhere. Find the complete docs on setting up a Google DV360 audience exporthere.",
        "Amazon DSP": "Amazon DSP takes a different approach than Google in helping you comply with DMA requirements. When setting up export of cookies or emails to Amazon DSP, you will be asked to select all the countries where the user info was collected in theCountry Codesinput. Amazon DSP uses the source countries of your audience to prevent DMA requirements from being applied to out-of-scope countries such as UK, IN, US, JP etc. Amazon will treat any audience without a country code as in-scope for DMA. Please note that if no country codes are selected, your export is likely to report a 0% match rate. If you have an ongoing audience export to Amazon DSP, you can edit the job in Lytics, select the country codes that apply, and hit Complete. Lytics will then update the audience definition in Amazon DSP. Find the complete docs on setting up an Amazon DSP audience exporthere.",
        "LinkedIn": "LinkedIn made changes for members in the EEA and Switzerland to comply with the new requirements imposed by the DMA. This may affect targetable audience sizes in LinkedIn, however in Lytics, there are no changes to the workflow of exporting users to LinkedIn. Learn more about how the DMA affects LinkedIn Marketing Serviceshere. To learn more about sending audiences to LinkedIn visit our docshere.",
        "\ud83d\udcd8Snowflake authorization update": "Snowflake will block single-factor password auth in Nov 2025https://www.snowflake.com/en/blog/blocking-single-factor-password-authentification/. In order to comply, Lytics now takes key-pair login credentials instead of password. In order for continued use of the Snowflake workflows, new authorizations will need to be created, and works updated with the new authorization before Nov 2025.",
        "Snowflake Direct Authorization": "Enter the SnowflakeAccountthat contains the data you want to import. SeeSnowflake's Account Identifier documentationfor details on account name format.Enter the SnowflakeWarehouseyou will run the extraction queries against. For more information, seeSnowflake's Warehouse documentation.Enter the SnowflakeDatabasethat contains the data you want to import.Enter the SnowflakeUsernameof a Snowflake user who has access to the data you want to import.Enter your SnowflakePrivate Keyfor the Snowflake user.Enter the SnowflakeRoleyou will run extraction queries with. Make sure this role has access to the data you want to import.For import, the role will needusageon the databaseusageon the schemaoperateandusageon the warehouseFor more information, seeSnowflake's Roles documentation.Enter your user's SnowflakePassword.",
        "Snowflake Direct Authorization for Bulk Export": "Enter your SnowflakeAccountaccount locator, ex.xy12345.us-east-aws. Be sure to format the locator according to your cloud platform and region, as described here:https://docs.snowflake.com/en/user-guide/admin-account-identifier.html#locator-formats-by-cloud-platform-and-regionEnter the SnowflakeWarehouseyou will run the load queries against. For more information, seeSnowflake's Warehouse documentation.Enter the SnowflakeDatabaseyou wish to export to.Enter theUsernameof a Snowflake user who has access to the schema you wish to export to.Enter the SnowflakeRoleyou will run load queries with. Make sure this role has write permissions on the database and schema you wish to export to.For export, the role will needusageon the databaseusageandcreate tableon the schemaoperateandusageon the warehouseFor more information, seeSnowflake's Roles documentation.Enter your SnowflakePrivate Keyfor the Snowflake user.In theGCS Stage Service Accounttext box, enter your GCS Stage Service Account.Optionally, enter theStorage Integration Nameto use when connecting. If this is not setGCS_INT_LYTICSwill be used.",
        "Bulk Audience Export": "Export user profiles including user fields and audience memberships from Lytics to Snowflake.",
        "Required Snowflake Setup": "Exporting to Snowflake requires additional configuration in your Snowflake account. You will need to create a storage integration in your Snowflake account that references a Lytics-owned GCS bucket. You will then need to retrieve the GCS service account associated with your storage integration in order to authorize in Lytics. More information about creating GCS storage integrations can be foundhere. Note that step 3 in the Snowflake docs linked is not necessary, as the GCS bucket is owned and managed by Lytics. In your Snowflake account, run the following queries to set up a storage integration for Lytics.Note: you will need ACCOUNTADMIN permissions to run the following queries. First, create your storage integration: Note that you will need to replace{your-aid}with the AID for your Lytics account in the query above. If you don't know your AID, contact your account manager for assistance. Second, retrieve your storage integration service account, you will need this to authorize the workflow: Finally, you will need to grant the role you authorized with permissions to use the newly created storage integration:",
        "How to Integrate Tealium with Lytics": "After logging into your Tealium account, navigate to theTagstab and select+ Add Tag.Search for Lytics and select+ Add.Tag Configuration:Enter your Customer ID number, which can be found in the admin section of your Lytics account: Account -> Manage Accounts.Cookie Name: OptionalAdvanced Settings:Send Flag: YesWait Flag: YesSynchronous Load: NoCustom Script Source: OptionalPublish to Dev: YesPublish to QA: YesPublish to Prod: YesLoad Rules: Load the tag on all pages.Data Mappings: SelectDefine and Map New Data Sourceto add any custom variable that you would like to pass to Lytics.  You can also see our suggested variables underSelect Destination. ClickApplywhen you are done adding variables.Note:Failing to specify a set of fields to pass to Lytics will result in all of them being sent.  This can be a very large number of fields which might impair performance.Suggested Variables:User ID (user_id)First Name (first_name)Last Name (last_name)Username (username)Email (email)Campaign ID (campaign_id)Product ID (prod_id)Transaction ID (transaction_id)Facebook ID (fbid)Twitter Handle (twitter_handle)Order Total (order_total)ClickSave and Publish\" to finish. Data will now be flowing into Lytics in real-time. Monitor incoming data on theData Streamspage.",
        "User account": "This option uses your Google user account to access your BigQuery data. Note that you must haveprojectEditorpermissions on the project you are importing from in order for BigQuery exports and imports to run properly with User Account authorization. SelectBig Query OAuth.From the user selection window, select the Google account you want to use from the list of accounts.ClickAllow.",
        "Service account": "This option uses a service account to access your Google BigQuery data. You will need a service account credential file to use this authorization type. If you do not have one, follow the instructions inGoogle's documentationto create one.For imports and exports, the service account must be able to list projects, create and write to tables, and create jobs. The combination ofBigQuery Data EditorandBigQuery Job Userpermissions will satisfy the requirements. Learn more about the BigQuery permissionshere.In your Lytics account, selectBig Query Service Account JWT. Note there are different options forImportandExport.InCert JSONtextbox, copy the contents of the service account's credential file.",
        "Give 3rd Parties Access to Export Data in BigQuery": "When exporting data from Lytics to BigQuery, you can specify email addresses that you would like to give access to. However, if you later decide that you would like to give access to another person, you can do this from within BigQuery. See Google's documentation regardingaccess controls for datasets.",
        "Export Audience Changes": "Export audience changes from Lytics to Google BigQuery.",
        "Export Audience Definitions": "Use theExport Audience Definitionsto export your Lytics audience definitions to Google BigQuery. You can use the exported audiences to do segmentation of users in other tools.",
        "Exporting table data to BigQuery": "Please see the Google BigQuery documentation onExporting Table Datafor options and instructions for exporting data to BigQuery.",
        "Google Cloud: Export All Users": "Lytics Conductorusers can export all profiles in their Lytics Account to BigQuery via theGoogle Cloud: Export All Usersworkflow. This workflow is optimized for scanning the Lytics user table without the need for considering audience membership, and offers superior performance to the audience export workflows as a result. This makes it a great option for Lytics users who wish to maintain a daily sync of all their Lytics profiles to a BigQuery data warehouse of their choice. Integration DetailsFieldsConfiguration",
        "Google Cloud: Export System Events": "Export Lytics system events to the BigQuery table and monitor jobs, authorizations, audiences, and queries in your Lytics instance.",
        "Navigating Accounts": "For users who have access to multiple accounts, you can quickly and securely navigate between Accounts using our account switcher at the bottom right of the primary navigation:",
        "2. Select Account": "A menu will appear outlining the accounts you have access to. Simply select an account and a new tab will open with this accounts dashboard.",
        "Client SFTP Server Authorization": "In theHosttext field, enter the host name or IP address of the SFTP server you want to connect to.In thePorttext field, enter the port number for the SFTP server.In theUsernametext field, enter the username for the SFTP server.In thePasswordtext field, enter the password for the SFTP server. If you wish to use aPrivate Key, leave this field empty.In thePrivate Keytext field, enter the ssh private key for the user for the SFTP server. This key is usually generated by you as a part of a SSH private/public key combination. Lytics is compatible with keys in the OpenSSH format. If you used a tool like PuTTY to generate your key make sure to export it in the OpenSSH format before adding it to Lytics.NOTE:If you choose to provide both, then Lytics will try to connect to the SFTP server with the private key first and if that fails then we will try with the password.In thePassphrasetext field, enter the passphrase for the private key you entered if any.In theFoldertext field, enter the relative path to the folder to import or export file. If the path is not given, then we will export the file to the user's home directory.",
        "Client SFTP Server Authorization with PGP Private Key": "This authorization lets you import JSON or CSV files to Lytics that have been encrypted using your PGP private key. In order to add this authorization, please make sure you have your PGP keys set up first. SeePGP encryptionfor more information. In theHosttext field, enter the host name or IP address of the SFTP server you want to connect to.In thePorttext field, enter the port number for the SFTP server.In theUsernametext field, enter the username for the SFTP server.In thePasswordtext field, enter the password for the SFTP server. If you wish to use aPrivate Key, leave this field empty.In thePrivate Keytext field, enter the ssh private key for the user for the SFTP server. This key is usually generated by you as a part of a SSH private/public key combination. Lytics is compatible with keys in the OpenSSH format. If you used a tool like PuTTY to generate your key make sure to export it in the OpenSSH format before adding it to Lytics.NOTE:If you choose to provide both, then Lytics will try to connect to the SFTP server with the private key first and if that fails then we will try with the password.In thePassphrasetext field, enter the passphrase for the private key you entered if any.In thePGP Private Keytext field, enter your PGP private key. SeePGP private keysfor more information.In thePGP Private Key Passphrasetext field, enter the passphrase for your PGP private key.In theFoldertext field, enter the relative path to the folder to import the file from. If the path is not given, then we will connect to the user's directory.",
        "Client SFTP Server Authorization with PGP Public Key": "This authorization lets you export Lytics audiences to a CSV file while also encrypting the content using the provided PGP public key. In order to add this authorization, please make sure you have your PGP keys set up first. SeePGP encryptionfor more information. In theHosttext field, enter the host name or IP address of the SFTP server you want to connect to.In thePorttext field, enter the port number for the SFTP server.In theUsernametext field, enter the username for the SFTP server.In thePasswordtext field, enter the password for the SFTP server. If you wish to use aPrivate Key, leave this field empty.In thePrivate Keytext field, enter the ssh private key for the user for the SFTP server. This key is usually generated by you as a part of a SSH private/public key combination. Lytics is compatible with keys in the OpenSSH format. If you used a tool like PuTTY to generate your key make sure to export it in the OpenSSH format before adding it to Lytics.NOTE:If you choose to provide both, then Lytics will try to connect to the SFTP server with the private key first and if that fails then we will try with the password.In thePassphrasetext field, enter the passphrase for the private key you entered if any.In thePGP Public Keytext field, enter PGP public key. SeePGP public keysfor more information.In theFoldertext field, enter the relative path to the folder to export file to. If the path is not given, then we will connect to the user's directory.",
        "Lytics Managed SFTP Server": "Lytics provides a managed SFTP server where you can provision an account to move data into and out of Lytics. The Lytics managed SFTP server URL and port are: For credentials to connect to Lytics Managed SFTP, you will create it according to your requirements using one of the following authorizations: Lytics Managed SFTP Server AuthorizationLytics Managed SFTP Server Authorization with PGP Private KeyLytics Managed SFTP Server Authorization with PGP Public Key When you add an authorization, Lyticscreatesa new user using the information provided in the Lytics managed SFTP server. You can use those user credentials to upload or download a file.",
        "Lytics Managed SFTP Server Authorization": "In theUsernametext box, enter a username. This username must be between 3 to 32 characters. Username can have characters a-z, A-Z, 0-9, hyphen(-), and underscore(_). It cannot start with hyphen(-).In thePasswordpassword box, enter a password. The password must be between 8 to 32 characters and include at least one uppercase letter, lowercase letter, number and special character.In thePublic Keytext box, enter your SSH public key. This key is usually generated by you as a part of a SSH private/public key combination. Lytics is compatible with keys in the OpenSSH format. If you used a tool like PuTTY to generate your key make sure to export it in the OpenSSH format before adding it to Lytics.NOTE: Either password or public key must be provided.",
        "Lytics Managed SFTP Server Authorization with PGP Private Key": "This authorization lets you import JSON or CSV files that have been encrypted with your PGP private key. In order to add this authorization, please make sure you have your PGP keys set up. SeePGP encryptionfor more information. Create a user forLytics managed SFTP serverif you have not already.Use the credentials of that user and Lytics managed SFTP server host details (Host name:sftp.lytics.ioPort:22) to create SFTP Server authorization with PGP Private Key usingClient SFTP Server Authorization with PGP Private Key.",
        "Lytics Managed SFTP Server Authorization with PGP Public Key": "This authorization lets you export Lytics audiences to a CSV file while encrypting the content using the provided PGP public key. In order to add this authorization, please make sure you have your PGP keys set up. SeePGP encryptionfor more information. Create a user forLytics managed SFTP serverif you have not already.Use the credentials of that user and Lytics managed SFTP server host details (Host name:sftp.lytics.ioPort:22) to create SFTP Server authorization with PGP Public Key usingClient SFTP Server Authorization with PGP Public Key.",
        "JSON Import": "Lytics allows you to import data via a JSON file for data sources we do not natively integrate with.",
        "CSV Import": "Lytics allows you to import data via a CSV file from data sources we do not natively integrate with.",
        "Export CSV for Download": "Export Lytics users to a CSV file that can be downloaded. You will receive an email with a link to download the file. Follow these steps to set up and configure an export job for Lytics File Service in the Lytics platform. SelectLytics File Servicefrom the list of providers.Select theEmail CSVjob type from the list.Select the Authorization you would like to use orcreate a new one.Enter aLabelto identify this job you are creating in Lytics.(Optional) Enter aDescriptionfor further context on your job.(Optional) From theCompressinput, select the compression type.(Optional) Select theInclude Headerscheckbox to include the field names in the first row of the exported file.(Optional) Select theAudience Namecheckbox to add a column with the name of the exported Lytics audience.(Optional) Select theAdditional Audiencescheckbox to add a column containing all the Lytics audiences a user is a member of.(Optional) From theFields to Exportinput, select a list of user fields to export.(Optional) In theUser Limitnumeric field, enter the maximum number of users to export. Leave blank to export all users.(Optional) In theCustom Delimitertext box, enter a custom delimiter for your file. Default delimiter is \",\". A recommended custom delimiter is \"|\" (pipe).(Optional) From theCustom Joininput, select a custom join character for fields that have multiple values. Default is to join multi-value fields with \"&\", e.g., emails=test@gmail.com&test2@gmail.com.(Optional) Select theContinuous Exportcheckbox, to email CSV file every day.(Optional) From theTime of Dayinput, select the time of day to start export each day.(Optional) From theTimezoneinput, select the timezone for time of day.ClickComplete. Once the CSV file has been created, you will receive an email with a link to download it. The download link will expire after 24 hours.",
        "Export the CSV to an SFTP Server": "You can opt to export your CSV to an SFTP server. This server can be your own or a Lytics managed SFTP server. The audience CSV file will be namedlytics_{segment_slug}_{date_time}.csv, and can optionally be compressed into a zip or gz archive. Follow these steps to set up and configure an export job for Lytics File Service in the Lytics platform. SelectLytics File Servicefrom the list of providers.Select theExport CSV via SFTPjob type from the list.Select the Authorization you would like to use orcreate a new one.Enter aLabelto identify this job you are creating in Lytics.(Optional) Enter aDescriptionfor further context on your job.From theAudience Namedrop-down, select the Lytics audience that contains the users to export.(Optional) From theCompressinput, select the compression type for the file to be exported.(Optional) Select theInclude Headerscheckbox, to include the field names as the first row of the exported file.(Optional) Select theAudience Namecheckbox, to select to add a column with the name of the exported Lytics audience.(optional) Select theAudience Slugcheckbox, to add a column with the slug of the exported Lytics audience.(Optional) Select theAdditional Audiencescheckbox, to add a column containing all the Lytics audiences a user is a member of.(Optional) From theFields to Exportinput, select the list of user fields to export. If none are selected then every user field will be included.(Optional) In theUser Limitnumeric field, enter the maximum number of users to export. Leave blank to export all users.(Optional) In theCustom Delimitertext box, enter a custom delimiter for your file. Default delimiter is \",\". A recommended custom delimiter is \"|\" (pipe).(Optional) From theCustom Joininput, select a custom join character for fields that have multiple values. Default is to join multi-value fields with \"&\", e.g., emails=test@gmail.com&test2@gmail.com.(Optional) Select theContinuous Exportcheckbox, to export at the specified frequency selected inFile Export Frequencyoption.(Optional) From theFile Export Frequencyinput, select how often to export the new file.(Optional) From theTime of Dayinput, select time of day to start export each day.(Optional) From theTimezoneinput, select timezone for time of day.(Optional) From theTimestamp Formatinput, select the format for the timestamp that will be added at the end of the filename. MM = month, mm = minute. IfNone (omit timestamp)is selected, the file name will not contain a timestamp. For continuous exports, this will result in the file being overwritten on each run.ClickComplete.",
        "LQL & Data Import Basics": "Let's take an in-depth look at Lytics Query Language (LQL) and custom data imports. We'll cover a basic overview of LQL itself, how data identities are resolved across streams, how you can map custom data and finally a few of the different ways to import custom data into your account. Difficulty: Intermediate TLDR:Checkout thedata upload documentationand thequery documentation.",
        "1. Lytics Data Processing Review": "Due to the intermediate nature of this course it is important that you already possess a strong understanding of segmentation, user fields and data streams. For those new to the Lytics data pipeline we recommend reviewing the following documentation before proceeding with the rest of the steps: Introduction: Lytics Data CollectionIntroduction: Lytics User Profiles & User FieldsIntroduction: Lytics Audiences",
        "2. Sending Data from a Website (JavaScript)": "When the Lytics JavaScript tag is installed we will automatically begin collecting standard data. This includes page-view level information such as URL, browser, Lytics Cookie Id, etc. In addition to the standard page-view event the Lytics tag empowers marketers to push more sophisticated data related to the end user's behavior or identity. An example of such data to a custom stream calledcustom_datamight look something like: While visiting your site that has the Lytics JavaScript tag installed you can leverage Chrome's developer tools, specifically the console, to manually fire this \"send\" request and pass the payload above to Lytics. Keep in mind, all data sent from your browser will automatically be associated with your current cookie. When testing and playing it is best practice to always use a fresh incognito browser. Chrome tools has greatdocumentationif they are new to you as well. Check out our JavaScript tagdata collection docsfor more details on the power of the Lytics tag.",
        "3. What is LQL?": "Lytics Query Language(LQL) is the transformation layer that makes passing arbitrary, non-schema specific data to Lytics. For the most basic implementations it is simply the translation of raw event data to user field data, the Lytics schema. In more advanced implementations complex calculations can be performed on events across streams to create sums, maps, and more. An example might be rolling allpurchase_totalvalues for individual conversions into apurchase_total_over_timefield that holds the total revenue for that particular profile. LQL is very powerful and we recommend working directly with our services team to get the most out of it. We also have some greatdocsto give you some more detail on the pre-built power. As mentioned in our documentation, LQL can be broken down into 6 key components:",
        "SELECT": "Select data to be added to user profiles. Including Maps, Counts, and other complex data types. As an example from the above event we sent from the browser you might mapstatusto the custom fieldcurrent_statusin order to unify data across streams. In that case your select statement would be something like: wherestatusis the key name from the event stream andcurrent_statusis the field name that is used across multiple streams. TheSHORTDESCallows you to add more context around that particular field in order to simplify working with the data inside the Lytics admin.",
        "FROM": "The stream to select from. In our example case this would simply be:",
        "INTO": "This is USER for all user profiles. (technically you could create other types, such as \u201caccount\u201d).",
        "WHERE": "Filters out entire records to not be included/analyzed. Bots, Employees, Test data. In our example we might want to exclude anyone not from Winterfell.",
        "BY": "What field are we going to identify this entity by. This is among the most important aspects of LQL. This defines which fields can be used as keys to merge data fragments together into a user profile. For instance: Means that anytime an email address or userid matches, regardless of its origin, we can assume that is the same user, thus merging all data associated with that key.",
        "ALIAS": "The alias is primarily an internal value of how to reference this particular LQL statement. When accessing LQL through the UI it would be referenced by this alias:",
        "4. Writing LQL": "Now that we know the basics of LQL we can put it all together in such a way that our sample data sent can be mapped into user fields. Though this can be totally customized to your liking the end product might look something like: All that is left once we have completed writing is to save our LQL file. This file name is completely up to you but should end with.lql. This will simplify the next step when we test our LQL before uploading it to our account.",
        "Install Lytics CLI": "Now that we have our LQL saved locally using the.lqlextension we are ready to test. To test we will use the Lytics CLI tool. The CLI tool and installation instructions can be found onGithub. Once we have installed the CLI tool lets verify it is working properly. In the terminal of your choice run the following command(s): If you are presented with a help menu you are good to go, if an error comes back or something was not found we recommend going through the CLI tools docs again to get it properly configured.",
        "Create a Sample CSV": "The Lytics CLI can test LQL in two ways. First, it will read some of the recent events from the target stream and show you the processed output. This is valuable when it comes to altering existing LQL but may present challenges when you are writing net new LQL on data that has not been sent to Lytics yet. For that use case we'll generate a CSV file to read from locally. In addition, though not demonstrated here, Lytics watch can also use a JSON file locally in addition to the CSV. CSV's can be created using a variety of tools. When creating your own CSV from scratch just ensure that the first row in the CSV is used to define your field keys(headers) and then each following row represents an individual user's data. When saving your CSV ensure the file name matches that of your lql with.csvin place of.lql. For instance if your LQL file is namedmy_sample_file.lqlyour CSV should be namedmy_sample_file.csvand be located in the same directory. The Lytics CLI will look for CSVs with the same file name as your LQL file.",
        "Activate Lytics Watch": "Lytics watch will listen for changes to your LQL file. When that happens it will take the CSV or recent data from your stream and process it using that newly altered LQL. This translation is then output into your terminal in order to validate that all mappings are working properly.",
        "6. Uploading LQL to an Account": "NOTE:Please be sure you are using the correct API token when performing the next operation. To check which API you currently have set simply run the following in your terminal: The value output should match the API token pulled from your account. If this is not the case please revisit the prerequisites section above for setting up you API token.",
        "Query POST Request": "To upload the query you will make a simple POST request to our API. Upon a successful upload you will get a response of200. Lytics handles everything else from there.",
        "7. Importing a CSV": "Finally we can import our CSV using a standard cURL command. Be sure to update your file and stream name in the example below:",
        "Consuming Metrics Downstream": "Once you have connected Lytics to your monitoring tool, there are various ways you can apply metrics from Lytics to support your operational and IT audit processes. Below are a few examples: Active monitoring - defined by alerting, requires additional configuration within your downstream tools to monitor Lytics.On-call distribution lists - control who within your operational teams to inform.Quiet hours - manage within a tool where you are already doing that for other metrics.Correlation of metrics - show existing metrics (e.g., website performance) in the context of Lytics metrics.Operational users - watch for signals without creating a Lytics admin user account.Anomaly detection - use threshold-based alerts that most monitoring tools have that go beyond the capabilities Lytics provides natively.",
        "Platform Monitoring via Metric API": "The Metric API provides access to a variety of metrics that are recorded in the Lytics platform. This API allows you to access segment size metrics, events received per hour, and many workflow-specific metrics. Heartbeats: metrics with a value of 1 for \"up & healthy\" and 0 for \"not healthy\" (or missing). *Availability of Metrics: the Lytics Metric API and all Lytics export workflows run inside Lytics' work runtime system in Kubernetes. During deploys or scaling events, these processes can move between servers, potentially resulting in 1 or 2-minute gaps in metrics. Therefore, alerts on single heartbeat misses are not recommended. Instead, look for a missing window of 5.",
        "Viewing Topics": "To viewTopicsin your account, click on theStandardInterest Engine and select theTopicstab. The table above displays the top 500Topicsassociated with theStandardInterest Engine. Each row indicates the number of documents tied to that Topic, as well as the number of users who have a Topic-score for it. For theStandardInterest Engine, Topics are generated using Lytics' Natural Language Processing (NLP) tools, as well as meta-tags from webpages. To learn more about how Topics are enriched, visit ourEnrichmentdocumentation.",
        "Blocking Topics": "In some cases, Lytics' NLP tools may identify irrelevant Topics. To address this, you can block unwanted Topics by selecting the checkbox next to each one. Once blocked, the Topic will appear in the Blocked Topics table and will no longer show up on user profiles or in documents.",
        "Topic Summary": "When you click on aTopicin the Topics table, you\u2019ll be taken to the Topic Summary page, which provides detailed insights about the selected Topic. Header: Displays the number of documents associated with the Topic, the number of users with a score for the Topic, and the average level of Affinity across all users.Chart: Visualizes the distribution of Topic scores across your user base.+ New Audience: This button lets you create a new Audience based on this specific Topic by redirecting you to the Audience Builder.Recent Content: Lists documents associated with the Topic. Clicking on a document will take you to its detailed page.",
        "Content Map and Related Topics": "The Topic Summary page also displaysRelated Topics, which are identified based on their frequent co-occurrence with the selected Topic. These related Topics appear in the bottom-right corner of the page and are also visible in the Content Map view. TheTopic Taxonomy(or Content Map) is built by analyzing documents and the Topics they contain, revealing patterns of co-occurrence and Topic prevalence. You can use the slider at the bottom of the Content Map to highlight strong connections\u2014Topics that frequently appear together. In the example below, we can see that theFurnituretopic is highlighted, as well as each of its related Topics (such asHouse, andFaux Fur Bean Bag).",
        "Segmentation With Topics": "When you click the+ New Audiencebutton, you\u2019ll be redirected to the Audience Builder. The Topic score distribution chart can help you decide on an appropriate threshold when creating segments. To include all users who have shown any interest in a Topic, use theExistsoperator.",
        "Slack: Alerts": "Monitor jobs, authorizations, audiences, and queries in your Lytics instance with real-time alerts to a Slack channel",
        "Slack: Event Quota Alerts": "Monitor the event quota in your Lytics instance with real-time alerts to a Slack channel.",
        "Sitecore": "Unlock the power of personalization in your CMS! Connecting Lytics to Sitecore enables you to access segments created in Lytics and use them to drive rules in Sitecore's native personalization engine. Lytics combines behavior from your other marketing tools with known user identities to create predictive segments based on how your users interact with your brand. Use your custom segments to create personal and powerful experiences. The integration setup in Sitecore is simple but requires back-end Sitecore and .NET development experience and access to your site's codebase. Install Sitecore connector as explainedhereOnce the Sitecore connector is installed, establish content to be personalized in Sitecore's Content Editor and/or xEditor.Select to use Lytics segments for personalization in the Rule Set Editor, just like you would any other rule.Select the specific Lytics segment to use as the target segment for personalization.",
        "Deliver Audience Membership to Sitecore": "The above plugin was created and managed by a third party outside of Lytics' control. At the time of writing, it leverages legacy JavaScript tag functionality that is no longer available by default. The following code leverages the modern jstag3 listeners to polyfill thely_segscookie to ensure backward compatibility with the Sitecore module. This must be installed after the core Lytics JavaScript tag and should be installed by a site admin who is aware of the inherent risks and impact of introducing JavaScript code to your web properties.",
        "Install Now": " Install Extension:This extension is installed directly into your Chrome browser. To install, visit theprimary extension pageand follow the instructions.Pin Extension to Nav Bar:Once installed, we recommend pining the extension to your top bar for easy access.",
        "Debugger": "The debugger section of the extension serves multiple purposes. Firstly, as we did previously, it allows you to validate installation, ensuring that the Lytics tag is successfully implemented. Additionally, it provides access to the full active configuration of the JavaScript SDK through the configuration tab. Moreover, it includes a live event debugger that monitors and displays comprehensive details of any calls made to the Lytics APIs in real time. This feature is particularly valuable for reviewing data sent viajstag.sendcalls, offering insights into the interaction between your website and Lytics APIs.",
        "Profile": "In the \"Profile\" section of the extension, users gain a live look at the current visitor's profile, providing valuable insights into their browsing behavior and demographic information. This section offers a snapshot of key details, such as the visitor's unique identifier and behavioral scores, allowing for a quick assessment of their engagement level. Additionally, users can access a detailed view of all information available to the browser, including demographic data, past interactions, and any custom attributes stored in the visitor's profile. This comprehensive overview enables users to tailor their strategies and personalize experiences based on individual visitors' specific characteristics and preferences.",
        "Classification Dashboard": "The Classification Dashboard displays theClassification Activity,Content Flow, and theContent Dashboardcomponents. Learn more about each component below.",
        "Classification Activity": "TheClassification Activitysection shows how many documents the Lytics Content Engine is classifying. By default, Lytics will classify up to 20,000 documents per month, which includes new documents as well as periodic reclassification of existing ones. This section helps you track whether you\u2019ve reached your monthly quota and ensure the Content Engine is functioning as expected. The Lytics Content Engine runs multiple workflows in the background, including the content classification workflow, which updates hourly. As long as you haven\u2019t exceeded your monthly quota, you can expect hourly updates to the classification activity.",
        "Content Flow Visualization": "(The Content Flow chart visualizes how your documents are ingested, processed, and enriched with Topics by the Lytics Content Engine. It also highlights any errors or problematic URLs such as URLs that are blocked byrobots.txtdirectives,Account Settings, or URLs that encounter non-200 status codes. Each state in the Flow diagram is described in detail below: Valid URLs: URLs that are properly formatted and can be fetched by Lytics. This step checks for invalid characters, symbols, and protocols.Allowed By Settings: URLs permitted by theDomain Allowlist,Path Allowlist, and other account settings that control if the URL can be scraped.Denied By Settings: URLs not included in theDomain Allowlistsetting.Path Blocklist: URLs blocked from being scraped based on thePath Blocklistsetting.Allowed By Directives: URLs that are allowed to be scraped according torobots.txtdirectives.Blocked By Directives: URLs that are blocked or disallowed according torobots.txtdirectives.200 HTTP Status: URLs that return a 200 status code, indicating they were successfully fetched.404 HTTP Status: URLs that return a 404 status code, indicating the URL was not foundOther HTTP Status: URLs that return a status code that is not 200, 404, or 401. This means the URL was not successfully fetched.Has Content: URLs that have content to be scraped. This means the URL was successfully fetched and contains content.Enriched: Content that has been successfully enriched with metadata and TopicsEnrichment Error: Content that encountered an error during enrichment, meaning it was not successfully enriched and may lack metadata or Topics.",
        "Content Dashboard": "TheContent Dashboarddisplays various attributes of your Content, including site name, author, URL path, and Topic information. TheClassification Dashboardis intended to provide a high-level understanding of your Content and can be used as a starting point for your Content-oriented use cases.",
        "Manual Content Classification": "TheManual Content Classificationmodule allows you to manually add a URL to Lytics or preview how a specific document will be classified. This feature is useful for troubleshooting any setup issues on your page before it\u2019s added to the Lytics content corpus. To use it, simply enter the URL of the document you want to preview and clickClassify. You\u2019ll be able to see the extracted Topics as well as any metadata that Lytics scrapes from the document. Once you're satisfied with the results of the classification, clickComplete Classificationto add the Documentto the content corpus. The document and its associated Topics will then be available for use in personalization efforts, such as recommendations or content affinity.",
        "\ud83d\udcd8Adding and Removing Topics": "When previewing a document, you can manually Add or Remove topics from the Classification. To add a Topic,  click theAdd Topicbutton, select the desiredRelevancescore (between 0 and 1) and then clickAdd topic to Document. To remove a Topic, simply click theXnext to the Topic tile.",
        "URL Normalization": "As Lytics ingests web-based content, it attempts to resolve duplicate URLs and create links between documents, much like a search engine would.  As such, Lytics does things like respectrobots.txtdirectives, resolve canonical URLs when present, etc. Lytics attempts to sanitize URLs as much as possible before ingesting them into the Content Affinity Engine.  Sanitization includes removing all URL parameters and cleaning URL syntax.  This happens via an LQL function calledurlmain.",
        "The Lytics' Graph": "The system records each unique identifier (ID) as separate nodes within the graph structure. A collection of nodes linked by edges corresponds to a singular profile. The data associated with the most robust ID present in an incoming event is stored. While not directly accessible via the API, this information is made available through the \"Identity\" tab in the conductor. The graph undergoes immediate updates upon the arrival of new events. Generally, the response time from a graph update to the corresponding entity ranges from milliseconds to seconds. Graph view of consumer profiles in Conductor",
        "Raw Data Store": "The system efficiently stores every event received from Lytics, both in batch and real-time modes. Each event includes a timestamp indicating when the event took place and when it entered the Lytics system. Although this information is not directly accessible through the API or user interface (UI), it is possible to transmit the events downstream using our out-of-the-box (OOTB) integrations, such as theBigQuery (BQ)export feature. This scenario seamlessly transfers the events from the raw storage to designated downstream tables. These operations can be initiated or concluded at any given time. The raw storage option also serves as a valuable resource for replaying data in the event of corrupted or malformed data, ensuring data integrity throughout the account.",
        "Entity/Profile": "The system diligently maintains and stores the latest Lytics profiles, ensuring their accuracy and relevance. These profiles encompass various attributes, such as scores and segment memberships. Users can conveniently access these profiles through the user interface (UI) using the Decision Engine functionality. Additionally, the profiles can be retrieved programmatically via the API, utilizing any of the existing profile IDs. To facilitate this process, you can reference the profile's URL in the UI, which contains the necessary ID value for API access. The transition from data entry to the updated entity is typically swift, with response times ranging from milliseconds to seconds. The updated entity is a valuable resource for real-time activations, including trigger-based exports, membership in client-side experiences or audiences, and client-side attribute updates. These functionalities enable seamless and immediate user interactions based on their up-to-date profiles. Entity view of consumer profiles in Decision Engine  ",
        "BlueHornet": "Connect with BlueHornet to import and export your subscribers and activity.",
        "Importing data to Lytics": "Authorize Lytics with BlueHornet, if you have not previously done so.SelectImport Listfrom the list of workflows available.Select the authorization you want to use for this import.From theSegmentsmulti-select, select the segments you want to import subscribers and activity from. If no segments are selected all subscribers and activity will be imported.Note: Activity for all messages that were sent to the selected segment will be imported. This will include activity for users in segments other than the selected one that the message was sent to.Under theAdvancedtab, select theOne time subscribers/unsubscriber synccheckbox to import your subscribers/unsubscribers only once, and skip importing activities.Click on theStart importbutton to begin the work.",
        "Export an audience to a BlueHornet list": "Authorize Lytics with BlueHornet, if you have not previously done so.SelectExport List.Select the authorization you want to use for this export.Configure the integration options:Audience: Select the Lytics segment to export to BlueHornet. A group will be created for the exported subscribers.Email Field: Select the Lytics field that contains the subscriber's email.ClickShow Advanced Optionsfor the following setup options:Keep Updated: Select this to keep the BlueHornet group up to date with the Lytics segment. Subscribers will be added and removed based as they enter and leave the lytics segment.Existing Users: Add users who already exist in the segment. If this is unselected users in the segment will only be added to the BlueHornet group if they experience an event in Lytics.Click on theStart Exportbutton to begin the work. Users should start appearing in BlueHornet after a few minutes.",
        "Import Audiences and Activity Data": "Import your Pardot prospects, prospect accounts, opportunities, visitors, visitor activities, and lists into Lytics. You can use this detailed data to build and refind your existing Lytics audience to power better, cross-channel campaigns.",
        "Import Data from Mailgun into Lytics": "Authorize Lytics with Mailgun, if you have not previously done so.SelectSetup webhooksfrom the list of workflows.This workflow doesn't require any additional configuration. ClickStart setupand Lytics will begin collecting data in realtime as users open and click through emails sent from Mailgun.",
        "Export Audience Triggers": "Send Lytics audience event triggers to your AWS Kinesis Data Streams to trigger a message when a user enters or exits a Lytics audience.",
        "Update Account Details": "Use the left-hand navigation to selectAccount>Settings>Details.Ensure theAccount Nameused is accurate and descriptive.Verify that theDomainreflects your primary web address.Note: Your domain should include the relevant subdomain (www, etc.) and exclude the protocol (httporhttps).EnsureAllow access via APIis checked.PressSave Changesto confirm the updates.",
        "Update Content Settings": "Use the left-hand navigation to selectAccount>Settings>Content.Scroll down toContent domains allowlist.Update theContent domains allowlistto include all domains and subdomains where the tag will be installed.Note: Be sure to update the domains allowlist, not the paths allowlist.\u2757\ufe0fAutomated classification requires a valid robots.txt file on your configured domain(s). In addition, we highly recommend using your production URL rather than a development URL for classification. This will greatly reduce confusion when it comes to recommendations and moving to production. PressSave Changesto confirm the updates.",
        "Step 1: Create a renewals audience": "Consider the fields available for renewal in your audiences.  Some common ones might include the time of last purchase, subscription start date, and subscription end date.  You might add additional criteria such as filters excluding complementary subscriptions. Consider that you will likely have several renewals audiences - one per subscription.",
        "Step 2: Export audience to all key integrations": "The focus of renewals and winbacks will be cross-channel which can include:Google,Facebook,Verizon Media (Yahoo Gemini), your email service provider (ESP) available asIntegrations, and the LyticsJourney Canvas(not an export).",
        "Step 3: Create on-site modals targeting renewals": "The most common renewal tactic is to deliver on-site modals to users with an upcoming renewal.  Take the renewal audience you created in step one, and then add custom personalized Experiences to target them.  See the slide-out modal on the left side below as an example.  Further instructions are available forbusiness usersand fordevelopers.",
        "Step 4: Add targeted email to your renewal journeys": "The email inbox is one of the most impactful places to reach your renewal users.  First, build an email in your ESP that can be triggered based on a Lytics audience.  You\u2019ll need to follow the instructions in your ESP to do so, and this varies considerably by tool. Once you have a triggered email built in your ESP, make use of the triggered email feature your ESP likely has. Whatever ESP you have, you probably do not want to include \u201cExisting Users\u201d in the audience, so we recommend leaving that box unchecked.  Once this export is configured, new customers who enter your renewal audience will be sent the automated message.  For examples of how you can go a step further and embed custom content in your email templates, check out thisIterable use case.",
        "Step 5: Involve telesales (optional)": "Imagine incorporating your message across channels to even include telesales.  Your telesales team likely has a stronger closing rate on renewals than your site or your email programs ever could.  There are two ways to incorporate telesales: Encourage visitors to call inbound.Deliver a list of leads for targeted outbound. Encourage Visitors To Call Inbound Leveraging very simple HTML, you can createclick to callcampaigns inOrchestrate.  Then, users on a mobile phone will be able to immediately talk to a rep on the phone:  Deliver a List of Leads for Targeted Outbound Build a list of all the upcoming renewals you care about.  Here\u2019s an example using several audiences of subscriptions expiring within the next 30 days as building blocks for a master telesales list: ",
        "Step 6: Incorporate direct mail (optional)": "Using the same example as above, consider adding address information to the export and sending it to a fulfillment provider who can send direct mail to your renewals.  As a final note, keep in mind there are many other areas to cover including copy choices, step-by-step optimization, and common pitfalls to avoid. These are beyond the scope of this playbook, but should be taken into consideration while developing your marketing strategy to retain customers.",
        "Import Profiles": "Import profile data from Localytics to add mobile user data to user profiles in Lytics.",
        "Triggered Push": "Trigger push notifications through Localytics when users enter a Lytics audience.",
        "Templating": "User profile fields can be dynamically inserted into an alert using  templating of supported fields including the alert title, alert subtitle, and alert body fields. Lytics usesGolang's template packageto provide this functionality. The data available to you will vary based on the data available on each individual profile. You can download a customer profile to see a list of fields available on your user profiles. For example, if you want to include a user's name in an alert and the field containing the name is calledfirst_name, a template like the following could be used: Notice the \".\" before the name of the profile field. Some profiles may not have a first-name field. To avoid sending an incoherent alert when you don't have the first name field available, a template like the following could be used: Alternatively, if you would rather include a default name when the field does not exist on the profile, a template like the following could be used: If you would like to make sure that multiple fields on a profile exist, a template like the following could be used: To check equality on a particular profile field, you can create a template like the following:",
        "Building the audience": "From theLytics dashboard, selectAudienceand thenCreate New Audience. Using a Custom Rule, build an audience of users withHas Accessed Mobile Webast(true).  This audience will include anyone who has reached your site from a mobile device. Note:You do not need any identifying information for this audience. Be sure to select theEnable API Accesscheckbox. This lets Lytics target anonymous users who have visited from a mobile device.",
        "Targeting the audience": "Now you can build campaigns in your chosen marketing platforms to effectively target this audience of mobile web users. You can connect this audience toGoogle AdWords, or if you are using Facebook, follow these steps totarget with Facebook ads. Upon completion, your advertising team or agency can build these mobile engagement campaigns inGoogle AdsorFacebook.",
        "NetSuite Overview": "NetSuiteis a Customer relationship management platform that supports marketing, sales and service operations and customer insights. Integrating Lytics with NetSuite enables you to import consumer data for use in Lytics audiences.",
        "NetSuite Authorization": "If you haven't already done so, you will need to set up a NetSuite account before you begin the process described below. Ensure you have followed the instructions in theNetSuite authentication documentationto create the 4 keys you need to authorize Lytics to access your NetSuite account. SelectNetSuitefrom the list of providers.Select the SuiteTalk TBA method for authorization.Enter aLabelto identify your authorization.(Optional) Enter aDescriptionfor further context on your authorization.In theAccount IDfield, enter your NetSuite account ID. To locate your account ID, in Netsuite go toSetup > Company > Setup Tasks > Company Information. The account ID field is located near the bottom of the right column.In theToken Keyfield, enter your token id you created.In theToken Secretfield, enter your token Secret you created.In theConsumer Keyfield, enter your consumer id you created.In theConsumer Secretfield, enter your consumer Secret you created.In theDescriptionbox, enter a name for your authorization.ClickAuthorize.",
        "NetSuite: Import Audiences": "Importing your consumer data from Netsuite allows you to use your NetSuite users in Lytics audiences.",
        "Campaign Tactic": "Once an Experience with the Campaign tactic has been activated in Lytics, follow these steps to finalize your email in Mailchimp: In Mailchimp, find your campaign and edit it.Under theTostep, selectAdd Recipients.From theAudiencedropdown select the Mailchimp audience you selected during the configuration of the Experience in Lytics.UnderSegment or Tagselect the tag that matches the name of your Experience.ClickSaveto save these changes, and proceed with the setup process of your campaign, seeMailchimp's documentationfor detailed instructions on these steps.Once you're ready to deploy, clickSendto send immediately orScheduleto send the campaign later.",
        "Email on Tag or List Add Tactics": "ForEmail on Tag AddandEmail on List AddExperiences, you must havecreated an automationbefore importing your Experience. You need to select a list for the automation before you can save it as a draft. Ensure that when youconfigureyou select the same list you choose when creating the automation in Mailchimp. The dropdown should be pre-populated with the list you chose to make this easy. Once you activate the Experience, use the following steps to finalize your email in Mailchimp: Locate the automation in Mailchimp under theCampaignslist.Click theEditbutton to edit the automation.If the Experience has theEmail on Tag Addtactic you will need to select the tag to trigger the email.Under theDesign Emailsection there should be a statement about the trigger of your automation. Select toEditthe trigger.Select a delay for triggering the email if you would like one, and underSettingsselect the tag with the same name as your Experience.Click onUpdate Triggerto save the changes.Finish configuring your email refer toMailchimp's documentationfor assistance.ClickNextto continue to the review step. Confirm your configuration looks correct and clickStart Sendingto finish the activation of your Mailchimp automation.",
        "Account Activity Monitoring via Audit Logs API": "Audit Logsprovide visibility into system changes such as creating segments, updating schema, deleting items, new authorizations added, and users added to roles. This is a wealth of audit data about changes in your account. These events are shown in many places inside Lytics, such as the history of work sync events, status changes, work failures due to expiring authorization, or password changes on the source for OAuth tokens.  See ourSystem Events API documentationfor more information.",
        "The Lytics app for Zapier": "In addition, the Lytics has built an app that allows Zapier to interact with Lytics. For more information on triggers, actions, and workflows see theZapierdocumentation. The Lytics app supports the following functionality: Trigger: Lytics user enters an audienceTrigger: Lytics user exits an audienceAction: Ability to load user data into Lytics Zapier supports public and private apps. Currently, the Lytics app is a private app. In order to get access to it, you must be invited. Contact your Lytics representative to get an invitation.",
        "Getting access to the Lytics app": "This section describes how to log into Zapier and get access to the Lytics app. Request an invitation from your Lytics representative to use the Lytics app. Your Lytics representative will need your email address.You will receive the invitation by email. Click the link in the email to accept the invitation.A browser window will open with a prompt to log into Zapier. If you have a Google account you can log in with that. Otherwise, you will need to create a new Zapier account by clicking theSign Uplink.  ClickAccept Invite & Build a Zap. ",
        "Connecting Zapier to Lytics": "In order for Zapier to interact with Lytics, aconnected accountmust be configured in Zapier. In Lytics UI, create a new API token with administrator rights. Information on how to generate an API token is availablehere.In Zapier, navigate toConnected Accounts.  In the dropdown select the Lytics app.  Enter your Lytics API token and clickYes, Continue.  The connection will appear in the sectionMy Connections. You will see the Lytics account name and the Lytics AID value (4-digit number). ",
        "Using the Lytics triggers": "The Lytics app provides triggers for when Lytics users enter and exit Lytics audiences. These triggers can be used in any zap you create. The following instructions demonstrate how to create a new zap that starts with a Lytics user enters an audience, and sends you an email notification when that happens. In Zapier, navigate toZaps.  ClickMake a New Zap.Click the Lytics app.  ClickUser Entered Segment.  ClickSave + Continue.The Lytics connection you configured earlier will be selected by default. ClickSave + Continue.  Enter the following values and clickContinue.  ClickContinue.  Zapier will tell you that your zap is missing an action step. Click+ Add a Step.  ClickAction/Search.  ClickEmail.  ClickSave + Continue.  In the fieldTo, enter your email address.In the fieldSubject, enter: \"New Lytics User Created!\"In the fieldBody, enter: \"New visitor browser: \"Click the buttonInsert a Field.  SelectUser Agent.  Scroll down and clickContinue.You can see the information that will be used to send an email. The email body includes values from Lytics. ClickSkip Test.  ClickFinish.In the field for the name, enter: \"Send Email Notification for New Lytics Users\"  If you want to enable the zap, click the toggleOFF.",
        "Using the Lytics \"add user data\" action": "The Lytics app provides an action that lets you add user data to Lytics. This action can be used in any zap you create. The following instructions demonstrate how to create a new zap that writes some hard-coded data to Lytics. This is not a very useful example, but it allows you to configure the action without having to set up connections to any other systems. Just imagine that the hard-coded data is coming from one of your own systems. In Zapier, make a new zap.For the trigger select the appCode.  SelectRun Javascriptand clickSave + Continue.  In the fieldCode, enter the following. This code represents the data that would be provided from",
        "Import Cohorts": "Importing cohorts from Amplitude results in new and/or updates to existing user profiles in your Lytics account containing fields from Amplitude. Once imported, Lytics can use this Amplitude data to inform its ML-based enrichments, and this data enables you to target your multi-channel campaigns orchestrated by Lytics.",
        "Import Events": "Importing user and activity data from Cloud Pub/Sub in real-time results in new users or existing user profiles supplemented with activity data. You can use this data to build and refine your existing Lytics audiences to power better, cross-channel campaigns. The integration is similar to AWS Kinesis Streaming or Azure Event Hubs. It requires you to set up a Cloud Pub/SubTopic, which you publish JSON messages to. Lytics then listens (subscribes) to this Topic and receives those messages.",
        "Additional fields": "The following fields are not included in the default mapping, reach out to thecustomer support teamto add them to the mappings for your account.",
        "Import SparkPost Data into Lytics using a Webhook": "Log into yourLytics account.ClickData>Integrationsand selectSparkPostfrom the integrations list.Click theSetup Webhookstab.Identify the authorization you would like to use and clickSelect.ClickStart Setup. SparkPost user data and activity will now be imported into Lytics in real-time and can be used to build or refine Lytics audiences. You can check theWebhookssection of your SparkPost account to verify the webhook is active, it will be namedLytics Webhook.",
        "Sending Triggered Emails": "You can use Lytics to trigger SparkPost to send an email to your customers when they enter a Lytics audience. You could send a welcome email when a customer moves from unknown to known or a retention email when a customer becomes disengaged. Log into yourLytics account.ClickData>Integrationsand selectSparkPostfrom the integrations list.ClickNew Workflow, and then click theSend Triggered Emailstab.Identify the authorization you would like to use and clickSelect.In theAudiencedrop-down list, select a Lytics audience. Customers entering this audience will trigger SparkPost to send them an email.In theTemplatedrop-down list, select the SparkPost email template you would like to use.Send emails to existing members of the choosen Lytics audience by selecting theImmediately send an email to users who already exist in the selected audiencebox.ClickStart Send. Lytics will now trigger SparkPost to send an email to a customer as they enter the specificed audience.",
        "Redshift Only": "To set up AWS Redshift User you will need the following credentials: database username and password. See theRedshift Users Guidefor instructions on how to create a user in Amazon Redshift. If you are new to creating authorizations in Lytics, see theAuthorizationsdocumentation for more information. SelectAmazon Web Services(AWS) from the list of providers.Select theAWS Redshift Usermethod for authorization.Enter aLabelto identify your authorization.(Optional) Enter aDescriptionfor further context on your authorization.In theUsernametext box, enter your Redshift cluster admin user name.In thePasswordpassword box, enter your Redshift cluster admin password.(Optional) In theDB URLtext box, enter your Redshift database endpoint. Followthese instructionsto obtain yourDB URL; copy theEndpointfrom the general information page (the area as the JDBC URL in the Amazon instructions). This will be of the form:redshift-cluster-name.VCP-cluster.region.redshift.amazonaws.com:port/db-name. If left blank the import will need to have theDB URLset in the job configuration.(Optional) From theSSL modeinput, select your SSL mode credential. Leave blank to disable SSL verification.ClickSave Authorization.",
        "Redshift and S3": "To setup theAWS S3 Keys And Redshift Useryou will need the following credentials: Redshift database usernameRedshift database passwordS3 access key IDS3 secret keyRedshift database URL See theRedshift Users Guidefor instructions on how to create a user in Amazon Redshift. If you are new to creating authorizations in Lytics, see theAuthorizationsdocumentation for more information. In theLabeltext box, enter a name for the authorization.(optional) In theDescriptiontext box, enter a description for this authorization.In theS3 Access Keytext box, enter your S3 Access Key credential.In theS3 Secret Keypassword box, enter your S3 Secret Key credential.In theRedshift Usernametext box, enter your Redshift Username credential.In theRedshift Passwordpassword box, enter your Redshift Password credential.In theRedshift DB URLtext box, enter your Redshift DB URL credential. Follow these instructions to obtain your DB URL; copy the Endpoint from the general information page (the area as the JDBC URL in theAmazon instructions). This will be of the form:redshift-cluster-name.VCP-cluster.region.redshift.amazonaws.com:port/db-name.From theRedshift SSL modeinput, select your Redshift SSL mode credential.",
        "Data Structure": "Effective consent management requires a comprehensive set of properties that can be deployed to ensure compliance and maintain customer trust. These properties should cover various aspects of data collection, storage, and usage and provide businesses with the tools to manage consent-related data effectively.",
        "Examples": "Lytics deploys a flexible data model, and though the following is not representative of every method of collection, we've highlighted two working examples leveraging our available SDKs as a baseline:",
        "\ud83d\udcd8We are here to help!": "Consent is not a one size fits all scenario. Each customers data and business goals are unique. Before deploying any of the following examples it is always recommended to consult with your Technical Account Manager or a Solutions Architect.",
        "Profile Materialization": "Building from the collection strategy outlined inCollecting Consent Data, we must determine how to materialize the consent-related data we've just collected to user profiles for segmentation. This can be achieved in a variety of ways. Regardless of the approach, however, it is essential to consider the level of granularity of the consent.",
        "Granularity": "When collecting consent from users, obtaining granular consent for each specific use of a customer's personal data is an important consideration. Granular consent means that customers are provided with a clear understanding of the exact purposes for which their personal data will be used and can choose to consent or withhold consent for each specific use case. For example, a business may seek granular consent from a customer to collect their email address and use it to send them promotional emails but not to share the email address with third-party partners for advertising purposes. This approach allows customers to make more informed decisions about how their personal data is used and provides greater control over their privacy. Obtaining granular consent not only requires careful planning and clear communication with customers about the specific use cases for their personal data but also a rock-solid means for enforcing an individual's consent wishes across all future touchpoints.",
        "Field Definition": "When defining profile fields, there are two primary considerations. What type of field should be used, such as astringor amapand how do you want to handle data merging when profiles are stitched? Type StringBecause consent is something that may change for a consumer over time, the values must represent the most recent data. As such, thelatestmerge operator is always recommended. This means that as additional data is stitched to a user profile, the most recent events that are mapped to a field will win.  Type MapFor a more complex or granular consent strategy, the map field type can be very helpful in accurately managing consent. As such, themergemerge operator is always recommended when trying to keep the key-value pairs up to date to the most recent consent state. This means that as additional data is stitched to a user profile, the most recent events that are mapped to a key-value pair will win. ",
        "Mapping Definition": "Mappings are then leveraged to determine how data from any number of streams map to the defined field. In the example below, we take a simple approach to map the boolean value of true or false to the consent field if the consent is related tomarketing-consent. Though this is one elementary example, the same practice can be replicated to ensure a consent attribute has the status, timestamp, and context of any important policies or sources for segmentation. ",
        "Building Blocks": "Building Block Audiences provide the perfect means to maintain consent-related rules and extend the ruleset to all campaign audiences. The number and these audiences will depend on the granularity of your consent strategy. Still, as a basic example, we recommend creating both a \"has consented\" and \"has not consented\" counterpart for each level of consent. This can be done simply through the GUI for our powerful segmentation engine, as shown below: ",
        "Campaign Segments": "Once you have the necessary building blocks constructed, you can quickly integrate that rule set into your campaign audiences again through the GUI for our segmentation engine. The example below outlines a use case where you want to target high-value users who have opted in. ",
        "Global Job Segment Filter (Consent)": "Configure a segment to prevent profiles from being sent to downstream destinations based on consent status or another relevant filter. This ensures that individuals who should be excluded will not be activated downstream. API documentation can be foundhere.",
        "Exploring LQL Queries": "The Browse Queries section in Lytics lets you see the Lytics Query Language (LQL) used in your account. To view this on your Lytics dashboard, navigate toData>Queries. From there, you can click on any item in the table to view an individual LQL file.  This is useful if you need to find out the exact definition of a specific user field and cannot access your LQL directly. Remember that you can't edit or remove LQL from the Lytics UI. For a more detailed breakdown of LQL and how it works, see the full article onQueries & LQL.",
        "Managing LQL Queries": "The Lytics Query Language (LQL) is used to define the transformation of uploaded records and event data intouser fieldson a customer profile. It transforms row-level event data into document-oriented user info. This Query language is similar to the HIVE or SQL query languages. However, it departs from these to offer more of aRich Document(JSON user profile) construction. The following section will explain how to write, validate, and upload queries using LQL. Query ExampleQuery ManagementStandard SyntaxFunctionsKinds (Data Types)Merge Operations",
        "Query Example": "The following is a simple example of an LQL file that translates events from a website into profile fields.",
        "Query Management": "Once you have written your LQL file, you can save it with the.lqlextension. And use the following request to validate the query using the Lytics API. You can upload your query to your account if it is valid.. To look at the user schema generated from the LQL:",
        "Standard Syntax": "There a few common keywords used in LQL syntax: SELECTSelect data to be added to user profiles, including Maps, Counts, and other complex data types.FROMThe stream to select from.INTOThis isUSERfor all user profiles.  (Technically you could create other types, such as \"account\".)WHEREFilters out entire records to not be included/analyzed such as Bots, Employees, Test data.BYWhat field are we going to identify this entity by.ALIASWhen a selection query has an alias, that is the profile-fragment(table) name to use. Use the following reference for a full syntactical guide when writing your own LQL:",
        "Functions": "LQL has many built-in functions for transformation and logic evaluation that can be applied to raw fields in LQL.",
        "Kinds (Data Types)": "TheKINDsyntax allows you to explicitly define the data type of a field. Often this is optional as it is inferred from functional expression. int64 bit signed integernumber64 bit signed Float valueboolBooleandateDate-Timestringstring[]timeArray of times[]stringArray of stringsts[]stringTime ordered Unique set of strings (useful for keeping track of order in which they performed set of unique events)map[string]intMap of key/integersmap[string]numbermap[string]stringmap[string]time",
        "Merge Operations": "TheMERGEOPsyntax allows you to define merge behavior, that is, do you want to keep new incoming values or values from previous events? In the UI, you will only see the mergeops that will be valid for the data type. Single Value Fields (Scalar) Holds the latest value passed in to the fieldold_score                   KIND INT     MERGEOP latestHolds the first value seen for my_datemy_date                   KIND DATE     MERGEOP oldestHolds the oldest value passed in to the fieldold_score                   KIND INT     MERGEOP oldest Multi Value Fields (Non-Scalar) Only Store Latest Set (all previous values of set discarded)set(lists) AS lists KIND []string MERGEOP latestOnly store data for keys that were seen in the last event (The latest event will overwrite what is in the map. If only one key is found in the last event there will only be one row returned on the profile)map(key, attribute) AS mergeop_latest  KIND map[string]number  MERGEOP latestOnly store data for keys that were seen in the first event (The oldest event will overwrite what is in the map. If only one key is found in the last event there will only be one row returned on the profile)map(key, attribute) AS mergeop_oldest  KIND map[string]number  MERGEOP oldestStore the latest attribute for for each key (This will overwrite attributes NOT keys)map(key, attribute) AS mergeop_latestmap KIND map[string]number MERGEOP latestmapStore the oldest attribute for each key (This will overwrite attributes NOT keys)map(key, attribute) AS mergeop_oldestmap KIND map[string]number  MERGEOP oldestmapStore the latest attribute for for each key (This will overwrite attributes NOT keys. This will behave like mergeop latest for maps)map(key,attribute) AS mergeop_merge KIND map[string]number  MERGEOP mergeSum the attribute value per key as new events come in, map(\"attribute\") AS mergeop_sum  KIND map[string]int MERGEOP sumStore the minimum attribute for each keymap(key,attribute) AS meregeop_mapmax KIND map[string]number  MERGEOP mapminStore the maximum attribute for each keymap(key,attribute) AS meregeop_mapmax KIND map[string]number  MERGEOP mapmax",
        "Contentstack API Key": "Syncing your entry metadata to Lytics requires a Contentstack API Key for your desired stack Stack. Select \"Stack API Key\".Add your ContentstackStack API Key and Delivery Token.Click \"Complete\".",
        "Contentstack Authtoken": "Syncing profiles to Contentstack Personalize requires you to first generate anauthtokenfrom Contentstack.  Then, you'll add that authtoken as a new Lytics authorization. Select Contentstack Authtoken.Add your authtoken and the base URL of your Personalize Project.Click \"Complete\".",
        "Contentstack Entry Import": "Once you've created an Authorization for Contentstack, you'll start a Contentstack \"Import Entries\" job in Lytics to sync your entries and taxonomy with your LyticsContent Graph. Navigate to \"Data Pipeline > Jobs > Create New\" and select Contentstack as the provider.Select the \"Import Entries\" job type and then select your entry type (likely \"article\") and provide the domain on which you are serving content and have the JavaScript Tag installed.Click \"Complete\".",
        "Enable JavaScript Tag Plugin for Contentstack": "Once you've imported your Contentstack entries into Lytics, you'll want to enable Lytics' JavaScript Tag to sync profile data with Contentstack's Personalize Edge API. Navigate to \"Account > Settings > JavaScript Tag\".Enter theFieldsyou want to send to Contentstack's Personalize Edge API, and your Personalize Project ID.Click \"Save\". Once connected, you'll be able to use any LyticsProfile Fieldas anattributeto tailor content for different audiences, manage variants, and run A/B tests to enhance engagement and conversions.",
        "Contentstack Schema Sync": "You can send all of your profile fields to Contenstack automatically by starting a new \"Schema Export\" job. Navigate to \"Data Pipeline > Jobs > Create New\" and select Contentstack as the provider.Select the \"Schema Export\" job type and select which profile fields you'd like to sync over.  If you leave the field list empty, it will export all of the available fields in your schema.Click \"Complete\".",
        "Providing your AWS Keys": "Follow the steps below to authorize AWS with Lytics using your AWS keys. For more information on obtaining your keys, see Amazon's documentation onsecret and access keys. SelectAmazon Web Servicesfrom the list of providers.Select theAWS Keysmethod for authorization.Enter aLabelto identify your authorization.(Optional) Enter aDescriptionfor further context on your authorization.Enter yourAccess KeyandSecret Key.ClickSave Authorization.",
        "Lytics Delegated Authorization for Export": "Below is a set of instructions for how to set up delegated authorization. This method of doing authorization is more complicated than the AWS Keys method but some people prefer it: Set up Policy and Roles in your AWS Kinesis account.Contact Lytics support with your role Amazon Resource Name (ARN) that we grant permission to. Here are related Amazon reference documents: http://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_create_for-user.htmlhttp://docs.aws.amazon.com/streams/latest/dev/controlling-access.htmlhttps://docs.aws.amazon.com/IAM/latest/UserGuide/tutorial_cross-account-with-roles.html Stream Name: The name of the Kinesis stream that you will be writing to. It can be any alphanumeric string plus underscores, periods, or dashes. This example showslytics_triggers_stream.",
        "Contact Lytics to grant permission": "Lytics will need the role Amazon Resource Name (ARN) from your AWS account to grant permission to. ContactLytics Supportwith the ARN. It will look like this example if you followed the instructions above.",
        "Accessing Data from JavaScript Tag": "Once user fields have been surfaced, they can be used to personalize a user's experience. For example, a theoretical car buying site has surfaced some basic info about a user's last vehicle search results in the following data available in the browser.",
        "Populate a Form": "Create a JavaScript function that populates form fields. Initialize the callback handler. Create a callback: This will execute an action via JavaScript once the visitor has been identified and the profile has been loaded (using the function outlined above). Once the profile has loaded, Lytics will call the JavaScript function and populate the form on the site. Once all the above steps are completed, the final code is as follows: This is one basic example of leveraging the current visitor's profile information. The personalization powered by Lytics builds from here. Using a similar pattern, you could surface new listings on the site's index page that match a user's last search or present other vehicles that user may be interested in.",
        "Recent activity by channel": "This module gives an overview of which channels (email, web, mobile, etc.) an individual user has been active in. When appropriately mapped, it will also provide information on when users were active on each channel (within the last day or within the last 30 days). NOTE: If this module is empty, please see below for details on how to configure the user fields. As with all data mapping exercises, it is best to include your Customer Success representatives for optimal results. If you are customizing LQL or handling custom streams, it is important to understand how this module should be configured. For all our built-in integrations, Lytics includes this data mapping by default. At its core, the \"Recent activity by channel\" module is built from a single user field. This field will not exist in legacy accounts and must be added to the existing LQL manually. For all new accounts that get the latest default LQL files, the baseline should be included. In order to handle either custom data mapping for an account or backfill an old account, the user fieldlast_channel_activitiesmust be defined in a streams LQL file. This field is defined as amap[string]timeand should not be altered. In each stream you'll simply map a string, such as \"web\" or \"email\" to a timestamp such asepochms()in order to update the field. A more lengthy example for an email LQL might look something like: The example above is for when you only want to map the activity event in the case of a click or open. This definition will vary from stream to stream. When it comes to which keys to use, we recommend following these standards for common channels: emailadwebmobilesupport",
        "Unique identifiers": "Highlights the identifiers that have been used to materialize this particular user's profile.",
        "Event chart": "The event chart shows a user\u2019s activity by hour and day of the week. The top right of the chart highlights when a user is most active, for example Tuesdays at 10am.",
        "Audience Membership": "This tab displays a list of all the current Audiences a user belongs to and the total size of each audience. Audiences include factual information such as \u201cHas Email Address\u201d and \u201cKnown Location\u201d along with Lytics score-driven characteristics such as \u201cDeeply Engaged\u201d or \u201cBinge User\u201d. ",
        "Profile Details": "The details tab gives an under-the-hood display of the exact user fields contributing to a user\u2019s behavioral data and their audiences. Unique identifiers are shown first, which are essential for enabling cross-channel mapping. For all user fields, the associated stream and value is provided. ",
        "Profile Picture": "Some profiles may include a profile picture when viewed in the user interface. This image comes fromGravatar. If the profile in question has an email address, Lytics converts it to an MD5 hash. That hash is then used to look up the corresponding image on Gravatar. This image is only displayed in the Lytics UI and is not part of the profile itself.",
        "Cordial Real Time Authorization": "In theCordial API Keyfield, enter your Cordial API key.ClickSave Authorization.",
        "Cordial Bulk Authorization": "During the configuration process for authorizations of typeCordial Bulk, in addition to Cordial API key, you will also need the SFTP server credentials like hostname, port, username and password. In theCordial API Keyfield, enter your Cordial API key.In theHosttext field, enter the host name or IP address of the SFTP server you want to store files before importing to Cordial.In thePorttext field, enter the port number for the SFTP server.In theUsernametext field, enter the username for the SFTP server.In thePasswordtext field, enter the password for the SFTP server.In theFoldertext field, enter path to the folder to save the file. If the path is not given, Lytics will export the file to the user's home directory.ClickSave Authorization.",
        "Export Audiences (Real Time)": "Export Lytics audiences in real time to refine the targeting of your Cordial campaigns with behavioral insights from Lytics. For this job, all existing members of the selected Lytics audience are exported to the chosen list in Cordial and new members are exported in real time.",
        "Export Audiences (Bulk)": "Export Lytics audiences in bulk for use in your Cordial campaigns, leveraging rich, behavioral audiences powered by Lytics data science.For this job, all existing members of the selected Lytics audience are exported to the chosen list in Cordial and new members are exported in batch.",
        "What is a Document?": "Lytics considers every piece of content as a \"document\", and calls the collection of every document a \"corpus\". Lytics automatically processes every web page for an account's site, where each web page is a \"document\". In the Lytics app, documents are surfaced in association with topics and in content recommendations. Keep in mind that the Lytics Content Affinity Engine isn't limited to your website content. All sorts of content can be sent to Lytics, including product catalogs to power Product Recommendations.",
        "Finding Documents": "To search for a document in Lytics, go to theSearchpage under theContentmenu. Enter a URL or search term, then clickSearchto find the relevant documents. Clicking on a row will lead you to theDocument Summarypage, which contains more details about the Document.",
        "Details Tab": "On an individual document page you are able to see all the fields associated to that particular document within the Details tab.",
        "Collections Tab": "To see if this document is used in any Content Collections, you can view theCollectionstab. If you make adjustments to any of your documents, such as updating a blog post or refreshing a product landing page, you can request Lytics to manually re-classify the document. This will ensure the Lytics content corpus has the most up-to-date information to serve in any of your content or product recommendations.",
        "Adding New Documents": "By default, Lytics observes new URLs in all data streams to identify content with which the user is registering activity.  New URLs are enriched as they're observed in incoming data \u2014 that is, Lytics can crawl a domain, but proactively indexing a domain and looking for new content is not part of the Content Affinity Engine's workflow. To add new documents to your corpus: You can send new documents directly to our Content Corpus API.  The Corpus API respects three parameters: url: An optional URL for a new document.  While most documents are identified with a URL, documents aren't constrained to be web accessible.  In the event that you have custom content that is not web accessible, you'll need to supply the content via thetextparameter.text: An optional string of content for a document.  This is only really necessary in the event that the new document is not web accessible.topics: An optional list of topics relevant to the document. When Lytics tries to enrich the new document, the resulting topics will be appended to this list.  In the event that you only want your own custom topics added to a document, you'll need to contact your account representative atLytics Supportto remove all external enrichment settings. You can also use the manual classification module to preview the enrichment of a URL and then add it to the content corpus.",
        "Customizing Document Properties": "Adding custom properties to a document allows you to create advanced content collections based on those custom properties. Imagine creating recommendations from a pool of custom categories, promoting pages with custom seasonality components for particular holidays, adding a custom SKU hierarchy to better reflect your product catalog within Lytics, etc. The Lytics crawler can detect custom document properties from custom meta tags \u2014 any meta tag with a name property prefixed withlytics:will be ingested and appended onto a document.  For example, a meta tag with the namelytics:skuwill update theskufield on that document. When Lytics scrapes new documents, it will append those properties on the newly generated content entity within Lytics.  When adding a field that isn't currently represented within the content schema, the content query will need to be updated with the new property.",
        "Mandrill": "Connecting your Mandrill account gives you the ability to begin collecting email data in real-time. As users open and click on emails sent through Mandrill, Lytics will collect this information and make it available to build segments with.",
        "Import Data from Mandrill into Lytics": "Authorize Lytics with Mandrill, if you have not previously done so.SelectSetup webhooksfrom the list of workflows.This workflow doesn't require any additional configuration. ClickStart setupand Lytics will begin collecting data in realtime as users open and click through emails sent from Mandrill.",
        "Pinterest Sign-In": "If you haven't already done so, you will need to set up aPinterest Business accountbefore you begin the process described below. If you are new to creating authorizations in Lytics, see theAuthorizationsdocumentation for more information. SelectPinterestfrom the list of providers.Select thePinterest Sign-Inmethod for authorization.Enter your Pinterest login credentials in the login popup.In theLabeltext box, enter a name for the authorization(optional) In theDescriptiontext box, enter a description for this authorizationClickSave Authorization.",
        "Pinterest Conversion Token": "If you haven't already done so, you will need to set up aPinterest Business accountbefore you begin the process described below. Pinterest Conversion Token you will need to generate a conversion token, followthese instructionsto generate a token. Select thePinterest Conversion Tokenmethod for authorizationIn theLabeltext box, enter a name for the authorization(optional) In theDescriptiontext box, enter a description for this authorizationIn theTokentext box, enter your Pinterest Conversion Token credential.In theAccount IDtext box, enter your Pinterest account ID.",
        "Pinterest: Conversions Export": "Push your conversions to Pinterest. These conversions can then be used for re-targeting in campaigns and can be reviewed in conversion reporting for improved conversion visibility. Integration DetailsFieldsConfiguration",
        "Viewing existing Access tokens": "If this is your first time creating an access token, you will see a message prompting you to create a new token. Once you have created at least one access token, this page will display your tokens, their expiration dates, and the user who created each token. You can also click on a specific token to see the list of roles the token has been granted, the lifetime of the token, the created date, and an option to delete the token.",
        "Creating a new API Token": "To create your first token, click theCreate Newbutton and fill out the following fields on the modal that appears: ClickGenerate tokento create your token. You will be prompted with aone-timedialog window that contains the access token. Once you dismiss this dialog, it is impossible to see it again, so please copy and paste it somewhere secure.",
        "Deleting an existing API token": "When an API token is no longer needed, or you otherwise wish to remove one or more tokens, click on the token from the list, then click theDelete Access Tokenbutton to proceed with the deletion.",
        "Using Lookalike Model Percentiles": "Another option to build a Predictive Audience is by using theLookalike Model Percentilesfield. Similar to theLookalike Model Predictionsfield, the Lookalike Models are keys for theLookalike Model Percentilesfield. The percentile for a model represents the value at which a percentage of the predictions fall below. For example, the 80th percentile represents the prediction score at which 80% of all other scores fall below, or more simply put; the top 20% of users. Percentiles help account for the shape of a model's prediction distribution, as it can sometimes be hard to determine who the best users are based solely on the prediction scores, if the distribution is skewed is any direction.",
        "Criteo Real Time": "You would need Client ID and Client Secret in order to use this authorization. For this, you will need to setup anApp in Criteo's Developer Center. To set up an app, you must first create a Developer\u2019s Account and an Organization in Criteo\u2019s Developer Portal.  Upon creating an app, you will be asked whether you would like to use a developer key or a client ID & secret. Select client ID and secret and store your credentials in a secure place.  Please refer to thisCriteo documentationfor more information on how to create client id and client secret. Follow the steps below to add this authorization in Lytics: SelectCriteofrom the list of providers.Select theCriteo Real Timemethod for authorization.In theLabeltext box, enter a name for the authorization(optional) In theDescriptiontext box, enter a description for this authorizationIn theClient IDtext box, enter your Client ID credential.In theClient Secretpassword box, enter your Client Secret credential.ClickSave Authorization.",
        "Criteo OAuth": "When you select this authorization type, a Criteo LogIn pop up window will be visible. Once you log in, you will be redirected to theAdvertiser Consent Dashboard. You can select which portfolio would you like give Lytics access to in order to export audience.  Once provided access, Lytics will be able to send users to the advertiser. Please refer toCriteo OAuthdocumentation for more information on how OAuth works with Criteo. Follow the steps below to add this authorization in Lytics: SelectCriteofrom the list of providers.Select theCriteo OAuthmethod for authorization.Enter your Criteo login credentials in the login popup and confirm the authorization.In theLabeltext box, enter a name for the authorization(optional) In theDescriptiontext box, enter a description for this authorizationClickSave Authorization.",
        "Leveraging user data": "This use case will provide a few suggested audience definitions to support your retargeting effort. These audiences will leverage the following building blocks: Lytics preconfigured audiences such ascharacteristicsandbehavioral audiences.Commerce user fields such asLifetime Value (LTV)orLead Status.Lytics data science scores such asfrequency,recency, andintensity. Note:The Lytics StartSmart data schema supports a standard set of commerce fields including the two used in this use case. If you aren't using the StartSmart schema these fields may also be mapped to your account, for example if you've imported data fromSalesforceyou should have access to the fieldSalesforce: Most Recent Lead Status.",
        "Building your target audiences": "Using the building blocks mentioned above you can build one or more of these suggested audience to use in ad campaigns.",
        "Known High LTV": "Combine theEmail Capture Status: Known emailwith theUser's Lifetime Valuefield using the \"AND\" rule to create a target audience of just your best customers. This audience would be a great source audience to generate a lookalike audience. ",
        "Known Leads": "For B2B marketing you can substitute LTV with a field likeLead Statusto build an audience of open leads for retargeting or for use in a lookalike audience. ",
        "Refining your audiences with user fields": "Want to further narrow your audience to only the most qualified users? Try experimenting with additional rules using thefrequency scoreto bring in more consistent users, or therecency scoreto target users who recently interacted with your brand. These qualifiers can help you create a further refined audience of users who are more likely to convert.",
        "Activating your audience": "Once you've built your audience(s) you can export them to your ads platform for use using Lytics' build in integrations.",
        "Retargeting anonymous users": "Anonymous users can be advertised to directly in Facebook. You can access theAnonymous Deeply Engagedyou built in Facebook with theCreate Web Traffic Audiences workflow. You can also target anonymous users in Google platforms such as Adwords through ourJavaScript integrationwith Google Analytics.",
        "Reach new, qualified users with Lookalike audiences": "Known users can be targeted across both Facebook and Google, as well as drive lookalike audiences. You can export yourKnown High LTVandKnown Leadsaudiences viaFacebook custom audiencesandGoogle Adwords remarketing lists. To power lookalike audiences which will find users similar to your best customers, read more on creating alookalike audience in Facebook, or usingGoogle Customer Match.",
        "Recommend Content Campaign": " The Recommend Content campaign type is used to surface content (an article, a blog post, a product page) to visitors. Each recommendation is personalized to the individual. This could be used to keep visitors engaged by surfacing what to read next, or assist casual perusers by prompting a starting point. Furthermore, the campaign can be configured to only a set of pages or a subset of your total audience. This allows you to limit the recommendations from showing up on auxillary pages like the home page, company page, or checkout page. It can also only be shown to users with high usage. This way low usage visitors can be targeted with a more aggressive campaign.",
        "Objective": "This guide streamlines the integration of Customer Data Infrastructure (CDI) and Customer Data Platform (CDP) into your marketing technology framework. Given the overlapping functionalities of both tools, distinguishing their roles in marketing activation is crucial. By clearly assigning responsibilities and adhering to best practices for each platform, businesses can improve customer profiling, segmentation, and activation efforts, all while eliminating data redundancies and streamlining processes.",
        "Roles and Responsibilities": "Give clear roles and responsibilities to each tool in order to limit duplicative processes. CDI Responsibilities:Focus on data collection, routing, and transformation. Establish how it will handle website tagging and funnel web-based data appropriately. CDP Responsibilities (Lytics): Concentrate on activation, segmentation, and customer interactions. Define the specific data that should be directly ingested into the CDP.",
        "Key Implementation Steps": "Define Tool Roles:Clearly outline why each tool was chosen based on its strengths and intended use in your martech strategy. Data Flow and Schema:Align data types and formats with primary activation use cases, ensuring CDI processes non-marketing data and CDP focuses on customer engagement data. Identity Resolution:Develop a joint strategy for both platforms to unify customer profiles promptly, ensuring a seamless customer journey across all touchpoints. Activation Channel Allocation:Specify which platform will manage client-side and server-side activations to prevent overlapping functionalities and increased costs.",
        "Specific Considerations": "Client-Side vs. Server-Side:Clearly distinguish between the two, utilizing each for its strengths in anonymous and known activations. Unique Integrations:Treat similar functionalities, like JavaScript tags, as distinct integrations based on their roles within each platform.",
        "Ongoing Maintenance": "Regularly update your integration strategy within the Martech Center of Excellence to accommodate new tools and data sources, ensuring consistent and efficient use of your CDI and CDP platforms. This involves: Integrating new data sources as per established guidelines.Continuously evaluating and adapting to new features to solve existing challenges.Monitor the current solution to make sure roles should not be modified. By following this focused approach, businesses can efficiently leverage the strengths of CDI and CDP platforms, ensuring a cohesive and powerful marketing technology ecosystem.",
        "Types": "The implementation type refers to the development methods used to implement the job. Usually this depends on the third-party provider's capabilities and the resources they provide to outside developers such as APIs or SDKs. Integrations may be client-side or server-side.",
        "Client-side Integrations": "Client-side integrations are implemented in browser-facing code. These are implemented through the Lytics JavaScript tag and usually require you to have a tag from the third-party provider also installed on your web page. Given the nature of these integrations being tightly coupled to the browser session, these integrations collect and send data in real time. There are two types of client-side integrations:",
        "Push Integrations": "A push integration pushes data from Lytics, such as surfaced fields or audience membership of users to the third party. Examples of push integrations include personalization tools such asPathfora SDKwhich uses Lytics user data to personalize the page with content based on audience membership, or data management platforms (DMPs) such as Krux, which transmits Lytics user information to the DMP.",
        "Pull Integrations": "A pull integration pulls data from the third party and imports it into Lytics. This data will differ based on the provider, and the exact fields available in the audience builder will be listed in the respective integration document. An example of this type of integration includes the Lytics JavaScript tag communicating with a data layer object on your website to collect data from various third-party vendors such as Tealium or even a custom implementation.",
        "Server-to-server Integrations": "Server-side integrations allow Lytics and the third-party service to communicate directly, usually in the form of a job. These jobs are available to set up and execute in the Lytics platform underData > Jobs. There are two main types of Server-side integrations:",
        "Import Integrations": "As the name implies, import integrations ingest data into Lytics. This data will differ based on the provider and job type. It can be anything from identifying information, demographic data, subscription status, or activity data for a user. Examples of import integrations include the Amplitude Cohorts Import which imports user data from Amplitude.",
        "Export Integrations": "Export integrations send user profile data from Lytics to a third-party tool, usually at the audience level. You can select which audience(s) you want to export, and include user data such as user fields, audience membership and more such as the SendGrid User Export. However, some integrations may export raw event data or metrics for monitoring such as the BigQuery Event Export or the New Relic Insights Monitoring job. An enrichment integration uses a third-party service to enhance and enrich existing user profiles within Lytics. These integrations require Lytics to push a list of user identifiers (defined by an audience of known users) to a third-party enrichment tool. That tool then returns additional data on that user such as information on the user's company (for B2B marketing), or additional demographic and social data. Examples of enrichment integrations include Full Contact Audience Enrichment and Clearbit User Enrichment jobs. Many providers support both import and export job types for a complete two-way data sync between Lytics and the third-party tool.",
        "Implementation Techniques": "Client and server-side integrations may be implemented in a number of different ways. The implementation technique often determines factors such as Tier, Frequency, and Resulting Data.",
        "APIs": "API integrations utilize REST or SOAP APIs the third-party vendor provides to send or receive data. The specific endpoints used are listed in the documentation for the integration. Lytics prefers to integrate with third parties via API as it provides a deeper out-of-the-box integration.",
        "File-Based Transfer": "Lytics supports file-based import and export jobs. Files may be obtained or pushed to a server viaSecure File Transfer Protocol (SFTP). SFTP integrations may be provided for third-party vendors who do not have API capabilities or as part of a custom integration. Lytics has a managed SFTP server that can be used to transfer files, or it may retrieve files from a third-party SFTP server. Another example of a file-based export is viaEmail Attachment, such as the CSV Email Export.",
        "Backfill": "Jobs that use audience triggers also often have an option to perform a backfill of users. Without a backfill, only new users who enter the audience after the time of job setup will be processed. Enabling a one-time backfill as part of the job configuration will immediately process a batch of all existing users in the audience to the third-party tool and then rely on real-time triggers to update user information once the backfill is finished. Some jobs support this feature as a configurable option. If so, it is listed under the \"Frequency\" section of \"Integration Details\" in a job's documentation.",
        "Batch": "Batch integrations send or receive user data in larger batches. Depending on the configuration of the job, it may only import/export a single batch of users once, or it may run continuously, checking for new users and updates to existing user data. One-time run- You may want to run a one-time job to import, export, or enrich users at a specific point in time. For example, if you're running a one-time promotional blast email to a unique audience, exporting your audience once before sending the email would make sense.Continuous Update- Many batch jobs support a configuration option for continuous updates which will re-run the job on some cadence (hourly, daily, weekly, etc). This is common for import jobs for providers that don't support webhooks, as Lytics will want to keep the user profiles up to date with what is available in the third party.Scheduled- Some jobs may support specific scheduled run times, which can be especially helpful for continuous updates. It allows you to specify the time of day the job should run and the timezone.",
        "Resulting Data": "The final way Lytics categorizes integrations is by the type of data it handles and how the job output is perceived. The resulting export and push integration data will vary based on the structures supported by the provider tool, and each integration will document the specific output type. Here is a list of common data types in Lytics for import and pull integrations:",
        "User Profiles": "AUser Profileis the living record of an individual that interacts with your brand. Lytics user profiles provide a view of your customers across your connected channels. Known profileshave at least one known identifier such as an email or CRM ID.Anonymous profileshave only anonymous identifiers such as a cookie ID.",
        "User Fields": "The new user fields that are created as part of an integration are listed in the \"Fields\" section of each doc. These fields will be added to new and existing user profiles as described in the section above, and they will become available for segmentation in the audience builder.",
        "Raw Event Data": "If data is imported to Lytics without being mapped by LQL, this data is stored as raw fields in data streams, which are un-actionable. For example, if you are importing custom data via CSV, you will need queries in place for the raw data to be meaningful. In addition, some pre-built integrations may import additional fields that are not mapped as part of the default queries provided by the out-of-the-box integration. You may contactsupportfor assistance with mapping these fields.",
        "PGP Encryption": "File imports and exports can be decrypted and encrypted, respectively usingPretty Good Privacy(PGP). To create or ingest PGP encrypted files, use an authorization that includes PGP keys. PGP Public Keyauthorizations are used to encrypt exports.PGP Private Keyauthorizations are used to decrypt imports. Follow the instructions below when creating a PGP authorization.",
        "PGP Public Key": "Use a PGP Public Key authorization to encrypt file exports. You will need togenerate an armored PGP key pairif you haven't already. In the PGP Public Key field, enter your public key. Be sure to include the header and footer of the armored key. This means your entry should begin and end with the following: -----BEGIN PGP PUBLIC KEY BLOCK-----and-----END PGP PUBLIC KEY BLOCK----- If your key does not begin and end with these strings respectively, it is possible it has not been ASCII armored.",
        "PGP Private Key": "Use a PGP Private Key authorization to decrypt file imports. You will need the private key that is paired with the public key used to encrypt the file. In the PGP Private Key field, enter your private key. Be sure to include the header and footer of the armored key. This means your entry should begin and end with the following: -----BEGIN PGP PRIVATE KEY BLOCK-----and-----END PGP PRIVATE KEY BLOCK----- If your key does not begin and end with these strings respectively, it is possible it has not been ASCII armored. If your private key has been protected with a passphrase, enter the passphrase in the field labeledPGP Private Key Passphrase. If your private key is not passphrase protected, leave this field empty. Note: in order for Lytics to decrypt the file successfully, you will need to provide the private key that is paired with the public key that was used to encrypt the file.",
        "Share Lytics Audience Definitions with AdRoll": "Log into yourLytics account.ClickData>Integrationsand selectAdRollfrom the integrations list.Click theAdRoll Segment Synctab.Identify the authorization you would like to use and clickSelect.Determine if you would like to keep the audiences in Lytics and AdRoll in sync. If not, ensure the \"Keep Updated\" option is deselected.Click theStart Exporttab. Lytics will begin adding Audiences to AdRoll immediately but it may take a few minutes to see all audiences depending on total number.",
        "Configure your AdRoll Campaign to Use a Lytics Audience": "Log into yourAdRoll account.SelectCreate Campaign.ClickWebsites & Appsto get started.ClickAudienceand thenChooseat the top right.Select one or more audiences from the list and then clickChoose.Continue on with your standard ad selection or creation process.",
        "Collect Leads Campaign": " The Collect Leads campaign type is used to convert unknown visitors into known leads by presenting a form to gather personal information at the right time. It can be configured down to just an email address or up to a full form, requiring job title, phone number, and organization name. Furthermore, the campaign can be configured to only a set of pages or a subset of your total audience. This allows you to limit the collect leads form to only engaged readers who are likely to sign up, whitepaper download pages where a gated collect leads form will perform well, or only unknown visitors so the people loyal to you don't get bothered every visit.",
        "Message format for Subscription Events": "When a user profile is updated, it may be due to: A new data event.Scoring gets updated ocassionally.A scheduled trigger evaluation. A trigger of \"has not logged in last 7 days\" may get scheduled to be evaluated 7 days after last x event. When the user gets updated, segment membership is reevaluated and segmentsa user has moved into or out of triggers a message. Here is an example of the message that is produced.",
        "Azure Storage Connection String": "Use the Microsoft Azure Storage Connection String to import or export files to and from Azure Blob storage. Please refer toAzure's documentationto setup the connection string for the Blob storage, and then follow the steps below to connect Lytics with Azure: In theConnection Stringbox, enter the connection string for the Blob storage.(Optional) In thePGP Keybox, enter a PGP public key to encrypt Azure Blob files.",
        "Azure Event Hub": "Use the Microsoft Azure Event Hub authorization method to export Lytics Audiences to Azure Event Hub. You will need theEvent Hub Connection String. Once obtained, enter the connection string in theEvent Hub Connection Stringbox.",
        "Azure SQL Database": "For Azure SQL Database you will need the following credentials. Note that if your Azure SQL server enforces IP firewall rules, you will need to add Lytics production IPs to the allowlist. Please contact your Lytics account manager for a range of IPs. In theServertext box, enter your Azure SQL server.In thePorttext box, enter your Azure SQL server port.In theDatabasetext box, enter your Azure SQL database.In theUsertext box, enter your username.In thePasswordpassword box, enter your password.",
        "Import SQL Table": "Azure SQL Databaseis an intelligent, scalable, relational database service built for the cloud. By connecting Lytics directly to your Azure SQL instance, you can easily import full tables of audience and activity data to  leverage Lytics' segmentation and insights.",
        "Export Audiences (Blob Storage) JSON": "Export your Lytics audiences to Azure Blob Storage asnewline JSON files, which can then be imported into other systems for improved segmentation and targeting.",
        "Export Audiences (Blob Storage) CSV": "Export your Lytics audiences to Azure Blob Storage as CSV files, which can then be imported into other systems for improved segmentation and targeting.",
        "Export Audiences (Event Hub)": "The export job to Azure Event Hub allows you to send audience triggers when users enter or exit your Lytics audiences.",
        "Export Events (Blob Storage)": "Export events into Azure Blob Storage so you can access, archive, or run analysis on Lytics events in the Azure ecosystem.",
        "Pushing a one-off audience to Lytics": "In this example, we\u2019ll use Google BigQuery as our data warehouse and querying tool. Once notifying your account manager, your Lytics instance will be configured to pull data from a table called lytics_custom_audience_push with the following schema: customer_id(string)custom_audience_name(string)custom_audience_value(string)",
        "Notes": "Thelytics_custom_audience_pushtable is configured to accept strings as values. If you need to push a different data type for the value, let us know. We can also configure a table and mappings for integers, floats, and timestamps.One-off audience pushes are typically used for one downstream campaign. If you find yourself repeatedly targeting a one-off audience, talk to your Lytics account manager to have that audience reconfigured as a user attribute.If you are using a different data warehouse, you can send the table as a CSV to anSFTP serverorS3 bucketfor Lytics to pull.",
        "Present a Message Campaign": " The Present a Message campaign type is used to show any general purpose message to visitors. This could be used to broadcast sitewide promotions, mention in-store deals, or one of many other messages. Furthermore, the campaign can be configured to only a set of pages or a subset of your total audience. This allows you to limit the message from being seen on landing pages with highly focused calls to action, or to only users who meet the behavioral conditions for the message.",
        "Goals": "Lytics Goals allow you to guide users through highly personalized experiences that increase customer lifetime value and drive progress towards your primary business goals. You will use the Lytics Canvas to create, edit, and monitor your Goals. Navigate toGoalsand clickNew Goalin the top right of your screen to create a new one.",
        "Choose an audience": "The first step in configuring a Goal is picking your starting audience. Select this audience by clicking the \"Audience\" button (blue circle) on the left-hand side of the Lytics Canvas. You can choose an existing audience or you can build a new one by clickingNew audience.  Lytics recommends keeping this starting audience as broad as possible, as there will be ample opportunity to add filters and refine your targeting through Stages and Experiences.",
        "Stages": "Stages are the individual steps that move customers towards your Goal. Users in your overall audience will automatically enter the first Stage. When a user completes the first conversion event, they will move to the second Stage, and so on through the last Stage within your Goal. A user can only be in one Stage at a time. ",
        "Moving through Conversion Events": "Users move to the next Stage when they complete the selected conversion event. Goals are always configured so that users move through your Stages from left-to-right as they complete conversion events. Keep in mind that users don't always complete conversion events in the order anticipated, which means they may end up skipping a Stage. If you want to prevent this, you can include additional logic in your conversion event definition. And remember, users can never be in more than one Stage per Goal at a time.",
        "Experience Drawer": "The Experience drawer contains Experience templates and Experiences that aren't part of a Goal yet. Add any of these to your Goal by dragging them into a Stage. Open the drawer by clicking the icons labeledExperience templateorExperience.  If you drag an Experience that is activated into your Goal, you will need to pause it before being able to save your Goal. You can do so from the Lytics Canvas by hovering over the Experience card and then clicking thePausebutton.  Once you're ready to reactivate the Experience in the context of the Goal, clickResume.",
        "Prioritizing Goals": " Much like prioritizing Experiences within Stages, you can also prioritize Goals. Since membership is not mutually exclusive (that is, users can be moving towards multiple Goals at a time), it is possible that a user will be eligible for two simultaneous Experiences from two different Goals. When that happens, Lytics will take into account Goal priority when determining which Experience will be delivered. The higher on the Goal list, the higher the priority.",
        "Conversion events": "Conversion events within Lytics are behavior-based audiences such as purchasers, highly engaged users, known users, etc. These determine which users are eligible for the Experiences within a Stage. When a user fulfills conversion event applied to a Stage, they move to the next Stage until they reach the last conversion event for that Goal. If users have completed the conversion event applied to a Stage, theywill notbe eligible for any of the Experiences in the Stage. This prevents overlapping targeting such as continuing to show a Facebook ad to a user who has already converted on a particular campaign.  To assign a conversion event to a Stage, click the \u201cTrophy\u201d icon to open a menu of available conversion events. A red dot next to the \u201cTrophy\u201d icon indicates a Stage that still needs to be assigned a conversion event. A conversion event is required to populate the metrics for a Stage.",
        "Stage metrics": "Lytics tracks three key metrics to measure Stage performance:Potential reach,Converted, andConversion rate. The table below defines these metrics and how often they are updated in the Lytics UI.",
        "Adding and rearranging Experiences": "Experiences can be added to a Stage in several ways. You can add an Experience directly to a Stage by clickingAdd a new Experiencealong the bottom of a Stage.You can drag an Experience template into a Stage, which will create an Experience with boilerplate settings to use as a starting point. You can also drag an existing Experience into a Stage, either from another Stage or from the Experience drawer. Note:This will significantly change the audience for that Experience.",
        "Prioritizing Experiences": "To ensure that your audience receives the right amount of messaging, each user will only be eligible for one Experience per Stage at a time. That means that no matter how many web modals you add to a Stage, your audience members will only see one at a time. Lytics won't just choose one at random though; there is a sophisticated decision engine that determines which Experience will be delivered to the end user. Part of the input into that engine is the order of Experiences within a Stage. The closer to the top of a Stage an Experience is, the more likely it is that it will be delivered to the end user.",
        "Goal Canvas Audiences": "You can think of the Lytics Canvas as an intelligent audience builder. When you create new Stages and add conversion events, you are effectively building new audiences based on the desired flow of users towards achieving your Goal. Unlike drip campaigns or more traditional journey builders, Lytics doesn't require you to build complicated audiences. Keep them simple, and the Lytics Canvas will do the heavy lifting for you.",
        "Selecting audiences for your Goals": "The following tips will assist you when building and selecting audiences for the conversion events and Experiences within your Goals.",
        "Building block audiences": "When you\u2019re creating audiences to use in a Goal, it\u2019s helpful to think about each audience as a \"building block\" rather than a complete definition of who you want to be targeting. Below are examples of good building block audiences: All users Users who have shared their email address Users who have purchased something",
        "Excluding users": "Building block audiences work best when they don\u2019t exclude anyone. It can be tempting to try to exclude users from a building block audience \u2014 for example, excluding \u201cKnown Users\u201d from \u201cAll\u201d in order to target only anonymous users \u2014 but the Lytics Canvas will do this for you automatically if one of your future conversion events is the audience \"Known Users\". NOTE:If you do end up doing this yourself, you can create a \u201cdead-end\u201d path, where users are unable to progress due to the definition of the automatically generated audiences, which are explained in more detail below. There is one important exception to this rule.If you want tosuppressa subset of your entire audience from your Goal, you can exclude them from your \"overall audience\". This will work so long as the exclusion rule isnotrelated to the downstream conversion events. An \u201coverall audience\u201d is the first audience you select on the Lytics Canvas, which is represented by the left-most blue circle as shown below. For example, if you used the audience below as your overall audience, you would exclude \"Realtors\" from the entire Goal. This will not create a \"dead-end\" if none of the subsequent Stages have conversion events dependent on the \"Realtors\" criteria. All users, excluding Realtors",
        "Filter audiences": "If you want to target with more granularity than a Stage audience allows, you can add a filter on individual Experiences within a Stage.  This is a great point to target users based on behavior, such as engagement level. The audience you choose here will be added to your Stage audience with an \"AND\" statement.",
        "Experience conversions": "A common pattern may be that you want any users who converted on the Experiences in your Stage to enter the next Stage. Learn how to target users based on Experience interactions to build a conversion event like this.",
        "How audiences are used in the Canvas": "Now that you've learned how to select and build audiences, take a deeper look into how the Lytics Canvas uses these audiences to help users flow towards your Goal.",
        "Automatically generated audiences": "The Lytics Canvas automatically generates audiences based on your selected conversion events. For example, let's say you want to create a the first Stage to target users who haven\u2019t shared their email address, and the second Stage will target known users who haven\u2019t purchased yet. Even though you haven\u2019t explicitly created an audience for users who haven\u2019t shared their email, or users who haven\u2019t purchased, Lytics will create these audiences for you. This example would look as follows in the UI: The following graphic shows how the Lytics Canvas takes your simple building blocks and creates more refined audiences.See the full size diagram. Remember, if you exclude converted users on a building block audience, it can cause problems and prevent users from moving between Stages. Let the Lytics Canvas do the work for you!",
        "Mutually exclusivity": "Stage audiences aremutually exclusive, meaning a user can only be in one Stage at a time. This is a result of the automatic audience building done by the Lytics Canvas. If a user in \u201cUnknown Users\u201d shares their email and now qualifies for \u201cKnown users\u201d they will automatically move into that Stage without any manual intervention.",
        "Goal Intelligence": "The Goal Intelligence Report provides a holistic view of how your existing campaigns are contributing towards the marketing goal you selected during the onboarding process. This report will help you answer questions about how effectively your marketing campaigns are driving conversions, how many engaged customers you have, and whether you are producing the right content for your users. The Goal Intelligence Report contains modules that focus on answering a specific marketing question. Each module is described below, along with definitions for the associated metrics and how often they are updated. ",
        "Goal Progress": "The Goal Progres module contains three top-level metrics for the campaigns you\u2019ve imported into Lytics. Use this summary to measure how your campaigns are working together to achieve your goal.",
        "Connected Customers": "Connected Customers represent the number of active users who are engaged with your brand across multiple channels. This is a Lytics proprietary KPI that offers a quick and comprehensive understanding of the performance of your account\u2019s identity resolution strategy. Lytics defines a Connected Customer as a user who has: Activity data collected from 2 or more channels, including web, email, or mobile.A positive and increasing Momentum Score, which represents the rate at which users are interacting with your brand. You can find the Connected Customer KPI in the Goal Intelligence Report. These  metrics come from a Lytics audience that is automatically generated under-the-hood using the criteria described above. ",
        "Content Classification": "The Content Classification module shows a list of the topics, documents, and relevance scores for the content used across your connected channels. Lytics automatically analyzes the topics within your website content and determines user-level affinities for those topics, which can help you decide what content to produce. Use these metrics to ensure you are creating the right content to engage your target audience.",
        "Why is this KPI important?": "Connected Customers are valuable to your business. This KPI helps you understand how many Connected Customers you have and the impact of your marketing programs on user engagement. Lytics helps you gain more Connected Customers by refining your audience targeting and messaging strategies to improve engagement and ultimately increase the ROI for your marketing efforts.",
        "What can I do with it?": "Track your Connected Customers for a pulse on engagement levels across your active campaigns. Use this KPI to create a stronger base of users who are more likely to convert on current and future campaigns. You can also activate this Connected Customer audience in your downstream marketing tools such as Facebook, Salesforce Marketing Cloud, and more.",
        "Import Custom Object": "Importing custom objects from Salesforce allows you to enrich Lytics profiles with sales and CRM data from your Salesforce account.  This will aid in targeting for your marketing efforts.",
        "Salesforce Administrator Setup": "You'll need to involve a Salesforce administrator when setting up the export. The administrator will need to perform the following setup: Set up aDuplicate Rulein Salesforce for leads (configuration pictured below).Set up the following Custom Fieldson the Salesforce \"Lead\" object: For \"Send Audiences To\"Field Type: Text Area (Long)Field Label: Lytics AudiencesField Length: 32,768 characters (32 KB)Field Visible Lines: 3Field Name: Lytics_AudiencesField Description: This user is a member of these Lytics AudiencesVisibility: Checked for all usersRead-Only: Not checked for any usersLayouts: Add to all availableOptional: For \"Send Content Affinity To\"Field Type: Text Area (Long)Field Label: Lytics Content AffinityField Length: 32,768 characters (32 KB)Field Visible Lines: 3Field Name: Lytics_Content_AffinityField Description: The top three kinds of content this user prefersVisibility: Checked for all usersRead-Only: Not checked for any usersLayouts: Add to all availableOptional: For \"Send Scores To\" (optional)Field Type: Text Area (Long)Field Label: Lytics ScoresField Length: 32,768 characters (32 KB)Field Visible Lines: 3Field Name: Lytics_ScoresField DescriptionVisibility: Checked for all usersRead-Only: Not checked for any usersLayouts: Add to all available ",
        "API Limitations": "Salesforce has limitations for the maximum number of daily API Calls. An API call is a connection made to a Salesforce server on your account's behalf. Lytics provides some guidelines below for working with these limits for your integration connection. Different accounts have different limits on the number of API calls allowed per 24 hours. You can find more information about these limits in theSalesforce documentation.In addition to your Lytics integration, your Sales team may have additional integrations using the Salesforce API. For example, they may use a tool to capture LinkedIn information and send it into Salesforce, which would also use their API. To estimate an appropriate number of API calls to enter for the Salesforce import, Lytics recommends you use the following formula: (Total Daily API Calls Allowed - Daily API Calls For Other Integrations) / 2 Dividing by two allows you to allot the same number of API calls to the upcoming Salesforce Export. To roughly estimate how long your initial import will take, use the following formula: (Total Leads + Total Contacts + Total Opportunities + Total Accounts) / Total Daily API Calls Allowed for Lytics Import This will yield the approximate number of days that will be required. Note: this estimate will be slightly low because each time the import encounters an API limit, it sleeps for 24 hours rather than restarting at the same time the following day.",
        "Styling Tweaks": "You may style the form usingcustom CSS. The example in this guide applies some small CSS changes to adjust the size and spacing of elements within the widget. Remember you candownload the code for this exampleto get the complete JavaScript, CSS, and API override. This can act as a starting point for your own custom form widget  Once you've tested and are happy with the look and feel of your form slideout, it's time for the most important step: defining the Lytics audience that should receive this experience.",
        "Event Stream Authorization": "This authorization is used for theImport Eventsjob. You will need tocreate an access tokenfor Lytics to use to authorize with Airship. After creating an access token and retrieving your app key, follow these steps configure the authorization between Lytics and Airship. In theApp Keyfield, enter your Airship app key.In theAccess Tokenfield, enter your Airship Token.(Optional) From theAirship Project Locationdropdown box, select the location of your Airship project location. The default is \"US\". ",
        "Basic Master Authorization": "This Authorization is used for theImport Compliance Eventsand theExport Audiencesjobs. In theApp Keyfield, enter your Airship app key.In theMaster Secretfield, enter your Airship Master Secret.(Optional) From theAirship Project Locationdropdown box, select the location of your Airship project location. The default is \"US\". ",
        "Platform Limits": "The following limits exist to ensure the optimum performance of the Lytics platform and your connected sites, data sources, and applications. The tables contain default limits for all accounts unless stated otherwise. If you have questions, please get in touch with your Lytics Account Manager.",
        "Browsers and Cookies": "The following limits apply to client-side integrations, which are implemented in browser-facing code through the Lytics JavaScript tag and typically a third-party tag as well.",
        "Custom Modeling": "Lytics enables you to build custom Lookalike Models as a proprietary service that uses your first-party data to evaluate audiences and conversion rates.",
        "Creating Components": "You can select from a list of Component types by clicking the+ Add New Componentbutton on the Report Page. Click on the desired component to add it to your Report.",
        "Managing Components": "Lytics provides a handful of methods of managing your Report Components. Configuration: Once you've created your Component, click on theEdit Componentbutton in the lower right corner to configure your Component.Minification: To minimize, or hide a component, click on the minimize(-) icon in the upper-right corner of the Component.Stacking: Merge multiple data points into a single visualization or separate each point into its unique component for streamlined consumption.Shrink/Expand: If your Report has many components, it may be desirable to 'shrink' your components to display more information on a single page. To shrink your components, click on the \"Shrink\" icon in the upper-right corner of the Component.Deleting: Click on the expanded menu in the upper-right corner to delete your component.Hiding/Showing Missing Values: By default, \"null\" values are hidden from view. This prevents individual component visualizations from being offset by profiles with missing data. For instance, in the two examples below, you'll see an overview of visitor geo, but if we \"show missing values,\" you'll see a large portion of the users in the audience do not have a geo value at all. This may be preferred in some cases and can be toggled on or off to meet your needs.Organization: As you add components, you will likely want to customize where and how they appear. The location of a particular component can be adjusted easily by simply clicking any edge of a component and dragging it to its desired location. Don't forget to save your changes so they are retained the next time you visit your report.",
        "Size Comparison": "Size Comparisoncomponents allow you to display multiple Audience Sizes on one chart. This component can help compare multiple audiences simultaneously, commonly used for variation testing.",
        "Creating and Editing Size Components": "To create aSizecomponent, select one or many Audiences in theEdit Componentmenu. In the configuration menu, you can set the Component'sname,description, andAudiencesbased on the selectedtable. After selecting and saving the Audience(s), the Component will display a time-series chart of the Audience size(s). ",
        "Customizing Size Components": "Once aSizecomponent is created, thetime-rangeand thestackedoptions can be selected. By default, theSizecomponent will chart the last seven days of data. To customize the start and end dates, click on the dates in the upper-right section of the component.  To display the Audience sizes on separate charts, toggle theStackedoption in the lower left section of the component.  Sizecomponents can also be visualized as a single number by selecting theNumberchart type. ",
        "Audience Overlap": "Audience Overlapcomponents allow you to understand the intersection or lack thereof between two or more Lytics audiences.",
        "Creating and Editing Overlap Components": "To create anOverlapcomponent, select one or many Audiences in theEdit Componentmenu. In the configuration menu, you can set the Component'sname,description, andAudiences. After selecting and saving the Audience(s), the Component will display a Venn diagram representing each audience and their collective overlap. ",
        "Data Flow": "Data Flowcomponents allow you to see how data flows through your connected Providers, Data Streams, Audiences, and Destinations.",
        "Creating and Editing Data Flow Components": "To create aData Flowcomponent, select up to 5 Audiences in theEdit Componentmenu. In the configuration menu, you can set the Component'sname,description, andAudiences. After selecting and saving the Audience(s), the Component will display a Bipartite diagram containing each of the selected Audiences and the connected Providers, Streams, and Destinations. TheEdit Componentmenu also provides options to hide (or display) the Providers, Streams, or Destination columns. This allows the user to \"zoom in\" on different aspects of the Data Flow diagram. ",
        "Composition": "Compositioncomponents allow you to visualize a field in theuserorcontentschema across one or many Audiences. This can be useful to compare a field across Audiences or to analyze the data distribution for a group of users.",
        "Creating Composition Components": "To create aCompositioncomponent, click on theEdit Componentbutton. In the configuration menu, thename,description(optional),Audiences,FieldandSubFieldcan be selected. Depending on the field type, Lytics provides a variety of ways to visualize the data. The data can be viewed as a Bar Chart, Line Chart, Pie Chart, Table, or Stats view for numeric fields. The following table shows all of the chart types for each field type.",
        "Chart Types": "Bar Chart: Display your data as a horizontal Bar Chart, where each Audience is represented in a different color. The labels correspond to each value of theField, and the values are the number of users with the field value.Example: the Bar Chart below shows the number ofRewards Usersacross 2 Audiences.  Line Chart: Display your data as a Line Chart forfieldsof typenumber. The x-axis displays the field value, and the y-axis displays the number of users with a given value.Example: the Line Chart below shows the distribution of users with anAffinityfor Candles.  Pie Chart: Pie Charts are particularly useful for categorical fields (iestring, []stringtypes).Example:the Pie Chart below shows the number ofRewards Usersacross 2 Audiences.  Table: the Table view presents categorical or numeric data in a table format.Example:the table below shows the most common _UTM Sources_across 2 Audiences.  Stats: the Stats view displays statistical information fornumericfields.Example:the table below shows themean, min, max, standard deviationandnumber of usersacross two different Audiences. ",
        "Managing Composition Components:": "Once you've created aCompositioncomponent, the bottom navigation bar allows you to customize your component. Here, you can select your desired chart type, toggle between stacked and unstacked charts, download data, and customize your component.  Stacked: depending on the chart type, selecting theStackedoption will display the data on different charts. The image below shows an \"unstacked\" Bar Chart.  Chart Types: The chart icons in the navigation bar allow you to switch between different chart types. Once you change a chart, you can save your changes by clicking on theSavebutton in the pop-up message:  Editing Your Component: theEdit Componentbutton on the right side of the navigation bar allows you to modify thename,description,Audiences, andField/SubFieldassociated with your component. Once your changes are saved, the data and charts will update.",
        "Example: Charting UTM Data": "As a first example, we'll walk you through creating your firstCompositioncomponent usingdevicedata. ",
        "Example: Using Sub-Fields": "As a more complex example, we'll chart the distribution of users' affinity for a Content Topic across two different Audiences. ",
        "Google Sign-In": "Click Google Sign-in to initiate the authorization flow.Log in to the Google account you want to authorize and follow Google's instructions.",
        "GA4 Measurement Protocol API Secret": "To authorize with a GA4 Measurement Protocol API Secret you will need an API secret created in Google Analytics. Your GA4API Secretcan be found in the Google Analytics UI underAdmin > Data Streams > choose your stream > Measurement Protocol > Create. See the GA4documentationfor more information In theLabeltext box, enter a name for the authorization(optional) In theDescriptiontext box, enter a description for this authorizationIn theAPI Secrettext box, enter your API Secret credential.",
        "DV360 Delegated Auth": "The Google DV360 export workflow does not require an authorization, as authorization is delegated to the Lytics DMP Google Ads account. In order to export to DV360 successfully, you will need to link your DV360 advertiser account with the Lytics DMP account. This is done in Google DV360 via the process describedhere. The Google DV360 export workflow requires the DV360 tag sync to be enabled in the Lytics account settings first. Thisconfigurationcan be found in Vault, underSettings > Javascript tag.",
        "Google CM360 Sign-In": "Click Google Sign-in to initiate the authorization flow.Log in to the Google account you want to authorize and follow Google's instructions.",
        "Google Analytics 4 (GA4)": "GA4, which was first released into beta in 2019, has replaced Universal Analytics (UA) as the default version for Google Analytics installations. GA4 offers a great deal of flexibility and power over its predecessor. The following document outlines the preferred method for enriching GA4 with an individual's profile information, such as audience membership and Lytics user ID. There are a variety of robust use cases enabled by this integration, most notably the ability to filter GA4 dashboards by Lytics Audiences as well as distribute those audiences efficiently across the Google ecosystem from GA4 directly. Lastly, web properties are complicated by nature. The following recommended approach aims to focus on ease of installation while also preventing a limitation of the power of GA4. Please note that it is always recommended that you consult with your technical teams or partners before proceeding.",
        "Step 1: Creating Dimensions": "To attribute onsite events such as page views or purchases, you must first define newCustom User Scoped Dimensionsto store the set of audiences and/or User ID depending on preference. This can be done through our server-to-server integration, in which we'll create two properties:Lytics User ID (ly_user_id_dim)andLytics Audiences (ly_segments_dim). Alternatively, because we'll pass the data to Google via JavaScript, you can create whatever dimensions you want by followingGoogle's documentation. If taking the manual approach, just be sure to use theuserscope option during creation. For details on our server-to-server integration please view ourGoogle Analytics: Setup Custom Dimensionsinstructions.",
        "Step 2: Enabling Lytics Audiences": "Once the necessary dimensions exist within your preferred Google Analytics account, you'll need to ensure all audiences you would like to send to GA4 are flagged asAPI Enabledfrom the Audience edit area of Lytics. This feature ensures no data is ever shared or passed to another tool without your explicit instructions to do so.",
        "Step 3: Configuring GA4": "Finally, leveraging the profile listener feature of our JavaScript tag, we'll get details of the current visitor and pass those along to GA4 via your new dimensions. Since there are a nearly infinite number of ways sites can be configured and constructed, it is always best to consult your technical team on this step. Here we'll outline the most common approach assuming that GA4 tag installation default instructions were followed. A typical installation of GA4 is completed by installing a few lines of JavaScript on your site. Typically in the header or footer. This will also need to be modified to include the setting of your custom dimensions as well as optionally altering how the initial pageview event is fired. The following example shows ultimately how we'll be sending data to GA4. To pass the current user's audience information as well as optional user ID, you'll first need to retrieve the profile via our personalization API. Luckily the Lytics tag makes this simple using ourentityready listener. The Lytics tag will call the desired function once it has identified the user and accessed their profile. Using the information returned, we'll send the user properties as outlined above. We'll alter the default listener example to verify we did receive audience membership data and then pass that along to GA4 as ourlytics_segments_dimdimensions or whatever you named it during the manual creation process. In this case, we also log a warning to the console when a set of audience memberships are unavailable for debugging purposes. For demonstration purposes, a full HTML example of loading the GA4 tag, Lytics tag, and listener function might look like the following: At this point, you should successfully pass audience membership to GA4 for each user. Given the flexibility of GA4, there are a variety of configuration options. That said, there are a couple of common points that are worth considering. The load order of tags is essential. For this to work correctly, you'll want to load tags in this order:GA4Lytics TagLytics ListenerIt may be desired that the GA4 default pageView tag is delayed until the profile information has been set. This ensures all events are associated with the user but could impact the efficiency of tracking page views. This is only recommended for advanced users with a firm understanding of JavaScript. In this case, you would disable the default page view asdescribed hereand then fire it manually following the setting of the user properties in our example.It may be desired to send an event to GA4 noting when the properties are set. This can be done following the entityready function and the user properties being set. For more information on the various ways to configure events, consultGoogle's documentation.When testing your integration, Google supports adebugsetting in their tag configuration settings. These settings allow access to a debug view where you can see the events stream in real-time to confirm proper behavior.",
        "Step 4: Add GA4 Audience": "The final step of this integration is to followGoogle's recommended approachfor building an audience inside of GA4 using custom dimensions. When prompted which dimension to use, you'll want to select thely_segments_dimor whatever custom parameter you created and then match based on the value of your audience or a regex match. When passing the audiences to GA4, Lytics converts them to astringseparated by commas as Google does not accept arrays. As such, the dimension value will look similar toaudience1,audience2,audience3.",
        "Google DV360: Cookie Matching Export": "Combining this export job with the latest version of the Lytics JavaScript Tag allows you to create powerful audiences for targeting in Google DV360.",
        "Google DV360: Customer Match Export": "Export your Lytics audiences to Google DV360 Customer Match to reach and re-engage with your customers across Google Search, Google Shopping, Gmail, and YouTube. Learn more aboutGoogle DV360 Customer Match. Refine your targeting efforts using Lytics audiences containing rich information on user behavior and content affinities across channels. This workflow targets audiences within Google Display & Video 360 (DV360), which is part of Google\u2019s programmatic advertising platform. DV360 allows you to manage and run display, video, audio, and even native advertising campaigns across the web.",
        "Google Marketing: GA4: Measurement Protocol Event Export": "The Google Analytics Measurement Protocol for Google Analytics 4 allows users to augment their automatic collection via gtag, Tag Manager, and Google Analytics for Firebase. Export Lytics event streams via GA4 Measurement Protocol to tie online to offline behavior, or to send events to Google Analytics that happen outside user-interaction (e.g. offline conversions). Integration DetailsFieldsConfiguration",
        "Google Marketing: GA4: Measurement Protocol Audience Export": "The Google Analytics Measurement Protocol for Google Analytics 4 allows users to augment their automatic collection via gtag, Tag Manager, and Google Analytics for Firebase. Export Lytics audiences via GA4 Measurement Protocol to tie online to offline behavior, or to send events to Google Analytics that happen outside user-interaction (e.g. offline conversions). Integration DetailsFieldsConfiguration",
        "Google CM360: Offline and Enhanced Conversion [Beta]": "Export Lytics audience user data to Google CM360 to update the existing conversions or introduce new ones to help improve your campaign performance.",
        "Enforce two-factor auth.": "Two-factor authentication is a technique that helps to make your account more secure. It does this by adding a second step to your login process. Single-factor authentication uses your email address and password to authenticate your Lytics session. The second factor comes from the Authy app using an Authy SoftToken, a secret token that changes every 20 seconds. Entering a correct token provides an extra level of verification.",
        "What is an Authy SoftToken": "An Authy SoftToken is a secret token that is broadcast to the Authy app every 20 seconds. This unique token serves as a second factor by which Lytics can authenticate your session. Authy is available to download for free as a desktop and mobile app:Download Authy.",
        "Using Two-Factor Authentication": "The only difference between two-factor authentication and single-factor authentication is an extra step during login. After providing your password, you will then be asked for your Authy SoftToken. Using Two-Factor Authentication also requires every user to provide their phone number. Logging in with two-factor authentication for the first time will walk a user through this workflow.",
        "Defining your Audiences and Content": "In this example you will use three audiences to determine what content should be shown in a section to the right of the main hero for the website. This is a small scale example, but as you can imagine this technique could be applied to multiple, major parts of your website to completely customize the site for your users. Before your team does any design or copy writing it may help to determine which key Lytics audiences you would like to target. If your organization has the concept of \"personas\" you may want to think about how your website should communicate and look to each of these personas and then build or use existing audiences that represent these user archetypes. Regardless of the scale of your project, the audiences should inform the messaging. Once you have finalized the audience definitions, designs, and copy for your inline personalized modules, simply start by building out the content as you normally would with HTML and CSS on your website. Start with the content to be displayed to a single audience. This is the HTML example of content displayed to first time visitors of the website:  Go ahead and build out all the code for each of your audiences. It may help to comment out the options that will be shown to different audiences as you go, so that you can simulate what an end user in each audience will see instead of viewing all personalized content options at once.",
        "Implementing Content Modularization": "When all of your content is good to go it's time to apply the audiences. Consider the following definitions: Ablockis a unit of content to be displayed to a single audience within a group.Agroupmay be made up of one or many content blocks, each with unique audiences. The end user will only be able to see one block on the webpage per group. Thus blocks within a group might need to be formatted similarly as they will take up the same space on the page. In the example code, there is a single group which will control the content displayed to the right of the hero, and four blocks within that group: one for each audience, and a fourth block to be displayed to users who are not a member of any of the audiences associated with the other blocks in the group. These concepts map to code with the followingdata-attributes: data-pftrigger- The ID of the audience you wish to display the block to. Each unique value for this attribute defines the element as different block within the group.data-pfgroup- A string name unique to the group which differentiates it from other personalization groups defined on the page. This name should be descriptive regarding a commonality between its content blocks such as placement on the page. Remember to check out thePathfora documentationfor more information on these attributes and additional code examples. Putting it all together, here's what the full content modularization example code looks like: ",
        "Reddit Ads User": "Reddit Ads User authorization method is an Oauth2 based authorization for the Reddit Ads platform. Enter your Reddit login credentials in the login popup.In theLabeltext box, enter a name for the authorization(optional) In theDescriptiontext box, enter a description for this authorization",
        "Reddit Conversion API Access Token": "This authorization required access token for conversion events. The access token can be generated inEvents Managerin Reddit Ads dashboard. In theLabeltext box, enter a name for the authorization(optional) In theDescriptiontext box, enter a description for this authorizationIn theAccess Tokentext box, enter your conversion api access token. ",
        "Reddit: Conversion API Export": "Send Lytics user profiles to Reddit using their conversion api. Use these conversion events to improve performance of your ad campaign. Integration DetailsFieldsConfiguration",
        "Reddit: Custom Audience Export": "Export your Lytics audiences to a Reddit Custom Audience to target specific customers in Reddit. Refine your targeting efforts using Lytics audiences containing rich information on user behavior and content affinities across channels. Integration DetailsFieldsConfiguration",
        "Configuring the Webhook": "Follow these instructions to set up the webhook integration. Log into yourContentfulaccount.In the space you wish to configure, clickSettingsand thenWebhooks.ClickAdd Webhook.Add a name for your webhook such as \"Lytics Content Collection\" or something similar.UnderURL, select thePOSTmethod and enter the following URL:https://api.lytics.io/collect/json/contentful_webhooksUnderTriggersyou can configure what events the webhook should fire on by selectingSelect specific triggering events. For this integration it is recommended to selectEntryevents onPublish.Note: Currently this integration only supports content entry ingestion and updates. There is no way to remove, mark as unpublished, or archive entries from Contentful in Lytics, thus these event types are not recommended for use in the webhook.You can add additional filters such as Environment or Content Types.Note:If your content types differ vastly in the naming of common fields, you may need to create one webhook per content type as you will need to map individual Contentful fields to Lytics fields.UnderHeadersyou will need to add two custom headers:Authorization- your generated Lytics API token.Lytics-Auth-ID- the ID you created during theauthorization step.  Selectapplication/jsonunder theContent Typedrop down.UnderPayloadselectCustomize the webhook payload. You will need to construct a custom payload to map fields to names that Lytics will recognize and can ingest. Use thewebhook transformationdocumentation and the section onpayload fieldsbelow to construct your payload.Once you're ready to begin sending webhooks to Lytics, clickSaveto activate the webhook. The next time a qualifying event (such as publishing an entry) takes place in Contentful, you should be able to see that document in Lytics.",
        "Payload Fields": "When constructing the custom payload of your webhook, you will want to include the following fields:",
        "Sample Payload": "Use the following sample payload as a reference when constructing your own. Be sure to rename your Contentful fields and locales as necessary and omit any fields that don't apply to your content types. Be sure totest your webhook. If you've mapped anything incorrectly, individual fields may not appear in Lytics as expected.",
        "Testing and Validation": "After you've configured the webhook you should test it to ensure data is being received in Lytics properly. There are some steps you can take in both tools to troubleshoot the webhook.",
        "Contentful": "First you will need to trigger the webhook by taking an action in Contentful. Navigate to an entry in Contentful which is qualified to trigger the webhook.Perform an action on this entry which will trigger the webhook (such as making a change to the content and publishing the entry).Navigate toSettings > Webhooksand clickView detailsnext to your Lytics webhook.In theActivity Logtab you should see a log of webhook requests. If the webhook was received successfullyHTTP 200should be present in theCall resultcolumn.ClickView detailsto view the full request and response payload. Ensure that all the expected fields are included and formatted properly. ",
        "Lytics": "If your webhook request looks as expected, you can validate the content has synced and is mapped correctly in Lytics: To ensure data has been received by Lytics, navigate toData > Data Streamsand select thecontentful_contentstream from the drop down. If this stream is not present, Lytics has not received content from Contentful yet.If raw data is present, you can check that it is being processed by building aContent Collectionsof all your Contentful entries. Navigate toContent > Collections.SelectNew Collection.You will use thesoucrefield to build this collection - which contains the ID of the content from Contentful, since it is a non-standard field. You can select theAdvanced Editorfrom the...menu in the Collection editor.UnderCustom Rulefind and select thesourcefield.Select theequalsrule and then typecontentfulinto the text box to include all documents received from contentful in the collection. Then clickAdd Condition.UnderSample Documentson the right side of the editor, you should see a list of Contentful documents that have been imported via the webhook. You can click on one to see the detailed view of the document.\ud83d\udcd8If no sample documents appear this may be because your data is processing. It may take longer to process the first time you receive content from Contentful. If the document does not appear, but you are able to see data in the data stream, contactLytics supportto ensure the content queries for your account are up to date.Validate that all the fields you expect to see are present and formatted correctly in the document summary.\ud83d\udcd8Documents with a rich text body field will have an additional field, \"HTML Body from Contentful Rich Text\" which contains the rich text body parsed ad HTML, while the \"body\" field itself includes plain text.Back in the advanced collection editor, you may opt to name and save your collection, or use any of the imported fields to further refine your collection for use inContent Recommendations If you run into issues during any of these steps, feel free to reach out to theLytics support teamfor assistance.",
        "Options for sending data from Lytics to Adobe:": "Sending file based data directly to Adobe using aLytics File ServceSending data in near real time from Lytics to one of our supported data warehouse integrations (Google BigQuery,Microsoft Azure,Snowflake,Amazon Web Services (AWS) Redshift).Then from the data warehouse, send the data to Adobe CloudClient Side - Lytics surfaces data in Web Layer via the Lytics JavaScript Tag. Learn how you can via the Lytics taghere.",
        "Options for sending data from Adobe to Lytics:": "From Adobe send data to a Lytics Supported data warehouse  (Google BigQuery,Microsoft Azure,Snowflake,Amazon Web Services (AWS) Redshift). The data from the data warehouse can be imported into Lytics. In many cases this is the recommended approach to avoid duplication of processes and data storage.Sending file based data from Adobe to Lytics using aLytics File ServiceClient Side - Using the Lytics JavaScript Tag [link] to pass Information to Lytics from the website / data layer",
        "Further details on integrating with Adobe products": "Adobe Campaign Lytics can export file based data to Adobe Campaign via hourly/daily CSV file export to an SFTP location. Adobe Campaign can import CSV files from its own secure location. This is more of a limitation on the Adobe Campaign side regarding limited API access. There are many ways to get data into Lytics from Adobe Analytics. One common method is to use the Lytics Javascript tag (jstag.send) function to send eVars directly from the data layer into Lytics on page load. This method will simplify the onboarding process and minimize the need to do a data transfer from Adobe Analytics on a regular basis. As part of onboarding, we can help identify which data you would like to pull in for activation and identify the right mechanisms for ingestion. Lytics provides ways to ingest this data from scheduled SFTP pickups to Bulk API imports or transactional APIs. Our Javascript Tag will communicate user audience membership to Adobe Target, which will respond with the appropriate site personalizations.  This Lytics data will be returned from our platform on page load and the Users Profile, segmentation information and content recommendations will be placed in the web page data layer. You can pass this information simply by leveraging the lio.data.segments object that is loaded onto every page where the Lytics JS tag is deployed. Similar to other Adobe products, Adobe Ad Cloud supports the consumption of user audiences via SFTP, Json file upload, API (Lytics can create a webhook to stream data - but more conversation with Adobe will be needed) as well as direct from other Adobe products like Adobe Audience Manager. Lytics can also directly send audiences from Lytics into ad platforms (e.g. Facebook, Google, Linkedin, Twitter, Snapchat, Instagram, LiveRamp.  With our Google partnership, Lytics has early access to Google\u2019s API-based integration for custom audiences within DV360. We can deliver audiences into Audience Manager via SFTP or Data Warehouse integration, similar to how the above integrations have been documented.",
        "\"WHERE\" Clauses --> Route Rules": "V2 Conductor Schema does not have a direct analog forWHEREclauses in V1 schema, or LQL. In V1, for example, it was possible to describe a clause to filter data that is mapped for a stream, such as the following, which ignores events whose_urlfield contains the stringlocalhost: To achieve the same result in accounts with Conductor Schema turned on, we need to use the Stream Route Rules API, whose endpoints include the following: GET /v2/stream/ruleretrieves a list of all route rules defined in the accountGET /v2/stream/rule/:idretrieves a single route rule by its IDPOST /v2/stream/rulecreates a new route rulePOST/PUT /v2/stream/rule/:idupserts an existing route rule by its IDDELETE /v2/stream/rule/:iddeletes an existing route rule by its ID Route rules define data to route from oneinputstream to anotheroutputstream. On ingress, the stream on the record is overwritten frominputtooutputif the rule is active and evaluates to true when executed against the incoming record. The mechanics of this imply one important distinction relative to V1WHEREclauses: since route rules describe which records to redirect away from the input stream, rather than which records to evaluate for the input stream, they are in effect theinverseofWHEREclauses. If, for example, we want to ignore urls containing the stringlocalhostfrom our input streamappin theWHEREclause above, we would need to define a route rule containing the following expression (which is the precise inverse of ourWHEREclause): This tells the system to redirect events matching this expression to a new output stream we define on the route rule object. By convention, if we don't care about mapping records that match our route rule, we call that output stream{input-stream}_divert, and define the following route rule object (note the need to escape double quotes inside the expression so that our object is valid JSON): Our object includes several properties which should be self-explanatory:account_id,aid, andname. The rest are as follows: activedefines whether the rule should be activated against incoming data. Marking a rule as\"active\": falseturns it off.priorityindicates the order in which the rules are evaluated (higher priority first) against an event. secondary sorting according the age (newer rules first) is performed when priorities match",
        "Exporting Routed Data": "When data is routed from theinputstream to theoutputstream, Lytics will associate the raw routed data to theoutputstream. This means when a raw activity data is exported, the data that was routed will be associated with theoutputstream only.",
        "Behavior Scoring on Streams": "An optional feature with all streams is to addBehavioral Scoringto selected streams. If a behavioral scores are desired for a specific stream make sure the stream that is being targeted is the routingoutputstream andnotthe input stream.",
        "X Ads Sign-In": "This authorization allows Lytics to access your X Ads account. SelectX Sign-In Oauthauthorization method.From the user selection window, select the X Ads account you want to connect from the list of accounts.ClickAllow.",
        "Import Webhook Events": "In order to import data from Customer.io, you need to setup the webhook to send event data to Lytics. This webhook sends behavioral events from Customer.io to Lytics in real-time. This activity data can then be used as targeting criteria for campaigns using Lytics audiences.",
        "Configuring Webhooks in Customer.io": "The following steps are required before running the import job. In Lytics, underAccount Settings,create a new API token.Login to Customer.io, navigate to production,Admin > Integrationand thenWebhooks. In the URL textbox, enter the following URL:https://api.lytics.io/collect/json/customerio?key=YOUR_API_TOKEN. ReplaceYOUR_API_TOKENwith the Lytics token you generated in the previous step.ClickUpdateto save the changes. Customer.io webhooks should now begin sending to the Lytics data streamcustomerio.",
        "Build your audiences": "In Lytics you'll need a target audience - the group of users to send the email to - and one or more additional audiences that the email can be personalized for. In this guide we havePromotional Listas the target audience, and two audiences built withcontent affinity,Interested in Dress ShoesandInterested in Athletic Shoeswhich we will build out separate content pieces for. ",
        "Sync the audience to your email tool": "Once you have your audiences ready, you will sync your target audience to an ESP. This can be done with a Lytics export integration. In this guide we willexport to Campaign Monitor. If you're using another ESP browse our list ofintegrationsand the read the instructions for exporting an audience. SelectPromotional Listas the target audience to sync. For Campaign Monitor the other two audiences will be recorded in a custom field calledLyticsAudiences. This is the field we will use in the template to determine what to show and hide for the user.  Regardless of the provider you choose you should see the Lytics users in your email tool not long after you click theStart Exportbutton.",
        "Send your campaign to the target audience": "Create a new campaign in Campaign Monitor. On theContentstep upload the HTML template you've created that includes your conditionals. After you've successfully uploaded, you can preview the email and toggle between theLyticsAudiencedynamic content options to view each of the expected outputs.On theRecipientsstep be sure to select the audience you exported to Campaign Monitor from Lytics.Continue on to theDeliverystep. You can send a test email to yourself and/or schedule your campaign for delivery. This guide has demonstrated how you can send three distinct, personalized emails to users based on their interests in a single campaign. Instead of sending a generic email to all your customers, you can send an email that speaks to a user's interests. This is just the tip of iceberg, think of how you can personalize and modularize your email templates for more advanced use cases. Conditionals can be used for any number of audiences, and you can show and hide entire content sections in an email based on audience membership. Your Lytics representative can help! If you have any further questions or would like to talk through use cases reach out to theLytics team.",
        "Monitoring": "Monitoring and alerting is available on every job and every authorization within Lytics. To set up alerting on your jobs or authorizations, you can set up a monitoring job from either the Job API or the Lytics UI for alerting toSlack,Microsoft Teams, or directly toemail. If a source or destination job has failed, Lytics will show the latest error message on the Conductor Diagnostics Dashboard and on the Logs tab of the Source/Destination Job Summary interface, and allow the job to be restarted if needed. The most detailed information for troubleshooting can be accessed from theJob Logs API Additional generic monitoring on the Lytics system is available on our status page atlytics.statuspage.io.",
        "Import Subscriber and Activity": "Importing subscribers and activity data from Marketing Cloud results in new users or existing user profiles supplemented with Marketing Cloud campaign activity data. You can use this data to build and refine your existing Lytics audiences to power better, cross-channel campaigns.",
        "sfmc_events fields": "The following fields are included in the default mapping of thesfmc_eventsstream:",
        "sfmc_subscribers fields": "The following fields are included in the default mapping of thesfmc_subscribersstream:",
        "Export Audience To Existing Data Extension": "Exporting a Lytics audience to Salesforce Marketing Cloud data extensions allows you to send email, push notifications, or SMSs to your users based on your own, relevant targeting criteria, such as cross-channel behavior, content affinities, and more.",
        "Export Audience To New Data Extension": "Exporting a Lytics audience to Salesforce Marketing Cloud data extensions allows you to send email, push notifications, or SMSs to your users based on your own, relevant targeting criteria, such as cross-channel behavior, content affinities, and more.",
        "Export Lists": "This job type allows you to sync Lytics audiences and profile data in real-time to Salesforce Marketing Cloud for targeting your email communications.",
        "Trigger Journey": "Push users into a Marketing Cloud journey when users enter a Lytics audience to send event-driven, responsive campaigns across any channel.",
        "API Event Tactics": "The API Event tactic will push users via theTriggered Journey export. The users sent to Salesforce Marketing Cloud will bestored in a data extension. NOTE:For this tactic, you will activate the Salesforce Marketing Cloud journey before activating the Lytics Experience so that it is ready to process incoming events. If Delivery Optimization is enabled, the API event will be sent at the best time to email the user. No delay should be configured in the Salesforce Marketing Cloud journey between user entry and email send. Navigate to the Journey Builder in Salesforce Marketing Cloud.Select your journey from the list of journeys in Salesforce Marketing Cloud.Click theActivatebutton to activate the journey in Salesforce Marketing Cloud.Navigate to yourLytics Experience.Click on your Experience.ClickActivate. If theActivatebutton is disabled, you will need to follow the instructions in theConfigurationsection above to set up the Experience.",
        "Data Extension Tactics": "The Data Extension tactic will push users via theData Extension export. The Salesforce Marketing Cloud journey will evaluate the Data Extension once the journey is activated. NOTE:For this tactic, you will activate the Experience in Lytics before activating the journey in Salesforce Marketing Cloud, which gives the export time to populate the Data Extension. Exact timing will depend on your audience size, among other factors. Navigate to yourLytics Experience.Click on your Experience.ClickActivate. If theActivatebutton is disabled, you will need to follow the instructions in theConfigurationsection above to set up the Experience.Navigate to the Journey Builder in Salesforce Marketing Cloud.Select the journey from the list of journeys.Click theActivatebutton to activate the journey. If the journey is configured with a schedule, the journey will trigger on the selected date(s). If the journey is configured to trigger immediately, then  the journey will run once you clickActivate. There are optionsfor configuring a data extension journey. ",
        "SP-Initiated SSO": "Lytics supports enterprise Single Sign-On (SSO) by usingGoogle Cloud Identity Platformas a service provider usingSAML protocol. Lytics integrates with Identity Providers (IdPs) in such a way that the Service Provider (SP) initiates SSO. Once implemented, users will log in to Lytics via aspecial SSO formthat only requires an email address. Lytics will recognize the email address and open a pop-up to the user's IdP to complete the login. Once the IdP verifies credentials, the pop-up will close, redirecting the user to a logged-in instance of their Lytics dashboard. Behind the scenes, a customer's IdP will communicate with the Lytics APIs, which useGoogle Cloud Identity Platformto validate the login. This document describes the process for integrating with a new IdP that uses SAML.",
        "Account Structure with SSO": "It should also be noted that primary accounts (master accounts) are decided as the first account that a user was added to.  Due to this,  users from a single group/organization will often have different primary accounts. This is important for SSO as it will also be the account the user is logged into at the start of their session. If SSO is enabled as the only means of login on one account and a user attempts to log in using Google OAuth or their username and password, the login session will fail. The following options are possible solutions: Add that user to your IdP.Add other logging methods (Google OAuth, username/password).Remove that user from all accounts and then add them back, with the first account being the one you want to be their primary account.",
        "Assigning Roles using SSO": "Configuring Role Assertions for SSO in Lytics The Lytics team can configure Single Sign-On (SSO) to assign roles via SSO. This configuration must be applied to the primary (master) accounts and any additional accounts to which users need access. For more information, refer to theaccount structure with SSO. Important Note:Users cannot be assigned to additional accounts via SSO alone; they must be manually invited to each Lytics account to which they need access. To set up role assignments via SSO, customers must configure their Identity Provider (IDP) to include a group assertion for each account the user can access. The role of each user must be specified in the format:lytics_<AID>_<Role> Example of a SAML assertion for a user whose default account is 123 and also has access to account 234: Note: This example is simplified to focus only on the group's assertion; a real-world example would be more detailed.",
        "Assignable roles": "The following roles can be assigned:",
        "Implementing SSO with Okta": "This document will walk you through how to implementSingle Sign-Onto the Lytics application withOktaas an identity provider. Lytics has applied to become an official Okta partner application. Still, while the partnership is being established, you can follow these instructions to set up the custom application in Okta, which covers the IDP configuration portion of the implementation. From your Okta Administration panel, navigate toApplicationsand thenAdd ApplicationandCreate New App.Under Platform, selectWeb, and for Sign on Method, chooseSAML 2.0.UnderGeneral Settings, you can enter the following values:App name: LyticsApp Logo: You can save and upload the following image of the Lytics Logo:UnderSAML Settings, enter the following values:Single Sign-on URL:https://api.lytics.io/api/user/ssoMake sureUse this for Recipient URL and Destination URLis not selected). Also selectAllow this app to request other SSO URLsto enable more URLs to be added.Requestable SSO URLs: Add the following two URLs:https://api.lytics.io/api/user/verifyauth(index 0)https://api.lytics.io/api/user/sso(index 1)Recipient URL:https://api.lytics.io/api/user/verifyauthDestination URL:https://api.lytics.io/api/user/verifyauthAudience URI (SP Entity ID):app.lytics.comName ID format:EmailAddress\ud83d\udcd8NOTE: Thehttps://api.lytics.io/api/user/ssoURL should only be used for Okta. Other SSO providers use the default URLhttps://api.lytics.io/api/user/verifyauthClick through the next step, and selectFinish.You can add users to view this application in their portal using theAssignmentstab.Navigate to theSign Ontab. And under settings, click onView Setup Instructions.You will need to gather the information on this page and send it to Lytics for configuration of the Service Provider:Identity Provider Single Sign-On URLIdentity Provider IssuerX.509 Certificate Once Lytics has completed the service provider implementation, you may begin to test the SSO implementation through your Okta portal.",
        "Implementing SSO with OneLogin": "This document will walk you through how to implementSingle Sign-Onto the Lytics application withOneLoginas an identity provider. You can follow these instructions to set up the custom application in OneLogin, which covers the IDP configuration portion of the implementation. From theAdministrationmenu, selectApplicationsand then clickAdd App. Search forSAML Test Connector (Advanced)and select that app type.UnderConfiguration > Portalenter the following:Display Name:LyticsMake sureVisible in Portalis selected.For the rectangular icon, you can save and upload the following image:For the square icon, you can save and upload the following image:ClickSaveto continue to the configuration process. Then click on theConfigurationtab to set up the SAML details.Enter the following into theApplication details:Audience (EntityID):app.lytics.comRecipient:https://api.lytics.io/api/user/verifyauthACS (Consumer) URL Validator:^https:\\/\\/api.lytics.io\\/api\\/user\\/verifyauthACS (Consumer) URL:https://api.lytics.io/api/user/verifyauthLogin URL:https://app.lytics.com/login/ssoSAML initiator:Service ProviderSAML nameID format:EmailClick on theSaveto save your configuration changes.You may configure any additional access details, such as users accessing this app in their portal for testing the integration under theUsertab.Click on theSSOtab, and you will need to gather the information on this page and send it to Lytics for configuration of the Service Provider:X.509 Certificate (clickView Detailsto see the full cert).Issuer URLSLO Endpoint (HTTP) Once Lytics has completed the service provider implementation, you may begin to test the SSO implementation through your OneLogin portal.",
        "Configure service account": "You will need a service account credential file to use this authorization type. If you do not have one, follow the instructions inGoogle's documentationto create one. You will need to give Lytics one of the following roles depending on how you want to configure your workflow: Pub/Sub Adminlevel permissions allows Lytics to create and receive new subscriptions without any problems.Pub/Sub Editorlevel permissions will also work. For importing into Lytics, the following IAM roles need to be added to the service account: pubsub.topics.listpubsub.topics.getpubsub.subscriptions.getpubsub.subscriptions.consumepubsub.topics.attachSubscriptionpubsub.subscriptions.create(Optional, but you must provide an existing subscription if your role does not have this action.) For exporting events to PubSub, you will need the following IAM roles added to the service account: pubsub.topics.listpubsub.topics.getpubsub.topics.publishpubsub.topics.create(Optional, but you must provide an existing topic if your role does not have this action.)",
        "Authorize integration": "If you are new to creating authorizations in Lytics, see theAuthorizationsdocumentation for more information. SelectGoogle Cloudfrom the list of providers.Select theCloud Pub/Sub Service Account JWTmethod.Enter aLabelto identify your authorization.(Optional) Enter aDescriptionfor further context on your authorization.In theCert JSONtextbox, copy the contents of your service account's credential file.ClickSave Authorization.",
        "Export Stream": "Export event data from any Lytics data stream to Google PubSub topic. NOTE: Unlike user fields, events are not represented within the Lytics dashboard. Events are the raw data received from integrations as seen in your Data Streams.",
        "Campaign Monitor Overview": "Campaign Monitoris an email marketing platform that allows you to execute email marketing campaigns. Integrate Lytics with Campaign Monitor to improve the personalization of your email marketing. Lytics combines your Campaign Monitor user activity data with your other marketing tools so you can create custom audiences based on how users interact with your brand.",
        "Export List": "Exporting your Lytics Audiences to Campaign monitor allows you to leverage Lytics' powerful data science driven segments for targeting. This integration will keep Lytics and Campaign Monitor in sync as users enter and exit your Lytics audiences. Leverage the export integration along withCampaign Monitor Automated Journeysto trigger emails to your users as they enter or exit a Lytics Audience. You can find a full guide of setting up an automated journey in Campaign Monitorhere.",
        "Import List": "Import your Campaign Monitor lists into Lytics allows you to combine your Campaign Monitor data with your other marketing tools, giving you a single view of your users. This can ultimately lead to running more efficient and better targeted marketing campaigns.",
        "Segment Overview": "Segmentis a platform that collects, stores, and routes customer data to various other tools. Segment's connections allow you to collect and standardize data, while their protocols can help you validate and cleanse data. Combine Segment and Lytics to add predictions to your marketing data management. You can install the Lytics tag via Segment as a data destination, or export your Lytics behavior-based audiences to route to multiple tools or executions via Segment's integrations.",
        "Export": "With this export you can send Lytics audiences to Segment as Identify or Track events. Using Segment you can send this data downstream to multiple other tools."
    },
    "Zeotap": "No documentation found."
}