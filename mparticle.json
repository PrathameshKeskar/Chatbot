{
    "Guides": "This section contains in-depth information about the mParticle platform and features.For information about our SDKs, APIs, and tools, visit theDeveloperssection.For information about our latest product releases, visit themParticle Changelog.",
    "CDP": "Customer data platform. mParticle includes functionality for CDP, as well as decision making powered by machine learning with Cortex, and behavioral analytics with Indicative.",
    "Analytics": "Analytics DocsOverviewGetting StartedDevelopersExpert Training",
    "Predictive AI": "Cortex DocsOverviewGetting StartedDevelopersUsing Cortex",
    "Learn More": "BlogPodcastResourcesCommunityChange log",
    "Get in Touch": "Help CenterDemoBecome a Partner",
    "Incoming event batches": "The following limits apply to event batches being ingested.",
    "SDK event data": "The following limit applies only to data being ingested with an SDK.",
    "Events per workspace and user": "The following limits apply to events per workspace or user. mParticle reserves the right to restrict average events per user to ensure platform quality of service.",
    "Events API": "The following limits apply to resources in theEvents API.",
    "Platform API": "The following limits apply to resources in thePlatform API.",
    "Profile API": "The mParticle user profile can be delivered in JSON format by the Profile API, in order to deliver personalized experiences based on user attributes or audience memberships. To request a profile from the API, supply either the corresponding MPID or a specially configured immutable ID (usually the Customer ID). For more information, seeProfile API.",
    "Data storage": "The following limits are explained in detail inData Retention.",
    "Data Subject Request API": "The following limits apply to resources in theData Subject Request API.",
    "Data plans": "The following limits apply to resources used indata planning. Similar to our event limit for workspaces, data plans support up to 1,000 data points.You can upload data plan JSON files smaller than 10 MB.Managing plans with more than 400 data points in the UI becomes unwieldy. Manage plans outside of the UI for larger plans. For more information, see theData Planning API guide.You can block data only for unplanned violations: events and attributes with names that diverge from the schema defined in a data plan.",
    "Data Planning API": "The following limits apply to resources in theData Planning API.",
    "Warehouse Sync API": "All functionality of Warehouse Sync is also available from an API. To learn how to create and manage syncs between mParticle and your data warehouse using the API, visit the mParticledeveloper documentation.",
    "Workspaces, users, and name length": "The following limits apply to workspaces, users, and the name length limits for audiences and tags.",
    "API throttling": "mParticle APIs have two types of rate limits in place to protect mParticle\u2019s servers from high demand: Speed: limits the rate of traffic.Acceleration: limits the rate of increase of traffic. You can configure your mParticle integration to respond programmatically to prevent data loss according to the 429 response header you receive. Throttled resources include a percentage representation of your current total usage in 2xx response headers. These percentage-used headers allow you to modify your request speed or acceleration to prevent exceeding a rate limit.",
    "429 rate limit exceeded": "An API request that exceeds the rate (speed) limit of a resource receives a 429 response and a header with the format: X-mp-rate-limit-exceeded: \"LIMIT\" where\"LIMIT\"is the ID of the source of the limit as described in the table below: An API request that exceeds any applicable acceleration limit receives a 429 response and a header with the format: X-mp-rate-limit-exceeded: \u201cacceleration\u201d",
    "2xx percentage used": "Rate limited endpoints return anX-mp-rate-limit-percentage-usedheader with 2xx responses including the percentage of the limit used. If a resource has multiple limits, the response header includes the greatest consumption percentage of all applicable speed and acceleration limits, omitting any user-based limits. For example, if a request is not throttled and is subject to both a speed and acceleration limit, and the current consumption is 92% of the speed limit and 50% of the acceleration limit, then the header lists the consumption of the speed limit because it is higher than the current acceleration limit usage: X-mp-rate-limit-percentage-used: 92",
    "Recommended actions": "If you receive a 429 response for exceeding a speed limit, reduce the frequency of your requests. Use exponential back-off with jitter and respect theRetryAftervalue which is a non-negative decimal integer indicating the number of seconds to delay your request. If you receive a 429 response for exceeding an acceleration limit, you can still submit requests but you should slow the increase of your request speed. Use exponential back-off with jitter when determining the new frequency of your requests.",
    "SDK Integrations": "We offer packaged integrations written in a variety of languages including Java and Python, as well as special integrations for iOS and Android apps. These integrations are designed to be fail-proof, asynchronous, and able to be modified as suits your needs. Please see our SDK documentation below. AndroidiOSJavaJavaScriptPythonReact Native",
    "1. Create CSV files": "Prepare the CSV files for import. Files must follow the guidelines in this section.",
    "File guidelines": "Files must adhere to theRFC4180 standardsfor CSV formatting.Files must be sent in one of the following formats:A plain CSV (.csv)A ZIP file containing one or more CSV files (.zip)A gzipped CSV (.csv.gz).A PGP/GPG-encrypted file with the additional extension.gpgappended, for example,.csv.gpgor.csv.gz.gpg). Only encrypted OR unencrypted files can be accepted, but not both.  You must use PGP encryption with mParticle\u2019s public key. SeeEncrypted filesfor additional instructions.File sizes should be between 5 MB and 2 GB. If you upload files outside these limits, processing speed is slower. If possible, split the data across multiple small files, because their processing can be parallelized.Each file can only contain events of the same event type. You can\u2019t mix events of different types in the same file.Don\u2019t use subfolders.Each row size should be under 80 KB. Larger rows may impact performance.All column names must be unique.Each CSV file must contain fewer than 50 columns.File name requirements:Do not use any dashes ( - ) or dots ( . ) in your file name, other than what is described below.End the file name based on the event content in your file:-custom_event.csv-commerce_event.csv-screen_view.csv-eventless.csvfor eventless uploads of user identities and attributesColumn names: specify fields according to ourJSON Schema, using dot notation.Column names described are case sensitive.",
    "Data guidelines": "Environment: include a column nameenvironmentset todevelopmentorproduction. If anenvironmentcolumn is not included, data is ingested into the production environment.User and Device IDs: as with any data sent to mParticle, you must include a column with at least one user ID or device ID.Device IDsdevice_info.android_advertising_iddevice_info.android_uuiddevice_info.ios_advertising_iddevice_info.att_authorization_statusdevice_info.ios_idfvdevice_info.roku_advertising_iddevice_info.roku_publisher_iddevice_info.fire_advertising_iddevice_info.microsoft_advertising_idUser IDsmpiduser_identities.customeriduser_identities.emailuser_identities.facebookuser_identities.microsoftuser_identities.twitteruser_identities.yahoouser_identities.otheruser_identities.other2user_identities.other3user_identities.other4Important: CSV files must have all the required identity columns, and the rows must have valid values in those columns to prevent processing errors.User attributes:If you include user attributes, for each, include a column named asuser_attributes.key, wherekeyis a user attribute key. For example:user_attributes.$FirstNameuser_attributes.communication_preferenceuser_attributes.Member TierAttribute names with spaces are allowed and do not require quotes. All the keys listed in theJSON Referenceare supported.Events:Use a column namedevents.data.timestamp_unixtime_msto set the event time.Use a column namedevents.data.custom_attributes.key, wherekeyis an event attribute key, to set custom event attributes.Attribute names with spaces are allowed and do not require quotes. All the keys listed in theJSON Referenceare supported.Screen view events: use a column namedevents.data.screen_nameif you want to include the screen name.Custom events: use columns namedevents.data.event_nameandevents.data.custom_event_typeto include custom events.Commerce events: use columns with the following names for commerce events.events.data.product_action.actionevents.data.product_action.products.idevents.data.product_action.products.nameevents.data.product_action.products.categoryevents.data.product_action.products.brandevents.data.product_action.products.variantevents.data.product_action.products.positionevents.data.product_action.products.priceevents.data.product_action.products.quantityevents.data.product_action.products.coupon_codeevents.data.product_action.products.added_to_cart_time_msevents.data.product_action.products.total_product_amountevents.data.product_action.products.custom_attributesOnly one product per event can be included for commerce events uploaded via CSV.Data types:All data in the CSV is converted to a string. The only exceptions to this are values that require a particular data type, such as MPID or IDFA.Only standard custom events and screen views, and eventless batches (eventless drops of user identity and attributes), have been tested.Attributes sent as arrays are not fully supported. When the entire array is present in a single cell of the CSV file, it is supported and is converted to string. Because there is no way of specifying anything but the first item in an array, repeated header columns, each subsequent column overwrites the previous one. Multiple columns don\u2019t append to the array. This is why you can only include one product for ecommerce events. Commerce events in the Events API support arrays in multiple places, but with CSV files, you can only populate a single item in each of these arrays.Custom manifest: You can use a custom manifest to drop files created in another system without transforming them. For details, seeUse a custom manifest.",
    "2. Get credentials for the mParticle SFTP server": "mParticle maintains an SFTP server where you will drop your CSV files.\nUse the following instructions to securely retrieve your credentials and find the hostname and path to use when you drop your files on the SFTP server. To get your SFTP username and password: Sign up for a Keybase account with your work email athttps://keybase.io/. Keybase is a secure tool which includes end-to-end encrypted chat.Provide your Keybase account name to your Customer Success Manager or your mParticle Solutions Consultant so that they can pass it on to our Ops team.Expect to receive your SFTP access credentials in a Keybase chat from mParticle. Note that if you need to use credentials that you already have, you\u2019ll share those credentials in the Keybase chat.",
    "3. Configure the Custom CSV Feed": "Configure the Custom CSV Feed as input. This step provides the hostname and folder path on the SFTP server where your CSV files must be dropped. To configure the Custom CSV Feed: VisitSETUP > Inputs > Feedsin the mParticle UI and click theAdd Feed Inputbutton, then select Custom CSV Feed from the list.If you\u2019ve already added the Custom CSV Feed, it won\u2019t show up in the list. Scroll through the list of feeds until you seeCustom CSV Feed, and then click the large plus sign in the gray bar to create a new feed. You need one feed for each different event type.Enter the following values:Configuration Name: enter a name that makes this feed easy to recognize in your list of feeds.Custom Event Name: if you are importing a custom event, enter the name that will be used for the custom event.Custom Event Type: if you entered a custom event name, select the event type.Custom Manifest: if you are using acustom manifest, paste it in the text box provided.Expect Encrypted Files: if you will import a PGP/GPG-encrypted file, select this option.After you complete the connection configuration, clickIssue SFTP Details. mParticle displays your hostname and path for mParticle\u2019s SFTP server.",
    "4. Drop CSV files on the SFTP server": "Connect to the mParticle SFTP server using the credentials provided. Once you have connected, the mParticle creates thedropfolder. If you don\u2019t see one, create a folder nameddrop.Create a new folder inside thedropfolder, and name it using the pathname provided in the mParticle UI as shown in the previous section. For example, based on the previous example, the folder path and name issftp.mparticle.com:4422:drop/us1-123456789123456789/.Hint:Verify that there are no trailing spaces in the name.Use your credentials to upload your CSV files to mParticle\u2019s SFTP server, using the correct path and folder name from the previous step. Files on the SFTP location are added to the processing queue nearly immediately.  Depending on file count and file size, a backlog may develop. You can observe how much data has been processed using Data Master and your outbound connections. There is no notification of processing progress or completion.",
    "Installation & Setup": "Download the library on GitHub After downloading the client, add theindicative.pyfile to your project and import Indicative. import indicative Before you start recording events, call theinit()method and pass in your project\u2019s API key: indicative.init('Your-API-key-goes-here') You should only have to do this once.",
    "Sending Events": "Next, to record an event, set up a dictionary object with your property names and value, and then call therecord()method. Its usage looks like this: Therecord()method takes three arguments: the event name, the user\u2019s unique ID, and a dictionary containing the event\u2019s property names and values. This method creates a JSON representation of your event and sends it to our API endpoint. Note that this is done synchronously.",
    "Create a Scheduled Report": "To create a scheduled report, you must first create a dashboard to send it from. For more information on how to build a dashboard, seeGetting Started: Dashboards. If you already have a dashboard that you\u2019d like to use, navigate to that dashboard via Saved Analyses & Dashboards in the left navigation menu. For best results, it\u2019s recommended that your dashboard be inPrint Modebefore scheduling a report. If you desire to keep your report in Screen Mode, then Analytics recommends duplicating the dashboard and creating a Scheduled Report from the duplicate. Once you have selected your desired dashboard and changed your dashboard layout mode to Print Mode, open the Reports dropdown in Dashboards settings at the top right of the dashboard. Here, you may create a new Scheduled Report or view your existing Scheduled Reports. We recommend that you check and see if someone else within your organization has already created a Scheduled Report from this dashboard. If there is an existing report, you may open the report to edit the recipients. Once you have selected \u201cNew Scheduled Report\u201d, a sidebar will appear on the right side of your screen. The default title of a Scheduled Report will be the title of the underlying dashboard. You may edit the title, select the frequency of the report, and select the recipients. You may schedule your report to be sent daily, weekly, monthly by day (i.e. first Monday of the month), or monthly by date. Next, choose the time at which the dashboard will refresh and the report will be sent. Scheduled reports are sent on the hour. Please note your project time zone when saving scheduled reports, taking into acco\u2026 In the recipients field, enter the email addresses to which you would like to send the report. Recipients do not need an Analytics account to receive a scheduled report: Analytics users will receive a PDF of the dashboard and a link to the dashboard itselfNon-Analytics users will receive a PDF of the dashboard and a public URL Remember to review the reports\u2019 editing privileges. You may choose to enable editing privileges to your entire team.",
    "Send a Test": "Analytics recommends sending a test report to your email to confirm that the scheduled report is configured correctly. Remember to review the attached PDF to ensure that results are correct and up to date. The test report will be sent to the email associated with your Analytics account. Click \u201cSave Report\u201d to save your scheduled report settings. Once you click \u201cSave Report,\u201d your scheduled report is confirmed, and it will be sent at the interval selected in the previous steps.",
    "Manage your Reports": "View your existing Reports by selecting the Settings icon on the left navigation menu and navigating to Scheduled Reports. Here, you can view all of your organization\u2019s Scheduled Reports in a table format, including the following information: Report titleThe time the report is scheduled to be sentThe creator of the reportThe time that the report was created Additional report actions are available by choosing the three-dot menu on the right of each report, including: View DashboardEdit SettingsUnsubscribeArchiveDelete",
    "Archiving and Deleting": "Archiving and deleting are the two methods of disabling an existing scheduled report. Archiving a report will disable the report indefinitely. Archived reports will be greyed out in the Scheduled Reports list, which can be filtered using the settings on the top right. To unarchive a report, select the three-dot menu to the right of the report and choose \u201cActivate\u201d. Deleting a scheduled report is permanent and irreversible. Only do so if you are sure that the report is no longer relevant. The underlying dashboard will remain, but the report will be permanently deleted.",
    "Manage Saved Analyses": "If changes are made to a query that is already saved, Analytics will prompt you to update the saved analysis. If you would like to continue editing the query before updating the saved analysis, click hide. A banner at the top of the page will persist as a reminder that there are unsaved changes in your query. If you\u2019d like to create a new saved analysis instead, change the title of your query, or select a different folder to place it in.",
    "Overview": "Prior to the release of Portfolio Analytics, an Analytics project only had access to data from its corresponding mParticle workspace. Portfolio Analytics introduces the notion of multi-workspace projects, or portfolios. Within a portfolio, you can now ingest data collected from multiple mParticle workspaces into a single Analytics project.",
    "Sequential order": "Typically, each user in the funnel must complete each event in a chronological order, one after the other. In the example below, this means thatSubscribemust happen beforeCreate Profile, with a distinctly different timestamp. The same logic applies for each event that follows. ",
    "Flexible order": "If you select Flexible Conversion Order, then each user in the funnel may complete each event in a chronological order,or up to one day prior. The options for Conversion Order Limit start at one second, and are configurable to minutes, hours, and to one day. This would mean that users can perform subsequent events in the funnel with a timestamp prior to the first event in the funnel, and still be counted in the results. In the example below,Subscribecan happen with an earlier timestamp thanCreate Profile, and this user will count as having converted between the two steps. The same logic applies for each event that follows.  In the case of events with the same timestamps, timestamps are defined as the same if they occur within the same second. This means that if a user performsSubscribeandCreate Profileat 00:00:00.99 and 00:00:01.00, then this user willnotcount as having converted. If a user performsSubscribeandCreate Profileat 00:00:00.01 and 00:00:00.99, then this userwillcount as having converted. Custom, joined and repeat events are supported in Flexible Conversion Order, but please note that there must be distinctly separate instances of each event for a user to count as converted.",
    "Conversion logic": "Which event is prioritized in the conversion rate \u2013 for example in the case of the Average Conversion Time metric? When the conversion rate is calculated, Analytics\u2019 logic prioritizes different events based on their timestamps, prioritizing the shortest amount of time, orabsolute value, of their occurrence to event A. Take a funnel with two events:Download App(Event A) andStart App(Event B). Conversion Order is set to Flexible, with a time limit of one hour.  In this example, the timestamps for events performed by a single user are as follows: Start Appat 1:15 PMDownload Appat 2:00 PMStart Appat 2:01 PM Here, the absolute distance fromDownload App, or event A in the Funnel, is counted. Therefore, the instance ofStart Appthat occurred one minute afterDownload Appwill be counted instead of the instance ofStart Appthat occurred 45 minutes prior. Some additional logic affects how these events are treated in the results: In the case of identical values (say, 30 minutes before and 30 minutes after) for such events, the event which occurred after event A will take precedence.In terms of Conversion Time results, any events occurring before event A will be counted as occurring with zero difference, as if they occurred at the same time as event A.",
    "Quality": "Quality reflects the overall accuracy of your prediction. Each time you create a Predictive Attribute, Cortex sets aside a portion of your data to compare the predictions to real-world outcomes. The results of these tests are converted into a score that indicate how well the model performed. The values in the Quality column correspond to the following scores: Excellent:0.85 - 1.0Very Good:0.75 - 0.85Good:0.65 - 0.75Average:0.55 - 0.65Below Average:0.5 - 0.55Unknown:A quality score could not be determined. The scores reflect the percentage of the predictions that aligned with what was observed in the real world. A score of 1.0, for instance, means that every prediction in the model was correct when compared with real-world data. Conversely, a score of 0.0 means that every prediction was incorrect.",
    "Status": "Each rule has a master switch in the Settings panel. If there is a problem with your rule, you can switch it off and it will be disabled for all connections until you enable it again. To disable, clickEditin the right sidebar and set theStatusslider toinactive.",
    "Last Calculation": "The time at which the most recent calculation was completed.",
    "View prediction details": "Clicking on the name of any of your Predictive Attributes will direct you toEnrichment / Predictive Attributes / {Name of Prediction}:  The tiles in theUser Likelihood Rangessection tells you about the users who fall into three default percentile ranges:  The default ranges displayed in this section are: Most Likely Users:90th percentile and above. (More likely to convert than at least 90% of other users in the group.)Somewhat Likely Users:70th - 90th percentile. (More likely to convert than at least 70% of other users, but less likely to convert than the top 10%.)Least likely users:0th - 10th percentile. (Less likely to convert than at least 90% of users in the group.) Each User Likelihood Range tile also displays: Total users in the rangeA predicted conversion rate for the rangeHow likely users in this range are to convert relative to the average across the group",
    "Custom Ranges": "Using theCustom Rangetile, you can specify a custom prediction percentile or fixed number of users. The tile will then display the total users, predicted conversion rate, and conversion likelihood relative to the average for the range specified: ",
    "Troubleshoot a failed pipeline": "The most common reason for a prediction to fail is because there are not enough users in the pipeline, and therefore not enough real-world training examples for the model to generate accurate predictions. This can happen for a number of reasons, including: The selected conversion window is too short.The conversion time frame you select is the lookback time that Cortex will use to analyze which behaviors lead to conversion. If not enough users converted within this time frame, Cortex will not be able to draw meaningful connections and create accurate predictions. Consider selecting a longer conversion time frame to increase the number of users in the pipeline.Exclusion criteria are too stringent.While usingexclusion criteriato narrow a prediction to a subset of users can help improve its accuracy, an overly restricted prediction can result in an inadequate number of total users and a failed pipeline. If you are using exclusion criteria in your prediction, consider loosening or eliminating certain  conditions.",
    "Time Zones": "Each dashboard in your project will be in the same timezone as designated in your project settings. To change a project timezone, see our Projects, Roles, and Teammates article.",
    "Refresh Interval": "The analyses on your dashboard will be refreshed as designated by your dashboard refresh interval. Each query is then independently run and your charts will refresh upon completion. Your dashboard refresh interval can be set to: Every 15 MinutesEvery 30 MinutesEvery 1 HourEvery 2 HoursEvery 24 HoursNo Refresh The default dashboard refresh interval is Every 24 Hours. To adjust your dashboard refresh interval, navigate to the menu bar above your dashboard, and click on the \u201cEvery 24 Hours\u201d dropdown. Then, select your desired dashboard refresh interval.",
    "No Refresh Option": "Sometimes, you will want to create a one-off dashboard for a quick analysis that you don\u2019t necessary want to refresh on a regular interval. For these cases, select the \u201cNo Refresh\u201d option. You can select a refresh interval at any time after setting a dashboard to \u201cNo Refresh.\u201d ",
    "Identity scope": "mParticle data is organized in three tiers: organization \u2192 account \u2192 workspace. Your identity scope determines how user data is shared between multiple workspaces and accounts under your organization. In other words, an identity scope is a set of user data in which each user profile and \u2018known user\u2019 identity is required to be unique. Multiple accounts or workspaces under a single mParticle organization can share the same scope, but a single workspace cannot be connected to more than one scope. For some use cases, it might be beneficial for an organization to maintain more than one scope. For example: Food delivery apps have both customers and couriers as users of their app ecosystem, but analytics requirements for each group are very different. Additionally, a courier may also use the app as a customer. Storing the data from both roles against the same profile could create confusion. By creating a separate Identity Scope for each set of users, data is kept clean and relevant.Large enterprise organization may not yet have a consistent way of identifying users across branches and subsidiaries. Creating separate Identity Scopes allow pools of differently identified users to be kept separate.Businesses that operate internationally may need to separate their customers geographically to comply with local laws.Multi-sided organizations, such as social media organizations, may conduct separate B2C and B2B business. For example, a user of a social media app may use the same login to post personal status updates and also to purchase advertising. Multiple Identity Scopes allow these activities to be considered separately.",
    "Identity strategy": "The default identity strategy is set toprofile conversion. The profile conversion strategy is designed to help you build a complete record of a user\u2019s journey through the entire signup funnel, from an initial page view or app load to the creation of an account. This identity strategy also supports the use ofaliasing. On Anonymous BrowsingIf match on device ID or cookie, use existing user.  Else, create new user. On New Known UserConvert anonymous user to known, persist previous event history. On LoginResolve existing known user.  Business decision to explicitly copy event history from anonymous session to logged in user or not. On LogoutCreate new user.",
    "Identity hierarchy": "The default identity hierarchy is:",
    "Considerations": "Replaying event attributes requires replaying of eventsReplaying event attributes is not possible without replaying their associated events, which can lead to event duplication.Avoid additional MTU chargesIf the backfilled MPID and the original MPID do not match, the user will be counted twice and the number of unique MPIDs that determines your mParticle bill will be impacted.Sooner is better than laterWe advise replaying data no longer than 2 weeks from the date it was quarantined. Many downstream tools will not accept data over a certain age. The sooner you replay data, the better.Batch and event timestampsTo send data to mParticle via our Events API, events are stored in a batch (see our Events APIdocs pagesfor additional detail). Both mParticle batches and events have a timestamp attached to them. To ensure that events are backfilled with the original timestamp, it\u2019s essential to preserve the value stored in thetimestamp_unixtime_msfield that each event object contains (the timestamp attached at the batch level can be ignored).Avoid batch deduplicationTo avoid batches from being deduplicated in mParticle\u2019s internal data pipeline, make sure to remove thebatch_idfrom the blocked batch before backfilling it to mParticle.Backfilling data requires some coding skillsTo fix and replay data, you need to know how to code.",
    "Understanding Data Points and Data Plans": "For every mParticle workspace, you may have many data plans.Every data plan contains one or more data plan versions.Each data plan version contains data points that you have defined.  A data point is a unit of data collected by mParticle. Data points represent Events such as Custom Events or Screen Views, or User attributes such as Customer Id or Email. Each data point includes criteria used to select the data point from an incoming data stream, and a definition called a schema that validates the contents of the data point. You can view data points in all three Data Master tools: Catalog, Live Stream, and Data Plan. For example, here is a view of the Play Video data point in Catalog:  Here is a view of the a Commerce Event data point in Live Stream:  This view of a data point is from Data Plan: ",
    "Profile data": "Data about users are stored as attributes of individual profiles. These attributes include identities, device types and IDs, and several custom attributes such as membership status and demographic information. An attribute value may not be current, depending on how often it is updated. For more information about how profile data is associated with users, seeStore and Organize User Data.",
    "Event data": "Events describe actions that your users have taken. Event data is stored as attributes of event types. The value of an event data attribute is valid at the moment the event was triggered. For example, the event \u201cSign up\u201d could have an event attribute of \u201cmembership tier,\u201d the membership status at the time of signing up. A complete description of the mParticle event schema is inJSON Schema.",
    "Data retention limits": "The maximum period that mParticle stores profile and event data is governed by your long-term data retention policy, which is defined in your contract. Usually, the long-term data retention policy is the same for event and profile data. However, you can have different ranges for added control and flexibility for events and profile data.\nFor example, during org and account setup, you can set long-term data retention for events at two years and reduce the time profile data is available to 12 months. Another factor may affect the data that is available for audiences: the Real-Time Audience Storage Lookback specified in your contract. Typically it is set to 30, 60, or 90 days, but can be changed. It is also overridden by Unlimited Lookback. SeeData retention and Unlimited Lookbackfor details.",
    "Data retention and Unlimited Lookback": "Unlimited Lookback is a premium feature that extends your audience and calculated attribute lookback to your long-term retention for events. Without this feature, audience lookback is limited to the Real-Time Audience Storage Lookback specified in your contract. Data retention for personalization features (audiences, journeys, and calculated attributes) behaves differently depending on whether or not your account has the Unlimited Lookback feature enabled: Unlimited Lookback:Uses long-term data retention for events.All events in the date range for your long-term data retention for events are available for data evaluations.In the criteria builder, the number of days you specify for recency can go back to the long-term data retention for events.When a personalization feature (journey, audience, or calculated attribute) is activated, it is initialized using data available for the entire range specified by your long-term data retention for events. For the initialization of personalization items, a longer lookback usually means larger data volume, which may incur an additional expense.To help with cost estimation, you can set an estimated lookback value. This informational value alerts anyone creating a calculated attribute when a specified date range exceeds that organization\u2019s expected date range.Without Unlimited Lookback:mParticle uses the Real-time Audience Storage Lookback for real-time evaluation, which is specified in your contract and is often set to 30, 60, or 90 days.The behavior described for Unlimited Lookback features is not available.",
    "Examples with no Unlimited Lookback": "If your long-term retention for events is set to two years, you can view events that occurred up to two years ago.If your long-term retention for profiles is set to seven days, if a user has no activity for more than seven days, then the profile data for that user expires and isn\u2019t available for calculated attributes, journeys, or real-time profile enrichment.",
    "Examples with Unlimited Lookback": "If your long-term retention for events is set to two years, audiences, journeys, and calculated attributes are initialized using data up to the long-term retention for events and long-term retention for profiles.When choosing a recency or frequency value in the criteria builder for audiences or journeys, you can specify a value up to the long-term retention for events.",
    "Date range measurement": "mParticle determines the beginning of a date range for retention purposes differently for event and profile data: Event data: The event batch timestamp (timestamp_unixtime_ms) added to the top level of every batch, representing the time the batch was received by mParticle. Note this is different from the timestamp associated with an individual event.Profile data: The last time a profile was updated by an inbound data stream. For example, a profile may be created, attributes may be added, updated, or deleted, or other profile information may change. Each change triggers a timestamp change. Events and profiles need different ways of calculating age because an event isn\u2019t usually updated. Since several processes may modify a profile\u2019s timestamp, the date is measured differently than event data.",
    "Date range example": "Assume the following facts: A screen view event (screen_view) occurred and is time-stamped 1657934165001 (6 June 2023, at 21:29:24).This event was received by mParticle (ingested) in a batch time-stamped 1657934165102 (6 June 2023, at 21:30:55).Your long-term data retention for events is two years. In this example, mParticle keeps the event available until 1749270655 (6 June 2025, at 21:30:55).",
    "Define your backfill strategy": "Backfilling blocked data is non-trivial because you typically are interested in backfilling data to several downstream event integrations. Based on your unique set of target event integrations, you should devise a strategy for your data backfill. The following questions will guide your backfill strategy: Which integrations do I need to backfill?Different integrations have different limitations when it comes to receiving historical data. Establish the limitation of a target integration by reading their developer docs or by sending a small amount of test data through an mParticle connection.Do I need to backfill unplanned event attributes?Event attributes cannot be replayed without their associated events. You\u2019ll need a strategy (e.g. deleting previously sent yet incomplete events) to avoid event duplication if you want to replay blocked event attributes.Which mParticle Input should I use to backfill my data?The cleanest solution is typically to create a newCustom Feedfor the purpose of your backfill. You can connect only the integrations that you want to backfill to that feed and then tear it down again once the backfill is complete.However, some integrations are not available through theCustom FeedInput. In those cases, you will need to either (i) use the keys and secret of the original Input (e.g. Web) in our backfill script or (ii) send data directly to the integration\u2019s API (after transforming it to match their data model).",
    "How to backfill blocked data": "Once you have a strategy for your backfill, here are the steps to backfill your data: Go to your Quarantine output and find the data you want to transform.In your data, find thecontextnode. Within the context node, you will see a node labeledblock_metadata. This node contains the data you have blocked. Reference our sample data below to understand the complete data structure.Pull out the events you want to fix and replay.Apply fixes to your data, then re-upload it to mParticle in its correct format. Use one of our sample scripts provided, or follow steps 5 - 8 for guidance on writing your own replay script.Using the JSON data file containing blocked events, get theblock_metadatanode.Create a new events array and user attribute object containing only the data you would like to backfill.Perform any data fixes.Using mParticle\u2019s Events API, create a new batch containing the events to be replayed along with any corrected event or user attributes. Send the fixed batch to mParticle\u2019s Events API.",
    "Resources": "Quarantine Replay Scripts Example Quarantine Batch",
    "Audiences Landing Page": "The Audiences landing page is a central hub from which you can view and manage all of your audiences in mParticle. To access it, selectSegmentationin theOverview Map, then clickAudiencesin the left-hand navigation. ",
    "Audiences Table Columns": "The columns of the audiences table display key information about each of yourAudience Strategiesandindividual audiences: Size:The count of MPIDs in the audience.(Note: This value is only displayed at the Audience level; Audience Strategies will show a \u201d-\u201d instead.)Total audiences:The total number of audiences that are part of the Audience Grouping.Activated audiences:The total number of audiences connected to an active output.Connected outputs:The total number of distinct tools receiving audience data.Adds:The number of MPID additions to this audience over the last 24 hours.(Only shown at the Audience level.)Drops:The number of MPID drops from this audience over the last 24 hours.(Only shown at the Audience level.)Volatility:The change in the audience calculated as:(adds + drops) / size in MPIDs.(Only shown at the Audience level.)Last updated (UTC):Allows sorting by time created. Some statistics, such asSize, Adds, Drops, and Volatility, are only displayed at theAudience leveland not forAudience Strategies/Groupings. If a value isn\u2019t applicable at the Strategy level, it will be displayed as\u201d-\u201din the table. To customize the columns that are visible in the table: ClickView Columns.Toggle on/off the columns you want to view.ClickSave.",
    "Audience Strategies vs. Audiences": "Individual audiences are contained within folders calledAudience Strategies. The main table in the Audiences landing page displays all of your audience strategies and their associated audiences. Click the+icon to the left of an audience strategy name to expand and hide the audiences it contains:  Note: The example above shows an A/B test, which produces individual audiences for the control and test variants.",
    "Audience Tags": "As you continue to create new audiences, you can use tags to keep them organized and allow team members to easily see the purpose of each audience at a glance. For example, you can use tags to group your audiences by campaign type, giving them names like retargeting, lead gen, and product launch. Once you have applied tags, you can use tag names as search queries to return all audiences that have that particular tag applied.",
    "Create a new tag": "On the Audiences landing page: Click the+icon next to the name of an Audience Strategy to expose its audiences.Click theTagsicon in the row for that audience.Enter the name of your new tag in the text bar.Note: Tags have an 18-character limit.Click the name of the new tag to apply it to the audience.  To add additional tags, select the name of the tag(s) that have already been applied, open the dropdown menu, and click the additional tags you would like to add.",
    "Update an existing Audience Strategy / Audience": "To update an existing Audience or Audience Strategy, click on its name in the audience table. This will display the Audience Builder modal (if you selected an audience) or the Audience Strategy Editor (if you selected an audience strategy). Here, you can update the inclusion criteria for this particular audience, or update / add features like new paths, additional audiences, or A/B tests. At any time after an Audience Strategy has been created, you can edit its data inputs. Click on the top node in the Audience Strategy, which has the heading \u201cAudience Strategy Inputs.\u201dAdd or remove inputs as you see fit, then clickUpdate.",
    "Share Audiences between accounts": "You can share audiences between your organization\u2019s accounts, with detailed control over what data is shared. Share data broadly or restrict it to only what\u2019s needed for a campaign. This feature does not affect data shared with third-party tools. Navigate to theAudienceslist page.Open theActionsmenu for the audience you wish to share.SelectShareto open the sharing modal. In the modal, you can view the accounts the audience is shared with and their sharing permissions. There are four sharing permission levels: In the sharing modal, select the+icon.Choose the account and the desired permission settings. To view audiences that have been shared with an account, navigate toSegmentation, then selectShared Audiencesin the left-hand navigation.",
    "Instructions": "In Analytics, click onSettingsand selectData Sources.Click onNew Data Source.Select theGoogle Tag Managericon.You will need to use thisAPI Keyin step 9.Navigate to yourGoogle Tag Managerconsole and selectNew Tagin the top left corner.Click onTag Configuration.Click onDiscover more tag types in the Community Template Gallery.Select theIndicative Analyticstemplate, and add it to your workspace.Once you have added the Indicative Analytics tag to your workspace, you\u2019ll need to input your Analytics project\u2019s API key from step 4 into theAPI Keyfield.Once you have inputted your Analytics API key, you must select yourTag Type. You have the option to select Initialization, Custom Event, Clear Stateful Properties, or Clear Unique ID.Please note:all selections initialize the SDK, and send the Page View event to Analytics.Tag TypeMeaningInitializationInitializes the SDK and sends thePage Vieweventonly.Custom EventSends custom GTM events.Clear Stateful PropertiesClear properties stored as a persistent cookie (e.g. at user logout).Clear Unique IDClear the user ID stored as a persistent cookie (e.g. at user logout).If Custom Events is selected as your \u201cTag Type\u201d, an additional field called \u201cEvent Name\u201d will become available. This allows you to choose the Analytics event name from the available variables in GTM. Press the + on the right of the field to select a variable. \u201cEvent\u201d is the recommended variable so Analytics event names match GTM custom event names.If Custom Events is selected as your \u201cTag Type\u201d, you also have the option of adding Custom Properties. This allows you to choose the event\u2019s property names and values from the available variables in GTM. Press the + on the right of the property field to select a variable. Variables can be either normal event properties, which add context to an event, or they can be stateful properties, which remain the same for a user until the property is cleared (see above).Property TypeMeaningEvent PropertyAdd context to an event, like device type or marketing channel.Stateful PropertyEvent properties that remain the same for a user until the property is cleared.You may then optionally choose to set a custom unique ID. This variable will be used as each user\u2019s User ID within Analytics. You may then choose to set this ID as your alias. For more information on aliasing, see documentationhere.Next, click onMain Configurationto configure your session and cookie settings. Here, you can choose to track sessions and set the timeout threshold for each session. The default defines sessions as being separated by 30 minutes of inactivity.Then you must configure your cookie settings. Analytics recommends that you write cookies on yourMain Domainso that each subdomain on your website is tracked using the same cookie.More advanced Google Tag Manager users may choose to customize their Analytics-GTM integration further. In your Advanced Settings, you may set tag firing priorities, enable custom tag firing schedules, add tag metadata and more.Once you have configured your tag, select specifically which events you wish to send to Analytics in theTriggeringsection.Next, save the Indicative Analytics tag to your workspace by clickingSavein the upper right corner.Finally, in order to finish your Google Tag Manager integration, you must Submit your changes to your Workspace. Click on theSubmitbutton in the upper right corner of your \u201cOverview\u201d page.",
    "In Analytics": "In Analytics, click on the gear icon and selectProject Settings.Select theData Sourcestab.SelectNew Data Source.SelectAmazon S3andDefine your own schema. ClickConnect.You should see this screen.ClickNext.",
    "Connection Information": "Open theBigQuery consoleon Google Cloud Platform andSelect a project.Enter theGCP Project IDcontaining your data.Enter theDataset Name.Enter theTable Nameand clickNextin Analytics.",
    "Grant Permissions": "This integration works by sharing the dataset with Analytics\u2019 service account and only requires read-only access to that dataset. Analytics takes on the cost of the query and caches this data in Analytics\u2019 proprietary analytics engine.Within the BigQuery Console, select your Project and your dataset from the previous section.Click onShare Dataset.In theDataset Permissionspanel, in theAdd Membersfield, place the user below.integrations@indicative-988.iam.gserviceaccount.comIn theSelect a Roledropdown, selectBigQuery Data Viewerand clickAdd.",
    "Event Modeling": "Events FieldSelect the field that should be used for your Analytics event names. We recommend choosing a field that will result in 20-300 unique values.Timestamp FieldSelect the field that represents the time that the event was performed. This timestamp is the field that will be used for querying. ::: success\nAfter this step, we will perform a few checks on your data with the model that you provided. The checks are: Valid event field (Do at least 80% of your records have a value for the event field?Valid timestamp field (Do at least 80% of your records have a value for the timestamp field?Total number of unique events. We recommend 20-300 unique events and limit it to 2000.\n:::",
    "User Modeling": "After some basic checks, we can define your users within your data. For more information on User Identification (Aliasing), please refer tothis article. If you choose to enable Aliasing:Unauthenticated ID- Input the field used to identify anonymous users.Authenticated ID- Input the field used to identify known users.If you choose to disable Aliasing, pressDisabled:Unauthenticated ID- Enter the field used to identify your users. All users must have a value for this field. If you have a non-null value that represents null UserID values, please click on theShow Advancedbutton. In this field, please enter these non-null values. ::: success\nAfter this step, we will perform additional checks on your data with the user model that you provided. The checks are: User Hotspot (Is there a single UserID that represents over 40% of your records?)Anti-Hotspot (Does your data have too many unique userIDs? A good events table contains multiple events per user)Aliasing\n- Too many unauthenticated IDs for a single authenticated userID\n- Too many authenticated IDs for a single anonymous ID\n:::",
    "Scheduling": "Select theSchedule Intervalto adjust the frequency at which new data is available in Analytics.Set theSchedule Timefor when the data should be extracted from your BigQuery environment. It is critical that 100% of the data is available by this time to avoid loading partial data.SelectSave.",
    "Waiting for Data": "If you see this screen, you\u2019re all done! You should see your data in Analytics within 48-72 hours and will be notified by email.",
    "Advanced Settings": "For additional advanced settings such as excluding certain events and properties, please refer tothis page If you have any questions or concerns about the above Integration, please contact your Customer Support Manager, or emailsupport@mparticle.com.",
    "Prerequisites": "To create an mParticle audience and sync it with a user segment, you must first have access to mParticle Audiences and permission to create an audience. In the following cases, the Segment Activation option will be disabled in Analytics: Your organization is not currently an mParticle Audiences customer (You canrequest Early Access to this feature).Your organization does not have Segment Activation configured with the mParticle CDP as an Analytics feed. (Use the instructions inSet Up User Segmentation Activationto configure it.)You don\u2019t have an mParticle CDP account. (Contact your admin to request access.) If you have access to mParticle but don\u2019t have permission to create an audience, the option to automatically create an audience will be disabled. In this case, contact your admin to request access.",
    "Adding a Data Source In Analytics": "In Analytics, click on the gear icon and selectProject Settings.Select theData Sourcestab.SelectNew Data Source.SelectConnect via Data Warehouse or Lake.SelectBigQueryas your data connection andDefine your own schemaas the connection schema and clickConnect.You should see thisGoogle BigQueryoverview screen. Click Next.",
    "Data Loading": "Load Timestamp FieldSelect the field used to identify new data. We recommend using a timestamp that denotes when the event was published, not the actual event timestamp to allow for late data to be collected. This will not impact your analyses since we reference the event timestamp for our queries. If you select to load data every 3, 6, or 12 hours, make sure to select a load timestamp field with at least hour precision (not a date only field).For example, if an event with an event timestamp of 12/1 was published to the table on 12/3, this will not be collected unless we use the publishing timestamp since every daily extract would look for events that occurred on 12/3. Using the publishing timestamp will allow us to extract all new data that was published to the table on a nightly basis.Start DateSelect the date from where Analytics should load your data from.::: success\nIf your data history exceeds 1 billion events, a Solutions Engineer will contact you to assist with the integration.\n:::Schedule IntervalSelect the frequency to make new data available in Analytics.Processing DelaySelect when we should start extracting your data in UTC. This time should be when all of your previous day\u2019s data is fully available in your table for extraction.",
    "Assisted Modeling": "You should see a summary of your data based on the last 7 days in two main blocks. You should only be concerned if the margin of error is significant. If so, please reach out to a product specialist: Events SummaryYou should see a daily breakdown of your Total Event Count, and the number of Unique Event Names. If there are certain events to exclude, please click on the Exclude checkbox for those events.If you would like to exclude any events by regex or property value, please contact a product specialist.Properties SummaryHere you will see the number of Unique Property Names. If there are certain properties to exclude, please click on the Exclude checkbox for those events.If you require more advanced configurations such as parsing out JSON fields, creating derived properties, or excluding properties based on regex, please contact a product specialist.Users SummaryThis section lists the number of Unique users seen. If the numbers do not look correct, please go back to the User Modeling section to confirm that the correct ID was chosen.",
    "Waiting For Data": "If you see this screen, you\u2019re all done! You should see your data in Analytics within 48-72 hours and will be notified by email.",
    "User profiles": "Consent state is maintained per person on the User Profile using the structure defined above. For testing consent, you can useUser Activity Viewto check that a consent was recorded correctly. Here is an example of how CCPA data sale opt-out will appear: ",
    "User identifiers": "User identifiers, referred to asuser identitiesin the IDSync API, are attributes like email addresses or customer IDs defined as key/value pairs. Identifiers are used to identify users whenever an IDSync request is received. The complete list of supported identifiers are: customer_idemailfacebooktwittergooglemicrosoftotherother_id_2other_id_3other_id_4other_id_5other_id_6other_id_7other_id_8other_id_9other_id_10mobile_numberphone_number_2phone_number_3 There are several subcategories of identifiers with some unique characteristics, described below.",
    "Login IDs": "Login IDs are used to identify one, and only one, MPID for a known user when resolving an identification request. If your account uses the Profile Link strategy, then the first time IDSync receives an identification request containing a login ID with no associated user profiles a new user profile will be created. To designate one or more identifiers as login IDs, contact your account representative directly or submit a request to mParticle support. A login ID identifies a single known user. In order to maintain the integrity of known identity records, a record with at least one login ID can only be returned if the identify request includes a matching login ID. Identity records Scenarios One way identity strategies handle new known users is by applying rules about what to do when a new login ID is received for the first time. For example, theProfile linkstrategy always creates a new identity record when a login ID is received for the first time. TheProfile conversionstrategy does not create a new identity record when a login ID is first received. The new ID is added to the existing identity record. Identity records Scenarios",
    "Immutable IDs": "Immutable IDs are identifiers that cannot be changed once they have been set. In order to maintain the integrity of known user profiles, the value of an immutable ID may not be modified to protect against identity theft. A profile with at least one immutable ID can only be returned if the identification request includes at least one matching immutable ID. Immutable IDs may be used as query parameters for the profile API.",
    "Unique IDs": "A unique identity (unique ID) is a setting that specifies that that user profile identifier must be unique. This means that only one mParticle user profile can have that value of the identifier. If a modify request to theIDSync APIwould result in two identity records sharing the same value of a unique identity, mParticle will add or update the identifier on the requested user profile and remove it from any other user profile to enforce uniqueness.Note that this doesn\u2019t mean all other identifiers are removed from the user profile. The history of that profile remains intact. But removing the conflicting identifier from the profile means it can no longer be used to lookup that profile. User profiles with no remaining identifiers are effectively \u2018orphaned\u2019. They will not be deleted, but can never be returned by an IDSync API request. User profiles A user signs up for your iOS mobile app with the emailed.hyde@example.com. The same person also independently interacts with your helpdesk, using a different email addressh.jekyll.md@example.com. This results in two user profiles being created, one for each email. Each has a unique mParticle ID: Scenarios",
    "Identity records": "Behind the scenes, mParticle maintains a user profile for each user. You can think of a user profile as a folder of data that describes all the events, user attributes, identities, attribution info, and device info for a user. User profiles help determine which users are included in different audiences, and they enrich incoming data with any relevant user information before forwarding it to a connected output. The main purpose of IDSync is to assign incoming data to the correct user profile. However, to identify users in real time, IDSync doesn\u2019t look at the entire profile, but at that profile\u2019s identity record. Think of an identity record as a label on the front of your folder of user data (the user profile). The identity record contains a list of all identifiers that can be used to look up a profile. Identity records always have a 1:1 relationship with their corresponding profile. There are two key points to remember about identity records: Some uses of IDSync force identifiers to be unique to a single identity record. Email addresses are a good example. SeeUnique Identitiesfor more information.The identity record might not contain every possible type of identifier available in a profile, but it will contain the identifiers that are specified in your identity hierarchy.",
    "Use Cases": "Identity Reconciliation Users interact with digital environments in many different ways. When a new user accesses your website or app, they are unknown, so a unique Anonymous ID is assigned to track their online behavior. Users may remain anonymous for the duration of their visit or they might create an account, log in, or otherwise share identifiable information. Once the user becomes known, it\u2019s important to reconcile the known identity with any previously anonymous user identities, and to combine the activity timelines of each ID. This creates a unified user activity timeline and enables a complete analysis of the full customer journey. Without user reconciliation, an individual user may be assigned multiple user IDs, which can cause duplication of user counts and fragmentation of user activity in your data. The process of reconciling known and unknown user identities is called aliasing. Aliasing is also known as identity reconciliation, stitching, or merging. Cross-Device Identification Users can access your website or app from a variety of devices and locations. Maybe a user creates a profile on your website using their laptop, then downloads your app onto their smartphone and also onto their tablet. Maybe they log in on a friend\u2019s phone to place an order. In addition to reconciling known and unknown user identities, aliasing combines the activity timelines of each device, platform, and geographic location. This again creates a unified activity timeline to provide a complete analysis of the full customer journey. Cross-Domain Tracking Aliasing can also help with cross-domain tracking. To achieve this, pass the Anonymous ID from the second domain as an aliasing parameter. Then contact your Customer Success Manager for more information on how to enable cross-domain tracking with aliasing.",
    "How Does Aliasing Work?": "Depending on your integration type, aliasing is achieved in different ways. For example, if you send events to Analytics in real-time by using Segment, Amazon Kinesis, or via the Analytics API, the process is different than if you send events to Analytics daily through a data warehouse integration. The following documentation describes our process for aliasing user identities in real-time integrations. An aliasing API call consists of three components: Anonymous ID:The anonymous ID is an ID used to identify a user before they register, log in, or otherwise identify themselves.User ID:The user ID is the ID used to uniquely identify a user in your database. This should be an immutable field that is recorded in the user table. As mentioned, the user ID is created and assigned whenever a user makes themselves known.API Key:The project API Key is used to direct the alias call toward which Analytics project. After receiving an aliasing call, Analytics reconciles the anonymous ID and the user ID, and handles all incoming events from both IDs as the same user. In short, they\u2019re \u201cstitched\u201d together. Multiple anonymous IDs may be aliased to a single user ID, however you may not alias multiple user IDs to a single anonymous ID, nor may you alias a user ID to another user ID. Alias calls are processed once per day, so please allow up to 24 hours for best results. JavaScript (SDK) Alias Calls Indicative.sendAlias() The Analytics client automatically generates a universally unique identifier (UUID) to apply to all events until \u201cIndicative.setUniqueID(id)\u201d is called. \u201cIndicative.sendAlias()\u201d can be called to alias the UUID to the ID parameter defined within \u201cIndicative.setUniqueID(id)\u201d Indicative.setUniqueID(id, true) After setting the new unique ID, then this will automatically call \u201cIndicative.sendAlias()\u201d. If you perform \u201ccallIndicative.setUniqueID(id)\u201d without \u2018true\u2019, then the alias call will not be sent.",
    "Best Practices": "Annotation names should be brief, descriptive, and consistentUse categories to organize your similar annotationsAnnotations with a date range are displayed as a period of time, annotations with the same start time date and end date are displayed as a point in time",
    "Event Aliasing from mParticle": "When user profiles and events are sent from the mParticle CDP to Analytics, Analytics will inherit all aliasing functionality that was performed usingIDSync. This means that Analytics automatically has access to the same unified view of user profiles and events established within mParticle.",
    "Retroactive aliasing": "For customers on the Enterprise plan, alias calls can be performed retroactively to combine two activity timelines that have already been processed. Retroactive aliasing is useful when events performed anonymously before an aliasing API call is received need to be reconciled with a known user profile. Retroactive aliasing takes place once every 24 hours. For example, consider the events below, all performed by the same customer: Event 1:Customer views a registration page, establishinguser_id AEvent 2:user_id Asubmits registrationEvent 3:Customer successfully registers while using a different device, establishinguser_id BAliasing API call is made, establishing thatuser_id A is user_id BEvent 4:user_id Bviews an adEvent 5:user_id Aviews a login screenEvent 6:user_id Blogs in successfully In this sequence, only event 3 would need to be retroactively reconciled with the known user ID, while events 1, 2, 4, 5, and 6 would be immediately associated with the unified profile following the aliasing API call. Once daily retroactive aliasing is complete, the event in step 3 becomes associated with the same user profile as the rest of the events. This diagram further illustrates retroactive aliasing: ",
    "Custom Aliasing": "Custom aliasing rules may be supported for Enterprise customers only. This can be requested by contacting Support or your Customer Success Manager.",
    "Third Party Aliasing": "Aliasing calls from a third party are not supported. Analytics rejects these calls because they are considered \u201cchained\u201d alias calls (i.e. Anonymous ID > Third Party User ID > Indicative User ID). If you are concerned that your method of integration is causing aliasing issues, please contact Support or your Customer Success Manager.",
    "Aliasing with GDPR": "Under GDPR privacy regulations, users have the option to opt into or out of session tracking. If a user does not consent to tracking, then all personally identifiable information (PII) must not be collected and existing PII must be deleted. The only data that is tracked by the Analytics client is the Anonymous ID, a random string ID used to unify a single user session. This ID does not persist across multiple user sessions. It is not possible to alias the IDs of users who opt out of session tracking.",
    "Group ID": "When you create a group definition, you select a user attribute that becomes the group ID. Any user attribute can be used as a group ID, as long as it exists in your data catalog. Every Monday, on a weekly recurring basis, mParticle processes all profiles and groups together any profiles who share the same value for a user attribute that is a designated group ID. For example, if you create a group definition withaccount_idas the group ID, and three users all have the attributeaccount_idwith a value of1234, then all three users are added to a group that is identified by the group IDaccount_ID:1234. Group IDs must be unique hexadecimal or numerical user attributes within the following constraints: Minimum 1 characterMaximum 32 charactersIncludes only the alphanumeric charactersa-zand0-9 When you create a group definition, you are defining a set of criteria that is used to create multiple group instances, one for each value of the group ID. Using the example above, if mParticle ingested data for two more users who each had the attributeaccount_idwith a value of5678, they would be added to a separate group identified byaccount_id:5678.",
    "Group attributes": "A group attribute is a user attribute given to all members of a group. The value of a group attribute is the same for every profile in the group, but the value of a group attribute is calculated based on one of four possibleaggregation functions. When viewing a user profile, you can tell the difference between group attributes and other user attributes by a prefix equalingnameof a group ID. For example, all user profiles in a group with the group IDaccount_id:1234and the group attributepremium_subscriberwill display the new attributeaccount_id:premium_subscriber. This naming convention prevents any conflicts between group attributes and user attributes. For example, a user may have a premium subscriber group attribute, showing that they are in a group with a premium subscriber, even though individually they are not marked as a premium subscriber. Any user attribute that exists in your data catalog can be used as a group attribute.",
    "Group attribute aggregation logic": "The value of a group attribute is calculated by aggregating the values of each original user attribute in the group. The aggregation results from one of four functions that you specify when first creating a group attribute. These four aggregation functions are: Continue reading below for detailed descriptions and examples of each aggregation logic option. Group members only inherit aboolean orgroup attribute when another member has the same user attribute with a value oftrue. Only boolean user attributes can be used asboolean orgroup attributes. For example, imagine a group with a source attributehas_dog. Let\u2019s say this group contains three users and none of them had a dog when they joined the group, so each user\u2019s instance ofhas_dogequalsfalse. If a new user with thehas_dogattribute set totruejoins the group, then the other members will all have theirhas_dog groupattribute set totruethe next time the grouping job runs. The value of alatestgroup attribute is set to the most recently updated instance of the user attribute in the group. All members of a group with alatestgroup attribute inherit that attribute upon joining the group. User attributes of any data type can be used aslatestgroup attributes. For example, imagine a household group with two members: John and Jane. The group has alatestgroup attribute calledstreet_addresswhich is set to1234 Main Street. Next, a new user, Cindy, joins John and Jane\u2019s household group. Cindy\u2019s profile would inherit the group attributestreet_address:1234 Main Street. The value of asumgroup attribute is equal to the sum of all instances of the user attribute in the group. Only non-negative numbers or integers can be used assumgroup attributes. The value of anaveragegroup attribute is equal to the average of all instances of the user attribute in the group. Only non-negative numbers or integers can be used asaveragegroup attributes.",
    "Group Identity API": "In addition to the mParticle UI, you can also create and manage group definitions programmatically using the Group Identity API. For more information, see theGroup Identity API reference.",
    "Limitations": "Predictions created in mParticle have the following limitations: Predictions can only be created for custom and commerce event types.Predictions use the future event prediction model in Cortex.",
    "Predefined Events": "Snowplow has a set ofpredefined event typesthat can be instrumented: Page viewsPage pingsEcommerce transactionsErrors If instrumented, the integration will generate these events, where the \u2018event\u2019 field in the Unified Log is used as the event name in Analytics.",
    "Common and Platform-Specific Properties": "For all Snowplow events, a range ofdatetime, user and device fieldsare recorded along with the event. If instrumented, all of these fields are generated as Analytics properties. Additionally, anyplatform-specific fieldswill be recorded as well, such as page referer and URL information for a web-specific instrumentation.",
    "Structured Events": "Snowplowcustom structured eventsare generated using the \u2018se_action\u2019 field as the event name for Analytics (or the \u2018event_name\u2019 if \u2018se_action\u2019 is not populated). The \u2018se_category\u2019, \u2018se_label\u2019, \u2018se_property\u2019 and \u2018se_value\u2019 fields are added as Analytics properties in addition to all common and platform-specific properties.",
    "Unstructured Events": "Snowplow allows customers to model flexible customunstructured eventsas needed. The \u2018event_name\u2019 field is used for the event name in Analytics (or it defaults to \u2018unstruct\u2019 if \u2018event_name\u2019 is not populated).",
    "Custom Contexts": "Snowplow allows customers to define their owncontextaround events, such as extra user properties for a customer (membership information, age, etc.) or extra properties about a product for a purchase event (SKU, tags, product name, etc.).",
    "Aliasing": "Analytics supports aliasing between anonymous IDs and user IDs to allow customers to unify event streams submitted with separate unique keys.Click herefor a full walkthrough of Analytics\u2019 aliasing protocol.",
    "Filters": "Filter a query to include or exclude users who meet a certain condition. You can filter on event properties, user properties, or user segments. There is no limit to the number of filters you can apply to a single query row. In the following example, PetBox wants to exclude any users who downloaded their app on Android. Therefore, the user selects the filter where function, and creates a filter where device type is not equal to Android. ",
    "Pausing and Resuming": "To pause the event view, clickPauseor click on a particular event. When the event view is paused, the Pause button changes to a Resume button.  The stream remains paused as you select additional events.  It will resume by clickingResume.",
    "Examining a Specific Event": "To view the details of a specific event, select the event from the Live Stream list. The Live Stream pauses, the selected event expands to display additional message details, and a details panel is shown. The Event Details panel contains additional event information arranged by category.If you select a Batch message, the Event Details panel displays general batch details, user attributes, user identities and location information.If you select an event message, the Event details panel displays general event details, app version, event attributes, device information, platform information, and location information.Click the expand icon at the top of the details panel for a JSON representation of the data: ",
    "Clear Entries": "When initiating a new session on the test device, you may want to clear the Live Stream from the last test session. To do this, simply clickClear Entries. The Live Stream and event details clear immediately, but the filters retain their values.",
    "Why use Predictive Attributes?": "Traditionally, marketers and product managers look to user events like website visits, page views, and journey completions to build out customer segments. While manual segmentation can certainly be effective, it is subject to the limitations and inefficiencies of human decision making. Predictive Attributes let you avoid the drawbacks of manual audience building. Once you define your desired conversion goal, Cortex\u2019s Machine Learning models will analyze thousands of behavioral signals to determine which users are statistically most likely to convert, letting you accelerate decision making and execute campaigns with confidence. Additionally, Predictive Attributes update automatically based on real-time customer behavior. Since your highest value customers are always changing, Cortex continuously recalculates Predictive Attributes to ensure that your campaigns stay focused on the users who most closely align with your campaign goals.",
    "How do Predictive Attributes work?": "Predictive Attributes are stored on the User Profile, and you can use them in the same ways you would any other behavioral or demographic User Attributes (like audience building or querying via the Profile API). The difference is that unlike regular User Attributes, which are captured as soon as your users take specific actions, Predictive Attributes need to be defined and generated withing a workflow in mParticle. Once they exist on the User Profile, however, there is functionally no difference between Predictive and regular User Attributes in how you use them to achieve your business goals.",
    "Types of Predictive Attributes": "Predictive Attributes fall into two main categories based on the business outcomes they enable: These predictions tell you how likely your individual customers are to take a specific action that matters to your business, like purchasing a new product or upgrading a subscription (to name just a few). These predictions tell you which specific offer among a specified set is mostly likely to result in a customer taking a defined action.",
    "When to use Predictive Attributes": "The most fruitful use cases for Predictive Attributes tend to have the following characteristics: Behavioral events are an important factor in generating a prediction.User Attributes alone (e.g. Customer Lifetime Value, subscriber status, etc.) are not sufficient to predict user behavior.The action occurs frequently.The more frequently an action occurs, the more behavioral data Cortex will likely have on the user base. (Like purchases and membership upgrades, for example.)The action is intentional and meaningful.Conversion actions like purchasing an item, subscribing, and upselling reflect meaningful consumer decisions, whereas product views, page visits, and other browsing behavior can be less intentional. Specific examples of use cases where Predictive Attributes are likely to improve campaign performance include: Non-subscriber to subscriber conversionSubscription tier increasesUpselling users on a premium offeringChurn preventionPredicting media engagement(e.g. which users are likely to watch a particular show)",
    "UTM Tracking Within Analytics": "Analytics automatically parses out the UTM values from page URLs that contain UTM parameters. You\u2019ll be able to see the following properties if you are sending Analytics data with UTM parameters within the URL. These parameters will then appear withinUser Propertiesor withinEvent Properties. For example, when using these parameters within User Properties, Analytics would display users with these specific UTM parameters. However, when using UTM parameters within Event Properties, Analytics would display events that were completed with the UTM parameters. When you want to see the users who completed an event or the total number of times an event was completed with a specific UTM parameter, it would be best to use the UTM parameters within Event Properties. Let\u2019s say you have a group of users who completed [pageload] of the url: www.indicative.com/ and you wanted to narrow down your results to users who performed [pageload] with utm_source=Facebook or in other words with a url of www.indicative.com/?utm_source=Facebook. In this case, you would filter utm_source=Facebook as an event property. However, if you wanted to see the total count of users with a specific UTM parameter who completed an event, it would be best to use UTM parameters within User Properties. Let\u2019s say you have a group of users who completed [pageload] of the url: www.indicative.com/ and you just wanted to see if these users have an utm_source=Facebook, not if they performed a [pageload] of the url www.indicative.com/?utm_source=Facebook. In this case, you would use a user property. Note:Analytics uses the \u201d&\u201d character to delineate separate UTM parameters in a URL. For instance, an event with a property named pageUrl URL with a value ofhttps://www.website.com/?utm_source=SRC&utm_id=252325would have the following UTM properties: \u201cpageUrl.utm_source\u201d: \u201cSRC\u201d\u201cpageUrl.utm_id\u201d: \u201c252325\u201d If including an \u201d&\u201d character in a UTM parameter is important, you need to use \u201camp\u201d in the URL to escape it. For example, the URL below (assigned to a property pageUrl) would be parsed into the following UTM properties: URL:https://www.website.com/?utm_source=SRC&amp;&someOtherField=someValue&utm_id=ID \u201cpageUrl.utm_source\u201d: \u201cSRC&\u201c,\u201cpageUrl.utm_id\u201d: \u201cID\u201d",
    "User Properties": "User Properties are the properties associated with the user performing an event, such as demographic factors, an email address, or the marketing channel through which the user was originally acquired. While event properties can differ from event to event, user properties are associated with every event performed by a given user. In order to enable user properties for use in queries, the Attribution Mode under Manage Data must be set to either first or last. Consider the example table of events below: \u2018First\u2019 will store the first value seen for a property for each user across any event that they fire. From the image above, user 1\u2019s first value for this property will beNULL, user 2\u2019s will beSocial, and user 3\u2019s will beSearch.\u2018Last\u2019 will store the last value seen for a property for each user across any event that they fire. From the image above, user 1\u2019s last value for this property will beSearch, user 2\u2019s will beSocial, and user 3\u2019s will beSearch.Because user properties are stored at the user level, the scope of the analysis does not impact what value is stored at the user level. For example, if an analysis was looking at events that occurred between 8/18-8/20 from the example above, user 1\u2019\u2019s value for the property will still beNULLif this user property was configured for First attribution mode. Thus, user properties are selected to view results from users who performed a particular action, even if their first or last actions are beyond the scope of the analysis that has been created.",
    "Event Properties": "When an event property is used in an analysis, Analytics will look at each individual event\u2019s payload and reference the property associated with each event. The event property has the query ask\u201cwhat was the value of the property at the time of the event?\u201d, which is the most common scenario for most analyses. For rows set to display the \u201ctotal count of\u201d events, the results will include all of the events seen as long as they meet the query parameters, even if they were performed by the same user multiple times.For \u201cusers who performed\u201d rows, event properties will still filter or group based on each time that the event was seen, but the results will represent user counts instead of raw event counts.",
    "Metrics": "The following metrics are available: Invocations- how many times the rule was invokedThrottles- how many times a 429 throttling response was returned when calling the ruleErrors- how many errors have occurred when calling the rule These metrics are for the last 24 hours and apply to all connections. Summaries for each rule can be seen on the main rules page. Detailed graph of the previous 24 hours is available on theMonitoringtab of the individual rule page. ",
    "Path insights": " Path Insights displays a variety of important funnel metrics. To access Path Insights, select an event in your funnel. Pathdisplays which step users completed before completing the selected step.Total Dropdisplays the percentage of users lost in the progression from the previous step to the selected step.Total Conversiondisplays the percentage of users that made it from the previous step to the selected step.Average Step Timedisplays the average amount of time it took users to get from the previous step to the selected step.",
    "Conversion precision": "You may adjust the precision of your Funnel query using the Conversion Precision setting. In a sequential funnel, events in the must occur in sequential order. In an approximate funnel, events may occur simultaneously. All new queries default to Sequential. For a full explanation, see our article onConversion Precision. ",
    "Zoom": "You can zoom your view of the Overview Map in and out by clicking the + or - magnifying glass icons in the bottom left corner of the map. ",
    "Funnel flexibility": "Funnels typically follow the critical steps to complete a user journey. For example, a commerce app would have browse, cart and buy as steps. But Analytics Funnels support more complex business logic including optional steps. The only steps in Funnels that are required are the first step and the last step. All others can be flagged as optional. When users flow through the optional steps, there is path exclusivity. That means that a user can appear in only one path or if the user appears multiple times, it is once for each path that they completed. In the commerce example, this could be the product compare feature or a size chart. For more information, see theMultipath Funnelarticle. Funnels allow you to test the importance of funnel steps by setting steps to inactive without completely deleting them. This is useful for understanding the impact of a step in overall conversion. And once your key funnel reports are in use, the trending option helps you monitor funnel performance over time. When there are key user characteristics that significantly contribute to conversion patterns, funnels can be built to group together users by an additional attribute. User by age group or user by sign up channel are interesting ways to dig into how users interact with a digital property. Depending on the attribute selected, users may land in the funnel more than once. Consider a user who visits first from search and then shortly after from a social ad. The grouped funnel can handle the complexity of treating those visits separately to understand which inbound channel has a more positive impact on conversion.",
    "Conclusion": "Journey reporting lets you explore common flows through your digital properties. These flows can help you identify points of friction. They can also uncover different flows through the site that you can then formalize and monitor with Funnel reports.",
    "Introduction": "The purpose of this guide is to walk you through the basic steps of setting up mParticle in your app, unlocking core functionality, and troubleshooting common issues. Along the way, you\u2019ll cover some important concepts you need to understand to be successful with mParticle. This is not a complete guide to all of mParticle\u2019s features and capabilities. If you already know your way around mParticle and you\u2019re looking for in-depth docs, head to ourDevelopersorGuidessections.",
    "Authentication": "Analytics\u2019 Export API requires HTTPS/SSL and uses Basic Authentication header for all requests. Basic Authentication is a simple authentication scheme built into the HTTP protocol. The client sends HTTP requests with the Authorization header that contains the word Basic followed by a space and a Base64-encoded string username:password. Use your API Key as the username and your Access Token as the password. This information can be found on the Project Page in the Analytics web application. Please treat your Access Token as you would a password, as it is meant to only be known to you. An example curl:curl -v -u \"apiKey:accessToken\" https://query.indicative.com curl\u2019s -u parameter automatically encodes the username and password and inserts them into the Authorization header. Depending on your client implementation you may need to do this manually. The resulting HTTP Response should have a 200 OK status.",
    "Response Format": "All API responses are in JSON format.",
    "List User Segments": "This endpoint lists all User Segments in a project. Endpoint: GEThttps://query.indicative.com/service/v1/segments/listRequest Parameters: NoneRequest Headers: Basic AuthenticationResponse: A list of User Segments objects for the specified project. See User Segment Object Response below. User Segment Object Response Example: List User Segments $ curl -s -u \"5a6646c4-ae5a-4de4-9d47-eb6f228405b2:nux06hkn870v0ni4fhtd81kf\" https://query.indicative.com/service/v1/segments/list",
    "Request a User Segment Export": "This endpoint invokes a new export request for the specified User Segments. Endpoint:POST https://query.indicative.com/service/v1/segments/export/Request Parameters: POST body, containing the following fields:segmentId: the ID of the segment to exportoutputFormat: the output format of the export file (currently only CSV is supported)Request Headers:Basic AuthenticationContent-Type: application/jsonResponse: An Export Status object representing the status of the newly created export. See Export Status Object Response Example: Request User Segments Export",
    "Check Export Status": "This endpoint checks the status of a previously submitted export request. Once the request has finished processing, it will include a URL to download the results. Once the request has been completed, the file will remain in Amazon for no more than 48 hours. Otherwise, a new request must be submitted. Endpoint:GET https://query.indicative.com/service/v1/segments/export/{exportId}Request Parameters:exportId: the unique identifier of the export for which to check status retrieved from either the Status or List APIRequest Headers: Basic AuthenticationResponse: An Export Status object representing the status of the specified export. See Export Status Object Response. Example: Check User Segment Export Status $ curl -s -u \"5a6646c4-ae5a-4de4-9d47-eb6f228405b2:nux06hkn870v0ni4fhtd81kf\" https://query.indicative.com/service/v1/segments/export/8f9fdd71-204c-41e1-a2ab-de7f85002f65",
    "Supported warehouse providers": "You can use Warehouse Sync to ingest both user and event from the following warehouse providers: Amazon RedshiftGoogle BigQuerySnowflakeDatabricks",
    "Warehouse Sync setup overview": "Prepare your data warehouse before connecting it to mParticleCreate aninput feedin your mParticle account for your warehouseConnect your warehouse to your new mParticle input feedSpecify the data you want to ingest into mParticle by creating a SQL data modelMap your warehouse data to fields in mParticleForuser data pipelines, this mapping is done by your data modelForevent data pipelines, you must complete an additional data mapping stepConfigure when and how often data is ingested from your warehouse",
    "Observability for Warehouse Sync": "mParticle\u2019sObservabilityoffers support for tracing data ingested into your account through a Warehouse Sync pipeline. Simply select the pipeline feed you want to trace from the list of Feeds whencreating a new trace configuration. Trace configurations for Warehouse Sync pipelines will generate a new unique trace for each event batch generated by your pipeline within the timeframe you specify in your configuration. Like other trace configurations, pipeline traces display spans for each processing stage your data passes through once it\u2019s ingested into mParticle. Warehouse Sync traces do not currently provide row-specific details from your source database or details for diagnosing errors encountered during data ingestion. The current version of Warehouse Sync tracing is intended to help diagnose and troubleshoot issues encountered post-ingestion.",
    "1 Prepare your data warehouse": "Work with your warehouse administrator or IT team to ensure your warehouse is reachable and accessible by mParticle. Whitelist themParticle IP address rangeso your warehouse will be able to accept inbound API requests from mParticle.Ask your database administrator to perform the following steps in your warehouse to create a new role that mParticle can use to access your database. Select the correct tab for your warehouse (Snowflake, Google BigQuery, Amazon Redshift, or Databricks) below.",
    "2 Create a new warehouse input": "Log into your mParticle accountNavigate toSetup > Inputsin the left-hand nav bar and select the Feeds tabUnderAdd Feed Input, search for and select your data warehouse provider.  You can also create a new warehouse input from the Integrations Directory: Log into your mParticle account, and clickDirectoryin the left hand nav.Search for either Google BigQuery, Snowflake, Amazon Redshift, or Databricks.  After selecting your warehouse provider, the Warehouse Sync setup wizard will open where you will: Enter your warehouse detailsCreate your data modelCreate any necessary mappings between your warehouse data and mParticle fieldsEnter your sync schedule settings",
    "3 Connect warehouse": "The setup wizard presents different configuration options depending on the warehouse provider you select. Use the tabs below to view the specific setup instructions for Amazon Redshift, Google BigQuery, and Snowflake.",
    "4 Create data model": "Your data model describes which columns from your warehouse to ingest into mParticle, and which mParticle fields each column should map to. While mParticle data models are written in SQL, all warehouse providers process SQL slightly differently so it is important to use the correct SQL syntax for the warehouse provider you select. For a detailed reference of all SQL commands Warehouse Sync supports alongside real-world example SQL queries, seeWarehouse Sync SQL reference. Write a SQL query following the guidelines outlined below and theWarehouse Sync SQL reference. Make sure to use SQL commands supported by your warehouse provider.Enter the SQL query in the SQL query text box, and clickRun Query.ClickNext.  mParticle submits the SQL query you provide to your warehouse to retrieve specific columns of data. Depending on the SQL operators and functions you use, the columns selected from your database are transformed, or mapped, to user profile attributes in your mParticle account. If you use an attribute name in your SQL query that doesn\u2019t exist in your mParticle account, mParticle creates an attribute with the same name and maps this data to the new attribute. mParticle automatically maps matching column names in your warehouse to reserved mParticle user attributes and device ids. For example, if your database contains a column namedcustomer_id, it is automatically mapped to thecustomer_iduser identifier in mParticle. For a complete list of reserved attribute names, seeReserved mParticle user or device identity column names. Below is an example of a simple table and SQL query to create a data model: Table name:mp.demo.userdata Example SQL query: This SQL query selects the first_name, last_name, and email columns from the table calledmp.demo.userdata. In the next step, we will set up the mappings for this user data.",
    "5 Create data mapping": "After creating a data model that specifies which columns in your warehouse you want to ingest, you must map each column to its respective field within mParticle with a data mapping. To create a data mapping, first use the dropdown menu titledType of data to syncto select eitherUser Attributes & Identities OnlyorEvents, depending on whether you want to ingest user data or event data. To continue with our example user data table and SQL query from above, we\u2019ll selectUser Attributes & Identities Only: When mapping attributes and identities from your warehouse to fields in mParticle, you must always create auser_identitiesoruser_attributesobject first. You can then create the individual mappings for your identities and attributes within these entities, as shown in the next two sections. To map your user identities: ClickAdd Mapping.Under Mapping Type, selectObject.Under Field in mParticle, selectuser_identites.ClickSave.Within your newuser_identitiesobject, click the+button.Under Mapping Type, selectColumn.UnderColumn in warehouseenter the name of the column containing the identities you want to ingest. In this example, we\u2019ll use a column calledemail.UnderField in mParticleuse the dropdown menu to select the correct user identity in mParticle you want to map your identities to. In this example, we\u2019ll selectEmail Address.  ClickSave. To map your user attributes: ClickAdd Mapping.  Under Mapping Type, selectObject.Under Field in mParticle, selectuser_attributes.  ClickSave.Within your newuser_attributesobject, click the+button.  Under Mapping Type, selectColumn.UnderColumn in warehouse, enter the name of the column you want to map to mParticle. You can enter a specific column name, or$unmappedto select all currently unmapped columns. Entering$unmappedselects all columns that are not already mapped to mParticle. In our example, because we\u2019ve already mapped theemailcolumn to theEmail Addressfield in mParticle, it will be excluded. UnderField in mParticleenter the name of the field in mParticle that you want to map your column to. If you entered$unmappedfor your warehouse column selection, you can enter an asterisk (*) forField in mParticleto use each unmapped column name in your warehouse as the respective field name in mParticle. This allows you to map all of your attributes at once so you don\u2019t have to create a separate mapping for each individual attribute.  ClickSave. If you are ingesting event data instead of user data (as shown above), selectEventsunderType of data to synconce you reach the Review Mapping step. ClickAdd Mapping.  Under Mapping Type, selectObject.Under Field in mParticle, selectevents.  ClickSave.Within the neweventsobject, click the+button to add a new mapping.  Select the type of mapping you want to use from theMapping Typedropdown:Column- maps a column from your database to a field in mParticleStatic- maps a static value that you define to a field in mParticleIgnore- prevents a field that you specify from being ingested into mParticle The next steps will vary depending on the data you are ingesting and the mapping type you select. Following are several examples of how to use each mapping type. UnderColumn in warehouse, select the name of the column in your database you are mapping fieldsfrom.UnderField in mParticle, select the field in mParticle you are mapping fieldsto.ClickSave.  UnderInput Valueselect eitherString,Number, orBooleanfor the data type of the static value you are mapping.In the entry box, enter the static value you are mapping.UnderField in mParticle, select the field you want to map your static value to.ClickSave.  UnderColumn in warehouse, select the name of the column that you want your pipeline to ignore when ingesting data.ClickSave.  To add additional mappings, clickAdd Mapping. You must create a mapping for every column you selected in your data model.  When you have finished creating your mappings, clickNext.",
    "6 Set sync settings": "The sync frequency settings determine when the initial sync with your database will occur, and how frequently any subsequent syncs will be executed. Select eitherProdorDevdepending on whether you want your warehouse data sent to the mParticle development or production environments. This setting determines how mParticle processes ingested data. Input protection levels determine how data ingested from your warehouse can contribute to new or existing user profiles in mParticle: Create & Update: the default setting for all inputs in mParticle. This setting allows ingested user data to initiate the creation of a new profile or to be added to an existing profile.Update Only: allows ingested data to be added to existing profiles, but not initiate the creation of new profiles.Read Only: prevents ingested data from updating or creating user profiles. To learn more about these settings and how they can be used in different scenarios, seeInput protections. There are two sync modes: incremental and full. Incremental: Use this sync mode to only ingest data that has changed or been added between sync runs as indicated by your warehouse column you use as an iterator. The first run for incremental sync modes is always be a full sync.Full: Use this sync mode to sync all data from your warehouse each time you execute a sync run. Use caution when selecting this sync mode, as it can incur to very high costs due to the volume of data ingested. The remaining setting options change depending on the mode you select from theSync modedropdown menu. Navigate between the two configuration options using the tabs below: The value you select for Sync Start Date determines how much old, historical data mParticle ingests from your warehouse in your initial sync. When determining how much historical data to ingest, mParticle uses to the column in your database you selected as the Timestamp field in the Create Data Model step. After your initial sync begins, mParticle begins ingesting any historical data. If mParticle hasn\u2019t finished ingesting historical data before the time a subsequent sync is due to start, the subsequent sync is still executed, and the historical data continues syncing in the background. Historical data syncing doesn\u2019t contribute to any rate limiting on subsequent syncs.  After entering your sync settings, clickNext.",
    "7 Review": "mParticle generates a preview for Data Warehouse syncs that have been configured, but not yet activated. Use this page and the sample enriched user profiles to confirm the following: Your data model correctly maps columns from your database to mParticle attributesYour sync is scheduled to occur at the correct intervalYour initial sync is scheduled to occur at the correct timeYour initial sync includes any desired historical data in your warehouse After reviewing your sync configuration, clickActivateto activate your sync.",
    "View and manage existing warehouse syncs": "Log into your mParticle account.Navigate toSetup > Inputsin the left hand nav bar and select the Feeds tab.Any configured warehouse syncs are listed on this page, grouped by warehouse provider. Expand a warehouse provider to view and manage a sync.To edit an existing sync, select it from the list under a warehouse provider. This loads the Warehouse Sync setup wizard, where you can modify your sync settings.Connect a sync to an output by clicking the green+icon under Connected Outputs.Configure rules for a sync by clicking the+ Setupbutton under Rules Applied.Delete a sync configuration by clicking the trash icon under Actions.To add a new sync for a warehouse provider, click the+icon next to the provider.",
    "Manage a sync": "To view details for an existing sync, select it from the list of syncs. A summary page is displayed, showing the current status (Active or Paused), sync frequency, and a list of recent or in-progress syncs. To pause a sync, clickPause Sync. Paused syncs will only resume running on their configured schedule after you clickResume. To run an on-demand sync, clickRun Syncunder Sync Frequency. Use the Data Model, Mapping, and Settings tabs to view and edit your sync configuration details. ClickingEditfrom any of these tabs opens the respective step of the setup wizard where you can make and save your changes.",
    "Query Summary": "You may also view a written summary of your query builder for any dashboard analysis. To do so, select the three dots in the top right of any dashboard analysis, and choose Show Summary: When you select Show Summary, the analysis \u2018card\u2019 will flip to display the Query Summary:",
    "Prerequisites:": "To integrate with Snowplow Snowflake, you will need to access your Snowflake console. For this self-service integration, we also have some data requirements: All of your events must be unified into one singular table as opposed to having separate tables for each event type.There can only be a maximum of one authenticatedID and one unauthenticatedID for aliasing.The event timestamp must be in UTCAll joins must be done beforehand.",
    "User Identification (Aliasing)": "After some basic checks, we can define your users within your data. For more information on User Identification (Aliasing), please refer tothis article. If you choose to enable Aliasing, click onEnabled:Type- Select the Snowplow field typeAtomic- If the anonymous ID field is an atomic field, select this option.Field Name- Select the field that should be used to identify anonymous usersContext- If your anonymous ID is contained within the Contexts field, choose this optionField Name- Select the context field that contains your anonymousIDIf you choose to disable Aliasing, pressDisabled:Type- Select the Snowplow field typeAtomic- If the anonymous ID field is an atomic field, select this option.Field Name- Select the field that should be used to identify anonymous usersContext- If your anonymous ID is contained within the Contexts field, choose this optionField Name- Select the context field that contains your anonymousID If you have a non-null value that represents null UserID values, please click on theShow Advancedbutton. In this field, please enter these non-null values.",
    "Export Users": "To download a list of users from an entire series, click on any data point or table cell within your results. Within the dropdown, select \u201cExplore Users.\u201d Once your Users query results have loaded, click on the export icon located in the menu bar beneath the query builder, then select \u201cDownload CSV.\u201d  To download a list of users from a single point, click on any data point or table cell within your results. Within the dropdown, select \u201cDownload Users in this Point to CSV.\u201d A list containing all users associated with this data point will be emailed to the address connected to your Analytics account. There is no limit on the number of users that can be exported. ",
    "Forwarding data from feeds": "When planning a feed implementation, it is important to consider if and how you want the data to be forwarded to any event outputs. Event integrations connect an \u2018input\u2019, which is a single platform or feed, to a single \u2018output\u2019. Most outputs can only accept connections from a limited set of platforms. When setting up connections from a feed, you need to know that some feeds can \u201cact-as\u201d iOS, Android, Web, or other platforms.",
    "\u201cAct-as\u201d feeds": "Act-as feeds are feeds which mParticle can treat as if they belonged to a single platform. For example, if a feed can \u201cact as\u201d iOS, you can connect the feed to any output that accepts data from the iOS platform. When you configure an act-as feed, you will need to select the platform you want the feed to act as:  You can connect an act-as feed to any output that can support the platform it is configured to act as. Note that for act-as feeds, if you wish to capture data for multiple platforms, you will need to configure multiple instances of the feed \u2014 one for each platform.",
    "\u201cUnbound\u201d feeds": "If you do not see an option to select an \u201cact-as\u201d platform when you configure a feed, the data from that feed is treated as \u201cunbound\u201d: not tied to a particular platform. For example, a feed that forwards data about user actions in response to an email campaign will be an unbound feed. These events don\u2019t specifically belong to a platform, like iOS or Android, and won\u2019t have the necessary identifiers to be processed as iOS or Android events. Outputs that only support iOS / Android / Web will not accept data from unbound feeds. However: Unbound feeds can still be forwarded to Data Warehouse outputs and webhook outputs, like Amazon S3.Events from unbound feeds can be used to power audience selection criteria.Some unbound feeds primarily forward user attributes. User attributes updated by a feed can still be forwarded to event output partners through the enrichment process.",
    "Create an \u201cAnd\u201d Clause": "After using a Filter Where clause to filter an event, add another filter below the one you just created, or drag another event property or user property onto the Filter Where parameter to build an \u201cAnd\u201d or an \u201cOr\u201d clause. For example, you may want to look at user who did View Content where the Platform is equal to Desktop and the user\u2019s Marketing Channel is Affiliate. This would help understand how many users acquired through an affiliate are viewing content on their desktops.",
    "Create an \u201cOr\u201d Clause": "Alternately, use \u201cOr\u201d to view users who did View Content where the platform is equal to Desktop or the platform is equal to Android to see the total content views made by anyone who used either a Desktop or an Android device. This can help when looking for very granular behavior patterns.",
    "Changing an \u201cAnd/Or\u201d Clause to a \u201cBy\u201d Clause": "Within the \u201cAnd/Or\u201d dropdown menu, there also exists an option to instead create a\u201cBy\u201d Clause. After selecting the \u201cBy\u201d option in the dropdown menu, the clause will be re-applied as a \u201cBy\u201d clause breakout.",
    "In-Line \u201cAnd/Or\u201d Clauses": "You may also add And/Or clauses in-line within the filter dropdown using the symbols \u201d&\u201d (shift+7) and \u201d|\u201d (shift+) to represent \u201cAnd\u201d and \u201cOr\u201d respectively. Rather than create a new line in the query for each additional value in the row, click within the box and use an ampersand (and) or a pipe character (or) to quickly add And/Or clauses in the same line. For example, you may want to see how many content views came from the users on the platforms iPhone and Android. In the Filter Where clause: Select iPhoneClick inside of the drop down boxAdd an ampersand after iPhoneType Android",
    "How to Configure an Embedded Widget Filter": "Create an embedded widget or public dashboardCreate a Variant ID by calling the Variant Creation APIRequest a Dashboard or Widget variant by including the query string parameter \u2018dvid\u2019 with the Variant ID in the URL.",
    "Create a Dashboard or Widget Variant ID": "Analytics offers a secure API to create dashboard variants. Before requesting a variant on any of your public dashboards via the URL, it must have first been created using the API as described below. You may create dashboard variants by passing in your specified parameters as either a JSON encoded entity in your request or as query parameters on your request\u2019s URI. There are a number of prerequisites that must be completed  before calling this API:",
    "Prerequisite: Enable Dashboard Sharing": "To use Dashboard Variants, Public Access must be enabled for the dashboard. Navigate to a dashboard, click on \u201cManage Dashboard\u201d and select \u201cDashboard Settings\u201d from the menu at the top right.  Select Enable Sharing in the Public Access section. ",
    "Prerequisite: Retrieve the Dashboard ID": "To use the Variant Creation API, you will need the Dashboard ID for a dashboard. This can be found either in your Dashboard URL, or through the Analytics application. Dashboard IDs can be retrieved by observing the URL when viewing a dashboard in Analytics. Specifically, the \u201cdid\u201d parameter contains the Dashboard ID.  For example: https://app.indicative.com/#/dashboards?did=085045b7-f7bd-4378-9b56-f3ed0e571bd3&pid=04d5cec0-a25c-4d7a-864d-f7f7deece4fd The Dashboard ID and the full variant creation API URL can be retrieved through the Analytics Web Application. Navigate to a dashboard, click on Manage Dashboard and select Dashboard Settings from the menu at the top right.  Look for the section titled Dashboard Variant API Endpoint.  Click on the URL to copy to your clipboard. ",
    "Prerequisite: API Authentication": "Analytics\u2019 Variant Creation API requires HTTPS/SSL and uses Basic Authentication header for all requests. Basic Authentication is a simple authentication scheme built into the HTTP protocol. The client sends HTTP requests with the Authorization header that contains the word Basic followed by a space and a Base64-encoded string username:password. Use your Project API Key as the username and your Private Access Token as the password. This information can be found in the Analytics web application within Project Settings on the General tab. Please treat your Private Access Token as you would a password - it is meant to only be known to you.  An example curl : curl\u2019s \u2018-u\u2019 parameter automatically encodes the username and password and inserts them into the Authorization header. Depending on your client implementation you may need to do this manually.",
    "Creation API Endpoint": "https://web.indicative.com/service/dashboard/variant/{dashboardId}",
    "Creation API Request": "Requesting Analytics\u2019 Variant Creation API requires you to pass in your dashboard ID as part of the request URI as well as additional parameters to define the variant.",
    "Creation API Response": "The response to the Analytics\u2019 Variant Creation API is a JSON object including information about the dashboard variant.",
    "Date Ranges": "There are two methods of filtering by a date range, absolute and relative date ranges. If no date range is specified, the queries default date range is used. Parameter Name: dateRangeParameter Value: startRange,endRangeDate Format: YYYY-MM-DD (ISO 8601)Required: False To change the start and end date for all widgets, simply add&dateRange=to your URL string, followed by the start date and end date, comma delimited, in ISO 8601 date format. Use the word \u2018Today\u2019 to indicate that the endRange should always be inclusive of today. For example: &dateRange=2020-11-15,2020-11-30: This will filter the date range to be from November 15th, 2020 to November 30th, 2020 inclusive.&dateRange=2020-11-15,Today: This will filter the date range to be from November 15th, 2020 to Today inclusive. Parameter Name: dateRangeParameter Value: rangeNumber,rangeTypeDate Format: YYYY-MM-DD (ISO 8601)Required: False To change your date range to a relative date range for all widgets, simply add&dateRange=to your URL string, followed by therangeNumberandrangeTypefrom the options below, comma-delimited. HourDayWeekMonthYear Examples: &dateRange=7,Days: This will change the date range for all widgets in your dashboard to include the Last 7 days.&dateRange=4,Months: This will change the date range for all widgets in your dashboard to include the last 4 months. Special Cases:If a user changes the date range via the URL for funnel widgets with individual step based date ranges, we will remove the individual step date ranges, and apply the new date range to the entire funnel.",
    "Time Zone": "The time zone setting determines the display time zone for all events within the project. All new projects default to (GMT+00:00) UTC (00:00). Note, only one time zone may be selected per project. You are able to set up queries to be in a time zone that is not in your project time zone: The time zone selector is located in the top right corner of the query builder within each tool.",
    "Query Interval": "Parameter Name: intervalParameter Value:A string corresponding to the interval constant with which to run the query. The interval determines event timestamp grouping. For example, an interval of Day produces x-axis ticks for each day in the specified date rangeRequired: False In order to change the interval for all widgets, simply add &interval= to your URL string followed by the appropriate interval constant. If interval is not included, the query interval will not be changed. HourDayWeekMonthYear",
    "Query Filters": "Parameter Name: filter (multiple parameters can be included in one URL)Parameter Value: [match filter],[replace filter]Filter: [propertyName,propertyOperator,propertyValue],[propertyOperator,propertyValue]Required: False Details of individual queries within a dashboard or widget can be filtered based on matching specific parts of the query and specifying the values the corresponding match should be changed to.  The components of a query that can be modified include the property operator and the property value.  For reference, in the screenshot above, \u2018Browser\u2019 is the property, \u2018is equal to\u2019 is the property operator, and \u2018Chrome\u2019 is the property value. The syntax of a filter is a tuple corresponding to the matchingFilter, and the replaceFilter.",
    "Matching Filter": "The matching filter contains three comma-delimited parts, propertyName, propertyOperator, and propertyValue which will be used to isolate individual queries in a Dashboard or Embedded Widget. propertyName: The case sensitive property name to be matched. (This field corresponds to the raw property name, not the display value. See the Events and Properties section in the Analytics web application for more information. Must not contain commas \u2018,\u2019.)propertyOperator: The operator type to be matched. See the table below for valid values. Must not contain commas \u2018,\u2019.propertyValue: The case sensitive property value to be matched. Must not contain commas \u2018,\u2019. NOTE: In a Match Filter, the * symbol can be used as a wildcard match for any component.",
    "Replace Filter": "When a Matching Filter matches a query row, the replacement filter is applied to it. propertyOperator: The operator type to be replaced. See the table below for valid values.propertyValue: The case sensitive property value to be replaced. NOTE: In a Replace Filter, the * symbol can be used to preserve the Matching Filter value of a component.",
    "Multiple Filters": "Multiple filters can be applied to a single URL by including multiple filter query parameters.",
    "Property Operators": "The available property operators, and the corresponding constant. Change all instances of device_type is equal to \u201cweb\u201d to device_type is not equal to \u201cweb\u201d using the Query String:&filter=[device_type,eq,web],[neq,*] Alternatively, with JSON: Change all instances of device_type to device_type \u201cmobile\u201d using the Query String:&filter=[device_type,*,*],[*,mobile] Alternatively, with JSON:",
    "Viewing a Dashboard or Widget Variant": "Using the Variant ID generated using the Variant Creation API, add a \u201cdvid\u201d parameter (from the Creation API Response) to any Dashboard, Public Dashboard, or Widget URL Request.",
    "Example": "The mParticle organization has two accounts: mPTravel and mPDine. Cross-account audience sharing is enabled.mPTravel sets the following audience-level sharing permissions:The \u201cPotential Parisians\u201d audience is private from mPDine.The \u201cAspiring Athenians\u201d audience is view only for mPDine.The \u201cIbiza Dreamers\u201d audience is \u201cusable\u201d for mPDine.A user in the mPDine account will be able to:See the \u201cAspiring Athenians\u201d and \u201cIbiza Dreamers\u201d audiences in theirAudiencesview.Connect the \u201cIbiza Dreamers\u201d audience to an audience output.",
    "Updating a Dashboard": "Dashboard variants are read-only. They can only be modified by updating the original dashboard within your project. Changes made to the original dashboard will propagate to their variants the next time the variant is refreshed server-side. The timing for the server-side refresh may vary. When refreshing dashboards, Analytics uses an algorithm to balance data \u2018freshness\u2019 and efficient resource utilization. When first applying a new filter to a dashboard or widget, please allow up to 15 minutes for changes to be reflected in the dashboard\u2019s variants. Subsequently, each request to render a dashboard or widget will display the most recently cached result before initiating an update.",
    "Saving to a dashboard": "Once you choose \u201cSave into Dashboard\u201d, a pop-up window will appear. Use the dropdown menu to select the dashboard in which to save your Journeys query. If you want to save your query to a new dashboard, you can create a dashboard from the save window. ",
    "Saving your analysis": "Once you choose \u201cSave Analysis\u201d, a pop-up window appears. Here, use the dropdown menu to select the folder in which to save your Journeys bookmark. ",
    "Saving modified queries": "If you access and modify a saved query or a query within a dashboard, choose theSavebutton in the top right corner of the query builder. This will replace the existing saved analysis or dashboard query. If you want to retain the original analysis, then save the modified query as a new analysis or dashboard query (see above). Don\u2019t forget to change the query name in order to differentiate between the two! ",
    "Explore users from a funnel step": "To explore a list of users from a funnel, choose any data point or table cell within your results. Within the dropdown, select Explore Users. If your query contains a series, you may choose to view users from a single data point or from the entire series. From this Point creates a list of users from only one interval, whereas From Entire Series creates a list of users from all of the intervals within the date range.  To explore a list of users from a single point, choose any data point or table cell within your results. Within the dropdown, select Explore Users. ",
    "Explore all users who entered the funnel": "To access a list of all users who entered the funnel during the date range, click on or in the circle representing Step A in the results field. Within the dropdown, select Download Users in this Point to CSV.  Alternatively, you may select Column A in the table at the bottom of the results field. Be sure to find the row labeled Overall to view all users who entered the funnel.  If your funnel contains a breakout, you may choose to view users from a single breakout or from the entire step. From Breakout creates a list of users from only one breakout whereas From Entire Step creates a list of users from all of the breakouts. ",
    "Explore users from a single path in a multipath funnel": "To access a list of all users who completed a single path within a multipath funnel, select the circle representing the last step in the results field. Then, select the relevant path from the table within the Path Insights dropdown. Within the dropdown, choose Explore Users. The relevant path will be highlighted in the results field upon hover.  It is not possible to select a single path within a multipath funnel from within the table at the bottom of the results field.",
    "Explore users in Cohort": "To analyze a funnel path in Cohort, select the last funnel step in the visualization area. Then, find the path you would like to analyze, and select Analyze as Cohort.  Indicative will then map your user journey using our Cohort tool. Here, you can get more granular in examining retention on an interval basis. Please note that Indicative will automatically set the breakout to be a time generation. For more information about our Cohort tool, seeCohort: Getting Started.",
    "Build a multi-path funnel": "To create a multi-path funnel, you must indicate which step or steps is an optional step. You may haveup to four optional stepsin one multi-path funnel. Neither the first step nor the last step may be optional. There are two ways to indicate that a step is optional: Select the green pushpin icon that appears to the left of the row in the query builderSelect the gray pushpin icon that appears to the left of the event name within the results in the chart area To highlight a particular path, select the webbing between the steps of the path that you wish to view. Depending on the number of optional steps, you may need to select multiple webbings to highlight the path.  You may also view a list of all path combinations by clicking on the dropdown in the top right corner of the chart area. The conversion rate for each path is displayed next to the description of the path. Select a path to highlight it within your results.  The path list is also viewable in the Path Insights dropdown that appears when you choose a funnel step circle. In addition to the conversion rate, the table also displays: Total Dropoff and Avg Step Time. ",
    "Path exclusivity": "The Path Exclusivity setting is enabled if a funnel contains an optional step. It is located in the Settings dropdown within the menu bar beneath the query builder.  There are two options for path exclusivity. Exclusive: Users completed the steps in the selected path, and they did not complete steps outside of the selected path. In an exclusive multi-path funnel,users will appear in only one path. Consider the following exclusive funnel: (A) Site Visit, (B) Blog View (optional), (C) Create Profile, and (D) Subscribe. When the A-C-D path is selected, the results exclude users who did Blog View because Blog View is an off-path step. When the A-B-C-D path is selected, only users completed all the steps are included in the results.Inclusive: Users completed the steps in the selected path, and may or may not have completed steps outside of the selected path as well. In an inclusive multi-path funnel,users may appear multiple times, once for each path that they completed. Consider the following inclusive Funnel: (A) Site Visit, (B) Blog View (optional), (C) Create Profile, and (D) Subscribe. When the A-C-D path is selected, the results include all users who did Site Visit, Create Profile and Subscribe. Some of these users may or may not have also done Blog View, and they are included in the result regardless.",
    "Limits": "Similar to our event limit for workspaces, data plans support up to 1,000 data points.Managing plans with more than 400 data points in the UI can become unwieldy. Manage larger plans outside the UI, either via aData Plan Builderor theData Planning API.You can block data only for unplanned violations: events or attributes with names that diverge from the schema defined in a data plan.",
    "Getting Started": "In order to use the BigQuery Export Integration for Analytics, the customer must provide programmatic BigQuery access to the Analytics Platform. The customer is required to grant dataViewer BigQuery access to Analytics. In order to perform the following steps you must have administrative access to the BigQuery Console as well as your BigQuery database.",
    "More About Data Plan Builder and Templates": "Data Plan Builder is a Google Sheet add-on and template that helps you create a data plan: A template with full instructions to specify your events and attributes. You simply clone the spreadsheet and add your data.A one-button process for turning the specifications into a JSON that you load into the Data Plan UI (or use theData Planning API). Just selectmParticle > Generate Data Planfrom the Google Sheet menu, and copy the output onto your clipboard.  Choose from one of the industry-specific templates or the generic template. GenericTravelQSRGamingFinTechRetailMediaHere\u2019s an example of the first part of the JSON file that you create from the generic template with no changes of your own: Once you have the JSON from Data Plan Builder, paste it into the Data Plan import window (as explained in Step 1.3 below), or store the file and upload it using theData Planning API.",
    "Step 1: Create Your Plan": "To create a plan: In the mParticle UI, selectData Master>Plans>Create Plan.Choose how you will import data points, using the instructions in the dialog to pickthe most suitable method.",
    "Step 2: Activate Your Plan": "To start verifying incoming data against your plan, you first need to activate it. To do this, click theActivatebutton on your data plan\u2019s home screen. Then in the Activate modal, use the Status dropdown to select the environment in which you want to activate your data plan (devorprod). (You also have the option to save the plan as a draft to return to later.)  Now that your plan is active, you need to ensure that incoming data is tagged with your plan\u2019s id. Continue to the next step to learn how.",
    "Step 3: Validate Incoming Data with Your Plan": "Before mParticle validates incoming data against the plan, the data must be tagged with a plan ID, an environment, and optionally a plan version. This is the step that requires a small code change, as mentioned inPrerequisites. Plan ID: This is the \u201cslugified\u201d form of your data plan name. You can find it during plan creation, and on the plan listing page.Plan Version (optional): The plan version that the data should conform to. If omitted, mParticle uses the latest active version for your environment.Environment(developmentorproduction): The environment of your data. mParticle uses this value to look for plans that are activated in the given environment. To find your plan ID, navigate to theplan listing page. In the following image,fintech_templateis the plan ID and should be used in the code snippets below:  Include the plan ID and environment in all batches sent to mParticle. For client-side SDKs, you must provide this metadata on initialization of the SDK.For theEvents API, you must include it in every request body.\nIn addition to plan ID, you can optionally add a plan version, which pins your validation to a specific version. If the plan version is omitted, mParticle will choose the latest version active in a given environment. Example Code in Four Languages You can cut and paste the following example code in either JSON, Swift, Kotlin, or JavaScript for your developer to implement: Now that you have tagged incoming data, use Live Stream to debug violations as they occur. ",
    "Step 4: Monitor Your Plan": "Once your plan is validating data, violations reports help monitor your data quality. To view violations, clickUnique Violationsin the header row of your data plan\u2019s home screen. This will display a violation report like the one below: ",
    "Step 5: Update Your Plan": "Your data needs change over time. Data plans can be easily updated to reflect these changes. Smaller changes can be made directly to an existing plan version. Updates to active data plans are live immediately: simply update the plan in the UI and save your changes. For larger changes, we recommend creating a new plan version. Creating a new plan version allows you to track changes over time and to revert back to an older version if necessary. If you\u2019re using aData Plan Builder, make the update in the builder and follow instructions to export a new data plan version into mParticle.  To view the version history of a data plan: Log in to mParticle and navigate toData Master > Plansin the left nav bar.Select a data plan from the list.Hover your cursor over the details icon (\u2026), and clickView Plan History.",
    "Step 6: Block Unplanned Data from Being Forwarded to Downstream Systems": "Once you are confident that your plan reflects the data you want to collect, you can block unplanned data from being forwarded to downstream systems. Learn more about blocking data inthe next section.",
    "Blocking Bad Data": "Using Data Plans, you can block unplanned data from being forwarded to downstream systems. You can think of this feature as an allowlist (sometimes called awhitelist) for the data you want to capture with mParticle: any event, event attribute, or user attribute that is not included in the allowlist can be blocked from further processing. ",
    "Quarantine Connections": "To prevent blocked data from being lost, you can opt for blocked data to be forwarded to an Output with a Quarantine Connection. To illustrate a typical workflow, assume you choose to configure an Amazon S3 bucket as your Quarantine Output:  Anytime a data point is blocked, the Quarantine Connection will forward the original batch and metadata about what was blocked to the configured Amazon S3 bucket. You will then be able to: Examine the blocked data in greater detail.Backfill data that was mistakenly blocked by following ourbackfill guide.",
    "Blocking Data Sent to mParticle Kits": "In most cases, data collected by the mParticle SDK is sent to mParticle and then forwarded on to an integration partner server-to-server. However, in cases where a server-to-server integration cannot support all required functionality for an integration, an embedded kit can be used instead. You can learn which integrations are kits for a given SDK here: WebiOSAndroid By default, the current Block feature supports blocking for server-side integrations. If you would like to enable blocking for mParticle kits, you need to follow additional steps outlined below for each of our most popular SDKs: Web, Android and iOS. Before you can enable the blocking feature, you need to create a data plan and initialize the respective SDK with a data plan ID in your code. Read our\u201cGetting Started\u201d sectionfor detailed guidance. Our SDKs are served by a CDN that caches SDK configuration, including your data plan, for some period of time (the \u201cTTL\u201d). As a result, updates to a data plan can take time before they are reflected in your client code. To avoid caching a plan version while you are iterating on it: Explicitly mention the plan version in your code.Create a new plan version when you make changes.Update the plan version in your code to point to the latest version. The resulting changes in the URL will sidestep previously cached versions. You can now turn on Block settings for the type of data you would like to block by completing the following steps: Open your data plan version in the UI and navigate to the Block tab.Enable \u201cBlock unplanned events\u201d or any other block setting. Events are a good place to start blocking. For Web, you can use the developer console to verify when a kit\u2019s underlying SDK uploads an event to the partner\u2019s API. For iOS and Android, you can typically use verbose console logs or a proxy such as Charles Proxy. Depending on your block settings, you should see unplanned data removed from payloads. For example, if you have not planned \u201cBad Event A\u201d, \u201cBad Event A\u201d will not be forwarded to a specific partner integration. Follow your usual software development process to deploy your code changes to production. Remember to also promote your data plan version to prod through the mParticle UI to start blocking production data that does not match your plan. Plan versions active on production are locked in the UI to prevent accidental updates. The recommended flow for updating a production plan is to clone the latest version and to deploy a new version after testing.",
    "How does the memory quota work?": "To protect shared resources, every mParticle account includes a memory quota for active data plan versions. The byte size of a plan version\u2019s JSON representation is a good estimate of its memory footprint. The typical data plan version size is approximately 50 KB. You can verify your current usage, check the size of a data plan, and if needed, take action to reduce your memory quota usage: To find your quota limit and current usage, navigate toData Master > Data Plans.To download the JSON file for a data plan to check its size, navigate toData Master > Data Plans, click a plan name to open it, and then click theDownload as JSONicon above theUnique Violationsarea.To stay within an account\u2019s memory quota, deactivate plan versions that you aren\u2019t using. Draft plan versions don\u2019t count against your quota, only active plan versions do. Contact your mParticle representative if you need more memory provisioned for your account.",
    "How do I enable validation?": "To enable validation, you need to point your code to adata plan idwithat least one active version. For a version to be considered active, its status has to be set todevordev & prod. You can either pin your code to a specific data plan version or omit the version, in which case mParticle will match the data you send with the latest plan version that is active in a given environment (devorprod). Learn more about how to implement a data plan withGetting Started. ",
    "Which events are supported?": "You can plan for and validate the following events: Custom Events (including events emitted by the Media SDK)Screen EventsCommerce Events The following events are not yet included: Application State Transition EventsSession EventsAttribution EventsUser Attribute Change Events",
    "Which user identifiers are supported?": "You can plan for and validate the following user identifier types: Amp Client IDGoogle Advertising IDAndroid Device IDCustomer IDmParticle Device Application StampEmail AddressFacebook IDFacebook Audience IDFire Advertising IDGoogle IDApple IDFAApple IDFVMicrosoft IDMicrosoft Advertising IDMicrosoft Publisher IDMobile Telephone NumberPhone Number 2Phone Number 3Push TokenRoku Advertising IDRoku Publisher IDTwitter HandleYahoo IDOtherOther 2Other 3Other 4Other 5Other 6Other 7Other 8Other 9Other 10 TheOtheridentifiers allow you to enter up to ten different custom strings against which to validate data.",
    "How do I validate the shape of event schemas?": "Here\u2019s an example schema configuration for a screen event called \u201cSandbox Page View\u201d:  This configuration states the following: Thecustom_attributesobject is required and any additional attributes that are not listed below should be flagged \u2013 the behavior for additional attributes is implied by the validation dropdown for thecustom_attributesobject.An attribute calledanchoris a string, and it\u2019s required.An attribute callednameis a string, and it\u2019s optional. Let\u2019s look at a couple examples to see this schema validation in action. This eventpassesvalidation. This eventfailsvalidation since the requiredanchorattribute is excluded. This eventpassesvalidation: Thenameattribute is excluded but optional. This eventfailsvalidation: Thelabelattribute is unplanned andcustom_attributeshas been configured to disallow additional attributes. You can change this behavior by changing the validation of thecustom_attributesobject toAllow add'l attributes(see below). ",
    "What do valid events look like on the developer side?": "If you\u2019re looking for an example of how to implement events that conform to your data plan, download your data plan and check out ourSnippets Tool. This tool will show you how to implement every data point in your plan for any of our SDKs.",
    "How are attribute types validated?": "Since various mParticle features (Audiences, Calculated Attributes, Forwarding Rules, some integrations) will automatically convert string representations of numbers and booleans to their respective types, data planning does not distinguish between raw numeric or boolean values (e.g.42ortrue) and their string representation (e.g.\"42\"or\"true\").  As long as the value can be converted to a type, it is considered valid.",
    "How can I validate specific event, user and identity attributes?": "You can validate specific attributes differently depending ondetectedtype. Learn more abouthow type validation works here. Number can be validated in two ways: An inclusive numeric range as implemented by the JSON Schema\u2019sminimumandmaximumkeywords. Learn morehere.A fixed list of integers as implemented by the JSON Schema\u2019senumkeyword. Learn morehere.  String can be validated in three ways: A fixed list of allowed strings as implemented by JSON Schema\u2019senumkeyword. Learn morehere. Within an enum value, commas are not allowed.A regex pattern.A list of pre-defined formats defined by theJSON Schema standard, including email, URI, date, time, datetime and others. ",
    "Where in mParticle\u2019s data pipeline are plans enforced?": "Ingestion, plan validation (and blocking), and event forwarding occur in the following sequence:  Use any API client or SDK to send data to the Events API, and tag the data with your plan ID and, optionally, a plan version. For instructions, see Step 1 inGetting Started. If you are using an mParticle kit to forward data to a destination, and you have enabled blocking of bad data, you can configure popular client SDKs to block bad data before it is forwarded to a kit. Learn more about blocking bad data before it is sent to kitshere. Your data then passes through themParticle Rules engine. You can use your Rules to further enrich or fix your data. Data is then validated and, optionally, blocked. You can see dev data being validated in real-time withLive Stream. Data is then sent to the mParticle profile storage system. When you block bad data, it is dropped before being stored on a profile. Learn more about what happens when data is blockedhere. Your data then passes through the rest of the mParticle platform and is sent outbound, including: Outbound Rulesthe mParticle Audience systemall Event and Audience-based integrations",
    "What do the different violations mean?": "During plan enforcement, mParticle will generate violations when actual data does not match expectations. mParticle tracks the following types of violations: The event type and name combination is not expected. The attribute is not expected on a specific event. The user attribute or identity is not expected. This means the attribute is expected, but it has one or more data quality violations such as: Invalid Data Type: The data type of an attribute\u2019s value does not match expectations. Learn more about type validationhere.Invalid Expected Value: The value associated with an attribute does not match expectations. Learn more about attribute validationhere.",
    "What do I need to know before enabling block settings?": "You can\u2019t replay blocked data through the UI. If you have set up aQuarantine Connection, we offer instructions and sample scripts for replaying blocked data in ourbackfill guide.You can\u2019t block the following items:Unplanned identifiersInvalid dataThe eventuser_attribute_changecan\u2019t be blocked as unplanned data Blocked data is dropped from your data stream before it is consumed by other mParticle features, such as: User Profiles, as viewed inUser Activity Viewand accessed through theProfile APIFiltersCatalogAudiences For debugging and reporting purposes, blocked data is shown inLive Streamand theData Plan Report. Unless you create aQuarantine Connection, you won\u2019t be able to recover blocked data.",
    "Does blocking data impact how mParticle counts MTU or events?": "Blocking data does not impact MTU or (ingested) event counts. To prevent blocked data from being lost, you can opt for blocked data to be forwarded to an Output with a Quarantine Connection. To illustrate a typical workflow, assume you choose to configure an Amazon S3 bucket as your Quarantine Output.  Anytime a data point is blocked, the Quarantine Connection will forward the original batch and metadata about what was blocked to the configured Amazon S3 bucket. You will then be able to: Examine the blocked data in greater detail.Backfill data that was mistakenly blocked. Learn more about how to use quarantined data in theBlocked Data Backfill Guide.",
    "Linting": "We\u2019ve developed tools for you to be able to lint your Swift, Kotlin/Java, and JavaScript/TypeScript code. For more information, seeLinting Tools.",
    "mParticle Snippets Tool": "The mParticle Snippets tool helps you to generate example code blocks that log events using the mParticle SDKs in a way that conforms to a specified data plan. For example, if a data plan includes a data point for a custom event with 10 different attributes, you can create the exact code that will log that event with all of its attributes by running the data plan through the Snippets tool. This is helpful when integrating the mParticle SDK into your app if you are unsure which method to call to log a specific event or how to ensure that all of an event\u2019s attributes are captured correctly.  To use the Snippets tool: Copy the raw JSON of your data plan. For an example, you can test the Snippets tool using thedata plancreated for the mParticle sample web app, The Higgs Shop.Navigate to the Snippets tool atmparticle.github.io/data-planning-snippetsPaste the data plan JSON in the left column.Use the language dropdown menu to select the appropriate language for the SDK you are using. For the sample Higgs Shop web app data plan, select Web SDK.The right column will automatically populate with example code blocks for each data point in the data plan. The Higgs Shop web sample app plan includes 16 different events, and the generated code block for each event includes a comment describing the action that will trigger the event based on the data point\u2019s description plan. For example: For this data point, you must first create the product being added to the cart using themParticle.eCommerce.createProduct()method, passing in the attributesproductName,productId, and19.199for the product\u2019s name, ID, price, and amount. To log the event, your app must call themParticle.eCommerce.logProductAction()method passing in the product object just created and the product action type (AddToCart). Visit themParticle developer documentationto learn more about integrating the SDKs into your application. For more information about the Snippets Tool, visit theGitHub repo.",
    "Use cases": "The Profile isolation strategy is designed to maximize the integrity of each user profile and to prevent anonymous data from being kept together with the data of logged-in users. Under the profile isolation strategy, any time a user creates an account, a new identity record and a new user profile are created. Any anonymous data collected prior to the user signing up is not carried over to the new profile. One of the main reasons to choose the profile isolation strategy is to ensure compliance with consumer protection and privacy laws. For example, if your user agreement includes permission to collect user data, it may be important not to combine user data from before the user signed up, accepting the user agreement, and after. The profile isolation strategy is based on building highly reliably profiles around login IDs, so any time an identity request includes an email address or customer ID, mParticle will return a unique Identity Profile for the user, regardless of which device they are using.",
    "Identity flow": "Example identity priority: Customer IDEmail AddressIDFVRoku ID",
    "Real-time audience page": "VisitAudiences > Real-timeto see a list of your audiences, and a count of how many active audiences you are using. Audiences are separated intoSingle Workspace,Multi Workspace, andShared With Metabs. The ** tab shows metrics for each real-time audience, including: Size: count of MPIDs in this audienceAdds (last 24 hours): number of additions to the audienceDrops (last 24 hours): number of drops to the audienceVolatility (last 24 hours): change in the audience calculated as: (adds + drops) / sizeConnected Outputs: count of connected outputsTagsLast UpdatedCreated ByAccess: whether the audience is private, view only, or usableStatus: whether or not the workspace is activeActions you can take on the workspace",
    "Workspaces, inputs, and audience criteria": "Every audience is populated from data sources that you specify when you create the audience: The workspaces that you select become the data sources for user-based criteria types.The inputs you select become the data sources for event-based criteria types. Because user and device identities are scoped at the workspace level, not the input level, they are available to all audiences in a workspace, for all inputs defined in the workspace, regardless of which input is selected for an audience. For example, if a profile is seen in a workspace in the audience scope, mParticle extracts all identities from the profile and uses them to evaluate against the audience criteria. Thus, if an audience is scoped to input A in a workspace, and a profile is seen from input B in the same workspace, user and device identities will be available for evaluation in an audience with input A, as long as A and B are both inputs defined in the same workspace.",
    "Create a real-time audience": "The audience workflow is simple: Define your use case.Create an audience.Specify the audience criteria.Select an output and configure it.",
    "Define use case": "Before you create audiences, define your segmentation and engagement strategies: What user audiences are important and why?How will you engage and/or monetize each user audience?How will you evaluate the effectiveness of your strategy? These decisions drive your implementation. Example: Audience Suppression The following video shows how to create an audience that excludes users from a particular campaign or from all campaigns: ",
    "Create an audience": "Before you create your first audience, the following video may help you understand the overall process:  To create an audience: SelectAudiencesfrom the main navigation, and then selectSingle Workspace, orMulti Workspaceif your input sources are in multiple workspaces, and clickNew Audience.Enter theAudience Name. You also have the option to provide anExternal Name. If provided, the external name is forwarded to Audience connections.If noExternal Nameis entered, theExternal Namewill be the same as theAudience Name.If you have enabledUnlimited Lookback, the date range is not displayed.UnderInputs, check all the Platforms and Feeds whose data you want to use to define the audience.Click theCreatebutton. The screen refreshes with the new Audience added to the list of audiences and theAudience Detailsscreen shown. If you are ready to define the audience, continue in the next section. Otherwise, clickSave as Draft. This screen shows a single Workspace Audience. Clicking theMultiple Workspace Audienceselection from the main navigation shows a dialog asking if you would like to switch to theMultiple Audience Workspacescreen.",
    "Define audience criteria": "After you create an audience, you can specify criteria to further define it. The scope of data that is evaluated by your audience criteria is dependent upon: The configurations you have selectedThe amount of data the mParticle platform has available for the configurationsThe data storage limit of your current subscription planThe tier setting for events. If set to any tier exceptPersonalize, an event can\u2019t be used as criteria in a real-time audience and won\u2019t be evaluated. After being set to any other tier, the event is grayed out in real-time audience selection drop-downs. The full audience definition is available once it is created. This means, for example, that you can create an audience, and before it is finished calculating, create a second audience that excludes members of the first audience. The first audience definition is available in full to the second audience definition, even if calculations are not complete. To add criteria to the audience definition:  You can add one or more criteria and logic either between two different criteria (and, or) or you can exclude users from an audience with criteria (exclude).After you define a criteria with the real-time audience builder a number displays that represents the estimated audience size:This audience estimator only supports real-time audiences.The estimate is based on a sample of user data within your organization.Audience criteria are scoped per workspace.When the calculation is complete, you can see the estimated size for an individual criteria next to theAppicon, and the estimated size of the whole audience in the Audience Details section. If there aren\u2019t enough users in the sample data to estimate audience size, you\u2019ll see a~without a number as illustrated in the example above. To add criteria to your audience definition: If you don\u2019t already have your audience displayed, navigate toAudiences > Real-timeand click on your audience to open it. Make sure you are in theBuildtab.ClickAdd Criteria.Select the type of data to be used in the audience definition. If your audience is built from both platform and feed data you will need to specify where the data should be drawn from.Specify additional qualifying parameters (i.e. attribute value, recency, frequency, platform version, build version, etc.) and clickDone.Optionally add additional criteria to the audience by addingand/or/exclusivecriteria to the definition.Click one of the following buttons once you have completed the definition of your audience:Save as Draft- to keep the draft of your Audience to work on laterActivateto begin populating the audience with users and make it available for connecting to outputs The audience builder allows you to build criteria based on two sources of data: Events: Criteria that checks for specific events and their properties. These criteria are subject to the audience event retention policy of your account. Within thenew criteriaoption in the audience builder, the following options create event-based  criteria: Events: access custom eventsEcommerce: access ecommerce eventsCrashes: access app crashesInstalls: access install eventsUninstalls: access uninstall eventsSessions: access session eventsUpgrades: access upgrade eventsScreen views: access screen view events As you define your audience criteria, a list of suggested matching values will appear based on what you\u2019ve entered. This feature works both when building new audiences and fine-tuning existing ones, helping you save time, reduce manual effort, and improves accuracy. To use this feature, you must have one of the following standard Roles:User,Admin,Audiences-only,Support, orAdmin+Compliance. Alternatively, you can create a Custom Role with any of the following tasks:audiences:draft,audiences:edit,catalog, oraudiences. User profiles: Criteria that checks your active user profiles. These criteria are subject to the user profile retention policies of your account. Within thenew criteriaoption in the audience builder, the following options create profile-based criteria: Users: access user profile information such as user attributes, calculated attributes, current audience memberships, consent state, location, etc.Attribution: access user install and uninstall information to build criteria based on the attributedcampaignandpublisher. As mentioned above, you can build audience criteria based on user attributes from the user profiles. These attributes can be of any data type including: numbers, strings, dates, lists, booleans, etc. All user profile data is scoped and maintained within a single workspace; In multiworkspace audiences, you can select which workspace to use by pressing the number in the top right of the criteria in the audience builder. To build an audience criteria based on a user\u2019s profile information, press theadd criteriabutton and selectUsersto view options for user based criteria: Audience membership: Checking a user\u2019s membership in other audiences. Only audiences which do not contain nested definitions can be selected. When using a standard audience membership criteria, the population starts with the real-time audience and refines from there. This criteria is not affected by standard audience expiration.Calculated attributes: Check a users calculated attribute valueConsent: Check a users CCPA or GDPR consent stateDevice, OS, Carrier: Check a users device type, carrier and operating systemFirst seen: Check the date the user was first seenLocation: Check user locationUser and device identities: Check the format and presence of user and device identitiesUser attributes: Check user attributesUser attribute lists: Check user attribute lists  When building audiences based on string attributes, several matching rules can be applied. All matches are case insensitive. Contains / Does Not Contain- Will match substrings. For example, \u201cblue\u201d, will match \u201cblue\u201d or \u201cblue shirt\u201d.Exact Match / Does Not Match- Entire string must match, no substrings. For example, \u201cblue\u201d, will match \u201cblue\u201d, but not \u201cblue shirt\u201d.Pattern- Wildcard style matching.*represents any number of characters,?represents any single character. For example, \u201cbl?e\u201d or \u201cb*e\u201d would both match \u201cblue\u201d.Includes / Does Not Include- The specified list includes an attribute you will specify. If you select this operator, an additional field appears for the attribute value. The full attribute value must be specified. For example, if the list is movies and you specify \u201cChicago,\u201d the movies \u201cChicago\u201d (2002, 1927) are returned, but not \u201cChicago Cubs.\u201dPartial Match- The inverse of Includes/Does Not Include, the specified list includes an attribute you will specify. If you select this operator, an additional field appears for the attribute value. For example, if the list is movies and you specify \u201cChicago,\u201d all movies with Chicago in the title are returned. There are two ways to build time-based criteria for audiences: by recency, and by date. Recency criteria define a period in time in relation to \u2018now\u2019, when the audience is actually being calculated, for examplewithin the last 7 days. Date criteria are based on fixed calendar dates which do not move in relation to when the audience is calculated. For example,after 09/12/18. Keep in mind that audiences defined using fixed calendar dates will have a shorter useful lifespan, as the audience builder only uses data from within a set range (last 30 days for most customers). Recency-based criteria select events occurring between two moments in time, relative to \u2018now\u2019. A \u2018day\u2019 represents 24 hours, and not a calendar day. For example, consider the following criteria:  If this audience is calculated at 1:00pm on September 9th 2018, then the earliest qualifying event would occur at 1:00pm on September 3rd, and the latest qualifying event would occur at 1:00pm on September 5th. Date-based criteria are concerned with calendar dates in UTC time and are not defined in relation to when the audience is calculated. Before date criteria is NOT INCLUSIVE of the given date. For example,Before September 9th 2018means that the latest qualifying event would occur at 11:59pm on September 8th 2018 UTC.After date criteria is INCLUSIVE of the given date.  For example,After September 9th 2018means that the earliest qualifying event would occur at 12:00am on September 9th 2018 UTC.On Date criteria cover from 12:00am to 11:59pm UTC on a given calendar day.Between Dates criteria are inclusive of the given dates. For example,Between September 7th 2018 and September 9th 2018means that the earliest qualifying event occurs at 12:00am UTC on September 7th, and the latest qualifying event occurs at 11:59pm UTC on September 9th. Attribution criteria can be used to segment users who have installed your app from a specific campaign and publisher or users who have purchased or re-engaged based on an engagement campaign. There are two ways to add attribution criteria: Profile criteria: SelectAttributionand then eitherInstallorUninstallto build criteria based on thepublisherandcampaignfields from an installattribution eventor uninstall attribution events. Like other profile based criteria, this is subject to profile retention limits.Event criteria: SelectEvents,Attributionand the event name to build criteria based on attribution events such as install, engagement or re-engagement events. Use the event name ofattributionto target install attribution events. This allows you to select any information included with the event ascustom_attributes.  Like other event criteria, this is subject to audience event retention limits. Identity criteria allow you to segment users based on their stored identities to test existence of a given identity or write logic against the identity as a string. This criteria will still scope the audience based on the workspaces included; It will not automatically include all users in theIdentity Scope. For example, if the identity scope is set at the account level and the account has 3 workspaces, an audience created in one workspace will only include users with activity in that workspace (and not the other two). Segment users by their location these two options available underUsers,Location: Equals: Segment users that are in a specific city, state, zip or DMA, using geolocation of the usersIP address.Within: Segment users that are within a set distance to any global city, usinglatitude & longitude coordinates. When using our Ecommerce events, you can easily target users that have added products to their cart, but not completed a purchase by usingcart abandonmentcriteria: Cart abandonment:New criteria->Ecommerce->Shopping - Cart Level->Cart Abandonment From here you can define how long to wait without seeing aproduct actionto include them in this audience.  UseExists / Not Existsto check for the presence of an attribute. For example, User Attribute Gender EXISTS evaluates as true for Gender = \u201cFemale\u201d and also evaluates true for Gender ={undefined}.",
    "Set up an audience output": "The next step is to connect the audience to an output service that can use the data.  See ourIntegrationsdirectory for a full list of output options. To add an audience output: Find the integration you want in theDirectory. You can filter the Directory to show only partners with an Audience Configuration.Click the card for your chosen partner.Click+ Add {partner} to Setupand, from the popup dialog, selectOutput Audience.Complete theConfiguration Settingsdialog. Each partner will require slightly different information. Some require an API Key/Secret/Token, others require you to log in from mParticle using Oauth. See theIntegrations Centerfor details for your integration. Give the configuration a name and clickSave.You can update your configurations at any time by navigating toSetup > Outputs, and selectingAudience Configurations.",
    "Connect an audience": "Once you have set up your output configuration, you can connect the Audience you have defined in mParticle. From theAudiencespage, select theConnecttab and clickConnect Output.  Select an output and complete theConnections Settingsdialog. This will be different for every integration. See theIntegrations Centerfor details for your integration.Make sure theStatusswitch is set toSendingand clickAdd connection. Any users that fit your audience criteria will begin to be available in the output platform. Some integrations take longer than others for this to happen. See the documentation for your specific integration for details. When mParticle forwards an audience to an output, we are only sending identities. mParticle is capable of collecting many types of identities for both devices and users, but most Audience partners will only accept the limited set of identity types that they actually use. For example, a partner that handles email marketing may only accept email addresses, a push messaging partner may only accept push tokens, and a mobile advertising platform may only accept device advertising identifiers (IDFA for iOS and GAID for Android). When building your audiences in mParticle, you don\u2019t have to worry too much about this. You can simply define your matching criteria, and mParticle will forward to each output as many available identities for each matching user as that partner accepts. You define a set of audience criteria, and mParticle finds 100 matching profiles. All profiles include one Apple Advertising ID (IDFA), but only 65 include one email address. You create connections to two Outputs: Partner A accepts IDFA and GAID identity types. Partner B accepts only the email identity type. It\u2019s not necessary for you to know which profiles have which identity types. mParticle simply forwards the 100 available IDFAs to Partner A, and the 65 available email addresses to Partner B.",
    "User profiles and identities": "mParticle creates audiences by comparing your matching criteria with each user profile. If a profile fits the criteria, each accepted identity included in the profile is forwarded to any connected Outputs. User profiles can contain data \u2014 including identities \u2014 collected from multiple workspaces. Even if your matching criteria only concerns data from a single workspace, once a matching user profile is found,allaccepted identities are forwarded to the output, even if the identities were collected in a different workspace. You have created 2 workspaces in your account to track activity for two related apps, App A and App B. User John Smith signs up for both apps, using the email addressjohn.smith@example.com. However, he uses his iPad for App A and his iPhone for App B. This means that there are two different IDFA identities associated with John Smith\u2019s profile. (note: read ourIDSyncdocumentation to understand more about how profiles with multiple identities are managed). You create an audience in the App A workspace, and your criteria match John Smith\u2019s user profile. When you connect that audience to an output that accepts IDFAs, mParticle will forward both of John Smith\u2019s IDFAs.",
    "Common settings": "The following video explains some of the common settings you use when creating an audience. ",
    "Audience estimator": "Each audience that you create in a journey provides an estimated audience size immediately, so that you don\u2019t have to wait for the audience calculation to complete. Once an audience has at least one active connection, audiences and all parent audiences in same path begin calculating the real size. When an audience begins calculating it no longer shows the estimated size.  To estimate the audience size quickly, mParticle samples the total number of users. You see the estimated size of the audience with all criteria applied (as shown in the previous image). Estimated audience size per criteria is also displayed on the milestone. Use the audience estimator\u2019s immediate feedback to adjust criteria definitions and parameters if needed: The audience size is much bigger or much smaller than expected.Your team wants to explore different ways to target your customers.You see a big drop-off of users between milestones and want to target an intermediary moment in the journey, to nudge more users toward your conversion goal. In some cases, you may see different symbols instead of an estimated size: The symbol~indicates that the population is too small relative to the overall user base, which prevents a meaningful calculation. For example, imagine a company that has 100 million users. If you create an audience that will have 13,000 members when fully calculated, it\u2019s likely that the random sample won\u2019t encounter enough members to be represented in the estimate. This symbol doesn\u2019t mean your audience will have no members, just that it will have so few members relative to the total number of users that estimation isn\u2019t possible.If an audience can\u2019t be estimated for a technical reason, you\u2019ll see a red triangle with an exclamation point instead of an estimated size. For example, if the input is not configured correctly, you\u2019ll see this warning sign.",
    "Audience A/B testing": "Audience A/B Testing allows you to split an audience into two or more variations and create connections for each variation independently, to help you to compare the performance of different messaging platforms. For example, if you have an audience of low engagement users that you want to reengage with your app, you might devise a test like this: Send 40% of the audience to Messaging Platform ASend 40% of the audience to Messaging Platform BKeep a control group of 20% who are not targeted with any messaging You can then compare the engagement outcomes for each group and apply the most successful strategy to the entire audience.",
    "Create a test": "From the Audience details page, select theA/B Testtab. If no test is set up for this audience, you will see only one \u2018Control\u2019 variation containing 100% of the users in the audience. Begin setting up the test by clickingAdd A/B Test Variation.  Enter the percentage of users you want your variation to contain. You can also provide a custom name for your variation.Create as many variations as you need for your test, up to a maximum of 5. The total of all your variations must always add up to 100%. You will notice that the \u2018Control\u2019 variation adjusts itself to100 - [sum of all created variations]. If you try to assign a percentage to a variation that would cause the total to exceed 100, you will see an error message.  When you are satisfied with your variations, clickSave.",
    "Create a connection": "After defining your variations, you can connect each one, including the control group, to any output by clicking+ Connect Output. There is also an option to connect the full audience to an output by clicking+ Connect Outputon the original audience card prior to the A/B split. In theAudiencessummary screen, audiences with an active A/B test will be marked with a%symbol.  Note that whenever the Audience Name is used in forwarding the audience to downstream partners, variant audiences will be named using the format[Audience External Name] - [Variant Name].",
    "Change an audience definition": "You can edit an audience definition without affecting the audience split, even after connecting to an output. When the audience is updated, the variants will still be balanced as defined when you created the test. You cannot modify the percentages of an A/B test after creating the test.",
    "End a test": "When you are ready to end a test, navigate to theA/B Testtab and clickDelete Test  Deleting a test will delete all variations and any connections you have set up for each variation.",
    "Download an audience": "You can download a calculated audience as a CSV file. This is useful if you want to troubleshoot your audience criteria, or if you want to share your audience data with a partner without an official mParticle Audience integration. Audience downloads take some time to prepare depending on the volume of users in the audience, ranging from a few minutes up to ~6 hours for extremely large audiences. Audience downloads are available on Real-time audiences only. To download a Standard Audience, connect and send it to an infrastructure output, like Amazon S3 via a Kinesis connection, and download it from there.",
    "Initiate a download": "You can initiate an Audience download, either from the mainAudiencespage:  or from theAudience Detailstab of an individual audience page:  If the Audience includes A/B Testing Variants, you can select which variants you want to download.  You can also choose to download an audience sample. Downloading a sample will likely take less time than downloading a full audience (depending on the audience size), and is useful for testing or troubleshooting.  You also need to select the identity types you want. ",
    "Download the file": "The download takes some time to prepare. When your download is ready, you will receive an email with download link. ",
    "Download format": "The download will be a ZIP file which, when extracted, will contain a CSV file for each audience or variant, plus amanifest.jsonfile, with metadata about the csv files. Audience CSV files have a row for each identity in the audience. Remember that a single user profile can have multiple identities and, therefore, multiple rows. The four columns show a unix timestamp for when the audience membership was retrieved, the mParticle ID of the profile, the identity type, and the value: The Manifest file will be in JSON format. See the following example for included fields:",
    "Delete an audience": "An audience can be deleted in the UI in a few ways, described as follows. In the Audience Overview:  In the Audience itself:  An audience can also be deleted with thePlatform APIusing the/audiencesendpoint. Note that if an audience is nested in another audience for exclusion or inclusion criteria, it can\u2019t be deleted.  It must be removed as nesting criteria for all audiences before being deleted.  If attempting to delete an audience nested in other audiences, the following message displays:  Integrations behave differently downstream after an audience is deleted:",
    "Bulk audience connections": "If you have defined a large number of audiences that you want to send to an output, you can establish the connections for many audiences at once, rather than doing them one at a time. Navigate toSetup > Outputsand selectAudience Configurations.SelectConnect Audiencesto the right of the Audience Configuration you want to connect audiences to.  Select the audiences you want to connect and clickNext.  Choose your settings. The same settings will apply to all audiences. ClickConnect.  You will see a status message showing all successful audience connections. If any audiences cannot be connected, error details will be shown.",
    "Audience tags": "As you continue to add audiences, you can use tags to help keep them organized. A tag is simply a label you can use to sort and search for audiences. For example, if you give all of your retargeting audiences a tag named \u2018Retargeting\u2019, you can easily find them all by filtering for the tag. You can add/remove tags for an audience directly from the Audience page, or in the audience settings. If you select more than one tag, the Audience page shows only audiences with both tags. There is no limit to the number of tags you can create, but each tag name is limited to 18 characters or less. If you clone an audience, it\u2019s tags will be cloned, also. ",
    "Audience faults": "If mParticle encounters errors forwarding an audience to an output, it will mark the connection as faulted. Audience Faults are visible from the Audience page, the Audience Connection screen, and the Audience tab onSetup > Outputs. While an Audience is faulted, mParticle will stop trying to forward audiences until the fault is resolved. Click the fault icon to view a detailed error message. If you can\u2019t determine the cause of the fault, the most common causes of faults include: AuthenticationSome integrations use OAuth tokens and require you to sign into your account in the Configuration Settings. These tokens eventually expire. You may need to go toSettings > Outputsand login again.Your username, password, API Key, API Secret, etc, may be incorrect. Check any API credentials.Permissions- Some integrations require the creation of an API User, whose credentials are used to access the partner\u2019s API. If the user whose credentials you provided in the Configuration Settings does not have permission to update audiences, you will see a fault.Rate Limiting- Many mParticle partners have limits on how often their API can be invoked within a given time frame. If this is exceeded, they return a \u2018Too Many Requests\u2019 error. If mParticle receives this error, we perform a number of retries in an \u2018exponential backoff\u2019 pattern - leaving more time before each successive retry. If the retries are exhausted before a \u2018success\u2019 response is received, the audience will be marked as faulted and mParticle will cease forwarding data. If your error message contains the text \u2018Too Many Requests\u2019 or the code \u2018429\u2019, the fault was caused by rate limiting. Contact the partner to clarify your account\u2019s API limits. It may be possible to increase your usage limit. When you believe you have resolved the issue, open the fault notification and clickResumeto resume sending data.",
    "User attribute sharing": "mParticle\u2019s Audience User Attribute Sharing feature allows you to include user attributes along with identities when you connect a supported audience connection. This allows you to use richer data in your activation platform, such as LTV, lead score or propensity to convert.  This feature does not forward or share your user data to any company beyond what you are explicitly configuring as an audience connection.",
    "Set up user attribute sharing": "Set up user attribute sharing in three steps. Create the Audience Connection in the usual way. For affected partners, you will see the following notification:  If you want to forward User Attributes to this partner, make sure you set the Status to Inactive as you create the connection. This will make sure you do not begin forwarding data until you have selected the user attributes to forward. From the connection screen, select the User Attributes you want to include. By default, all attributes are disabled. It may take up to 15 minutes before attributes begin to be forwarded.  Once you have selected the User Attributes you want to forward,Save and Activatethe Audience, open theSettingsand set theStatustoActiveto begin forwarding identities.",
    "Profiles and user attribute forwarding": "mParticle\u2019s user profile stores user attributes across platforms, workspaces and accounts. This means that, if your audience output uses device IDs, and if you are tracking a user across multiple platforms (mobile and web, for example) you may be able to forward user attributes that were not collected on the targeted mobile devices.",
    "Score and percentile values": "When you create a predictive audience, you can choose between two types of results: score, the likelihood of an audience member performing a future eventpercentile, the likelihood of an audience member performing a future event as compared to other users scored by Cortex, mParticle\u2019s machine-learning engine: typically the number of active users in the last 90 days For example, if you wanted to know how likely it is that a user will purchase shoes in the next 7 days, you could see that likelihood displayed as a score or percentile: With a score of 80%, the user has an 80% chance of purchasing shoes.With a percentile value of .9, the user is more likely to purchase than 90% of users.",
    "How it works": "Creating a predictive audience is simple: Create an audience using the same criteria builder that you use for any audience or journey creation.In the criteria builder, select User Prediction and fill in the requested information.When you save the audience, mParticle creates two user attributes for the user prediction, one expressed as a score and the other as a percentile. Choose one.mParticle populates your audience. After a delay of up to 24 hours, your audience is available for use.Connect the output to the forwarding destinations as you do for real-time audiences. For step-by-step instructions, seeUsing Predictive Audiences. Predictions are rerun weekly to regenerate fresh predictions.",
    "More about Cortex": "Cortex is the machine-learning engine available with mParticle\u2019s CDP. To learn more, you can visitthe Cortex documentation.",
    "Date range": "In the date range dropdown, Enterprise customers may select from Analytics\u2019 3 default date ranges: Last 1 Day, Last 7 Days, and Last 30 Days. Pro customers are limited to Last 7 Day Journeys queries. You may also customize your date range. Select the +Add Custom Date Range button, and input the amount of days you want your Journeys query to look back. Please note that Journeys queries are restricted to a 30 day look back.",
    "Enabling BigQuery Export": " Sign into theFirebase Console.Click on theSettingsicon, then clickProject Settings.On the Project Settings page, click theIntegrationstab.On theBigQuerycard, clickLink.",
    "Manage Event Properties": "To access event properties, select any event. You may also select the Details tab while hovering the mouse over an event to access the event properties for each event, and vice versa.",
    "Archive/Unarchive Event Properties": "Select the Archive icon to archive event properties. Once hidden, event properties will not appear in any of the Analytics tools. To unarchive an event property, choose the Unarchive icon on the hover menu of an event property. This will unarchive it, making it re-appear in Analytics tools.",
    "Rename, Define, and Categorize Event Properties": "Under theDisplay Namecolumn, you may rename an event or event property. Events or event properties without a label will default to their original name.Under theDescriptioncolumn, you may add a description to an event. Definitions are useful for new users, and/or when your project contains similarly named events.Under theCategorycolumn, you may create categories and assign events and event properties to them. Creating categories helps keep your Data Panel organized.",
    "Data Types": "Within the properties menu, you may identify a property as a certain type to surface additional capabilities (like summing numbers). By default, properties will be set to Auto, which will automatically assign a data type. The following data types are available: Stringproperties enable the use of \u201ccontains\u201d based on query filters.Numericproperties enable the use of \u201cless than\u201d and \u201cgreater than\u201d based on query filters.Location (US States)properties enable the use of the Map Chart in Segmentation analysis.Geolocation (IP Address)properties are automatically collected to show your customers\u2019 country, subregion and city for integrations after May 15, 2019 using the Analytics SDK. For existing Analytics SDK integrations, go into project settings and turn on IP collection to use these properties. Note: IP Addresses cannot be retroactively collected.Date/Timeproperties enable the use of \u201cbefore\u201d, \u201cafter\u201d, and \u201cbetween\u201d based query filters.  This option will also display the underlying data as a human-friendly date, according to your project settings.",
    "Avoiding Duplicate Display Names": "When event properties are forwarded with names that match existing Display Names, the Display Names can become duplicated. This may lead to confusion in the Analytics and Segmentation interfaces, as multiple event properties with the same Display Name will appear, making it unclear which one to select. To ensure clarity and prevent ambiguity: Assign unique Display Names to event properties when configuring them.If duplication occurs, you can resolve it by editing the Display Names in the Analytics \u2192 Data \u2192 Events interface. While it is technically possible to assign the same Display Name to multiple properties, it is advisable to use unique names to differentiate between them.",
    "Driving Freemium-to-Paid Upsells": "Let\u2019s explore how a gym would use NBA to determine the membership tier offer that is most likely to turn trial customers into paying members. Focus on trial users or existing members on lower tiers who regularly attend the gym (e.g., moderate to high engagement with classes, personal training sessions, or equipment). Avoid targeting users who are already on the Elite membership plan. Use NBA (Next Best Action) to assess different gym membership tiers\u2014Basic, Premium, and Elite\u2014as potential upgrades based on the user\u2019s activity level and interests. Let NBA analyze factors like the user\u2019s workout habits, class attendance, and upcoming promotions (e.g., a new fitness class launch or personal training package) to recommend the best tier, maximizing the likelihood of an upsell.",
    "Driving New Product Sign-ups": "Let\u2019s explore how a retail or financial institution would use NBA to determine the best co-branded financial product offer for frequent shoppers. Focus on customers who frequently shop in-store or online and demonstrate financial engagement, such as frequent use of store loyalty rewards or payment methods. Avoid targeting customers who already have one of the available co-branded products. Use NBA (Next Best Action) to assess different financial products\u2014Savings Account,Credit Card, andLine of Credit\u2014as potential offers based on the user\u2019s purchasing patterns, credit needs, and spending habits. Let NBA analyze factors like transaction history, average spending, and upcoming promotional events (e.g., a seasonal sale with cashback bonuses) to recommend the most suitable financial product. This approach increases the likelihood of new product sign-ups.",
    "Driving User Engagement and Retention": "Let\u2019s explore how a media company would use NBA to determine the best type of content to promote for increased user engagement and retention. Focus on users who have shown moderate to high engagement but may need additional encouragement to stay active on the platform. Avoid targeting users who have already reached high engagement with frequent content consumption across multiple genres. Use NBA (Next Best Action) to assess different content types\u2014Comedy,Horror, andAction\u2014as potential promotions based on the user\u2019s viewing history, preferences, and genre exploration. Let NBA analyze factors like the user\u2019s past viewing patterns, session duration, and upcoming releases (e.g., a new season of a popular show in their preferred genre) to recommend the most engaging content. This approach boosts user activity and long-term retention.",
    "Driving Upsell and Cross-Sell": "Let\u2019s explore how a quick-service restaurant (QSR) would use NBA to determine the best product category to recommend for increasing user purchases. Focus on customers who have made recent purchases but have not yet added multiple categories to their orders. Avoid targeting customers who consistently order a full meal that already includes appetizers, mains, and desserts. Use NBA (Next Best Action) to assess different product categories\u2014Appetizer,Main, andDessert\u2014as potential recommendations based on the user\u2019s ordering patterns and preferences. Let NBA analyze factors like the user\u2019s past order history, favorite items, and upcoming promotions (e.g., a limited-time combo deal or seasonal menu item) to recommend the most appealing product category. This strategy encourages larger orders and higher purchase value.",
    "Save a Cohort to a Dashboard": "If you select \u201cSave into Dashboard,\u201d a pop-up window displays a preview of your cohort query. Here, use the dropdown menu to select the dashboard in which to save your cohort query. If you want to save your query to a new dashboard, you can create a dashboard from the save window. ",
    "Saving Your Analysis": "When you select Save Analysis, a pop-up window will appear. Here, use the dropdown menu to select the folder in which to save your Segmentation query.",
    "Saving Modified Queries": "If you access and modify a saved query or a query from a dashboard, select the Save button in the top right corner of the query builder. This will replace the existing query. If you want to retain the original analysis, save the modified query as a new analysis or add to a dashboard (see above). Don\u2019t forget to change the query name in order to differentiate between the two!",
    "Examples": "Use audiences to create sophisticated segments powered by both rules and machine learning.",
    "Inputs and Outputs": "One of the key functions of mParticle is to receive your data from wherever it originates, and send it wherever it needs to go. The sources of your data are inputs and the service or app where it is forwarded are outputs. A connection is a combination of an input and output. Inputs include:Apps or services built on any platform we support, such as iOS, Android, or Web. You can view the full list inSETUP > Inputsin the PLATFORMS tab.Data feeds of any other data you want to send into mParticle. This could be data you have collected yourself or from a feed partner. Once configured, feed inputs are listed inSETUP > Inputson the FEEDS tab.Outputs may be configured for events, audiences, cookie syncs, or data subject requests depending on what the output supports. You can see the list of configured outputs inSETUP > OutputsorSETUP > Data Warehouses. Outputs include:Analytics partners such as IndicativeAdvertising partners such as FacebookIn-app messaging partners such as BrazeData Warehouse partners, such as Amazon Redshift, Google BigQuery, or Snowflake To get started with mParticle, you need some data, which means you need to create at least one input.",
    "Create Access Credentials": "The first thing you need to do is to to create a set of access credentials that will allow a client-side SDK or a server-side application to forward data to this workspace. Login to your mParticle account. If you\u2019re just getting started, your first workspace is created for you. The first screen you see is an overview of activity in the workspace. Since you haven\u2019t yet sent any data, there\u2019s nothing to report, so far.Navigate toSetup > Inputsin the left column. Here you can see each platform type accepted by mParticle. Different platforms are associated with different sets of metadata, such as device identifiers, and most outputs only accept data from a limited set of platforms, so it is important to select the right platform. To capture data from your native Android app, chooseAndroid. Just click the+next to your chosen platform.ClickIssue Keys.Copy and save the generated Key and Secret.",
    "About Access Credentials": "mParticle labels the credentials you create for an integration the key and secret, but they are not exactly like an API key and secret, since you embed these credentials in the app. However, this is not the security risk that exposing API credentials would be: The client-side key and secret can\u2019t read data from the system.You canblock bad datato stop any traffic that doesn\u2019t match the data you expect as defined in your schema. Most anonymous client-server architectures, including Adobe, Braze, Firebase, Google Analytics, and Segment don\u2019t have per-session or per-instance credentials, nor does mParticle.",
    "Install and Initialize an mParticle SDK": "You need a developer to help you install and initialize an SDK. See the Getting Started guides for theiOS,AndroidorJavascriptSDKs to get set up before continuing.",
    "Verify: Look for Incoming Data in the Live Stream": "Navigate toActivity > Live Streamin the left column. The Live Stream lets you inspect all incoming data from your development environments. It\u2019s an easy way to check that you have correctly initialized mParticle in your app. When you first open up the Live Stream, it will be empty, as we haven\u2019t yet started sending data.Start up a development build of your app (get a developer to help you if necessary). The mParticle SDKs automatically collect and forward data about installs and user sessions, so just by opening a development build of your app, you should start to see data in the Live Stream.",
    "Advanced Platform Configuration Settings": "For the iOS, Android, tvOS, and Web platforms, some advanced configuration settings are available. To change these settings, navigate toSetup > Inputsin the left column and select either iOS, Android, tvOS, or Web from the list of platforms. Expand theAdvanced Settingsby clicking the + icon.",
    "Restrict Device ID by Limit Ad Tracking": "iOS, Android, and tvOS (Apple TV) devices allow users to limit the collection of advertising IDs. Advertising IDs are unique identifiers you may use to associate event and user data with a specific device. For both iOS and Android devices, if a user has not provided explicit consent to share their device\u2019s advertising ID, then the value of that ID is set to an all-zero value. By checkingRestrict Device ID by Limit Ad Tracking, mParticle will not collect advertising IDs from users who have enabled the Limit Ad Tracking setting on their device. Remember, mParticle will collect advertising IDs for both iOS and Android devices, regardless of whether or not a user has enabled the Limit Ad Tracking setting on their device. However, the IDs collected from users who have opted out will be all-zero values. Following are descriptions of Apple and Google\u2019s policies for device advertising IDs: After the release of iOS 14.5, Apple introduced the App Tracking Transparency (ATT) framework, which requires app developers to request users\u2019 explicit consent to share their advertising IDs. If a user of your app has not provided this consent, Apple\u2019s advertising ID (IDFA) will be set to all an all-zero value:00000000-0000-0000-0000-000000000000. Read more about Apple advertising identifiersin their documentation. For more information about the ATT framework, visit theiOS 14 Guide. Google allows Android users to opt out from sharing their devices\u2019 advertising IDs. Similar to Apple\u2019s policy, Google will set a user\u2019s advertising ID (GAID or AAID) to an all-zero value if that user has opted out from sharing their ID. Read more about Google\u2019s advertising identifiers intheir documentation.",
    "Collect Integration-Specific Identifiers": "The Web SDK can collect integration-specific identifiers to enrich the user data forwarded to your connected outputs. WhenCollect Integration-Specific Identifiersis checked, these integration-specific identifiers are collected and used to enrich your user data to help optimize the match rate of your audiences in downstream tools. Currently, these identifiers include Facebook\u2019sfbcandfbpfields.",
    "Troubleshoot": "If you don\u2019t see data arriving in the output service, navigate toData Master > Live Streamand select Message Direction Outbound. If you see messages in the outbound Live Stream, but none in the output service:You may just need to wait. For most event outputs, mParticle forwards information in close to real time. However, there are factors which can slow down processing and the amount of time it takes for data to become visible in an output service\u2019s dashboard can be different for each service.Navigate toActivity > System Alertsand see if there are any errors noted for the output you want to troubleshoot. The error type may give you a clue as to what is wrong.If the previous step doesn\u2019t resolve the issue:Check all of your Configuration and Connection settings. Make sure that all settings are correct, especially any access credentials, such as Project or App IDs, API Key & Secret, etc.It is common for a particular output service to require certain identifiers or other data points to be present to allow data to be forwarded. As an example, the Google Ads output requires information about a user\u2019s device, including the Device Advertising ID, in order to construct a User Agent Header. If the Device Advertising ID is not present, no data can be sent. Check thedocsfor the output service and make sure you\u2019re sending all the required information. If there are no outgoing messages in the Live Stream, then mParticle is not attempting to send any data to the output service. Some possible reasons for this include: Not all outputs support every platform or accept every event type. The Directory shows a list of available platforms and supported event types for each output. Make sure the data you are trying to send is supported.mParticle allows you to filter your data for each output. Check theData Filterto make sure you haven\u2019t turned off the data points you\u2019re trying to send.",
    "Next Steps": "Congratulations, you have created a working data input. Now it\u2019s time tostart capturing some data.",
    "Query Notes in Tool": "Query notes can be found to the right of the query builder in any analysis tool, just below theData DictionaryandProperties Explorer:  When you click on the Query Notes icon, a pop-out will appear on the right hand side of your screen.  Click into the notes area to add any text or images that are relevant to your query. If you add any additional query notes, don\u2019t forget to save your query before navigating away. If you do not modify your query notes, the form will be pre-filled with a written summary of your query.",
    "Objective-C": "After downloading the client, drop the Analytics project files into a Group in your app. To start using it, specify your Analytics API key by calling Indicative\u2019s\u00a0launch:\u00a0method in the\u00a0didFinishLaunchingWithOptions:\u00a0method of your app delegate: To record an event, simply call the\u00a0record:\u00a0method, passing in the name of your event. For example: To record an event with properties call the\u00a0record:withProperties:\u00a0method, like so: The\u00a0record:\u00a0and\u00a0record:withProperties:\u00a0methods add an event object to a queue. Every 60 seconds, the events on this queue will be asynchronously sent to our servers via HTTP POST requests. If debug mode is enabled via the debug BOOL in Indicative.m, the status code and body of our response will be outputted to your device\u2019s logs. To specify the user who performs an event, call the\u00a0identifyUser\u00a0method, passing in the user\u2019s unique identifier. If you choose to not identify the user, Analytics will automatically generate an identifier for the user based on their device\u2019s MAC address.",
    "Swift": "Analytics\u2019 iOS SDK is written in Objective-C and is compatible with Swift files. To learn more about importing Objective-C into Swift, please see Apple\u2019s documentationhere. After downloading the client, drop the Indicative project files into a Group in your app. To start using it, specify your Analytics API key by calling Indicative\u2019s\u00a0launch\u00a0method in the\u00a0didFinishLaunchingWithOptions\u00a0method of your app delegate: To record an event, simply call the\u00a0record\u00a0method, passing in the name of your event. For example: To record an event with properties call the\u00a0record withProperties\u00a0method, like so: The\u00a0record\u00a0and\u00a0record withProperties\u00a0methods add an event object to a queue. Every 60 seconds, the events on this queue will be asynchronously sent to our servers via HTTP POST requests. If debug mode is enabled via the debug BOOL in Indicative.m, the status code and body of our response will be outputted to your device\u2019s logs. To specify the user who performs an event, call the\u00a0identifyUser\u00a0method, passing in the user\u2019s unique identifier. If you choose to not identify the user, Analytics will automatically generate an identifier for the user based on their device\u2019s MAC address.",
    "Create a predictive audience": "To create and activate a predictive audience: In mParticle, navigate toAudiences > Real-timeand click New Audience.Enter your information in the New Audience form and clickCreate.In the Build tab, clickAdd Criteriato display a list of items:If you haven\u2019t already created a user prediction, click User Prediction from the list and enter the User Prediction information:If you\u2019ve already created a user prediction, select it from the list inUsers > Choose User Feature > User Attributes.ClickCreate prediction.mParticle displays the audience definition, where you can see that two user predictions have been created, one for score and one for percentile. Choose one.Specify the threshold. For example, if you wanted a likelihood of greater than 90 percent, you\u2019d set the following values:ClickDone.Choose eitherSave as DraftorActivate.Once you chooseActivate, mparticle prompts you to optionally create an A/B test, and thenconnect to an output. After you create a prediction, a pipeline is created in Cortex, and the pipeline begins calculating. Calculation may take up to 24 hours, though typically the delay is about an hour. If there\u2019s not enough data to learn from, the calculation may fail and the prediction won\u2019t exist, resulting in an empty audience. To correct this, choose different criteria or troubleshoot your data for issues. To check on the status of your pipeline, view it in Cortex.",
    "Include and exclude users from predictions": "When setting criteria for a new user prediction, you can specify whether Cortex should generate that prediction for all users or a specific subset. Narrowing predictions to a subset of users can help improve the accuracy of your predictions, and avoid generating predictive attributes for users who are not relevant to a specific use case. To focus on a subset of users, select the optionA subset of usersin theMake predictions forfield.  Once you\u2019ve selected this option, you can build queries with user attributes and behavioral events to select the users for whom Cortex will generate this prediction. ",
    "Why include or exclude users?": "While it may be counterintuitive, using more data to generate a prediction does not mean the prediction will be more accurate. Not all users are relevant to every prediction you want to create, and irrelevant users can make it more difficult for the ML model to identify meaningful patterns and trends. This is why it\u2019s best to consider the business outcome you want to achieve when defining the user segment that will generate a prediction.",
    "Use case examples": "Objective:Non-subscriber to subscriber conversionInclude:Non-subscribersExclude:SubscribersReasoning:Subscribers have already converted, so there is no benefit to generating a subscription likelihood for these users.Objective:Subscription upgrade (tier 1 to tier 2)Include:Tier 1 subscribersExclude:Tier 2 subscribersReasoning:Since Tier 2 subscribers have already upgraded, there is no benefit in predicting their likelihood of upgrading.Objective:Churn preventionInclude:Active usersExclude:Lapsed usersReasoning:Since lapsed users have already churned, they should not be part of the effort to prevent churn. As such, there is no value in generating an attribute predicting their likelihood of churning.Objective:Cross sell: Get purchasers of item A to buy a variant at a higher price pointInclude:Customers who purchased item AExclude:Customers who did not buy item AReasoning:Since customers who have already purchased item A are more likely to purchase its more expensive counterpart, predicting the likelihood of cross sell for these customers only would be more efficient.Objective:Predicting which viewers who have not yet watched a show will watch it in the futureInclude:Viewers who have not watched this showExclude:Viewers who have watched this showReasoning:Since the objective of this campaign is to acquirenewviewers of a show, this predictive attribute should exclude viewers who have already watched the show.",
    "Apply All Filter Where Clauses": "Apply All allows you to quickly update multiple query rows with Filter Where clauses at once. This function is available in theSegmentation,Funnel, andCohorttools.",
    "Notes:": "Notice the query rows below the overlay. The query rows and operators/values that will be affected by these changes will flash based on your selections.The Apply button will also indicate the number of rows that will be affected by this change.This will not affect any hidden query rows. Our example reads: We are making changes to multiple rows containing theCountryproperty and having the \u201cis equal to\u201d operator and any value. We will update these three rows with the \u201ccontains\u201d operator and the valueUS. Step 4: Finally, select the Apply button to complete the changes.",
    "Segmentation": "You may create a user segment inSegmentationfrom any result that contains at least one user. To create a user segment, click on any data point, series, or table cell within your query results, then select \u201cCreate User Segment\u201d.  A user segment createdfrom a pointwill only include users from that particular data point.A user segment createdfrom a serieswill include all users from all of the data points in the selected analysis results.",
    "Funnel": "You may create a user segment inFunnelfrom any result that contains at least one user. To create a user segment for a funnel up to and including a particular step, click on the numerical user count located within the table along the bottom of the chart area, labeled \u201cCount,\u201d then select \u201cCreate User Segment\u201d.  You may also select the number displayed within the circle that represents users who complete the step within the chart area, then click on the relevant path, and then select \u201cCreate User Segment.\u201d  There are two options for creating a user segment when thefunnel query contains a group by: A user segment created from a breakout selection will contain users in that particular breakout.A user segment created from the entire step will contain all users in this step, regardless of breakout. In amulti-path funnel, you must select a single pathway to create a user segment.",
    "Cohort": "You may create a user segment from any result that contains at least one user. A user segment created from a cohort generation will contain users in that particular cohort. This includes all of the users in the table row.  A user segment created from a cohort interval will contain users in the cohort who also completed the target behavior in the interval. This includes only the users in the table cell. ",
    "\u201cCreate a user segment\u201d modal": "Use the Create User Segment Modal to review the criteria that comprise your user segment, create a name, description, and category for the user segment, and select whether the segment never changes (One-time) or is updated daily (Daily).  This User Segment is composed of users who: \u2026 : This section describes the criteria for selecting users to be included in the segment.Segment Name: Give your segment a name. It is recommended that the name be easily understood by any user within your organization.Description: Provide additional context and notes here, for example, to describe how this user segment should be used or to provide business-specific context. The description will appear within the data dictionary, available within each tool.Category: Define the category that your segment belongs to. For example, if the segment is related to marketing, type \u201cMarketing.\u201dOne-time vs. Daily: A user segment with an update cadence ofOne-timeis never updated. It contains only the users that meet a set of fixed criteria. If the update cadence isDaily, the user segment is updated daily to include all users that meet a set of relative criteria. Users may enter or exit a dynamic user segment as it updates over time.",
    "Filter where": "In order to define additional parameters for your user lists, introduce filter where clauses to your query row. You may filter by any event or property value.  For more information on filter where clauses, reference our support articleFilter Where. Group by clauses are unavailable in the Users tool.",
    "Group by": " Group byclauses, also referred to as \u201cbreakouts,\u201d allow you to segment data based on event properties, user properties and now user segments to better understand its context. In the case of user segments, only two values will be expressed when a \u201cBy\u201d clause is selected: [Query row name]: in [segment name]: Only includes users who are also in the selected segment.[Query row name]: not in [segment name]: Only includes users who are not in the selected segment.",
    "Manage user segments": "To manage user segments, click Settings in the navigation bar, then select \u201cUser Segments.\u201d Here, you will see all of the user segments created and saved by members of your project.  Segment Name: A descriptive name. Analytics recommends that the name be easily understood by any user within your organization.Description: Additional context and notes. For example, describing how this user segment should be used or providing business-specific context. The description appears in the data dictionary, available within each tool.Category: The category that your segment belongs to.Segment ID: A unique ID for use with theUser Segments Export API.Type: One-time or daily. A one-time user segment is never updated. It contains only the users that meet a set of fixed criteria. A daily user segment is updated daily to include all users that meet a set of relative criteria. Users may enter or exit a daily user segment as it updates over time.Creator: The user who created the user segment.Created (Time Zone): The time zone in which the user segment was created. Hover over a user segment to view two additional options: Open Query: Opens the original query that defines the cohort within the relevant Analytics tool.View Users: Opens a list of users included in the user segment within the Users tool.",
    "Start In Analytics": "In Analytics, click on the gear icon and selectProject Settings.Select theData Sourcestab.SelectNew Data Source.SelectAmazon Redshiftand clickConnect.",
    "Create Security Group": "The next step is to grant access to your Redshift cluster when accessed from Indicative\u2019s IP addresses. This involves creating a new Security Group in your VPC. Go toIAM Managementin the Console and chooseRolesfrom the sidebar.ClickSecurity Groupson the left side.ClickCreate Security Group.Fill outName tagand copy theGroup nameandDescriptionfrom Analytics. Make sure you choose the VPC that also includes your Redshift cluster. Click \u201cYes, Create\u201d to continue. We recommend using the values below.Add Analytics\u2019 IP addresses to the newly created Security Group.a. Select your newly created Security Group.b. Click the Inbound Rules tab.c. Click the Edit button.d. Add the following three rules to allow access to your Redshift cluster to Analytics\u2019 IPs:- Type: Redshift; Source: 54.227.242.108/32\n  - Type: Redshift; Source: 104.196.66.86/32\n  - Type: Redshift; Source: 35.227.102.123/32\n  - Type: Redshift; Source: 35.227.125.106/32",
    "Prepare Redshift Cluster": " Go toRedshift Console.Click the name of your Redshift cluster.Go to Cluster > Modify.  Select the Indicative security group, in addition to your existing security groups.Set \u201cPublicly accessible\u201d to \u201cYes\u201d.  Select an Elastic IP from the list.When complete, your cluster status should look like this:",
    "Understanding mParticle Credits": "mParticle Credits are a universal currency that mParticle customers can use to buy any product on the mParticle platform.",
    "How mParticle Credits are purchased": "Your company makes an upfront contractual credit commitment to unlock access to a discounted rate for mParticle Credits based on your usage.\nThe higher the minimum commitment, the higher the discount tier that is available.\nYou can also choose to purchase additional mParticle Credits at any time.",
    "How mParticle Credit usage is calculated": "At the end of each billing cycle, mParticle measures your usage, and then calculates the number of mParticle Credits consumed, using the pricing in your contract. Remember that events are counted in units of one million (1M).",
    "Example mParticle Credit usage calculation": "Assume your contract specified the following unit prices in mParticle Credits (mPCs) for three billable items: 60 mPCs per million Preserve tier events.74 mPCs per million Personalize tier events.5 mPCs per additional six months of long term data storage. In one billing period you use the following quantities: Ingested five million Preserve tier events (300 mPCs)Ingested two million Personalize tier events (148 mPCs)Two additional six-month units of long term data storage (70 mPCs)\nTo calculate the additional storage cost, multiply the additional storage unit price (5 mPCs) times the number of units (2 additional units) times the number of Preserve and Personalize tier event units (7). The total mPCs consumed for the billing period is 518: 300 + 148 + 70 This guide explains how mParticle calculates the quantities for each billable item. Each quantity is then multiplied by the unit price in your contract to determine the number of mPCs used for the billing period. To ensure you receive the fairest usage calculation, mParticle evaluates all billable items as described in the following sections. Use the following quick reference table to understand at a glance how quantities are calculated.\nEach quantity calculation is explained in detail after the table.",
    "Where can I find my credit usage?": "Customers who have value based pricing contracts with mParticle receive a Credit Usage Report at the end of each billing period directly from their mParticle account manager. Your credit usage report breaks down your total credit usage per event tier and anybillable itemsfrom the past billing period by each organization, account, and workspace on your contract.",
    "Billable items quick reference": "The following quick-reference table lists the billable items for credit usage calculations.",
    "Billable items": "The usage for each billable item listed in the quick-reference table is calculated and the total value is drawn down as mPCs each billing period.",
    "Tiered Events": "Each event is a data point that records an action taken by a user in your app or system. Events may be of a predefined event type ready for use in mParticle, or custom events that require mapping.Events are collected automatically after initializing an mParticle SDK or including the JavaScript snippet for Web, or events are ingested via feeds or an API. Some events matter more to you than others. mParticle provides three tiers to help you map event types to how they are used, and billed, in the platform: To calculate the quantity of events, count the number of event units in each tier. Use the larger of:The number of events originally ingestedThe number of events after All Output rules have been appliedTo calculate the credit usage for events, multiply the number of event units in each tier by their unit price in your contract. That total is the number of mPCs used. A few additional factors affect event usage calculations: To maximize data delivery in all network conditions, an SDK sometimes sends the same event more than once. Even if we send an event more than once, we only count it once.mParticle charges for events blocked byData Planning.mParticle respects the final event type encountered in the pipeline. If the event type of an event changes using rules, we use the most recent one for metering per tier.Sometimes, mParticle can\u2019t determine the tier of an event dropped using \u2018All Output\u2019 rules. For events of unknown tier, mParticle counts them at the lowest tier available in your contract. For example, assume that 100 events are ingested into mParticle (33 Connect tier, 33 Preserve tier, 34 Personalize tier). After the \u2018All Output\u2019 Rules stage we have 70 Personalize tier events. In this scenario, the 70 Personalize tier events and the remaining 30 events are counted at the lowest tier, which is Connect tier for most customers.",
    "Additional long-term retention": "Additionallong-term retention for eventsis a billable item. For example, if you purchased two six-month units for an additional twelve months of long-term storage for Preserve and Personalize tier events, those extra storage units are calculated and included in your credit draw down. To calculate the quantity of events, count the number of event units in the Preserve and Personalize tiers.To calculate the credit usage, multiply the number of additional units by the unit price in your contract for each additional unit, and then multiply that value times the number calculated in step 1. That total is the number of mPCs used.",
    "Additional Real-Time Audience Storage Lookback": "Additional Real-Time Audience Storage Lookback (specified in your contract) is a billable item. For example, if you purchased two 30-day units for an additional 60 days of storage for Preserve and Personalize tier events, those extra units are calculated and included in your credit draw down. To calculate the quantity of events, count the number of event units in the Personalize tier.To calculate the  credit usage, multiply the number of additional units by the unit price in your contract for each additional unit, and then multiply that value times the number calculated in step 1. That total is the number of mPCs used.",
    "Eventless batches": "Batches that are sent server-to-server with only user attributes or user identities but no events are called eventless batches. Each eventless batch is counted as one Personalize tier event. If the event tier is not specified in your contract, we count each Eventless Batch as one event of the lowest tier available in your contract. To calculate the quantity of eventless batches, count the number of eventless batches in each tier. Eventless batches are counted the same as one event, and are calculated using the same one million per unit.To calculate the credit usage, multiply the number of eventless batch units in each tier by their unit price in your contract. That total is the number of mPCs used.",
    "Real-time products": "Active real-time products include Real Time Audiences and Calculated Attributes. Real-time products are metered per workspace on two dimensions: Number of active products in a given billing periodThe largest number of active real-time audiences and calculated attributes at any point in the given billing period. Five real-time audiences are included in the model, so mParticle subtracts five before using this number to calculate credit usage.Number of Personalize tier events in the same billing periodTotal count of Personalize tier event units ingested in mParticle based on the Event metering logic above. To calculate the quantity of real-time units for each workspace, multiply the number of Personalize tier event units by the count of real-time products (Maximum active Realtime Audiences + calculated attributes in the billing period minus five). Then sum the values for all workspaces.To calculate the credit usage: multiply the number from step 1 by the unit price in your contract for Real-time Products to determine the mPCs usage for real-time products.",
    "Real-time invocations": "Some early VBP customers are on a version of the pricing model where real-time products are priced based on the number of real-time invocations. To calculate the quantity of real-time invocations, count the number of invocation of events for each active real-time audience and calculated attribute in a billing period.To calculate the credit usage: multiply the quantity by the unit price in your contract to determine the credit usage for real-time invocations.",
    "mParticle-hosted rule invocations": "mParticle-hosted rules are lambda functions that mParticle maintains and administers on behalf of customers in mParticle\u2019s AWS instance. To calculate the quantity, count the number of times the lambda is invoked in a billing period.To calculate the credit usage: multiply the quantity by the unit price in your contract to determine the credit usage for hosted rule invocations. Note the following: Retries aren\u2019t reported or counted as additional invocations.Customers who host the lambda functions on their own AWS instance aren\u2019t charged under this category.",
    "Backfill and data replays": "Customers sometime have to retrieve events from long-term storage and evaluate them for standard audiences or real-time audiences with Unlimited Lookback enabled. Customers may also request that Support perform a data replay. For example, you may create a new audience, a new calculated attribute or request a historical data replay from mParticle Support. To calculate the quantity of backfill and data replay, count the total number of event units retrieved from storage.To calculate the mParticle Credit usage, multiply the number of event units in each tier by the unit price in your contract. That total is the number of mPCs used.",
    "Cortex Intelligent Attributes": "Customers can use mParticle\u2019s Cortex machine learning engine to create Intelligent Attributes (IAs). IAs are attributes generated by predictions, look-alikes, and classification models. Cortex usage is metered on two dimensions: Number of active intelligent attributes (IA)An active IA is defined as an IA which has been trained or had predictions run on it at least once in the previous billing period.Events forwarded to CortexNumber of event units forwarded to Cortex from mParticle in a given billing period. To calculate the quantity of Cortex units, count the number of events from mParticle used by Cortex and multiply that with the number of Active Intelligent Attributes (IAs) in the billing period.To calculate the mParticle Credit usage, multiply the result in step 1 by the unit price in your contract for IAs.",
    "Indicative": "Behavioral analytics in the mParticle platform. mParticle also includes functionality for customer data platforms (CDP), as well as decision making powered by machine learning with Cortex.",
    "Next Best Action Details Page": "Select the name of your Next Best Action inEnrichment > Predictive Attributesto view its details:  This section displays the total number of users that are included in the target segment you have specified. Predictions are generated for all users in your target segment, though you will often want to target the highest likelihood users. Using theUser likelihood rangedropdowns in this section, you can see how many users fall into each of your offer categories when targeting different percentile ranges to understand how targeting different likelihood percentiles will impact your campaign. Note: Experimenting with percentile values does not actually do anything with them.You will need to remember which percentile you want to target when youactivate your NBAs in a campaign. This section displays how many users within the selected likelihood range were selected for each of your three offers. (For the least likely converters, it displays the total number of users in this category.)",
    "Data in mParticle": "mParticle collects two important kinds of data:",
    "User data": "mParticle also captures data about your user, including their identities, information about the device they are using and any custom attributes you set. As with event data, some user data, such as information about the devices they use, is captured automatically by mParticle\u2019s native SDKs. Two important types of user data must be captured by writing code in your app: User identitiesare unique identifiers for your user, like an email address or customer ID. These are different from the device identities collected automatically by the SDKs, which don\u2019t identify an individual person but a particular cell phone, browser session, or some other device.User identities help mParticle keep track of unique users of your app and allow you to track a user\u2019s activity over time and across different devices. To learn a lot more about user and device identities, read ourIDSyncguide. For now, you just need to know that a user identity is a way of identifying aperson, independently of thedevicethey are currently using.User Attributesare key-value pairs that can store any custom data about your user. The value of a user attribute can be:A stringA numberA listA boolean value (trueorfalse)null- attributes with anullvalue function as \u2018tags\u2019, and can be used to sort your users into categories.",
    "Capture User and Event Data": "To start capturing data you will need to go back to your app code. Inthe previous stepyou should have installed and initialized the mParticle SDK in at least one of your app platforms. This means you\u2019re already set up to capture Session Start and Session End events, as well as basic data about the device. Grab a friendly developer again, if you need one, and try to add some additional user and event data to your implementation. Here are a few things you might try, with links to the appropriate developer docs: Add a Customer ID or Email Address for a user.Android/iOS/WebCreate a custom user attribute that tells you something about a user. For example:status: \"premium\".Android/iOS/WebCreate a page or screen view event that captures the name of a page or screen being viewed.Android/iOS/WebCreate a custom event to track a user action in your app. Include some custom attributes. For example, the mPTravel app sends a custom event when a user views one of its content videos. The event is called \u201cPlay Video\u201d and it has two custom attributes: thecategoryof content, and the traveldestinationthe video promotes. Later on, you\u2019ll see how events like these can be used to target custom messaging.Android/iOS/WebCreate a purchase event - track a purchase using mParticle\u2019s commerce APIs.Android/iOS/Web",
    "Verify: Look for incoming data in the Live Stream": "Once you\u2019ve added code to your app to start collecting some basic data, start up a development build of your app again and trigger some events. Have another look at the Live Stream. You should start to see new event batches, with the individual events you have added to your app. ",
    "Next steps": "To get started with mParticle, set up some data inputs and integrate with at least one output service. SeeWeb End-to-End Tutorialfor instructions on how to set up connections, and see your first data flow from input to output. If you aren\u2019t working with Web apps or services, visitGetting Startedfor more generic instructions. Once you\u2019re up and running, see the rest of this Platform guide for a more advanced look at the mParticle dashboard, or browseIntegrationsto see all available integrations. The following video explains more about how to access integrations in mParticle: ",
    "Widget Layout Flow": "In your Dashboard Settings, you may also toggle your Layout Flow between Free Flow and Auto Flow. If your dashboard is inFree Flowmode, then each analysis can be placed anywhere within the dashboard.If your dashboard is inAuto Flowmode, then each analysis can only be placed where there is space within the dashboard.",
    "Full-Screen Mode": "Often, you\u2019ll want to keep a dashboard open on a shared screen so that everyone is aware of KPIs in real time. To display a dashboard in full screen, navigate to the menu bar above your dashboard and click the \u201cFull Screen\u201d button. Your Dashboard analyses will update at regular intervals, as indicated by yourRefresh Interval settings.",
    "Print Mode": "Print Mode formats the dashboard for viewing on a printed page, which is especially useful for dashboards that are printed on a regular basis, or for those that are to be rendered in aReport. To toggle your dashboard to print mode, click on the \u201cManage\u201d dropdown in the top right, and select \u201cDashboard Settings\u201d. From the sidebar, toggle the Dashboard Layout Mode from Screen to Print. Keep in mind that analyses formatted for print mode may automatically adjust to fit. Once your dashboard is in print mode, you may print your dashboard by clicking \u201cPrint\u201d in the menu bar above your dashboard.",
    "Reports": "For more information on how to send scheduled reports, see ourReportsarticle.",
    "Use a custom manifest": "A custom manifest allows you to use files created by other systems without transformation. In the mParticle UI, when you\u2019re configuring the Custom CSV input, you can provide a JSON manifest to describe how you want to map your CSV data to mParticle\u2019s fields in one of two ways: With a header row: Map your column names to mParticle\u2019s column names.Without a header row:  Map your columns, which must appear in a specific order, to mParticle\u2019s column names. In order to guarantee that the new/changed manifest applies to your CSV files, please ensure a gap of about 5 minutes between the manifest change and uploading CSV files.",
    "With a header row": "If your CSV has a header row, you can map the column names in your header directly to mParticle fields. The manifest must set\"hasHeaderRow\": trueand contain an array of columns objects, each giving a column name, an action (\"keep\"or\"ignore\") and a target mParticle field. The order of entries in the column array doesn\u2019t need to match the order of columns in the CSV file. Example with a header row Assume that you drop the following manifest on the SFTP server: Then assume that you drop a CSV file with the following header row and one row of data: Event Name,Custom Type,Email,Facebook,Home City,Category,Destination,Time,EnvironmentViewed Video,other,h.jekyll.md@example.com,h.jekyll.md,London,Destination Intro,Paris,1466456299032,development The resulting batch will be:",
    "Without a header row": "If your CSV does not have a header row, you can map columns to mParticle fields by the order they appear in the CSV. The manifest must include\"hasHeaderRow\": falseand contain an array of columns objects, each giving an action (\"keep\"or\"ignore\") and a target mParticle field. For this method to work, you must be able to guarantee the same column order in each CSV file you upload. The following is an example of the same custom manifest as the previous example, but without a header row:",
    "Encrypted files": "The Custom CSV Feed can accept files encrypted with PGP, using mParticle\u2019s Public Key. You can use software like GPG Tools for OS X or Windows. Never use web-based tools to encrypt, hash, or encode data. To enable encryption, set the configuration settingExpect Encrypted Filestotrue. It\u2019s most efficient to send multiple files in a zipped, or gzipped format. Encrypt the final ZIP file instead of each individual file. Use the public encryption key corresponding to your pod: US1EU1AU1US2",
    "Sensitive data": "Use the following public encryption key for sending sensitive data.",
    "Processing speed": "For files within recommended size limits, processing speed is consistent with the Events API: about 270 rows per second. To increase the processing speed for your CSV feed, contactmParticle Support.",
    "Processing Behavior": "Files aren\u2019t guaranteed to be processed in sequence; files are not linked to one another. Each file is independent and there\u2019s no way to indicate if two files were split from a master file.You can observe how much data has been processed using Data Master and your outbound connections. There is no notification.Once dropped, files start processing at any time. Deleting a file from the dropped folder is not a guarantee that it won\u2019t be processed. Overwriting files can lead to partial or incomplete imports, or other errors may occur.Rows may be batched together for processing. Thus, there may be fewer processed batches than rows.\nThe only fields not considered unique identifiers for batching are event-specific, such as event name, custom event attributes, and the batch-level timestamp. If two rows have the exact same set of attributes and identifiers otherwise, then they may be batched together for processing.Each file is processed beginning to end. A file is never split or read asynchronously.\nHeader mappings are on a per-configuration basis and are applied to all potential files (if their headers are not valid already). There is no way to associate a mapping with either a filename or filename pattern.Processed files are deleted within 30 days.",
    "How Profiles Are Used": "Use profiles in mParticle to understand users and create personalized experiences: Inspect the profileof any user in the mParticle UI.Enrich a profilewith complete and up-to-date information about the user it represents.Deliver profiles in JSON format with the Profile APIto create personalized experiences based on user attributes or audience memberships.Drive real-time calculations and Customer segmentations.",
    "User Activity View": "Inspect the profile for any user in the mParticle UI via the User Activity View. To find user profiles, you can search by any user identity type you capture, or by the mParticle ID. You can also navigate to the User Activity View for active users in your development environment by inspecting a batch in the Live Stream. For more information, seeUser Activity.",
    "Profile Enrichment": "Profile enrichment allows mParticle to make sure that each of your downstream systems has the most complete and up-to-date information about each of your users. For example, mParticle receives data as batches from native SDKs, our HTTP API and third-party data feeds. Each batch is a JSON object containing an array of events and contextual information about the user, such as identities, user attributes, and device information. mParticle processes the information in each batch and forwards it to downstream systems through event integrations. Before a batch from a particular source is forwarded, mParticle compares it to the matching user profile and adds additional information.  If you collect the same user attribute from multiple sources, for example an iOS app and a web app, mParticle accepts the most recent version. For example, if Bob sets his favorite drink to \u201cBeer\u201d in the web app, and then changes it to \u201cCoke\u201d in the iOS app the next day, Bob\u2019s user profile will use the most recent value for enrichment. Enrichment ensures that all of your downstream tools can receive complete and accurate information about your users. Remember that you can still usedata filtersto prevent downstream systems from receiving user attributes that they don\u2019t need.",
    "Audiences and Calculated Attributes": "Audiences allow you to define segments of users based on rule based criteria of their event behavior and profile data. mParticle builds and maintains these segments of users over time which can then be connected to hundreds of outputs for activation. Audiences and Calculated attributes are built from all your profile data, including calculated attributes, and events that have been collected across all your data sources. For real-time audiences, these are within the audience look back window. Standard audiences utilize your extended Data Retention policy. For more information about data retention, seeData Retention. For more information, seeAudiencesandCalculated Attributes.",
    "Understanding Profile Data": "Two main classes of data provide context about your users and the events they trigger. User dataUser data describes the attributes of individual user profiles. It includes information such as what identities they have, device types and IDs, and a number of custom attributes such as membership status and demographic information. An attribute can reflect either current or previous values, depending on its nature and how often it is updated. User data is stored in profiles.Event dataEvent data describes actions that your users take. They contain information current for the moment at which the event was triggered. For example, the event \u201cSign up\u201d could have an event attribute of \u201cmembership tier\u201d, which denotes the membership status at time of signing up. Event data is stored in events.",
    "Profile Schema": "Each profile describes useful details about the user associated with the profile.  A user profile contains the following elements: mParticle ID - a unique identifier for the user within mParticleTheIDSync APIresolves identities like email, customer ID and device IDs to a single mParticle ID.Each mParticle ID maps to a single user profile.First-seen timestampUser responses to Limit Ad Tracking (iOS) and global opt-outAll known identities for the userAll Audience memberships for the userThe most recently seen IP address associated with the userThe IP address is always updated with the most recent value based on theX-Forwarded-Forheader for client SDK batches or theipfield for batches ingested from server-to-server connections. The IP address on a user profile is only forwarded to connected outputs if theEnrich IP Addresssetting is available and enabled for the connection.For each workspace the user has been seen in:All user attributes captured in the workspace, including Calculated AttributesAny consent information captured for the userFor each device the user has been seen on:Device Application Stamp - a unique identifier for a device within a workspaceDevice information, including device identitiesFirst-seen timestampApple App Transparency Tracking statusInstall attribution information More about the mParticle schema can be foundhere.",
    "How a Profile Is Updated": "Most of the time, mParticle automatically keeps user profiles updated as you capture new data with any of the following methods: ThesetUserAttributeSDK methodSending a \u201cbatch\u201d of user and event data to the HTTP APISending user data from a third-party feed However, sometimes it is necessary to make direct updates to your user profiles in bulk. This happens most often when you\u2019re loading large amounts of data from legacy systems. To directly update a profile, you can make a standard request to our HTTP API, and leave out theeventsnode. For example: User attributes updated in this way will not be immediately updated in all downstream event integrations. Most event integrations in mParticle will not process a batch with no events. However, as long as you are sending some event data to each integration, the enrichment process will make sure that user attributes are updated the next time an event is sent for each user. If it is important for profile updates to be reflected across all your systems immediately, add an event to the batch. For example:",
    "User attributes and input source priority": "For a given user, attributes are stord at the workspace level, not the device level. User attributes are ingested according to the following priority: Calculated attributesAccount-level attributes (a premium feature)Custom feed inputSDK inputCustom CSVPartner data feed (PDF), for examplethe Punchh integrationis a partner data feed. Once ingested from a particular source type, subsequent source types for that same data won\u2019t be ingested. For example, once you set a user attribute key value using the Web SDK, you won\u2019t be able to set that same value from a partner data feed.",
    "Input protections": "You can override the default behavior for input feeds, and specify whether data ingested from a feed creates new profiles or updates existing profiles. Choose from one of three input protection levels: Create & Update - Allows the input to both create new profiles and update existing profiles. All inputs default to this setting.Update Only - Prevents input from creating new profiles, but allows the input to update existing profiles.Read Only - Prevents the input from creating or updating profiles. The default profile protection level for all input feeds isCreate & Update. Profile protection levels are specific to each input configuration, providing more granular control over multiple configurations even if they are contained in a single workspace. When ingesting user data from an input, mParticle searches for an existing profile that corresponds with any of the user identifiers included with the data. The identity strategy configured for your account determines if a new profile is created when an existing profile can\u2019t be found. You can prevent unwanted changes by defining one or more input protections.",
    "Configure the profile protection level for an input": "To set the protection level for an existing input: Log into your mParticle account and navigate toSetup > Inputsin the left nav bar.Click the Feeds tab and expand the list of configurations for your input.Select the configuration you want to change the protection level for.Use the Input Protection Level dropdown menu to change the protection level for the input. When creating a new input feed, you are given the input protection dropdown menu, allowing you to set the protection level for that input from the start.",
    "Create & Update Profiles": "This setting removes all restrictions from an input\u2019s access to a user profile, meaning that new user profiles can be created and modified as data is ingested. For example, if mParticle receives data from the input that it can\u2019t associate with an existing profile and your account uses thedefault identity strategy, then it creates a new profile for the user. If a matching profile was found, the data from the input enriches the found profile.",
    "Only Update Profiles": "This setting ensures that only data from known, existing users in the workspace is ingested. If IDSync can\u2019t find a matching profile (in other words, if the user is unknown) then the data is discarded. Only existing users within the workspace can be updated, and all other unknown users are ignored. For example, if mParticle receives data from the input that it can\u2019t associate with an existing profile and your account uses thedefault identity strategy, mParticle will not create a new profile for the user and the data will not be associated with an MPID. However, if a matching profile was found, then the user data from the input enriches the found profile. If an event batch includes user data with an MPID or user identifiers that don\u2019t resolve to an existing MPID in your mParticle workspace, mParticle will drop the data and it will not be forwarded to any outputs. However, this is still a billable event due to the processing needed when searching for a matching MPID in your workspace. Imagine you have a business with a large customer base, but multiple storefronts or customer touch-points. These various touch-points are housed under separate brands and any one customer engages with many of them. Furthermore, you have chosen to use a centralized data warehouse to help keep your customer data consolidated. However, you want to ingest customer data into mParticle from your warehouse while maintaining a single user profile for each customer. To do this, you must prevent new profiles from being created. This is where theOnly Updateprotection level is useful. By setting your data warehouse input feed toOnly Update, mParticle ensures that as new data for a known customer is ingested, that data is attributed to a single user profile. Without this protection, it would be possible for a separate user profile to be created for each of your brands a customer engages with. This would prevent you from maintaining a holistic perspective of your customers\u2019 history.",
    "Only Read Profiles": "Prevents all write access to profiles and IDSync. Event batches are ingested normally, but existing profiles can\u2019t be modified and new profiles can\u2019t be added to protect user profiles and IDsync records from undesired modifications from the input. For example, if mParticle receives data from the input that it can\u2019t associate with an existing profile and your account uses thedefault identity strategy, then it will not create a new profile for the user and the data will not be associated with an MPID. If a matching profile was found, mParticle will not associate the data with the found profile. If an event batch includes user data with an MPID or user identifiers that don\u2019t resolve to an existing MPID in your mParticle workspace, mParticle ingests and forwards the event batch to other outputs, but it will not create a new profile for the MPID. Keeping your customer data clean and uncontaminated is critical. However, it\u2019s also essential to be able to test new connections from your data warehouses to mParticle. To configure and test a new input feed from a warehouse without allowing any ingested data to modify your existing customer profiles, you can set the initial protection level for the new feed toOnly Read. This allows you to verify that data is ingested properly, without any of that data leading to the modification or creating of new user profiles. After you\u2019re confident in your new input configuration, you can change the protection level to allow data from that input to begin enriching profiles.",
    "Data retention and profiles": "Profiles are available for different periods of time, depending on thedata retention policyfor your account.",
    "Enhance User Profiles with UID2": "The Professional Services team at mParticle provides the following guide for enriching your user profiles with UID2 for more effecive activation through The Trade Desk and others. If you have further questions while implementing the guide, please reach out to your Professional Services contact or account manager. This guide references resources that can be found onGitHub. There you will find source code and supporting files for a serverless application that you can deploy with AWS\u2019s serverless Application Model Command Line Interface (SAM CLI). It includes the following files and folders: Trade Desk Enrichment - Code for the application\u2019s Lambda function.Events - Invocation events that you can use to invoke the function.Trade Desk Enrichment/tests - Unit tests for the application code.template.yaml - A template that defines the application\u2019s AWS resources.",
    "Architecture Diagram": "",
    "Setting Up the Solution in mParticle": "In mParticle, create an audience with the criteria for all users who haven\u2019t had their user profile enriched in the last n days.  Connect the audience to the enrichment module. mParticle will then send emails that need a raw UID value. The enrichment service talks to either a private or public UID instance to retrieve the token.Once the UID value is obtained, it will send a custom event to mParticle via custom feed input, adding the UID value as a partner ID.",
    "Check the mParticle Status Page": "Any known issues affecting mParticle are tracked on ourstatus page(mParticle login required). If you are encountering problems in mParticle, first check this page to see if any service interruptions have been reported. You can also subscribe to receive service updates by email, SMS or RSS feed.",
    "Ongoing Incidents": "The status page displays information about any ongoing incidents above the first table, after the \u201cAbout This Site\u201d section.",
    "Table of Components": "A table displays past and current availability for the following mParticle components: mParticle Dashboard:app.mparticle.comDocumentation Site:docs.mparticle.comData Ingestion: data collection for mobile, JavaScript, pixels, partner feeds, the SFTP ingestion endpoint, and CookieSyncAPIs: mParticle\u2019s ability to receive data at HTTP endpoints:Events APIIdentity APIUser Profile APIPlatform APISDK Configuration API: a private API used to pass settings to client-side SDKsGDPR API: a private API for GDPR (not data subject requests)DSR API: data subject requestsJavaScript Tags CDN: a private content delivery network for JavaScript tagsData forwardingAudienceProfileUser Activity ViewRules You can also viewuptimesfor the last ten years (mParticle login required).",
    "System Metrics": "The status page\u2019s second section displays average latency for mParticle\u2019s key API endpoints, updated every five minutes. On this page, Latency means the average time, in milliseconds, between mParticle receiving a request at an API endpoint and sending a response. You can view the metrics by day, week, or month.",
    "Past Incidents": "The status page\u2019s third section displays any known incidents that caused recent service disruptions. You can also viewreported incidents for the last ten years(mParticle login required).",
    "Troubleshoot Events": "Many configuration settings or other circumstances may cause event data to not be forwarded: Time zones\nIf you compare mParticle event forwarding to numbers in a system using a different timezone, the numbers won\u2019t match. mParticle reporting uses UTC.Missing data points\nYour integration may require application, device, or user data available in order to forward events. Check that all required data points have been configured for forwarding.Data point mapping\nSome integrations require data points to be mapped. Check that the relevant data points are mapped.Rules\nYour connection may have sampling, a minimum app version, data filters, or conditional (event/user attribute, consent, identity) based forwarding rules that are reducing the amount of data being forwarded. You may also have a rule that limits the amount of events being forwarded.Server-to-server\nIf server-to-server data is being sent in with a duplicate, batch, or source request ID, that data won\u2019t be forwarded. Use the following techniques to find the cause of the data discrepancy.",
    "Events Fail to Arrive in mParticle (Input)": "For web apps, is the app able to see a network request to the mParticle Events endpoint? With verbose logging enabled, is the app able to see the events logged and uploaded in the browser\u2019s inspector or developer tools?For iOS or Android apps, with verbose logging enabled, is the app able to see the events logged and uploaded in the Xcode or Android Studio logs? If the answer is yes, then continue diagnosing the problem. If not, review the knowledge base or log a ticket withmParticle Support.",
    "Validate Connection Output Settings and Data Filters": "Confirm that the connection has been configured and is active for the correct workspace, environment (Dev or Prod), and input. SeeTroubleshooting Connectionsfor details.Confirm whether \u201cMinimum app version\u201d setting has been filtered - data from versions older than the filter will not flow.Confirm that data filters for the data points are set to on.Some integrations require that data points be mapped in order to be sent to a downstream service. No mapping results in no data forwarding. The following integrations have this requirement\u2014for other integrations, check thedocumentation for that integration:AdjustGoogle Marketing PlatformKruxSFMC EmailConfirm whether event attribute or attribution-based forwarding rules could be causing data to not be forwarded.",
    "Verify Event Forwarding and System Alerts": "Check theevent forwarding reportand note whether the numbers for the input in question line up to the output in question.Checksystem alertsand look for issues.For kit-based integrations, you may not have included the kit:Look for errors such as \u201cno route available.\u201dSome services require the presence of a kit, even to send data that is sent to mParticle server-side. For example, AppsFlyer, Adjust, Adobe MCID, and Airship all require a kit. Check thedocumentation for the integrationto verify whether a kit is needed.These integrations use the kit to obtain an identifier (an integration attribute). For data that is sent to mParticleserver-side, the user needs to have been in an app version that contains the kit in order to send the data. This means that for server-side data, not having seen the user in a version of the app with the kit would trigger errors in system alerts and cause data not to be sent. Data not being forwarded is expected in this case.Check for invalid credentials in output or connection settings which prevent data from flowing.Check for missing required data. Many integrations require device or user information in order to forward data. System alerts expose the missing data points.For data being sent in via SDK, a certain percentage of users (on average ~15%) won\u2019t have IDFAs or GAIDs. If a user limits ad tracking on their mobile devices, these identifiers are not available. For data being sent server-side, the customer is not sending the missing parameters to mParticle. You must send mParticle the missing parameters to start event forwarding. If you still can\u2019t identify the issue, review the knowledge base or log a ticket withmParticle Support.",
    "Troubleshoot SDKs": "For additional SDK troubleshooting: AndroidiOSWeb0",
    "Building Your Query": "In the Analytics app, events can be filtered by event or user properties, grouped by those properties, or both: In the finished analysis, the query row will indicate which type of property was used.",
    "When to Use Event vs. User Properties": "To better illustrate the differences between event and user properties in Analytics, consider the example of a query set to view the eventBannerImpressionwith the propertyChannel, where the date range is equal to 8/17 - 8/20. Here, filtering by \u201cevents\u2019ChannelequalsSearch\u201d, for example, would essentially pose this question: \u201cGive me the total count of events performed between 8/17 and 8/20 where the propertyChannelis equal toSearch. \u201d The results, filtered by event property, would show one event. Let\u2019s say that the user property forChannelis set to \u2018Last Seen\u2019. If the filter is set to \u201cusers\u2019ChannelequalsSearch, the query instead asks: \u201cGive me the total count of any events performed between 8/17 and 8/20 where the user who performed them had a last-seen value ofSearch. \u201d In this case, the results, when filtered by user property, would show five events: four performed by userID = 1, and one performed by userID = 5. userID = 1 shows up four times because they performed theBannerImpressionevent within the time window, while having a last-seen value ofSearch. Because user properties focus more on the individual users than on the events, they can be useful for cases focusing on the user journey. Questions that might benefit from filtering or grouping by user properties might be: What does a funnel look like for users who became a subscriber, vs. users who did not?What types of banner impressions were seen by users who started by viewing the blog?What is the most effective first or last channel (When grouping by users\u2019 first or last seen channel)?",
    "A/B Testing Best Practices": "When setting up A/B tests in Analytics, the best practice is to set up the data to use event properties, for several reasons: User properties will show whether the users are in an experiment or not, but will not take into account the time that they performed the event, as discussed in the above sections. Event properties will take into account the time of the event, as well as when the user entered or exited the experiment.Typically, users are placed into experiments based on their user ID, which means the variant will remain constant through the experiment. However, in experiments that use cookie ID, there is a risk that aliasing will combine users with different cookie IDs under the same user ID, making event properties a more accurate method for tracking these variants.",
    "Example Use Cases": "Retaining users is crucial for the sustained success of a mobile application. By identifying and re-engaging inactive users, you can boost overall engagement and reduce churn. Goal:Improve user retention by targeting users who have not opened the app in the last 7 days.Segmentation Strategy:Identify users who have been inactive for a week but used the app actively within the previous month.Engagement Strategy:Send personalized push notifications offering a discount or exclusive content to re-engage users. Leveraging predictive analytics allows businesses to anticipate user behavior and proactively engage those most likely to convert, thereby optimizing marketing efforts. Goal:Increase conversion rates by targeting users predicted to make a purchase in the next 7 days.Segmentation Strategy:UtilizePredictive Audiencesto identify users with a high likelihood of purchasing based on machine learning models analyzing past behaviors and interactions.Engagement Strategy:Deliver personalized email campaigns featuring product recommendations or special offers to these high-likelihood purchasers. Identifying subscribers at risk of churning enables proactive engagement strategies to maintain a stable subscriber base. Goal:Minimize churn by identifying at-risk subscribers.Segmentation Strategy:Identify users whose subscription renewal is within the next 30 days and who have decreased engagement (e.g., fewer logins or interactions).Engagement Strategy:Offer these users tailored incentives such as a loyalty bonus or a discounted renewal rate to encourage continued subscription.",
    "Create a New Audience": "Once you have clearly defined your use case, it\u2019s time to begin building your audience.",
    "Navigate to the Audience Creation Page": "From the mParticleOverview Map, selectSegmentation.On theAudienceslanding page, clickCreate New.",
    "Configure Your Audience Group": "Individual audiences are contained within folders calledAudience Groups. The first step in creating a new audience is to set the configurations for this folder:  Enter a name for your folder.Select your Inputs (the platforms and feeds that will supply data to define the audiences within your folder).ClickCreate.",
    "Define Your Audience": "After saving your audience group, you\u2019ll enter theEditor. Here is where you can add, view, edit, and connect audiences. Follow the steps below to create your first audience within this folder: Click theAdd criteriabox to open theAudience Builder Modal.In the audience builder modal, choose the environment (Production,Development, or both) from which the audience will receive data. (Note theaudience environment considerationsbelow.)ClickAdd Criteriato begin adding criteria for your audience.Select the type of data you would like to use for your first criteria (more oncriteria typesbelow).Referring to thedata types and matching rulesbelow, use the criteria editor to further target the users who fit your use case..Once you have added your first criteria, clickDone. After you have added your first criteria, a number displays that represents the estimated audience size:.\nThis estimate is based on a sample of data. As you continue to add criteria, you will see an estimated size for both individual criteria as well as for the whole audience. Continue adding more criteria usingAnd,Or, orExcludelogic to further refine your audience.",
    "Audience Preview": "After defining your audience criteria, you can use thePreviewtab to inspect a sample of users that match your current segmentation. Audience Preview helps validate audience composition before activation, reducing errors and improving targeting accuracy.",
    "How to Use Audience Preview": "Navigate to thePreviewtab in the audience editor.View a list of sample users that meet your audience criteria:Each row in the preview includes:mParticle ID\u2013 A unique identifier for each sampled user.Last Seen\u2013 The most recent interaction timestamp.February 27, 2025",
    "Use NBA Attribute and Percentile to Create an Audience": "In the Audience editor, give your audience a name that reflects the offer being promoted (e.g., \u201cShow Standard Plan\u201d). Then Define your audience criteria:  Add a user attribute(s) selecting users in the segment you want to upsell (e.g., Freemium users).Add another user attribute to select yourNBA attribute. (This will appear with_attributeappended to your NBA name.)Set a value for yourNBA attribute, in this case the product offer (e.g., \u201cPremium Plan\u201d).Narrow the audience further by adding aNBA percentile. (This will appear with_percentileappended to your NBA name.)Set the percentile to your desired range (e.g., >= 0.7, or top 30%). ClickDone, then selectSaveto finalize the audience.",
    "Create Audiences for Additional Product Offers": "Add a new path in your journey by clicking thefork iconabove the first audience\u2019s name.Open the audience editor for the new split.Follow the same steps as above to define the audience with the following exceptions:Use the NBA attribute to identify users for the next potential offer (e.g., \u201cPremium Plan\u201d).Adjust the conversion likelihood range as needed.Repeat this process to create separate audiences for all additional offers.",
    "Activate Your Audiences": "Under each audience, selectConnect Output.Follow the steps to forward the audience to your chosen engagement platform (e.g., an email marketing tool, ad platform).In the engagement platform, use these audiences to power personalized recommendations based on their assigned NBA. By segmenting users based on NBAs and conversion likelihood, you can efficiently target the right users with the most relevant offers, improving personalization and conversion rates.",
    "Create Audience Output": "To add an audience output: Navigate toSetup > Directory, and click the card for your audience partner of choice. (You can filter the Directory to show only partners with an Audience Configuration.)Select an output and complete the Connections Settings dialog. This will be different for every integration. See theIntegrations Centerfor details for your integration.Complete theConfiguration Settingsdialog. Each partner will require slightly different information. Some require an API Key/Secret/Token, others require you to log in from mParticle using Oauth. See theIntegrations Centerfor details for your integration. Give the configuration a name and clickSave. You can view and update your configurations at any time by navigating to Setup > Outputs, and selectingAudiencetab.",
    "Connect Your Audience": "Once you have created at least one audience output, you can connect your audience to begin sending data to that vendor. In the Audience Editor, selectConnect Outputin the card beneath the audience you want to connect.Select the output to which you want to connect and follow instructions in theConnect Outputdialogue. This will be different for every integration. See theIntegrations Centerfor details for your integration.Make sure theStatusswitch is set toActive.ClickAdd connection.",
    "How Audience Forwarding Works": "When mParticle forwards an Audience to an output, only identity data is sent by default. While mParticle can collect various identity types for both devices and users, most audience partners accept only a limited set of identities based on their specific needs. For example, an email marketing partner may only accept email addresses, a push messaging partner may only accept push tokens, and a mobile advertising platform may only accept device advertising identifiers (IDFA for iOS and GAID for Android). When building your Audiences in mParticle, you don\u2019t need to worry about which identities a partner supports. Simply define your Audience criteria, and mParticle will forward all available identities that each partner accepts.",
    "Selective Attribute Forwarding": "When forwarding data to an Audience output, you have the option to selectively forwardUser Attributesbeyond the identities that are required for engagement. Forwarding Calculated Attributes like LTV or Predictive Attributes like propensity scores, for example, enables you to use richer data in your activation platforms. Not all integrations support this \u2014 when you connect your Audience you will see what is supported. In the last step of the process to connect an output, select which account and workspace level attributes you would like to forward to that particular tool: ",
    "Connection Setup": "Log in to your Snowflake account.Enter theAccount Infointo Analytics.Account Infois everything to the left of.gcp.snowflakecomputing.com/\u2026Enter theWarehousename.Enter theDatabasename.Click into Warehouses and copy theSchema.Enter theTablename.ForAuto-Generated Password, we randomly generate a password for you to use. If you would like to create your own password, please replace the autofilled value in that field.",
    "User Modeling (Aliasing)": "After some basic checks, we can define your users within your data. For more information on User Identification (Aliasing), please refer tothis article. If you choose to enable Aliasing:Unauthenticated ID- Input the field used to identify anonymous users.Authenticated ID- Input the field used to identify known users.If you choose to disable Aliasing, pressDisabled:Unauthenticated ID- Enter the field used to identify your users. All users must have a value for this field. If you have a non-null value that represents null UserID values, please click on theShow Advancedbutton. In this field, please enter these non-null values. ::: success\nAfter this step, we will perform additional checks on your data with the user model that you provided. The checks are: User Hotspot (Is there a single UserID that represents over 40% of your records?)Anti-Hotspot (Does your data have too many unique userIDs? A good events table contains multiple events per user)Aliasing\n- Too many unauthenticated IDs for a single authenticated userID\n- Too many authenticated IDs for a single anonymous ID\n:::",
    "Add a Folder": "To add a new folder, click on the Add a New Folder icon located in the bottom left corner of the dashboard menu, below your existing folders. You must provide a name for your new folder. If you are a Pro or Enterprise customer, you must select whether the new folder is public or private. ",
    "Create a Dashboard": "A Default Public dashboard is created automatically for every new Analytics user. There are several ways to create a new dashboard. First, you can create a new dashboard from the Saved menu. In the bottom right corner, select Create a Dashboard. Next, provide a name for your new dashboard, provide an optional description, and select a folder to save the dashboard in.  You may also create a new dashboard from any existing dashboard. To save a new dashboard, click the New icon in the top right corner, then select Dashboard. Note that Growth customers are limited to 25 dashboards containing 20 analyses each. Enterprise customers have unlimited dashboards containing 50 analyses each. ",
    "Manage your Dashboard": "View ourManage Dashboards documentationdocumentation for more information about how to customize your dashboard.",
    "Anonymous and known user profiles": "A user who opens your app and is tracked by mParticle is referred to as the current user. mParticle stores data from the current user\u2019s session in a user profile. IDSync automatically searches for the best profile to use immediately after the current user begins a session. Depending on your identity strategy, if a profile cannot be found using the available user identifiers then mParticle creates a new profile. All user profiles can be either known or anonymous.",
    "Known profiles": "Known profiles have at least one login ID, which is a unique identifier like a customer ID, email address, or phone number. Known user profiles can only be returned in response to an identity request if the request includes at least one matching login ID.",
    "Anonymous profiles": "Anonymous profiles do not have any login IDs. Unless a new user supplies a login ID, they will always be given an anonymous profile.",
    "Transitioning from anonymous to known": "When a user supplies a login ID, IDSync transitions their profile from being anonymous to known. The default behavior for how data collected with the anonymous profile is carried over to the new known profile and whether or not the same MPID is used for the new known profile depends on your identity strategy.",
    "Default IDSync configuration and the profile conversion strategy": "The default IDSync configuration uses the profile conversion strategy. If you have explicitly selected the conversion strategy or your account uses the default configuration, then the appearance of a new login ID adds the login ID to the existing anonymous profile. This means that the new profile is now considered known, but it keeps the same MPID. Any historical data collected with the anonymous profile persists to the known profile.",
    "Profile link strategy": "If you are using the profile link strategy, the appearance of a new login ID results in the creation of a new profile with a new MPID. While the profile link strategy does not carry data from the anonymous to the known profile by default, you can configure your app to execute an alias request which (if successful) will attribute data from the anonymous (or source) profile to the known (destination) profile.",
    "Make an alias request": "The general process for making an alias request is the same regardless of the SDK you are using. To learn how to make an alias request with a specific SDK, refer to the SDK documentation forWeb,Android, andiOS. Remember that the mParticle SDKs always maintain a persistent \u201ccurrent user\u201d, or the user actively engaging with your app. Data from the current user\u2019s session is being associated to a profile, which is either known or anonymous. An alias request includes: MPID of the destination profile: the known profileMPID of the source profile: the anonymous profileStart time: only data collected after this time is aliased to the destination profileEnd time: only data collected up to this time is aliased to the destination profile If you do not specify the start and end time, then all data collected for the source profile will be aliased to the destination profile up to the point the user submits a login ID or your app otherwise submits an alias request. Alias requests are most often made when a user creates or logs into an account, or whenever they provide an identifier configured as a login ID in your account\u2019s IDSync settings. However, you can submit an alias request using the SDKs at any time.",
    "Supported identity strategies": "Aliasing is only available to accounts configured to use either the default identity strategy, the profile link strategy, or the profile conversion strategy.",
    "User profile requirements": "For an alias request to be successful: The source profile must not have been the source profile for a previous alias request with an overlapping start or end date.The source profile must not have been the destination profile for a previous alias request.The destination profile must not have been the source profile for a previous alias request.",
    "1. A user first downloads your app or opens your website": "The initial identification request includes only the device IDs collected automatically by the mParticle SDKAn anonymous user profile with the MPID of 1234 is createdAny events and attributes captured for the user are stored against this profile",
    "2. The user creates an account": "When the user creates an account, a login identity request is sent, including at least one login ID (e.g. an email address)A new known user profile is created with the MPID of 5678The login request returns objects containing information on the previous and current users. At this point, any user attributes or products in the cart (for ecommerce) captured for the anonymous user can be copied to the known user profile",
    "3. An alias request is sent": "The alias request contains four pieces of information:The source (anonymous) user profile MPIDThe destination (known) user profile MPIDA start date (optional) - only events collected after this date are copied to the new profileAn end date (optional) - only events collected before this date are copied to the new profile. The default value is the time the alias request is submitted. If the alias request meets the validation requirements, it will be processed after a 24 hour delay. This delay allows for any late-arriving events from the source profile to be included.",
    "Results of a successful alias request": "A successful request will result in a202 acceptedresponse. Errors are only returned in the cases of failed authorization or exceeded rate limits.",
    "Information from the source profile updates the destination profile": "The first seen date (a value helpful in the mParticle Audience Builder) of the source profile overwrites the first seen date of the destination profile.All events captured for the source profile, between the start date and end date (up to a 90 day period), will be copied to the destination profile.Any install attribution information captured for the source profile will be copied over to the destination profile.",
    "Not all information is automatically copied": "The following information is not copied as a result of an alias request: User identifiers and device IDs are not copied to the destination profile. However, the destination profile should already contain the same device IDs as the source profile, since it should have originated from the same device.User attributes and calculated attributes are not automatically copied as part of an aliasing request.If you are using Data Privacy Controls, consent information is not copied. You need to reobtain consent information from your users after a successful alias request. The mParticle SDKs provide a method for copying user attributes, identities and consent data any time the current user profile changes. For more information see theSDK docsfor iOS, Android, and Web.",
    "Status messages are added to both profiles": "A status message will be added to the source profile indicating that it has been aliased and noting the mParticle ID of the destination profile.A status message will be added to the destination profile, indicating that it has been merged and noting the mParticle ID of the source profile.",
    "Identity API": "The identity API is used by all of mParticle\u2019s SDKs to log users in and out of your app, to search for, and to modify a current user\u2019s identities. It is also available as an HTTP API. The identity API provides four endpoints for identifying users: Identify - called when a session begins with whatever identifying information is availableSearch - called to find current user identities or determine if a specific user existsLogin - called when a known user signs into the app.Logout - called when a known user signs out These four endpoints are called in response to different user actions, but they all perform the same function - resolving a request containing all known identifying information for the current user into a single, unique mParticle User Profile. That profile might be: An existing profile that matches all identifying information in the requestAn existing profile that matched some identifying information in the request, updated to include new information.A new user profile, created when no existing profiles matched the request.",
    "Identity strategies": "Identity strategies determine which user profile to add data to when the current user can be identified, and what to do when the current user cannot be identified. There are 5 identity strategies that have been designed to handle different business and privacy requirements.",
    "Profile conversion": "Theprofile conversion strategyis designed to help build a comprehensive picture of a user\u2019s entire journey through a traditional sales funnel. The main distinguishing feature of this strategy is that when a new login ID is received, IDSync will not create a new profile. Instead, it will simply add the new login ID to the previous profile used to store data collected when the user was anonymous.",
    "Default identity strategy": "Thedefault identitystrategy is a simplified version of the profile conversion strategy. The main difference is that the unique ID and login ID for the default identity strategy are set tocustomer_id, and they cannot be changed.",
    "Profile link": "Theprofile linkstrategy is optimized to track the events that drive users to create an account or make purchases.",
    "Profile isolation": "Theprofile isolationstrategy is built to prevent any anonymous data from being attributed to known users. This strategy is helpful when strict compliance with consumer protection and privacy regulation is required.",
    "Best match": "Thebest matchstrategy is not optimized to help uniquely identify users. It is best suited for businesses that do not have a login flow or that provide their primary services without requiring users to log in.",
    "Identity priorities": "Each identity strategy must define the order of precedence for matching user profiles. When an identity request is received, mParticle looks up matching profiles for each identifier in the order defined by the identity priority until a single profile can be returned. Keep in mind that some identity strategies impose minimum requirements that a request must fulfill in order to return a user even if they match (seeLogin Identities). For now, let\u2019s just look at how the identity priority can affect which profile is returned by a request.",
    "Feed specific user identifiers": "There are some input feeds that always include a specific user identifier that you may or may not want to include in your identity priority list. To ensure that IDSync requests from one-off feeds like this can still be resolved even if you decided to omit their specific identifiers from your identity priority, you can configure an additional identifier for that specific feed without having to modify your overall identity priority. If you ingest data from a feed with an extra identifier configured, mParticle still attempts to resolve any IDSync requests using your normal identity priority list first. Then, if no matching profiles are found, mParticle tries any additional identifiers configured for the feed. If a matching profile is still not found, then mParticle handles the anonymous user according to your identity strategy. To configure additional user identifiers for a feed, contact your mParticle account representative.",
    "Define a conversion window": "To define a conversion window, click on Conversion Settings, located within the menu bar beneath the query builder, then select Limited. Next, you may select whether to apply the conversion window to the entire funnel, or to each step within the funnel. ",
    "Entire funnel": "The options for funnel conversion windows are: 5 Minutes, 1 Hour, 1 Day, 1 Week, 1 Month, Custom, or Entry Date Range. Custom allows you to define a new conversion window, for example two minutes or six months. Entry Date Range will include only users who complete the Funnel within the date range defined in the query\u2019s date range. This appears after Entered Funnel in the date range selector located in the menu bar beneath the query builder.  In the following example, each user in the funnel parameters (A) Site Visit, (B) Blog View, (C) Create Profile, and (D) Subscribe must complete their conversion within one week.",
    "Each step": "Use Each Step Tracked By mode when you would like to track a funnel through an event property, but the event property has different naming conventions for each event. For example, PetBox may want to track the user journey Site Visit - Open App - Purchase Product tracked by Session ID, but the property that captures Session ID is named Cookie ID for the event Purchase Product. You may select 1 tracking property per step in Each Step mode. To switch to Each Step Tracked By mode, select settings from the menu bar, just below your query builder. From there, toggle the tracked by mode line from Simple to Each Step.  Once you have selected Each Step, you must then input an event property to track by in each row. A tracking property must be selected for each step in the user journey in order to run the query. Note that if you already had an event property selected in Simple tracked by mode, this property will populate the tracked by line in each query row in Each Step mode.  By using Simple tracking properties, PetBox is able to ensure that each user journey in this Funnel occurs whilst holding User IDandSession ID constant. However, what if the Purchase Product event uses Cookie ID instead of Session ID, but they represent the same values? In order to account for this, PetBox should use Each Step mode. Then, in the tracked by inline, PetBox should select Session ID for the events Site Visit and Open App, and select Cookie ID for the event Purchase Product. Now, PetBox has now ensured that all events in the user journey are completed by the same user, and in the same session. Thus, this Funnel analysis is counting user-property pairs, and not just users.",
    "Create a journey": "Following the instructions inCreate a journey, begin creating a journey.Once you\u2019ve added a milestone with criteria, click the plus sign beneath the milestone\u2019s audience and selectA/B Test (Random Split).  Click+ Add A/B Test Variationto add a new variation to your A/B test. Enter the percentage of users you want each variation to contain. If the sum of your variations\u2019 percentages is less than 100, the remainder is automatically assigned to the Control group. For example, if you create Variation A and Variation B with 40% in each, you will have a Control group with 20%. You can also provide a custom name for each variation.Create as many variations as you need for your test, up to a maximum of 5. The total of all your variations must always add up to 100%. If you try to assign a percentage to a variation that would cause the total to exceed 100, you will see an error message.  When you are satisfied with your variations, clickSave. After saving your variations, you will see each variation in a new branch within the Journey Builder. ",
    "Enable AI assistance": "On the Calculated Attributes landing page atEnrichment > Calculated Attributes, select theCalculated Attributesbutton in the top right. In theCreate Calculated Attributemodal, check theCreate with AI Assistanceoption. In theDescriptionfield, enter natural language that will be translated into a set of criteria for generating your Calculated Attribute.",
    "Write a description for your Calculated Attribute": "When writing your description, you should be as specific as possible about what you want your CA to do or represent. Use as many of the elements below that pertain to your use case: Calculation type:The type of calculation to perform, like count, total, average, etc. We support 13 calculations organized into four categories. View themhere.Event name / event attribute:The behavioral events and event attributes that your CA will track. (e.g. purchases, page views, video views, etc.)Conditions:Parameters that narrow down the event (e.g. \u201cwhere category is shoes\u201d).Time Period:The window of time in which the event must have taken place. (If no time period is specified, the CA will default to all time.) For most calculations, you\u2019ll need to specify an event attribute in your description. The only exceptions are the following: CountFirst_occurrence_timestampLast_occurrence_timestamp For all other calculation types, an event attribute is required.",
    "Description Examples": "Below are examples ofhigh-qualityandlow-qualitydescriptions for Calculated Attributes. High-quality descriptions include all necessary elements, while low-quality descriptions lack important details or contain errors that make it difficult for AI assistance to generate accurate criteria. Countofpurchaseswherecategory is shoesin thelast 30 daysIncludes all key elements: calculation type (Count), event name (Purchases), condition (Category is shoes), and time period (Last 30 days). Specific enough for the AI to generate accurate criteria.Total money spentonflight bookingsin thelast 30 daysClearly defines the calculation type (Total), event name (Flight bookings), event attribute (Money spent), and time period (Last 30 days). Specific and avoids unnecessary complexity.Totalofflight bookingsin thelast 30 daysProvides the essential elements: calculation type (Total), event name (Flight bookings), and time period (Last 30 days). Effective despite the simplicity. Purchaseswherecategory is shoesin thelast 30 daysMissing a calculation type, making it unclear whether to count or sum purchases.Totalin thelast 30 daysMissing an event name, so the AI cannot determine what data to analyze.Countofpurchaseswhere thecategory is shoes, andtotalofpurchaseswhere thecategory is electronicsCombines different calculation types, which is invalid since a single Calculated Attribute must represent one value.",
    "Journey workflow": "At the start of the journey, you have access to all the users available from all the inputs from all the workspaces in your account. You choose the workspaces and inputs you wish to select audience members from, and then build the journey: In a series of steps called milestones, you break down all the paths taken by your users within a customer lifecycle stage. For example, let\u2019s say that a user signs up for a trial account, interacts with free content, and then saves some content for later. Milestones define the steps that users take or that you want users to take to achieve a set of goals such as sign up, makes a purchase, or become a repeat customer.Each milestone generates an audience that you can forward to an integration to convert users from one step to the next in their journey. Following on the previous example, if you see a drop off between \u201csign up to trial account\u201d and \u201cwatches content\u201d milestones, you can activate the trial account audiences and send it to a CRM partner to send promo emails that help attract and bring content to non-engaged users.Keep defining milestones until you\u2019ve reached the final goal for the customer lifecycle stage. Using the previous example, the \u201cconverted to paid user\u201d milestone is the last milestone in the journey.Verify that the data flow is behaving as you expect using the same tools and techniques you use for an integration.When a journey needs to be changed, you can modify or delete milestones. The following diagram shows a simple journey with two milestones: In this journey, all customers who engage with the app are sent an email, and those who open the email are sent a mobile push notification. Notice the following: The audience is displayed in a box underneath its milestone. You can connect to an output here, or view the status of the output with the down-arrow.If it can be estimated, the audience size displays in the audience box.The path symbol appears above each milestone, and a plus sign appears at the bottom of each milestone. Click the path symbol to split off additional paths and milestones, or click the plus sign to create a new milestone on the same path.You can tell whether an audience has been activated by the green on/off icon in the audience. If grayed out, the audience isn\u2019t active, if green, it is.",
    "Journey path splits": "Each step in the customer journey can be split into additional paths. For example, you could define a set of milestones for customers who buy handbags, shoes, or winter coats. Each milestone becomes the start of a new path. You can also create a milestone for all audience members that haven\u2019t fit any previous milestone criteria. This split is called a remaining user split.",
    "Split behaviors": "The first milestone and audience definition is the left-most milestone, and milestones are added left-to-right.Splits are mutually exclusive and evaluated left to right. Audience members are placed in the first audience where they meet the milestone criteria.If you create a remaining user split, it displays as the last (right-most) milestone.You can create a remaining user split after the first milestone for a path is created.",
    "Use predictions to reduce churn and vary customer communications": "Powered by Cortex, customer-centric teams can predict a user\u2019s likelihood to churn. Using that prediction, teams can deliver a unique set of experiences for users who have a high likelihood to churn.  In this example, a brand engages high-churn risk users on multiple channels to ensure they get the message. In addition, the brand sets up a fail safe for the users who received an email but didn\u2019t open it, engaging them over SMS.",
    "Engage customers in the right channel based on customer consent": "One key piece of user preference is their consent status, what they\u2019ve told the brand about how they want their data to be used for marketing purposes.  In this example, an eCommerce retailer delivers two different kinds of experiences based on a user\u2019s consent to GDPR. If users have consented, the retailer sends a mobile notification as a reminder to convert. For the remaining users, those who have not consented, the retailer triggers an in-app coupon if the user returns after several days.",
    "Trigger post-purchase sequences based on a customer\u2019s lifetime value": "For many retailers, a user\u2019s purchase of a product is just the beginning of the relationship between the customer and the brand. After a purchase, the retailer can suggest to the customer many possible next steps to prolong and deepen that relationship: purchasing more products, leaving a review, referring a friend, posting on social media, and more.  In this example, retailers trigger the next best step based on a user\u2019s lifetime value.  For high LTV customers, brands can assume that they\u2019re fans of the brand, and would be more willing to refer a friend. If the retailer knows a loyal customer\u2019s preferred engagement channel, they can communicates with them there. For the remaining users, those who are not high LTV customers, a retailer can recommend products that pair well with the one that a customer just purchased.",
    "Journeys and billing": "When an audience is actively connected, that audience is activated and consumes a real-time audience credit. Unlike the real-time audience experience, there is no explicit audience status of Draft or Active. The status is now derived from the connection status. Activated audiences count toward your account limit.Parent audiences are calculated, but do not consume additional real-time audience credits. To view the number of audiences available to you, in mParticle go toAudiences > Journeysto display the list of journeys. The number of activated and available audiences is displayed under the New Journey button:",
    "Drive User Engagement Through Intelligent Segmentation": "Let\u2019s say you want to target users who installed your app to increase their engagement. There are two approaches to accomplishing this using Audiences: Rules-based:Taking this strategy, you would use audience criteria to build a customer segment that aligns with your target characteristics, like users who installed your app in the last 72 hours with fewer than three sessions.AI-powered:Using a predictive approach, you could simply define your business goal, then let Predictive Attributes to perform a comprehensive analysis of user data to predict which new users are most likely to take the engagement action you have defined. Once the Predictive Attribute has been generated, you could use it as an audience criteria to select users who are statistically most likely to engage with your app. No matter which approach you take, you can easily forward your audience to push notification and email partners.  Once these audience connections are established, mParticle instantiates a corresponding audiences both partners.  No coding is necessary.",
    "Drive app downloads": "Let\u2019s say you want to find more users like your currently highly engaged users and run an app download campaign in Facebook against that target audience. You start by defining your highly engaged users, using whatever criteria is important to you: lifetime value metrics, session activity, event activity, or any other data points you capture. Once your audience is defined, configure the Facebook integration and corresponding custom audiences in your Facebook account.  From there you can leverage the custom audiences like any other custom audience in Facebook. Because we want to target users that look like our highly engaged users, we will create a Facebook lookalike audience from our highly engaged user audience and run a Facebook app install campaign that targets that lookalike audience.",
    "General": "The organization display name is defined by the Owner of an organization during their initial account creation. To change your organization display name, click General, then type your new name into the text field, then click Save. ",
    "Projects": "The Projects screen within Organization Settings displays a list of existing projects, and provides the option to open or delete a project using the three-dot menu on the right side of the page.",
    "Default Project Access": "With Default Project Access, select which projects users will receive access to. Existing users retain access to their current projects unless otherwise modified. Project access from an invite takes precedence over Default Project Access. ",
    "Teammates": "To edit or modify a teammate\u2019s role, open the Account Settings dropdown- usually indicated by your initials in the top right corner of Analytics- then select \u201cTeammates\u201d. The Teammates table displays a full list of all users within your organization, including their Name, Email Address, Status, and Last Login. To view teammates within a particular project, select the dropdown in the top right, then select your desired project.",
    "Organization Administrator": "Organization administrators have access to all features, all organization projects, organization settings, and project settings within their organization. Only organization administrators may access the organization settings within the Settings menu.",
    "Adding Teammates": " Within the Teammates section, by selecting the New Teammate option, you may invite new teammates, and assign them to projects and a group",
    "Project Permissions": " Within Project Permissions you are able to change a teammate\u2019s group in any of their projects by ticking the checkbox next to the respective project name.",
    "Groups & Feature Permissions": " You can control teammate access permissions for different sections and features of Analytics within the Groups & Feature Permissions section. These groups can then be applied to teammates within the Teammates section withinProject Settings. There are three default groups: Admin, Member, and Read Only. Please note that these default groups cannot be edited or deleted. Default Groups settings (Growth and Enterprise only):There is a no limit to the number of teammates that can be assigned to these default groups Admin:For teammates in leadership rolesFull access to all projects and all features of AnalyticsMay add new teammates to projectsMember:For all general teammates within an OrganizationAccess to assigned projectsMay not access Organization Settings, Project Settings, or TeammatesMay not add new teammatesRead Only:For viewers not associated with an OrganizationView only access to assigned projectsMay not access Organization Settings, Project Settings, or TeammatesMay not use any of the features and tools of AnalyticsMay not add new teammates Growth:Pro users have access to the default three groups. Enterprise:Enterprise owners have access to the default three groups, and access to custom groups. Organization Owners and Organization Admins may create new groups by choosing New Group.",
    "Custom Groups": "  By creating a new group, you may change the access to specific features, settings, and support as shown below:",
    "Calculation credits": "Standard audiences are purchased by buying annual calculation credits. Each calculation credit lets you run a calculation across 365 days of your historical data, regardless of how many audiences are included. You can calculate many standard audiences at once. There are prompts in the product to select the audiences to calculate and confirm how many credits you are spending. Some example calculations and costs: 1 standard audience spanning from 1/1/2018 to 1/1/2020: this costs 2 credits as it scans 2 years of data.3 standard audiences spanning from 1/1/2019 to 12/31/2019: this costs 1 credit as it scans 1 year of data (with many audiences).",
    "Standard audience lifecycle": "Standard audiences have a 4 stage lifecycle: Draft: The audience is being drafted and has not yet been calculated. To calculate it, press \u2018calculate\u2019 and confirm that credits will be spent.Calculating: The audience is being calculated. Progress indications are shown in the UI and the time this takes depends on the date range selected (and thus the data volume scanned).Ready: The audience has been calculated and is ready for use by connecting and sending it downstream.Expired: 30 days after a standard audience is calculated, it is expired and appears in the Expired tab inAudiences > Standard.An expired audience can no longer be connected, but it can be cloned for recalculation.Any real-time audience criteria that checks user membership in a standard audience is not affected by standard audience expiration, as the users membership is saved in the users profile.Audience membership is not removed from downstream partners.",
    "1 - Create a new standard audience": "Standard Audiences are managed separately from Real-time audiences. ChooseAudiences > Standardfrom the main navigation menu, and clickNew Standard Audience. ",
    "2 - Define date range and inputs": "Just as withreal-time audiences, you can define which inputs you want use to calculate the audience. For Standard Audiences you also need to define a date range. You can chooseAll available dataor define any period within the available range. When you\u2019re ready, clickCreate. The start and end dates are inclusive and it uses the UTC timezone. ",
    "3 - Define audience criteria": "Define your audience by clicking the plus sign to create and define one or more criteria. This step is the same as inreal-time audiences. When your definition is ready, clickSave as Draft. Notice that after a moment, aCalculatebutton displays next to the grayed-outSave as Draftbutton. ",
    "4 - Calculate one or more audiences": "At the top of the Standard Audiences page from step 3, clickCalculate. Select any additional DRAFT audiences from the list to add them to the calculation. This modal shows you how many calculation credits will be deducted from your account. When you\u2019re ready clickStart Calculation. ",
    "5 - Set up one or more connections": "At first your audience shows asCalculatingin the list viewStatuscolumn underAudience Details. While you wait for the calculation to complete, you can set up one or more audience connections by clicking the green plus sign in theConencted Outputscolumn. Calculation can take many hours for large amounts of data. You can track progress via a popup in theSizecolumn. The Connections screen functions the same as forreal-time audiences. Add and configure one or more connections. The only difference is that when you save the connection, no data is forwarded until you explicitly send the audience.",
    "6 - Send your calculated audience": "Once your audience is completely calculated, you can see it in theReadytab. Click Send next to an Audience to go back to theConnectionsscreen.  From theConnectionsscreen clickSend. You can also adjust your output connections here as needed.  Select one or more audience Outputs and clickSend.  All members of the audience are forwarded to the output. Calculated audience will remain in theReadytab for30 days, after which they will need to be recalculated and can be found in theArchivetab. Once an audience is in the archive tab, you can clone and recalculate it.  Remember that the audience will not be updated in real time. If you want to update the audience, you must run the calculation again.",
    "Accessing Advanced Settings": "In Analytics, click on the gear icon and selectProject Settings.This will list all of your data sources. Find the Data Warehouse that you would like to edit and selectEdit Details.Click on theAdvancedtab.",
    "Exclude Events": " We typically pull all of the events in your data source, but if there are exclusions that you would like to make, place them here. We pull data with a blacklist approach, meaning everything will be ingested except specific exclusions. In theEvent Name matchesfield, include any event names to exclude when extracting your data. You can place a pattern or if there isn\u2019t one, separate multiple event names with a;.For example, if you want to exclude all events that start with \u2018test\u2019, put \u2018test*\u2019 in this field.ForProperty Value matches, we will exclude events based on property values.For example, if you want to exclude any events where device = \u2018test\u2019, place that in this field.",
    "Exclude Properties By": " Similar to the Exclude Events section, we will pull all properties and make exclusions with a blacklist. If there are any properties to exclude during extract, list them out in this field.For example, if you want to exclude the device_screen_size property, place that in this field.",
    "Context Field Exclusions": "",
    "Include Event Property Prefix": " Note: This option is only available for Snowplow schemas.",
    "User ID Null Values": " Some organizations may use non-null values to represent an undefined User ID. Please enter any values that are used to represent null User IDs in this field. We excludeNULLby default, so you do not need to specify that here. Note: If you use an empty string for null user IDs, put two single quotes and press Enter to mimic the picture above.",
    "Before you start": "Prepare to create a journey: Define what goals you wish to accomplish with the journey. For example, do you want to convert trial users to paid users, or encourage infrequent viewers to engage with their content? The goal you wish to accomplish will affect which inputs you select, and the criteria you\u2019ll define for each audience.Verify that you can create the number of activated audiences you plan to create in your journey. In mParticle, visitAudiences > Real-timeto see how many activated audiences are available:",
    "Step 1. Create a journey": "To create a journey: Log in to your mParticle workspace and go toAudiences > Journeys, and clickNew Journey.In the Create Journey configuration dialog, choose a name for your journey and select inputs from all the workspaces you wish to include.After selecting all the inputs in all the relevant workspaces, clickCreate.The Journey canvas displays your selected journey inputs.Click the plus sign to add a milestone, and then clickMilestone. An empty milestone is created.Click the milestone to add a name, and define the criteria. You are now in the real-time audience builder, and candefine criteria.ClickSave. The canvas displays the milestone you just created and the audience for that milestone.You can connect to an output now or wait until you\u2019ve created all the milestones. SeeConfigure connectionsfor details.You can either define the next step in the existing path by clicking the plus sign, or split the path by clicking the split icon, and create one or more additional milestones:Click the plus sign under theJourney Inputsyou just created to continue the journey, and clickMilestone. The following image shows a second milestone in the same path as the first one.Click the branch symbol above the milestone you just created to split the path into multiple journeys, and clickAdd pathto create a new path and milestone, orRemaining Users splitto create a milestone for all audience members who do not meet criteria for any other milestone at this level. The following image illustrates splitting the path with a remaining user milestone.You can create an A/B test for a milestone\u2019s audience by clicking the plus sign and selectingA/B Test (Random Split)To learn more, seeAudience A/B testing from a journey.You can\u2019t modify the criteria for a remaining user milestone.",
    "Step 2. Configure connections": "For each milestone: In the audience for the milestone, clickConnect Outputto select an output for the audience you just created. For more information about output connections, seeConnections. You can add one or more outputs to each audience. You can leave the audience inactive until you are ready to activate it, which starts sending audience members to the output. To change a connection status after you have created a milestone, click the down-arrow across from the connection name, then click the status badge to open the Connection Settings dialog, and click the button to activate or deactivate a connection. After a connection is activated, all parent audiences in the path are set to \u201cCalculating.\u201d Audience size estimates are updated to the actual audience size. After you\u2019ve added all the milestones and connected and activated all the outputs, you\u2019ve fully defined the journey.",
    "Deactivate an output": "If you wish to stop sending audience updates to one of the connections in a journey, you must either deactivate or delete the connection for that audience. To deactivate an output, click on the output name in the audience box, and then click theActivebadge to display the Connections Settings dialog where you can set the output toInactive.",
    "Delete a journey": "To delete a journey: Go toAudiences > Journeysand click a journey to open it.Click a milestone at the end of a path.ClickEdit milestone.ClickDelete.Delete all the milestones in the journey, then close the journey by clicking on the Journeys label at the top of the page.In the Journeys page, find the journey you wish to delete, and click in theActionscolumn for that journey, and selectDelete.",
    "About SSO": "Single Sign-on (SSO) allows a user to log into Analytics using any third-party identity provider that supports SAML 2.0. Examples include Azure Active Directory, Okta, OneLogin, Auth0, and others. SSO is available for Enterprise customers only. The walkthroughsin the examples sectioncover just a handful of the providers that are supported by the platform. Specific steps and processes may differ, but the overall setup process is similar for most identity providers. If you don\u2019t see your provider\u2019s instructions here and have additional questions, please contact Analytics support for further assistance. Note:Users are required to initiate login from Analytics in order to capture all of the information needed to securely complete authentication. Identity Provider initiated login is not supported at this time.",
    "SSO Settings Within Analytics": "As an Organization Admin, navigate to Organization Settings and thenSingle Sign-Onto view, enable, and edit Enterprise Single Sign-On.  With SSO enabled, users do not have to be added to use Analytics app before logging in \u2013 accounts will automatically be created for them as long as they are provisioned in the identity provider.",
    "Create a new group definition": "If you\u2019re using the classic UI, navigate toData Master > Group Identitiesfrom the left hand nav bar. If you\u2019re using the New Experience, SelectCustomer 360from the Overview page.ClickCreate Group.Use the dropdown menu to select the user attribute that will act as the source for your group ID, and clickNext.  Enter a name for your group definition. This is how you will find your new group in other mParticle features, like the Audience builder.Enter a group description.  ClickCreate Group Identity. After creating your new group identity, you are taken to the Group Identity Details page where you can begin adding group attributes. Any users with the same value for the user attribute you selected as the group identifier are added to an instance of this group. For any given group ID value, a group is only created if the group ID value is at least 5 characters long.",
    "Add attributes to a group": "To add user attributes to your group: ClickSelect Attributes.  Use the two dropdown menus to select the user attribute to add as a group attribute and its type, eitherBoolean_ororLatest.  To add additional attributes, clickAdd Attribute. You can add up to 10 group attributes.Finally, clickSavewhen you are done adding attributes. After users have been added to a group, they will appear in the User Activity View when searching for the group identifier or group attribute.",
    "Delete or modify a group definition": "To modify a group or its attributes: Navigate toData Master/Customer 360 > Group Identities.Select the group definition you want to modify.  To remove a group attribute, click thexdelete icon next to the attribute.To add another group attribute, clickAdd Attributeand follow steps 2 through 4 inAdd attributes to a group  To delete a group definition: Navigate toData Master/Customer 360 > Group Identities.Click the details icon on the right side of the group definition\u2019s row in the Group Identity page, and click \u201cDelete\u201d. ",
    "Configure Your Project Settings": "Owners and Admins have access to Project Settings, however Members and View Only teammates do not. Select the cog wheel icon on the left menu bar to review Project Settings.  Within Project Settings you\u2019ll be able to access General Settings, Teammates, Data Sources, Scheduled Reports, and Data Filters. ",
    "Date Format": "The date format setting determines the display style for dates in Analytics. Dates are commonly found along the x-axis of query results. There are three options for date format: US MM/DD/YYYY, European DD/MM/YYYY, or ISO YYYY-MM-DD. All new projects default to US MM/DD/YYYY. ",
    "Currency Format": "The currency format setting allows you to change the currency and symbol displayed for all Revenue results in Analytics. To use the Revenue event, you must complete a Revenue mapping in the Events and Properties settings. All new projects default to US Dollars $. This setting is applied to Revenue results for reports within your entire Analytics project.",
    "List of supported currencies": "If your currency is not currently supported, please contact support.",
    "IP Address Collection": "If your project is integrated using the Analytics SDK, then the IP Address Collection setting appears in your project settings. You may enable or disable the collection of IP address information for your project. All new projects default to enabled. Note: IP geolocation tracking is restricted under the Child Protection Regulation (COPPA). Please ensure that you have disabled this within any project that contains data from online services directed toward children 13 years of age or younger. Analytics reserves the right to disable IP address collection if we suspect any violation, intentional or otherwise, of this regulation.",
    "Public Sharing": "Permit the sharing of embedded widgets, dashboard links, and scheduled reports externally.",
    "AI Features": "Enable or disable AI-assisted features including Assisted Analysis.",
    "Data Sources": " The Data Sources section displays a summary of the active and inactive data sources for your project, including the name, type, and date that event data was last seen (or received). A green dot indicates that the project received data from this data source recently. A red dot indicates that the project has not received data from this data source recently. A gray dot indicates that there is no information available about the status of this integration. You are able to add a new data source to the project by clicking on New Data Source on the top right. This brings you to the Connect Your Data screen.  You may also view all data sources within an organization by accessing Data Sources within Organization settings. This will display data sources across all projects.  Data warehouse users may also view the specifics of their data warehouse integration by selecting Edit Details. ",
    "Scheduled Reports": "Project Settings also contains a tool to manage Scheduled Reports. The table shows when reports were created and by which user, and the three-dot menu on the right allows editing of the underlying dashboard, as well as the features of the report itself. See ourReportsdocumentation for more details. ",
    "Data Filters": " If your project contains any data filters, then the data filters section displays the defining criteria for the filter(s). Data filters are applied to all queries within a project; they do not modify or otherwise change the underlying data in Analytics.",
    "Create a Predictive Attribute within Journeys": "Predictive Attributes can be created in the process of defining a new customer segment using Journeys. Here are the steps to accomplish this: Navigate to the Journeys homepage atSegmentation > Journeys.SelectNew Journey.Give your Journey a name, and select the workspace(s) and data inputs you would like to have access to within this Journey.In the Journey canvas, create a new Milestone.SelectUser Predictionfrom the criteria dropdown options.Give your Predictive Attribute a name that describes the user behavior it is predicting.Use the query builder to specify the user action you would like to predict.Specify whether you want to generate predictions for all or a subset of users. If choosing the latter, use the query builder to define this subset.  Once you save your Predictive Attribute, Cortex will immediately begin calculating your prediction. Note that new predictions need up to 24 hours to calculate.",
    "Create a standalone Predictive Attribute": "Creating a new Predictive Attribute does not require using any of mParticle\u2019s customer segmentation tools. To create a standalone Predictive Attribute: Navigate to thePredictionshomepage.SelectNew Predictive Attribute.Give your Predictive Attribute a name that describes the user behavior it is predicting.Use the query builder to specify the user action you would like to predict.Specify whether you want to generate predictions for all or a subset of users. If choosing the latter, use the query builder to define this subset. ",
    "Calculated attributes basics": "A calculated attribute contains the following elements that you define: Name: a descriptive label for the calculated attribute. Once activated, the name can\u2019t be changed.Description: an optional text field to describe the attribute.Calculation definition: The logic for computing the attribute: the calculation type, criteria, and the date range. For example, you might specify a sum calculation type.Status: Calculated attributes are created as drafts, indicating that they are not being calculated. When you change a calculated attribute status to active, mParticle starts calculating the attributes and you can use them across the mParticle platform and downstream.",
    "Scope": "Calculated attributes are defined and calculated per workspace; calculations use data available within the same workspace where they are defined. You can create calculated attributes with the same name and functionality in multiple workspaces.",
    "Initialization": "After you activate a calculated attribute, it initializes using existing data in the mParticle CDP along with any seeded value that was sent usingSeeding API. Depending on the date range selected in the calculated attribute definition, this can take several hours. After initialization, calculated attributes continue to recalculate with new data.",
    "Calculation speed": " Calculations are eithersynchronous(sent with the batch of data being processed) orasynchronous(sent with the next batch): Synchronous calculations are evaluated immediately and updated values are included in the same outgoing event batch.Asynchronous calculations are evaluated with a small delay (usually a few minutes) and updated values are included in the next outgoing event batch or when the feed input named Calculated Attributes is connected (more about this feed). If your event output doesn\u2019t support the Calculated Attributes feed, user attribute values may grow stale until mParticle receives the next event or batch for the relevant user.",
    "Calculation categories": "We currently support 13 calculations organized into four categories: Count: calculate the number of times an event has occurred, such as the number of logins or cart views.Aggregation: calculate a standard statistic about an event attribute such as sum, minimum, or maximum.Occurrence: calculate the timestamp for an event, or the first or last value or timestamp.List: calculate the unique list of values for an event attribute. For example, all the unique game titles played or unique product categories viewed. For a list of calculations and details about each, seeCalculated Attributes Reference. For an overview of how to use calculation categories, view the following video: ",
    "Calculation date range": "The following date ranges in calculated attributes are supported: Within the Last: limit calculations to the period of a specified number of days or weeks ago to now.  For example, \u201cmost frequent product categories viewed over the last 30 days.\u201dSince: limit calculations to the period of a specified start date to now.",
    "Conditional logic": "After selecting an event, you can add conditions to the attribute in order to more precisely define results. For example, a retailer creating a calculated attribute might use the Contains operator with a Count attribute to count only the purchases that contain \u201cSock\u201d in the product name. For a complete list of operators for the four categories of attributes (Count, Aggregation, Occurrence, and List), seeConditions.",
    "Calculation attribute seeding": "Seeding allows you to pass in historic values for calculated attributes that mParticle builds on as new data arrives, without passing in the raw events. Seeding allows you to transition from your own calculations to mParticle calculation. For example, you may have data from outside mParticle about the last four months\u2019 bookings. You can create a calculated attribute, then send the seed data to mParticle usingthe Calculated Attributes Seeding API. mParticle then combines your seed data and live data so there\u2019s no interruption. You can seed calculated attributes in both draft (recommended) and active states; the calculated attribute must exist before you can seed it. Seeding requires two pieces of information: The seed values: the values required to calculate the attribute.The seed cut-off date: any data prior to this date is processed by your team into seed values, and mParticle only uses live data on or after this date in the calculation, combining the result with seed values. Using the correct cut-off date ensures an accurate transition into mParticle and avoids duplications of data in calculating a calculated attribute. Note the following calculated attribute behavior: After a calculated attribute is already in an active status, receiving data via historical API doesn\u2019t automatically trigger a recalculation. You must either create a new or update the definition of the calculated attribute to trigger a recalculation.Seeding of a rolling time window is not supported. Thus, seeded values don\u2019t decrement when they pass out of the calculated attribute window, for example a calculated attribute date range of \u201cwithin the last 4 weeks.\u201d",
    "Forwarding calculated attributes in an audience integration": "If a partner supports user attribute forwarding, you can forward calculated attributes in an audience integration alongside user attributes. Different partners have implemented user attribute forwarding in different ways. For example,Salesforceuses a separate data extension whileGoogle BigQueryuses the configuration setting Send User Attributes.",
    "Use Case Guide": "To walk through several different scenarios for using calculated attributes, download theCalculated Attributes Use Case Guide.",
    "Step one: create a calculated attribute": "Within theData Mastersection of your dashboard\u2019s side navigation panel, selectCalculated Attributes, and then select+ Calculated Attribute.Enter the Calculated AttributeNameand an optionalDescription.Select theCategoryof calculation. SeeCalculation Categoriesfor details.ClickNextto display the criteria section.Define the data used to run the calculation.a. Select values from the drop-downs to define and add criteria as needed.For Value Based Pricing customers, only events set toPersonalizecan be used as criteria in a calculated attribute.  Events set to other tiers are grayed out.Some operations require a specific data type to run a calculation. When a selected attribute is incompatible with the operation, a warning message is displayed. If you want to force the use of a specific event attribute, you can continue past the warning and activate the calculated attribute. For example, if you pass in the purchase amount as a string, you can force it to be a number for use in a sum calculation.b. ClickNextto select a date range.To choose the date range for your calculation, click on the Date Range drop-down:SelectSincefrom the dropdown for calculations that must be made from a specific start date within the audience retention period defined in your subscription plan.SelectWithin the lastfor calculations that must be made over a specific rolling time period. Enter a number and specify a time unit ofDaysorWeeksSelectAll Timeto use all the data available per yourlong-term data retention policy. Therefore, selectingAll Timemay incur additional expenses due to larger data volumes.You can't specify a period longer than the period specified in your subscription plan. If you need a longer period of time to match your long-term data retention policy, you can request enablement of [Unlimited Lookback](/guides/platform-guide/data-retention/#data-retention-and-unlimited-lookback).You can seed the calculated attribute with historical data if you selectSinceorAll Timefor your date range. To seed, add the date you want to start using incoming mParticle data\u2014it can be a date different from the last date of seeded data. If you don\u2019t want to seed this calculated attribute, click the X on the right to remove it. For more information about seeding, seeSeed a calculated attribute.Saveyour changes.",
    "Step two: activate a calculated attribute": "A calculated attribute must be activated before mParticle starts calculating its values across your users. To activate a calculated attribute: If it\u2019s not already open, go toData Master > Calculated Attributesand click the calculated attribute to open it.SelectActivate.Once activated, the calculated attribute is immediately available across the mParticle platform. You can then create audiences or set up connections and data filters. When activated, mParticle computes and initializes the value for the calculated attribute. Depending on the date range, volume of data in your workspace, and definition complexity, calculations require different amounts of time before they are available across your customer profiles. While calculating, the UI displays progress of the initialization.",
    "Optional step: using the Calculated Attributes feed": "You can use the default behavior for forwarding calculated attributes forwarding, or you can use\nthe special Calculated Attributes feed.",
    "Forward calculated attributes in event batches (default behavior)": "mParticle automatically enriches incoming batches with active calculated attributes for that user. Like regular user attributes, you can restrict which outputs receive them usingdata filters. ",
    "Forward calculated attributes in the Calculated Attributes Feed": "The Calculated Attributes feed allows you to send calculated attributes downstream whenever they change, without an event from the user. This feed is especially useful for keeping calculated attributes with asynchronous calculations synchronized throughout your stack, and for sending calculated attributes downstream alongside kit integrations. If your output partner supports the Calculated Attributes Feed, the input appears once you have activated a calculated attribute. When a new connection is made to this input, calculated attribute values for users who have not been seen since their asynchronous calculated attributes were calculated are sent. This feed sends an update when calculated attributes change (both synchronous & asynchronous); it does not send user attributes. To control which downstream system receives these updates, connect specific platforms to receive the calculated attribute updates. You can also filter out calculated attributes you do not wish to forward using the platform filters page.  The calculated attributes feed is available with the following partners: Amazon KinesisAmazon Kinesis FirehoseAmazon RedshiftAmazon S3Amazon SNSAmplitudeApache KafkaBrazeGoogle BigQueryGoogle Cloud StorageGoogle Pub/SubMicrosoft Azure Blob StorageMicrosoft Azure Event HubsSlackSnowflakeWebhook",
    "View calculated attributes in Live Stream, User Activity view, or Profile API": "Calculated attributes can be viewed alongside other user attributes in theLive Streamand theUser Activity view, and are accessible via theProfile API. ",
    "Use calculated attributes in audiences": "Use calculated attributes in the Audience builder by selectingUser > Calculated Attributes. Calculated attributes appear as \u2018string\u2019 types at first and then automatically switch to the correct type as they are computed across many users.  You can build audience criteria with a calculated attribute even if it is calculating. After the attribute values are completed for each user, their audience membership is updated. ",
    "Seed a calculated attribute": "Seeding allows you to pass in historical values for calculated attributes that mParticle will build upon as new data arrives without passing in all the raw events. Seeding allows you to seamlessly transition from your own calculations to mParticle\u2019s. To use seeds with a calculated attribute: Define a calculated attribute in the mParticle platform before sending any seeds.The calculated attribute can be in either draft or active state. However, mParticle recommends the calculated attribute be activated after seeds for all users have been sent to mParticle. Once a calculated attribute is activated, mParticle starts calculating it, and thus some users may show inaccurate values until all seeds have been received.Send seeds via theCalculated Attributes Seeding API.Calculated attribute creation or update takes up to five minutes to be included in the mParticle cache. Therefore, if you send seeds immediately after creating or updating a calculated attribute, mParticle may send a NOT FOUND error. Once mParticle has received seeds, mParticle combines them with calculated attribute results based on live data received after the cutoff date.",
    "Calculated attribute changes and seeding": "After seeds have been sent to mParticle, any of the following changes make the previously received seeds invalid and are deleted from mParticle. Calculated attribute nameCalculated attribute calculation type, such as from sum to countSeeding cutoff dateDeleting the calculated attribute",
    "Updating seeds": "To update the seeds after you have sent them to mParticle, send the updated seeds to mParticle again. mParticle overwrites previously received seeds.",
    "Historical data loads require recalculation": "If you load historical data usingthe historical endpoint of the HTTP API, after loading, you must recreate new calculated attributes or update the definition of an existing CA to trigger a recalculation.",
    "Create a BigQuery Dataset (Optional)": "Open the BigQuery web UI in theGCP Console.In the navigation panel, in theResourcessection, select your project.On the right side of the window, in the details panel, clickCreate dataset.On theCreate datasetpage:ForDataset ID, enter a unique datasetname.(Optional) ForData location, choose a geographiclocationfor the dataset. If you leave the value set toDefault, the location is set toUS. After a dataset is created, the location can\u2019t be changed.ForDefault table expiration, choose one of the following options:Never:(Default) Tables created in the dataset are never automatically deleted. You must delete them manually.Number of days after table creation:This value determines when a newly created table in the dataset is deleted. This value is applied if you do not set a table expiration when the table is created.ClickCreate dataset.",
    "Share Your Dataset with Analytics": "In the menu panel along the left side of your BigQuery instance, underResources, click on the triangle to the left of your project name to expand the view. This will display the available datasets.Select a dataset fromResources, then clickShare Datasetnear the right side of the window.  In theShare datasetpanel, in theDataset permissionstab, clickAdd members.In theAdd memberspanel, enter \u201devent-stream@indicative-production.iam.gserviceaccount.com\u201d into theNew memberstext box.ForSelect a role, selectBigQueryand choosebigquery.dataEditor.ClickDone.Contact your account manager with the following information once you have granted Analytics \u2018bigquery.dataViewer\u2019 access:ProjectId: the Analytics project ID to exportBQProject: the GCP project name to write data intoBQDataset: the BQ dataset name to write data into",
    "Explore Users from Cohort Cell": "To explore users within a certain cohort cell, simply select the desired cohort cell, and select Explore Users.  In the above example, PetBox is exploring users that have done Site Visit on 6/6/2020, and returned to do Purchase Product on Day 3.",
    "Heat Map Visualizations": "In order to explore users from an entire time generation or breakout, simply select the time generation or breakout on the left side of the visualization area, and choose Explore Users.  In this example, PetBox is exploring users that did Site Visit on 6/6/2020, and returned to do Purchase Product in any interval within the selected Date Range.",
    "Graph Visualizations": "If your cohort visualization type is set to Area or Line, then you may only explore users from a certain time generation or breakout. In order to explore users from an entire time generation or breakout, select into any data point within the visualization area, and choose Explore Users.  In this example, PetBox is exploring users that did Site Visit on 6/6/2020, and returned to do Purchase Product in any interval within the selected date range.",
    "Count vs. percent": "If you select \u201cCount,\u201d Analytics will calculate the net change in absolute numbers. For example, \u201cthere were 56 more users who completed the funnel this week than there were last week.\u201d  If you select \u201cPercent,\u201d Analytics will calculate the net change as a percentage. For example, \u201cthere were 35% more users who completed the funnel this week than there were last week.\u201d ",
    "Trends display": "The net change is displayed in the upper left corner of the results field and in the table at the bottom of the results field.  If your funnel results are higher than the previous period, then the text will be green and the arrow will point upward. If your funnel results are lower than the previous period, then the text will be red and the arrow will point downward. This indicates the direction of travel. Hover over the result to see a definition of the previous date range. In the table visualization, sometimes you will see a dash \u2018-\u2019 in the change column (represented by \u0394). That does not necessarily mean that there is no change between the two date ranges but rather, there isn\u2019t enough data for Analytics to provide you with the change. Circumstances that could produce this result include: The data point did not exist in the previous period.The data point did exist in the previous period, but was not among the top/bottom breakouts for the previous period. Trends are not available for conversion over time funnels.",
    "Apply a Filter Where Clause": "In order to apply a filter where clause, hover over the query row in which you would like to filter, and select +filter where. Then type to search for a property to filter, and then select your desired property.",
    "Property Value Type": "There are three different property types to consider when using filter where clauses. Numeric:A value that contains only numbers. Numeric values may be used in calculations.String:A value that contains letters, numbers, or other characters. String values are not used in calculations.Date/Time:A value that represents a date or time. Date and Time values may be in the ISO 8601, Unix Time Seconds, or Unix Time Milliseconds format. Property types can be toggled in theEvents and Properties Manager. The first 50 property values auto-populate when choosing Select a Value.",
    "Numeric": "When filtering by a numeric value, select one of the following options: Is equal todisplays data for values equal to that specific property value.Is not equal todisplays data for values that are not equal that specific property value.Is greater thandisplays all data for values greater than the selected property value.Is less thandisplays all data for values less than the selected property value.Is greater than or equal todisplays all data for values that are greater than or equal to the selected property value.Is less than or equal todisplays all data for values that are less than or equal to the selected property value.Is defineddisplays all data where there are values for the selected property value.Is not defineddisplays all data where are no values for a selected property.",
    "String": "When filtering by a string value, select one of the following options: Is equal todisplays data for values equal to that specific property value.Is not equal todisplays data for values that are not equal that specific property value.Containsdisplays data where the string contains the selected property value.Does not containdisplays all data that does not include the selected property value.Is defineddisplays all data where there are values for the selected property value.Is not defineddisplays all data where there are no values for a selected property. If you are filtering by string values, and you have selected either contains or does not contain, you may combine multiple property values. You may select to combine property values using Or or by using And. For example, PetBox may choose to analyze Email Clicked, filtered where Browser Name contains Chrome or Safari. This use case would not work with the and filter where. However, PetBox may choose to analyze Email Clicked, filtered where Marketing Channel contains Social and Content.",
    "Date/Time": "When filtering by a date/time value, select one of the following options: Is equal todisplays data for values equal to that specific property value.Is not equal todisplays data for values that are not equal that specific property value.Is beforedisplays data for values that occur before a specific date or time.Is afterdisplays data for values that occur after a specific date or time.Is betweendisplays data for values that occur in between two specified dates or times.",
    "Combining Filter Where Clauses": "You may also combine multiple filter where clauses. To do so, simply hover over the query row in which you are analyzing, and select \u201c+filter where\u201d again. Then, you must select whether to combine the filter where clauses using and or or. If you choose and, then users must satisfy both filters in order to count in the analysis. If you select or, then users may satisfy only one of the filters in order to count in the analysis.",
    "Getting started": "$ npm install react-native-indicative --save",
    "Mostly automatic installation": "$ react-native link react-native-indicative",
    "Manual installation": "In XCode, in the project navigator, right clickLibraries\u279cAdd Files to [your project's name]Go tonode_modules\u279creact-native-indicativeand addRNIndicative.xcodeprojIn XCode, in the project navigator, select your project. AddlibRNIndicative.ato your project\u2019sBuild Phases\u279cLink Binary With LibrariesRun your project (Cmd+R)< Open upandroid/app/src/main/java/[...]/MainActivity.java Addimport com.reactlibrary.RNIndicativePackage;to the imports at the top of the fileAddnew RNIndicativePackage()to the list returned by thegetPackages()method Append the following lines toandroid/settings.gradle:include ':react-native-indicative'\nproject(':react-native-indicative').projectDir = new File(rootProject.projectDir, \t'../node_modules/react-native-indicative/android')Insert the following lines inside the dependencies block inandroid/app/build.gradle:compile project(':react-native-indicative')",
    "Usage": "Find more onNPMandGitHub.",
    "Receive email notifications": "To start receiving email notifications, emailsupport@mparticle.comand request that you begin receiving failure alerts. Once enabled, you will be notified once per load if the load is at least three standard deviations over the average load time over the last 7 days To add additional people who receive notifications,submit a request.",
    "Stop email notifications": "Respond to an email notification with \u201cunsubscribe\u201d in the body of the email orsubmit a request.",
    "Toggle User Properties On/Off": " Analytics automatically turns on certain common user properties. Some of these include first marketing channel, last device, last location, and last user profile information such as email. Any traits and attributes sent through anIdentify Callwill be turned on automatically. Under the Attribution Mode column, you may toggle the Off/First/Last selector to indicate which event properties you want to appear in the platform.",
    "First vs Last": "Each user property can take the first observed value or the last observed value. In order to toggle a user property on, you must select the attribution mode.",
    "Rename, Define, and Categorize User Properties": " Under the Property Key, you see the raw property name that is sent to Analytics. This can be renamed in the Display Name Column.Under the Display Name column, you may rename a user property. User properties without a display name will default to their original name.Under the Description column, you may add a description to a user property. Descriptions are useful for new users, and/or when your project contains similarly named events.Under the Category column, you may create categories and assign user properties to them. Creating categories helps keep your Data Panel organized.Under the Data Type column, you may designate the format of your user property. You may select one of the following: Auto, String, Numeric, Location (U.S. State), Date/Time.",
    "Instructions:": "In Analytics, click on the gear icon and selectProject Settings.Select theData Sourcestab.SelectNew Data Source.SelectConnect via Data Warehouse or Lake:SelectSnowflakeas your data connectionSnowplowas the connection schema and clickConnect.You should see thisSnowflake + Snowplow Overviewscreen. ClickNext.",
    "Okta": "To configure single sign-on using SAML2.0 and Okta you must create a new Application within the Okta administrative portal, populate the appropriate configuration information from Analytics in Okta, copy and paste the Identity Provider XML file into Analytics and assign users. Follow the instructions below to get started: Log into Okta and then select ApplicationsChoose on Create App Integration  Within Create App Integration, highlight SAML 2.0 and then choose Next  In the General Settings tab, insert \u201cAnalytics\u201d in the App name field. If you would like to add the Analytics logo, you can retrieve it from the Extras section of Analytics\u2019SSO Settings. Otherwise, select Next.Select SAML Settings and enter the following fields for which the values are located in Analytics\u2019SSO Settingssection:Single sign on URLApplication URI (SP Entity ID)Default RelayState Choose Next at the bottom  On the Feedback tab, select \u201cI\u2019m an Okta customer adding an internal app\u201d and choose Next.On the Sign On tab, select Identity Provider Metadata to download the XML configuration file, open it using a text editor, and copy and paste into Identity Provider Metadata XML within Analytics\u2019SSO Settings.  Your application is now set up for SAML2 authentication. You may test your application using the dedicated sign-on URL which can be found on theSSO Settingssection within Analytics. To enable Home Realm Discovery, mapping your business domain to your SSO provider, configure the Identifier-First Authentication section and make sure to disable username and password authentication underEmail & Password Authentication.",
    "OneLogin": "To configure single sign-on using OneLogin, first add the SAML Test Connector to your Admin Portal atonelogin.com, then enter your custom organization settings. When your configuration is complete, copy and paste the metadata into your Analytics Organization Settings to complete your SSO setup. Log into your OneLogin Admin Portal, then click Applications.In the top right corner of the screen, select Add App.  Under Find Applications, enter SAML into the search bar, then select select SAML Test Connector.  For the Display Name, enter Analytics.Upload the Analytics logos, available at the bottom ofthis page.Use the custom field values fromhttps://app.indicative.com/#/organization/ssoto complete the required fields in Configuration:  Add the following Parameters:  Click Save.Click to open the More Actions dropdown, then click download your unique SAML MetaData.  Copy and paste XML file intohttps://app.indicative.com/#/organization/sso, then click Save.On the Default Project Access tab of Organization Settings, enable default project access for at least one project, then click Save. To test your SSO configuration, use the dedicated sign-on URL listedhere. Optional: Once your SSO configuration is tested and confirmed, disable Email & Password Authentication on the Email & Password Authentication tab of Organization Settings,here.",
    "Script for Non-AMD Sites": "Please note, the instructions in this section are only valid for sites that don\u2019t use AMD (e.g. require.js). First, you\u2019ll need to asynchronously load our script into your site. Add this script in either your site\u2019s\u00a0\u00a0or\u00a0tag: This script tag asynchronously loads Indicative.js from our CDN and initializes the JavaScript code with your unique API key. You will need to set your API key in quotes where it says \u201cYOUR_API_KEY_GOES_HERE\u201d. You can find a list of all of your projects and appropriate API keyshere. To choose between your site\u2019s\u00a0\u00a0or\u00a0\u00a0tags, note the pros and cons of each. The\u00a0\u00a0tag will allow you to access the Analytics object earlier (on load), however your site will not load until everything in the\u00a0\u00a0tag is loaded. So, if you do not need the Analytics object immediately, we recommend putting this snippet in the\u00a0\u00a0tag. If you would like a version of the script that does not ask require.js, please reach out tosupport@mparticle.com.",
    "Building and Sending an Event": "Recording an event is easy and customizable. It can be as simple as: The above line will build and send an event named \u2018event-name\u2019 with a unique ID set as a random UUID. You can also add your own user IDs and important properties to every event to further enrich your data for more impactful analyses like in the example below. Some properties may be stored as a persistent cookie, so that every page can share some common properties or unique ID instead of explicitly passing them every time you build an event. Additionally, Indicative automatically tracks some properties by default; learn more.",
    "Validating Integration": "Open up theDebug Console in Analyticsto view all incoming events. You should expect to see your data in Analytics",
    "Additional Information": "For advanced JavaScript settings, please refer to our documentation. For a full list of our Analytics Object API, please click here.",
    "Callbacks": "Indicative also allows callbacks, which will be fired after a successful or unsuccessful stat post. You can include a callback function in any of the buildEvent methods, like so: With so many different ways to build an event, you\u2019ll have a lot of flexibility to build and send any custom events you need. For further references, refer to the Analytics object API table below.",
    "Stateful Variables": "We allow you to set stateful variables across every page. Stateful variables are stored as a persistent cookie, so every page will be able to share the same common properties and a uniqueID for the user triggering events on your site. Anywhere in your JavaScript, after Analytics was initialized, call: Indicative.setUniqueID(\"unique-user-id\"); This will allow you to log events without having to refer to a unique ID every time you build an event. Analytics also allows for stateful properties, as well, which can be added with the following calls: These properties will be appended to subsequent event calls. They will not override the properties passed into a buildEvent call, rather append to the list of properties. If a common property isn\u2019t applicable anymore, call: Indicative.removeProperty('propertyName'); This will remove a single property. It\u2019s just as easy to clear the entire common properties list: Indicative.clearProperty();",
    "How to Track Links": "Tracking\u00a0href\u00a0link clicks can be challenging, because once the page changes we lose our chance to fire an event. To solve this problem, we\u2019ve added a callback to our build object. Use the following function to track link clicks and then send the user to the linked page: To call this function in your HTML, set up a link like so:",
    "How to Track Web Sessions": "We\u2019ve implemented Web Sessions in our JavaScript SDK track users\u2019 web sessions with just a slight change to one line of your code.\u00a0If a user has no activity for 30 minutes (no events are fired locally), upon any new event activity, the JavaScript SDK will also fire a \u201cWeb Session\u201d event to indicate the start of a new Web Session. Note:\u00a0The window of inactivity is customizable, but defaults to 30 minutes, with industry standards.",
    "How to Integrate": "Where you see the line in this snippet: Indicative.initialize(apiKey); Alter it to read: If you want to alter the inactive session length, change the line to be this instead: where 5\u00a0signifies\u00a05 minutes.",
    "Automatic Tracking": "We automatically track the following properties: browser:\u00a0browseroperating system:\u00a0browser_osdevice:\u00a0browser_devicereferrer:\u00a0browser_referrerlanguage:\u00a0browser_languagepage title:\u00a0page_titleurl:\u00a0page_url We also automatically track marketing channels provided by UTM search parameters. You\u2019ll be able to see the following properties if you have users loading your page with UTM properties in the URL:\u00a0campaign_source,campaign_medium,\u00a0campaign_term,\u00a0campaign_content, and\u00a0campaign_name. We will also provide these channel properties in their own section of the data panel, titled as User Properties - UTM (category).",
    "How to Track Users Across Subdomains": "We support tracking user sessions across various subdomains through the use of our SDK. If the option \u2018cookiesOnMainDomain\u2019 is set to true, it will store the cookie on the root domain. Where you see this line in the snippet: Indicative.initialize(apiKey); Alter it to read: Whenever the cookiesOnMainDomain option is set to true, it is recommended that you include the base domain name. If it is not set, our SDK will attempt to figure it out by taking the last two tokens of the domain name, and in some cases it may be invalid values (e.g. .com.mx). To explicitly add the domain name, add the domainName to the initialization parameters. Indicative.initialize(apiKey, {recordSessions: true, sessionsThreshold: 30, cookiesOnMainDomain: true, domainName: \u2018example.com.mx\u2019 }) Warning:\u00a0Changing or enabling this optionmaybreak existing cookie tracking.",
    "Establish your Identity Strategy": "This guide hasalready coveredcollecting identities, such as email addresses, for your users. mParticle\u2019s IDSync feature gives you a lot of control over how you identify and track your users over time, and selecting an Identity Strategy is one of the most important decisions you need to make when implementing mParticle. Read our fullIDSync guidefor more.",
    "Add more sources": "For most mParticle clients, the primary sources of data are native and web apps, instrumented with the mParticle SDK, but you can also use mParticle to leverage other sources of data to build a more complete picture of your users: TheEvents APIcan be used to send supplementary server-side data.Our mainAppleandAndroidSDKs can also be instrumented in AppleTV and FireTV apps, and we publish independent SDKs forRokuandXbox.If you use a cross-platform development framework, you can use our libraries forReact Native,Xamarin,Unity, andCordovato interface with our native SDKs.Use Feeds to bring in data from other services.",
    "Explore advanced Audience features": "If you want to compare different messaging platforms or strategies, you can use mParticle to conduct anA/B Testby splitting an audience into two or more variations and connecting each to different outputs.The more specific your audiences, the more you are likely to need to create. If you have a large number of audiences to forward, useBulk Audience Connectionsworkflow to speed things up.",
    "Transform your Data": "One of the core benefits of mParticle is the ability to capture data once and forward it to multiple outputs. However, you probably don\u2019t want to sendallyour data toeveryoutput. mParticle provides you with a full set of tools to filter and transform your data. Use these tools to control the flow of Personally Identifiable Information (PII), to customize the data you send to each output and to control your costs. TheData Filterallows you to individually filter each data point for each output.User Splits allow you to test competing services by dividing your users into groups and connecting each group to different outputs.Forwarding Rulesallow you to block data to an output according to simple predefined rules.User Samplingallows you to send only a subset of your data to a given output. This is usually done to control costs for services that charge according to data volume or unique user counts.For advanced transformations, theRulesfeature allows you to host a custom function on AWS Lambda which can change almost any aspect of your data.",
    "Manage your GDPR Obligations": "If you have users in the European Union, you may have obligations as a Data Controller under the General Data Processing Regulation. mParticle provides tools, available as premium features, to help you manage two aspects of GDPR compliance: User ConsentData Subject Requests",
    "Know your limits": "Part of the purpose of mParticle is to allow you to maximize leverage of your data without compromising performance. In order to protect the performance of both your app and the mParticle dashboard, we impose certain limits on the data you can send. If you\u2019re a dashboard user, you can read a brief summary of the default limits,here. If you need the full picture, you can read ourdetailed guide.",
    "Analyze as Funnel/Cohort": " In order to interlope your customer path from the Journeys tool, and analyze it as a Funnel or Cohort query, select the Analyze as Funnel/Cohort menu item. Then, you\u2019ll be prompted to select an event in each previous step until you reach a singular customer journey. As you click on events going backwards, notice that your customer journeys will start to slim into a singular Journey. When you reach the starting event, your customer journey will load into theFunneltool, or theCohorttool, depending on your selection.",
    "Exclude event": " In order to exclude an event from your Journeys query, select the Exclude Event menu item. Any excluded events will not be displayed in the chart - they will be treated as if they did not happen. To re-include an event, click on the Excluded Events menu bar item. Events expanded or excluded will be shown in the UI.  Excluding events will also affect the query performance.",
    "Breakout event": "Let\u2019s say you currently track a Page View event, with a Page Title property. You\u2019re most likely going to care about Page View & Page Title combinations, rather than just the Page View event. In order to display event+property combinations in your Journeys visualization, click on the event in which to break out, and select the Breakout Event menu item. Then, select a property to breakout by. To undo this, click on the Events with Breakouts menu item. At query time, an \u201cevent\u201d in a path can include property values. ",
    "Saving a users query": "Once you save your query, a pop-up window will appear. Use the dropdown menu to select the folder in which to save your Users analysis. ",
    "Posting Announcements": "Users with the Project Announcements permission enabled are able to post project-wide announcements. In order to post or edit an announcement, users must select the Edit button in the bottom right corner of the widget. The widget is flexible, so you can post text, tables, link to other areas of the app (like a new dashboard for example), and more! Once you save your changes, the widget will be updated immediately for all of your teammates to see. You can control who can edit and view announcements in theGroups and Permissionspanel of Organization Settings.",
    "Period vs Interval": "If you select\u00a0Period, Analytics will compare your query results with the results from the immediately preceding date range. For example, if your query\u2019s date range is Last Full Week, then Analytics will calculate the results for the last full week and also the previous full week \u2014 for example, \u201cthere were 18 fewer signups last week than there were the week before last week.\u201d Both results will be displayed within the chart area, with the previous range appearing in gray. If you select\u00a0Interval, Analytics will compare each interval within your results to the immediately preceding interval. For example, if your query displays daily intervals, then Analytics will calculate the difference each day \u2014 for example, \u201cthere were 12% more fewer signups on Friday than there were on Thursday.\u201d",
    "Count vs Percent": "If you select\u00a0Count, Analytics will calculate the net change in absolute numbers \u2014 for example, \u201cthere were 56 more purchases this week than there were last week.\u201d If you select\u00a0Percent, Analytics will calculate the net change as a percentage \u2014 for example, \u201cthere were 35% more purchases this week than there were last week.\u201d",
    "Trends Display": "The net change is displayed in the upper right corner of the results field. If your query results are higher than the previous period, then the text will be green and the arrow will point upward. If your query results are lower than the previous period, then the text will be red and the arrow will point downward. This indicates the direction of travel. Hover over the result to see a definition of the previous date range. To move this display, simply click to toggle between the upper right and upper left corners of the chart area. If your query contains multiple rows, you may toggle between trends by hovering over the relevant row name in the chart legend or by hovering over the relevant data points in the results field. If you\u2019re using a table visualization, sometimes you will see a dash \u2019-\u2019 in the change column (represented by \u0394). That does not necessarily mean that there is no change between the two date ranges, but rather that there isn\u2019t enough data for Analytics to provide you with the change. Circumstances that could produce this result include: The data point did not exist in the previous period.The data point did exist in the previous period, but was not among the top/bottom breakouts for the previous period.",
    "Hide / show filters": "Dashboard filters are displayed in the top row of a dashboard above the analyses, and they are shown by default. To hide the dashboard filters row, selectHide Filtersin the top settings bar. To show them again, selectShow Filters.",
    "Date range filter": "The date filter at the top of your dashboard lets you set a date range across all analyses within that dashboard. By default, there is no date range applied to a dashboard when it is created. The data displayed for each analysis reflects the date range set at the analysis level. Set a date range for your dashboard Click \u201cSelect Date\u201d to expose the date selector. Here, you can choose a custom or preset date range. ",
    "Property filter": "You can apply event property filters across all analyses on your dashboard. To do this, first selectAdd Filterto expose the event property query builder:  The query builder contains three components: Event Property (category and name):Select the event property you want to apply as a filter. The query builder organizes events into categories, so select the category first, followed by the event itself.Operator:Choose an operator for evaluating the logic in your filter. The default operator isis equal to, but you can change this to any other operator that is available to that the event property you have selected.Value:Provide a valid value for your chosen event property. You can continue stringing filters together to hone in on a precise subset of your users. For example, applying the filter settings below to a dashboard will result in all analyses displaying data from the last full month whereCityis equal toNew York,Subscription Planis equal toMonthly, andBrowser Nameis equal toChrome. ",
    "Remove a filter": "To remove a filter from your query, click the \u201cX\u201d icon to the right of the filter description. This will remove that single filter from your query.",
    "Apply filters": "Each time you add or remove a new filter your query, you must selectApply Filtersfor your changes to be reflected in the dashboard. After applying filters, there will be a delay before the filtered data is loading. The loading state is indicated by spinning to the right of the title of each analysis widget. ",
    "Impact of dashboard filters at analysis level": "Each dashboard filter will be applied to the individual analyses within the dashboard, provided the filter and filter value are available within the analysis. Once you have applied filters to a dashboard, each analysis will display a filter icon in the bottom right indicating how many of the filters are applied at the level of that analysis. The names of the filters applied to this analysis will appear on hover:  In cases where filters you\u2019ve set at the dashboard level do not apply to an analysis, both the applied and unapplied filters will appear on hover:  When do dashboard filters apply to individual analyses? Dashboard filters override analysis-level filters, provided the event property is available at the analysis level. For example, if a dashboard includes the filterState=CAas a filter, and an analysis within this dashboard has access to this property but is not using it as a filter, the analysis will have the filterState=CAapplied once the dashboard-level filters are applied. When do dashboard filters override analysis filters? If a filter applied at the dashboard has been previously applied at the analysis level, the dashboard filter will override the analysis filter at analysis level. For example, if an analysis in a dashboard has the filterState=CA, and you apply a filter at the dashboard level sayingState=MN, the dashboard filter will override the analysis filter, and theStatewill be updated toMNat the analysis level. When are dashboard filters not applied at the dashboard level? When a dashboard filter value does not exist in any of the analyses in that dashboard, this filter will not be applied at the dashboard level. For example, if you set a date range of 90 days on a dashboard that includes only Journeys, this filter will not be applied, since the lookback period for Journeys is 30 days. Modifying dashboard filters at the analysis levelWhen you modify a filter on an analysis that was set at the dashboard level, the change will persist in the underlying query, but will not impact the original dashboard filter. For example, say you have a dashboard with the geo filterState=New York, and you open an analysis on this dashboard on which that filter is applied. If you change that filter toState=Californiaat the analysis level, the analysis will haveState=Californiawhile the filter at the dashboard level will continue to beState=New York. Open Query This option opens the query with the most recent dashboard filters applied. Open Query without Filters Applied This option disregards any dashboard filters. It will display the results based on the filters applied previously on the analysis.",
    "Managing filter updates across teammates": "Dashboard filters include checks to ensure smooth collaboration between members of the same organization. Here are some of the scenarios these checks account for:",
    "Another user has recently made changes to the dashboard": "When viewing a dashboard for the first time after someone else in your organization has updated the filters, you will see a banner at the top of the dashboard indicating this:  In these cases, you will need to refresh the page to reflect the most recent filters before you are able to apply additional changes you have made to the dashboard filters. ",
    "Dashboard filters user access": "Dashboard filters mirror dashboard-level and project-level permissions: At the dashboard level: Edit privileges:Any user who can edit a dashboard can also edit that dashboard\u2019s filters.View privileges:Any user who can only view a restricted dashboard has read-only privileges at the dashboard level. These users can view but not edit that dashboard\u2019s filters. At the project level: Within project settings, users can be assigned one of three dashboard permission levels:Full, View,orNone. Full:Any user withFullaccess at the project level can view and edit all dashboards, including their filters.View:Users withViewproject-level permissions have read-only access to dashboards, and can view but not edit dashboard filters.None:Users whose project-level permission is set toNonemay not view or edit dashboards within the project.",
    "Optimize dashboard performance": "You can optimize the performance of your dashboards by adhering to best practices when creating and filtering them, as well as when building the analyses they contain. Learn morehere.",
    "Outputs": "Outputs are mParticle\u2019s term for the services where we forward your data. Outputs are also referred to as \u201cintegrations\u201d. Outputs come in two types: \u201cevent\u201d, and \u201caudience\u201d. Audiences are covered in thenext part of this guide. This section will show you how to set up an event output.",
    "Example - Connect an Output to Analytics": "mParticle has over a hundred event outputs, and the connection process for each is similar. This tutorial creates a connection to Analytics as an example. You can follow the same steps with a different output, or create afree Analytics accountto follow along exactly.",
    "Find Analytics in the Directory": "ClickDirectoryin the sidebar, and search for Analytics.Click the Analytics tile to display Output: Event Configuration.Enter the configuration information:Enter a configuration name.Leave the box checked to use the same settings for Development & Production.Select a field as the user identity field. Leave the default if you\u2019re not sure what to choose.Enter the Analytics API key which you can find in the Analytics project settings. Remember to save your new output configuration.",
    "Create the Connection": "Now that you have both an input and an output set up, it\u2019s time to connect them: ClickConnections > Connect, and select the input you\u2019ve already set up.ClickConnect Output.Select your Analytics configuration.Complete the Connection Settings, different for each output. For Analytics you can leave the default selections.Set the status toActiveand clickSave.",
    "Verify: Check that data is arriving in Analytics": "Once you have enabled the connection, open up the development build of your app again and create a few more events. Each output service displays the data it receives differently. For Analytics, you can view real-time data in theDebug Console.",
    "Step 1. Initiate a download": "To download an audience from the the journey where it is defined: From the journey where you have defined an audience, click the three dot icon in the active audience that you wish to download.If the audience includes A/B Testing Variants, you can select which variants you want to download.Determine whether you want to download the full audience or an audience sample. Downloading a sample will likely take less time than downloading a full audience (depending on the audience size), and is useful for testing or troubleshooting.Select the identity types you want, then clickDownload.",
    "Step 2. Download the file": "The download takes some time to prepare. When your download is ready, mParticle sends you an email with the download link. The download is a ZIP file which, when extracted, will contain a CSV file for each audience or variant, plus amanifest.jsonfile, with metadata about the files.",
    "CSV format": "Audience CSV files have a row for each identity in the audience. Remember that a single user profile can have multiple identities and, therefore, multiple rows. The four columns show a Unix timestamp when the audience membership was retrieved for download, the mParticle ID of the profile, the identity type, and the value: The manifest file is in JSON format.",
    "Question": "What is the most valuable marketing channel by conversion percentage?",
    "Build the Query": "The three-step funnel will include the eventsSite Visit,Blog View, andSubscribe.  Next, let\u2019s collapse the query builder so we can focus on the funnel.  This is a three-step funnel showing the conversion fromSite VisittoBlog ViewtoSubscribeover the last 7 days. The percentages in the webbing between steps indicate the percentage of users that have moved from one step to the next \u2014 keep in mind that your data may vary from this example. The percentages in the webbings should be interpreted as: 22.78% of users who visited the site went on to view the blog.45.05% of users who viewed the blog went on to subscribe The total conversion rate of 10.26% is displayed above the funnel. Percentages will change if thedate rangeis adjusted. For example, conversion percentages across 30 days will be different than those across 7 days.",
    "Modify the Cohort": "Once the basic cohort analysis has been created, we can go back into the query builder and modify it to fine-tune our results. Currently, our cohort includes all users who downloaded the PetBox app. However, to answer our initial question\u201cAt what rate do non-subscribers who downloaded the app convert to box subscribers over time?\u201d, we only need to see non-subscribers who took this action. We can achieve this by adding a\u201cWhere\u201d clauseto the first row in the query builder, which defines our cohort of users. Hover underneath the Define a Cohort step and select filter where, then choose the User Properties tab and select the propertySubscription Plan.Finish out our parameters by setting the \u201cWhere\u201d clause to \u201cis equal to None\u201d. Our new group only includes users who completed Download App and who were not subscribers: ",
    "Select Between First Time and Recurring": "For the purpose of our analysis, we only need to know about the first time a user subscribed after downloading the app. Selecting\u201cFirst Time\u201dwill limit our results to only show the first time this event was performed. ",
    "View Results": " Our cohort analysis now shows us the rate at which non-subscribers who downloaded the PetBox app are converting to subscribers over time.",
    "Observation": "We see \u201cnot enough data for visualization\u201d which means there is no data available\nfor this query. It looks like Cat Food Purchasers did not purchase cat toys\nat all since April. At this point, you can analyze further by changing the\ndate range to see if these users purchased any cat toys in the past or start\na marketing campaign to target these users for cross-selling.",
    "How to Use It": "To apply this feature, follow these steps: Select the user attribute you want to filter by.Choose either\u201cAt Event Time\u201dor\u201cMost Recently\u201das the filtering method.Analyze user behavior with precise, context-specific insights. ",
    "Filtering options": "When building queries, you have two options for filtering events by user attributes: At Event Time:Uses the attribute value as it was when the event occurred, preserving historical accuracy.Most Recently:Uses the latest attribute value in the user\u2019s profile, offering real-time insights.",
    "Implications of both filtering options": "Consider a user who made a purchase while living in Seattle and has since moved to New York: Selecting theat event timefiltering option would show Seattle as the location.Selecting themost recentlyfiltering option would show New York as the user\u2019s current location. This helps ensure that historical reports, audience segmentation, and campaign targeting remain aligned with either the historical or current context of user attributes.",
    "Example use cases": "This feature benefits customers who use mParticle\u2019s CDP for analytics, segmentation, and personalized experiences. It\u2019s particularly useful for: Historical audience segmentationCohort analysis over timeAccurate event filtering for reporting",
    "Supported Tools": "You can apply this filtering option in: SegmentationFunnelsCohorts",
    "Cumulative and Non Cumulative": "When you run a Segmentation query, the default will be set to the Non-Cumulative setting. The Cumulative setting essentially sums up the data points as you move from left to right and shows an increasing series. The Cumulative setting is not available when calculating the total count of the number of users who performed a particular event.",
    "Aggregate and Average": "When your Segmentation query is set to the\u00a0Table\u00a0or\u00a0Metrics\u00a0visualization type, you may choose to display either the aggregate or the average.",
    "Table": "Aggregate:\u00a0Select\u00a0the AGG button to view aggregate numbers. This will add an extra column at the far right end of the chart showing aggregate numbers for each row.Average:\u00a0Select the AVG button to view average numbers. This will add an extra column at the far right end of the chart showing average numbers for each row.",
    "Count vs. Statistics vs. Breakout": "If you choose to visualize using the\u00a0Table\u00a0visualization, you may also choose to show your analysis in Count, Statistics or Breakout modes. Count\u00a0mode displays counts of users, broken out by the selected time interval. These are the same values that you see when hovering over data points on the graph.Statistics\u00a0mode shows statistical summaries such as range, standard deviation and confidence intervals. This is calculated from the raw data, and will give you access to information that can help identify important trends in your data and build out analysis.Breakout\u00a0mode allows you to use the event and user property breakouts as column headers. To use Breakout mode, each query row must have the same event or user property breakout applied.",
    "Decimals": "You may also customize the decimal places for any visualization type in the Segmentation tool. For example, if you\u2019re analyzing revenue, you\u2019ll likely want to set the decimal places to .00. To do so, select the decimal type in the menu bar above the visualization area, and select your desired decimal format.",
    "Javascript Method Calls": "Indicative.sendAlias() The Analytics client automatically generates a default unique ID (a UUID) to use on all events untilIndicative.setUniqueID(id)is called. At that point,Indicative.sendAlias()can be called to alias the UUID to the \u2018id\u2019 parameter set withinIndicative.setUniqueID(id) Indicative.setUniqueID(id, true) This will automatically callIndicative.sendAlias()after setting the new unique ID. If you just callIndicative.setUniqueID(id)`\u00a0(without \u2018true\u2019) it will not send the alias call.",
    "Date Range": "To select fixed start and end dates, use the date range selector on the left side of the dropdown, you may select a start date and end date from the calendar. You can also enter a specific date by selecting on the date at the top of the selector and entering a value. Use the left and right arrows to navigate the calendar. Tick the Today checkbox to create a dynamic end date.  The right side of the dropdown lists all of the dynamic date ranges that are available. You may choose a dynamic date range, for example Last 7 Days or Last Full Month. This will automatically update the date range of your query each time you view it, counting backwards from today. If you select Last Full Week, then Analytics will analyze the most recent complete week, defined as Monday to Sunday. If you select Last Full Month, then Analytics will analyze the most recent complete month. You can quickly navigate the calendar to select full months using the links in the lower left corner of the dropdown.  To save a custom date range, for example Last 45 Days, simply click Add Custom Date Range in the lower right corner of the dropdown. Your previously used custom date ranges will be saved for future use and are viewable alongside the default dynamic date ranges. ",
    "Interval": "In Cohort, the interval describes the time period during which a user completes the Target Behavior, defined in Row C. You may choose hourly, daily, weekly, or monthly intervals. Interval options are dependent on the generation and date range selected. For example, if the selected date range is Last 7 Days, then the available time intervals will be hourly and daily. Weekly will not be available because there is only one week in a seven day date range. If the date range is changed to Last 90 Days, then the intervals dropdown will update with the additional options of weekly and monthly, but will no longer allow hourly analysis due to the inability to visualize. To calculate the total event count or user count across an entire date range, match the duration and interval of the report. Hourly intervals are only available if a cohort is defined by an hourly or daily generation.  In the results section, intervals are listed as columns. Interval 1 is the same interval during which the first event was completed. Interval 2 is the next interval after, and so on. If the interval is set to daily and a user completed the target behavior within the same day, then they will appear in Day 0. Note: For Cohort queries with the time interval set to Daily, \u201cDay 0\u201d captures users who completed the target behavior in the subsequent 24 hours after entering the cohort (meeting the definition for inclusion). For example, a user who performed Site Visit on Jan 1 at 1pm (and thus entered the cohort) would be included in Day 0 as long as they performed Subscribe (the target behavior) at any time before 1pm on Jan 2. Events are tracked down to the millisecond, and will be categorized in intervals by this level of precision.",
    "The Query Builder": " Within the query builder, you\u2019re able to construct a complex data search using a variety of field types. Every Segmentation query begins with an event. Start by setting whether you\u2019d like to calculate the total count of events or the number of users who performed a particular event. Additionally, you may categorize your results by using theGroup Byfunction or narrow your\u00a0search by using a Filter Where function. TheGroup Byand Filter Where function are akin to SQL \u201cgroup by\u201d and \u201cwhere\u201d clauses. You can use event properties, user properties or user segments with any event you select in your segmentation. Event properties are tied to a single instance of an eventGiven a scenario where a user downloads an app on iOS today and then downloads it on Android tomorrow: if the analysis looks at \u201cUsers who performed App Downloads grouped by event property Platform\u201d, the count would be 1 for iOS and 1 for Android.User properties can be chosen on any event regardless if it came in with an event since it\u2019s part of the user\u2019s profile.User properties have a chronological control and can be configured to be measured as the first value or last value. Using the example above, assume user property Platform is set to the last value. If the analysis looks at \u201cUsers who performed App Downloads grouped by user property Platform\u201d, the count would be 1 for Android and 0 for iOS. This is because Android is the last value seen for platform for that user.User Segments cross check user identities from their segment memberships.Special Note: User Segments using a specific time frame can be used with Filter Where to show members of a User Segment to help understand users who performed an action in two different time frames. For example, a User Segment looking at the count of users who were active yesterday can be contrasted with the count of users who were active this month, creating a DAU/MAU analysis. Finally, you can create a combination of events by using additionalFor Clauses(+Did [Not] Perform).",
    "Frequency Query": "You can also run aFrequencyquery to group users into different segments based on the number of times they performed a particular event.",
    "Additional Query Builder Settings": "All queries within Analytics are fully customizable using Settings such as thedate range and interval. Event queries may display results on a per-interval basis or as a cumulative count. There are six different chart types to choose from: Line chartArea chartBar chartStacked bar chartTableMetrics Optional settings includeAnnotations, which mark milestone events, and Data Labels, which communicate the numerical values at each point in the chart. For more advanced analysis, you may create queries with multiple rows, displaying and comparing information of distinct events within the same results. You may also create calculated queries to explore, for example, proportions or percentages using theCalculator. Finally, you may compare results to a previous interval by using theTrendsfunction. Certain analyses run in the past can change based on what user properties are set to since user properties can change over time. A common scenario where changes are seen when running analysis at different points in time, is a direct result of looking at users who were aliased since then. Note on Aliasing: Aliasing runs once on a daily basis and links unknown users to known users. The count of unique users triggering an event seen before the aliasing process may decrease after aliasing finishes processing. You can read more by visiting our docs onuser aliasing.",
    "Helpful Tools": "A few helpful tools are located to the right of the query builder: Data DictionaryProperties ExplorerQuery NotesAssisted Analysis",
    "Roles": "Roles enable you to manage the different permissions within your organization and its projects. You may assign different roles to your teammates, depending on your desired level of access. For example, trusted data engineers may have Admin permissions, while end users may have Member permissions. Similarly, you may limit some users to have view-only permissions. To access Roles settings by open the Account Settings dropdown- usually indicated by your initials in the top right corner of Analytics- then select \u201cRoles\u201d. The default roles enabled for Analytics are: Owner*: full edit accessAdmin**: full edit access to all projectsMember: may not edit Organization, Projects, or TeammatesRead Only: may not edit Organization, Projects, Teammates, or Dashboards, and View-only access for all other areas *There may be only one Owner per project. **Admins have access to all Projects within the Organization. You may also create a custom role by clicking \u201c+ New Role\u201d at the top right of the table.",
    "Rights of Data Subjects": "The GDPR defines some rights of Data Subjects, including: The right to have data concerning themerased. Also known as the \u2018right to be forgotten\u2019.The right toaccessdata concerning them.The right toportabilityof data concerning them, for transfer to another controller. The CCPA defines that consumers have rights of: The right to request the data saved concerning them.The right to request any data collected from the consumer be deleted.",
    "OpenDSR Request Framework": "mParticle is a collaborator on theOpenDSR framework, which provides a simple format for Data Controllers and Data Processors to collaborate towards compliance with requests from their Data Subjects to honor the above rights. This framework was formerly known as OpenGDPR; it was renamed in early 2020 to include CCPA support. To find out more about OpenDSR, read the full spec on theGithub page. mParticle\u2019s OpenDSR implementation handles three types of DSRs: \u201cErasure\u201d, \u201cAccess\u201d and \u201cPortability\u201d.",
    "General Request Workflow": "Each DSR follows the same basic workflow: The data subject submits a DSR to the data controller.The data controller must log, authenticate and verify the request. If they choose to accept the request, the data controller forwards a request to mParticle in its role as a data processor. The request provides:One or more identities for the data subjectThe type of request: \u201cErasure\u201d, \u201cAccess\u201d or \u201cPortability\u201dThe time the data subject submitted the requestAn optional list of status callback URLsOn receipt of the request, mParticle sets the status of the request to \u201cPending\u201d and sends a status callback request to all URLs listed in the original request. This callback includes an expected completion time for the request, which is calculated as: the time it will be scheduled for processing plus 48 hours to ensure the job completes in time.The Data Controller can check the status of the request at any time.When the request is complete, mParticle sends a status callback request to all URLs listed in the original request. For Erasure requests, this callback will simply confirm that the request has been fulfilled. For Access and Portability requests, a download link will be provided.For Access and Portability requests, the download link remains valid for 7 days. Attempting to access the download link after that time will result in a410 GoneHTTP response. This workflow can be managed in mParticle UI or programmatically via theOpenDSR API.",
    "Identifying affected user data": "mParticle stores data against user profiles, each identified by an mParticle ID (MPID). To respond to DSRs, mParticle first matches identities in the DSR against observed user profiles. This is handled the same way as mParticle\u2019s regular IDSync process: provided identities are resolved to MPIDs to identify affected user data. Data subject requests submitted without alogin IDwill not be fulfilled for known profiles that have an associated login ID. For example, if you submit a data subject request that only includes the device ID for a user, mParticle will not be able to find the correct profile to fulfill the request. When finding the correct profile for a DSR, mParticle follows the same identity resolution process used for general identification requests made to IDSync (the mParticle identity management system). The exact profiles returned for a data subject request depend on the specificuser identifierssupplied with the DSR and theidentity strategyconfigured for your account. All DSR requests are scoped to a single workspace by API authentication. If you need to apply a DSR to multiple workspaces, please submit it within each workspace.",
    "Data Subject Request Settings": "To get started, enable GDPR and/or CCPA compliance features on your workspace fromWorkspace Settings>Workspace>Regulation. This will allow you to see the DSR UI. mParticle will honor all requests received via API even with these features disabled. You have the option to include a copy of the live user profile in access/portability requests. Navigate toPrivacy>Privacy Settingsto include a copy of the users profile with GDPR and/or CCPA DSRs. This is for clients whose privacy teams determine that this is required for compliance. The profiles will include: devices, identities, audience memberships, user attributes and calculated attributes. By default, profiles are not included.  The following video explains how to use consent to control data forwarding with mParticle: ",
    "Develop a strategy for accepting Data Subject Requests": "As a Data Processor, mParticle will match user profiles for a Data Subject Request based on any identities we are given. As a Data Controller, it is your responsibility to determine how to accept and forward Data Subject Requests in order to best meet your GDPR responsibilities and manage risk. This decision should be managed in conjunction with your Identity Strategy. You also have the option of using theIdentity APIto identify for yourself the MPIDs you wish to include in the request and submitting them directly, rather than letting mParticle match IDs for you. Be sure to consult your internal privacy and compliance experts when determining your strategy for accepting and forwarding Data Subject Requests.",
    "Erasure": "After mParticle receives an erasure request, a 7 day waiting period starts. This waiting period gives you the opportunity to cancel a pending erasure request before it is initiated. After the 7 day waiting period, any pending erasures are initiated. Once begun, it may take up to 14 days before the erasure is complete. For each completed erasure request, mParticle sends a callback to any specified URLs indicating that the request has been fulfilled. By default, erasure requests are completed between 7 and 21 days after being received by mParticle. The initial 7 day waiting period provides an opportunity to cancel a pending erasure request before it is carried out. To skip the initial 7 day waiting period when submitting a data subject erasure request to mParticle, check the option labeledSkip waiting periodin theNew Data Subject Requestmodal. Skipping the waiting period shortens the request cancellation window. This reduces the total time required to complete an erasure request to between 1 and 14 days after it is received by mParticle. If you wish to remove users from audiences or from event forwarding during the waiting period, set a user attribute and apply audience criteria and/or forwarding rules to exclude them. In response to a data subject erasure request, mParticle deletes the data it stored, such as historical event batches, audience data, and profiles. A delete request will also not prevent additional data concerning the subject from being received and processed by mParticle. If the data subject wishes to prevent all future data processing, they will likely need to take additional steps, for example, ceasing to use your service/app.",
    "Access / Portability": "Access and Portability requests are treated exactly the same way, as follows: mParticle identifies the MPIDs that match the request.Just after midnight each Monday and Thursday, mParticle searches for data related to each MPID, including the user profile and historical event batches.mParticle compiles the data into a single text file. This data includes device identities, user identities, user attributes (including calculated attributes), as well as current audience memberships.mParticle sends a callback to any specified Callback URLs indicating that the request has been completed. The callback will contain a secure download link to the text file containing the Subject\u2019s data. If you submit an access and portability request for more than one profile using multiple MPIDs, the data for every profile returned will be included in a single file. Since the resolution process for DSRs is the same as the process for IDSync, an access and portability request that includes only a device ID will not return any profiles that are protected by a login ID. For example, imagine that a user opens your app and is tracked with an anonymous profile, but they do not create an account with a login ID. Later, a different user on the same device opens your app and logs in with a login ID. If you submit an access and portability request but only supply the device ID, then only the data for the anonymous user will be returned. The data gathered in response to an access or portability request will be delivered in a.zipfolder containing many.jsonlfiles (JSON Linesformat).  The zip may contain: profile.jsonl: A file that contains the live profile at the time of the request. This includes: device identities, user identities, current audience memberships and user attributes (including calculated attributes).one or more additional.jsonlfiles: These results are split into many files to avoid a single, large file to make them easier to transmit and process. Controllers are encouraged to re-process the files as they see fit. These files contain the event batches sent to mParticle. Each line of the data files represents a complete mParticle event batch. See ourJSON Referencefor a guide to the event batch format.empty.txt:  A file which indicates that mParticle found one or more MPIDs associated with the identities in the request, but that there is no data available for them. Note that if no records can be found matching the identities in the request, the request for the zip file returns a404error. A sample portability response can be downloadedhere.",
    "Managing Data Subject Requests in the mParticle Dashboard": "In addition to the OpenDSR API, users with theCompliance or Admin and Compliance rolecan create, delete and monitor DSRs directly in the mParticle Dashboard.  To view details about a request, click the Request ID number.",
    "Forwarding Data Subject Requests for Erasure": "You can configure mParticle to forward Data Subject Requests (DSRs) for erasure with one or more integrations.  This detail UI for a data subject request for erasure shows the forwarding status for a request that is being forwarded to three different outputs. The forwarding status field contains different values, depending on the situation: Pendingmeans that a request has been queued for forwarding, but hasn\u2019t been forwarded yet.Skippedmeans that a request for forwarding has been skipped because mParticle could not find suitable identities to forward, either from the original request or the user profile.Sentmeans that a request was forwarded and an acknowledgement of the request to delete the user from the integration was received by mParticle.Failedmeans that an attempt to forward the request was made, but an error occurred.Not Sentmeans that the request was not forwarded, because the request was made using an older version of the DSR API. You must upgrade to the DSR API v3 in order to forward DSR erasure requests. In addition to the forwarding status, the identities that were forwarded are also shown. mParticle determines which identities to forward based on the identities supplied in the original request, the identity resolution strategy, and what identities each output supports: When a single generic identity type (such as email address) is submitted in the erasure request ORWhen multiple generic identities of different types (such as email address and device ID) are submitted in the erasure request and:mParticle resolves it to a single user profile: mParticle enriches the request with all IDs found on the corresponding user profile. mParticle will include all identities supported by the output in the forwarded request.mParticle resolves it to multiple user profiles: mParticle will try to resolve it to a single user profile following your Identity resolution strategy. mParticle then enriches the request with all IDs found on the corresponding user profile. mParticle will include all identities supported by the output in the forwarded request.mParticle cannot resolve it to any user profile: The request may still be forwarded if the vendor supports the ID type provided in the original DSR request.When a single MPID is submitted in the erasure request and:mParticle resolves it to a single user profile: mParticle enriches the request with all IDs found on the corresponding user profile. mParticle will include identities supported by the output in the forwarded request.mParticle cannot resolve it to any user profile and nothing will be forwarded. In the case where the data in a user profile does not match what was provided in the original erasure request, mParticle will use the information from the original erasure request as the source of truth to process and forward the request. Once a request is forwarded, mParticle can\u2019t guarantee that data is ultimately deleted by the integration partner, so confirm that each vendor fulfills the request. If an integration supports forwarding erasure requests, the integration documentation contains a section \u201cData Subject Request Forwarding for Erasure\u201d and that section contains specific instructions and information about which identities are forwarded. To find all the integrations that support forwarding erasure requests, visitIntegrations.",
    "Retention of Data Subject Requests Records": "mParticle retains Data Subject Request records for up to 1 year.",
    "An event is blocked by a forwarding rule": "Forwarding rulesallow you to prevent events from being forwarded to an output according to criteria like the event\u2019s attributes, user consent preferences, or whether the user who triggered the event was logged in. Sometimes, a forwarding rule might block an event that you actually want to send to your output. Using Observability, you can more easily identify and troubleshoot these instances. For example, consider a scenario where we send the following event to mParticle when a user subscribes to a premium service, but we don\u2019t see the event appear downstream in our output. Example event batch First, let\u2019s check to see if Observability flagged any recent traces by logging into mParticle and navigating toObservability > Trace Activity.We clickSort & Filtersand select our input feed underInputs. UnderResultswe select \u201cWarning\u201d, \u201cInsight\u201d, and \u201cNeeds Attention\u201d and clickApply.  We see a trace matching our input and output with \u201cInsight\u201d as the result, which indicates some detected behavior (either intended or not) that we should investigate. We click on theTrace IDto open the details page to learn more.  In the Trace Details page, we scroll through the Timeline View until we see a highlighted span under the section calledTransformations. This includes the error message:\u201cEvent Batch Filtering: The batch was dropped because all events contained within it were filtered out by event attribute forwarding rules. If this is not expected, check the event attribute rules for this connection.\u201dNow we know our event was dropped due to a forwarding rule.  We navigate to the connection between our custom feed input and our webhook output to view our forwarding rules.We go to the Forwarding tab and see that we have a Forwarding Rule set to \u201cOnly Forward\u201d events if they contain the event attributeplan_typeset tobasic.  After re-examining our original event batch, our event\u2019splan_typewas set topremium, which explains why it wasn\u2019t forwarded to our output.We can now either remove our feed filter or adjust it to forward subscription events with either basic or premium plan types.",
    "An event is blocked by a feed filter": "Filtersprovide granular control over exactly which data points are forwarded to a specific output. You can switch OFF certain data points to exclude them from the data you forward, or you can switch ON data points to forward them. Sometimes, you might mistakenly toggle a filter for a data point that you actually want to send to an output. For example, consider a scenario where we send the following event to mParticle when a user subscribes to a premium service, but we don\u2019t see the plan_type event attribute appear downstream in our output. Example event batch First, let\u2019s check to see if Observability flagged any recent traces by logging into mParticle and navigating toObservability > Trace Activity.We clickSort & Filtersand select our input feed underInputs. UnderResultswe select \u201cWarning\u201d, \u201cInsight\u201d, and \u201cNeeds Attention\u201d and clickApply.  We see a trace matching our input and output with \u201cInsight\u201d as the result, which indicates some detected behavior (either intended or not) that we should investigate. We click on theTrace IDto open the details page to learn more.  In the Trace Details page, we scroll through the trace timeline until we see a highlighted span underTransformations. If we click on this, we see the error message:\u201cEvent Batch Filtering: The batch was dropped because all events contained within it were filtered out by event attribute forwarding rules. If this is not expected, check the event attribute rules for this connection.\u201dNow we know our event was dropped due to a feed filter. Let\u2019s check our filter settings to see if we\u2019re unintentionally removing any event attributes.  To find our feed filter settings, we navigate toConnections > Feed Filtersusing the left hand navigation, and then we select theEventstab. The column for Webhook (our output that didn\u2019t receive the event) lists each event type that will be forwarded (or filtered).If we expand theOtherevent type section, we see our subscription event. If we expand this, we see that theplan_typeevent attribute is switched OFF. As long as this filter is enabled, mParticle won\u2019t forward theplan_typeattribute with subscription events.  Now we know we need to remove the feed filter so theplan_typeattribute is always forwarded.",
    "An event is blocked by a data plan": "Data plansallow you to describe the extent and the shape of the data you collect using mParticle. Each data plan contains a list of data points which describe the event names, event attributes, user attributes, and user identities that you want mParticle to ingest. If your input sends data that doesn\u2019t match your data plan, it can be either marked as invalid or blocked entirely, depending on your data plan settings. Sometimes, data plans will block data that you actually want to ingest. We can use Observability to help identify and troubleshoot these instances. For example, consider a scenario where we send the following event to mParticle when a user subscribes to a service, but we don\u2019t see thelanguage_preferenceuser attribute appear downstream in our output. Example event batch First, let\u2019s check to see if Observability flagged any recent traces by logging into mParticle and navigating toObservability > Trace Activity.We clickSort & Filtersand select our input feed underInputs. UnderResultswe select \u201cWarning\u201d, \u201cInsight\u201d, and \u201cNeeds Attention\u201d and clickApply.  We see a trace matching our input and output with \u201cInsight\u201d as the result, which indicates some detected behavior (either intended or not) that we should investigate. We click on theTrace IDto open the details page to learn more.  In the Trace Details page, we scroll through the trace timeline until we see the highlighted span called\u201cEvaluating Data Plan \u2018higgs_data_plan\u2019\u201dunderTransformations. If we click on this, we see the error message:\u201cData Planning: All user attributes in the batch were dropped by the data plan. Not all integrations and use cases support batches with no user attributes.\u201dNow we know our user attribute was blocked by the Higgs Data Plan.  TheSetup Detailssection on the left side of the trace details page gives us a direct link we can click to take us to our Higgs Data Plan.Once on the Higgs Data Plan settings page, we clickBlock Volumeto view the data plan report.  From the Report tab, make sure to select \u201cBlocked\u201d. Under the list of blocked data points, we see our language preference user attribute.  At this point, we can modify our data plan\u2019s block settings to allow unplanned user attributes, or we can add the language preference attribute to our list of expected data points. Note: if you have a quarantine connection configured for your data plan, you can backfill any blocked data.",
    "Accounts, organizations, and workspaces": "mParticle creates a unique organization for you. It\u2019s the container for all data and metadata related to your mParticle. Within an organization, mParticle will create one or more accounts for you, and within each account, you can create one or more workspaces. Your choices for account and workspace setup are important because these choices affect identity and feature provisioning.  These three nested containers provide scoping and functionality for multi-brand and multi-geo use cases, as well as edge use cases. The scope and advantages of each are explained in the following sections. OrganizationMost mParticle customers have one organization which contains one or more accounts. However, some large companies have multiple organizations. No information is shared across organization boundaries. mParticle creates the organization(s) for you.A few features apply at the organization level, includingprofile strategies.You can think of the organization as representing your company.AccountEach organization has one or more accounts. Accounts often represent different functional groups or goals within an organization, for example regional divisions, or Sales, Marketing, and Customer Support. mParticle creates the account(s) for you.Some information is shared across accounts either by default or by enablement:Audiences can be shared across account boundaries withcross-account sharingenabled.mParticle users (people authorized to access your mParticle organization) are shared across accounts and workspaces.User data is sharedacross workspaces by default, but you can request it be shared across accounts.WorkspaceEach account contains one or more workspaces. A workspace is the basic container for data in an mParticle account.  mParticle creates your first workspace, but you can add more at any time.For most use cases, each workspace is its own domain, separate from other workspaces. Some information is shared across workspace boundaries:Audiencesare shared, allowing you to build an audience using data from more than one workspace.mParticle users (people authorized to access your mParticle organization) are shared across accounts and workspaces.Some mParticle accounts have over a dozen workspaces, while others have only one. How you organize data from your app ecosystem is entirely up to you.",
    "Using organizations, accounts, and workspaces": "Use organizations, accounts, and workspaces to manage multiple brands, regions, and to manage custom identity configurations and unique input/output requirements.  You can also use mParticle features likecross-account audience sharingormultiple workspace real-time audiences. To see which accounts and organizations are available: To display your current organization and account, log into mParticle and in the left navigation bar toward the bottom, click the buildings icon. If more than one organization or account has been created, you\u2019ll be able to search for it.To see your current workspace name, log into mParticle and in the left navigation bar, the square in the upper left corner displays your current workspace name. Click anywhere in the square to search for other workspaces or see the settings for this workspace. The Best Bags company sells handbags under several different names, and in several regions of the globe. They can use organizations and workspaces to provide differentiation when needed: Best Bags created workspaces that correspond to three regions: North America, APAC, and Europe. Each brand defines their own audiences and users within a separate organization, since most customers purchase from only one region.Best Bags created accounts that correspond to their different brands: BestieBags, BlingBags, and CarriageBest. In this way, they can create unique inputs and outputs for the same data sources and forwarding destinations, address different governance and compliance requirements, while still being able to share audience membership, since their customers may buy different brands at different times. You can also useidentity scopeto manage how user data is shared between workspaces and accounts. And if you need to share audiences across accounts, you can request that mParticle enablecross-account audience sharingfor your organization.",
    "Managing workspaces": "Click on the name of your current workspace in the top-left corner of the dashboard to open the workspaces menu. From here you can switch into any of your current workspaces, or clickSettingsto open the Workspace Settings page.  From the Workspace Settings page, you can: View daily, monthly and quarterly statistics across all workspaces in this account, including data from both development and production environments.Browse a list of all workspaces in your account.Download the Event Volume Report, which lists all events ingested in the selected timeframe. The report provides visibility into the calculated attributes and audiences created using those events. This is the same report you can download fromData Master > Catalog > Download Report.Create a new workspace - all you need to do is provide a name for the new workspace.Delete a workspace - this will also delete all the workspace data and connection settings. This action cannot be undone, so proceed with caution.Edit a workspace - view theApple App Transparency Tracking (ATT) Defaults, enable GDPR and CCPAregulations, and retrieve the workspace Key/Secret to use with theOpenDSR API. Note that you can\u2019t delete a workspace that is part of aMulti Workspace Audience. First delete or modify the multiworkspace audience, then you can delete the workspace.",
    "Working with web data": "mParticle handles Web data \u2014 collected from a browser client \u2014 a little differently from data collected from native apps. In most cases, data collected by the mParticle SDK is sent to mParticle, and then forwarded on to integration partners server-to-server. There are exceptions to this rule: in cases where a server-to-server integration cannot support all the required functionality of an integration partner, an Embedded Kit may be used. Embedded Kits are extra components added to the mParticle SDK that communicate directly with an integration partner from the app client. While direct communication between the client and partner is the exception for native apps, it is common for web data. A key reason for this is that most of mParticle\u2019s integration partners are not set up to receive web data server-to-server, as they rely on cookie data only accessible to the cookie owner. To support these integrations, the mParticle Web SDK uses the following workflow: On initialization, the SDK checks to see which Web integrations are enabled for your workspace.For each enabled integration, mParticle SDK will fetch the Partner\u2019s javascript and the mParticle wrapper specific to that Partner. For example, if you have enabled the Google Analytics integration, the mParticle SDK will fetch Google\u2019sanalytics.jssnippet and mParticle\u2019sGoogleAnalyticsEventForwarder.jssnippet. We fetch only the integrations that you have enabled in order to keep the page size to a minimum.Any supported events are mapped directly onto the equivalent partner method. For example, when the mParticle SDK logs a Page View it automatically calls Google Analytics\u2019pageviewmethod. To make it easier to work with web integrations, we provide the source code in a public repository, so you can work with the Integration Partner\u2019s documentation and see exactly how we map mParticle methods onto the Partner code. See themparticle-integrations organizationfor a complete list of client-side web integrations.",
    "Platform limits": "mParticle imposes limits on the number and length of attributes that can be associated with events and users. A quick summary of some of the most important limits is below. For more information, see our fullDefault Service Limitsguide. Events An event can have up to 100 attribute key/value pairs.Event names and attribute keys are limited to 256 characters.Event attribute values are limited to 4096 characters. Users A user can have up to 100 attribute key/value pairs.User attribute names, including user identities like email or Customer ID, are limited to 256 characters.A user attribute value can be a list. These lists are limited to 1000 entries.An entry in a user attribute list is limited to 512 characters.A user attribute value that is not part of a list is limited to 4096 characters. Note that Output Services often have their own limits, which can differ from mParticle\u2019s. When planning your implementation, check the documentation for your Output Services in theIntegration Centerto make sure you are complying with their limits.",
    "Tracking protection": "Browsers add third-party tracking protection for end users. The protections affect third-party trackers and their cookies and work in different ways. For example, Firefox Enhanced Tracking Protection (ETP) relies on a list of known trackers to decide what to block. Safari, Chrome for iOS and other browsers with the Apple WebKit engine use Intelligent Tracking Protection (ITP). ITP prevents the browser from loading cookies from a third-party domain. mParticle aligns with this privacy stance. Firefox Enhanced Tracking Protection (ETP)Apple WebKit engine and ITP",
    "User attributes and event attributes": "mParticle ingests data points that are composed of event attributes and user attributes.",
    "Timestamps and ingested data": "When mParticle ingests data, there are two timestamps associated with events: Each event batch has a timestamp.Each event in the batch may have a timestamp. If batches or events have a timestamp that is more than 15 minutes in the future, relative to the server processing the data, that timestamp will be reset to the current server UTC time. Attribute timestamps remain unchanged. If you load data using CSV Import, the batch timestamp is reset to the current server UTC time.",
    "Forward-looking statements": "mParticle strives to be as transparent as possible. Part of this transparency is to share information about products, features, or functionality that we expect to deliver in the future. Forward-looking statements are as accurate as possible given the knowledge at the time of publication. However, no purchasing decisions should be made on the basis of any forward-looking statement, and mParticle may withdraw or change the products, features, or functionality mentioned in such statements.",
    "Conversion vs. Conversion Over Time": " You can toggle a funnel between Conversion and Conversion Over Time analyses. Conversiontracks users through a multistep sequence of events, or \u201cfunnel\u201d. Users who complete steps within the funnel are counted only once.Conversion Over Timedisplays how the funnel performs on a historical basis. Users who complete steps within a funnel can be counted more than once if they complete the funnel over multiple intervals.  Choosing daily, weekly, or monthly determines the length of the funnel entry window. ",
    "Conversion over time between steps": "When Conversion Over Time queries are visualized as a table, you may view the 3 core conversion metrics (Conversion Rate, Average Conversion Time, and Converted Users) between each step in your funnel. With this feature, you can identify what steps in your funnel are driving drop-off or increased conversion, just like you can with a standard funnel analysis. Take the following funnel: Blog ViewDownload AppStart App  In the Table visualization, the three funnel metrics are displayed not only for the entire funnel, but for each step within the funnel:  The specific funnel steps shown in the table can be toggled using the Steps dropdown in the query settings bar: You may also toggle on and off the metrics displayed via the Metrics dropdown in the query settings bar. For each step, you can view: Total Conversion Rate(the percentage of users who completed that step compared to the previous one).Converted Users(the number of users who completed that step).Average Conversion Time(the average time it took to move from that step to the next).",
    "Visualization area": "Your aggregated Customer Journeys will be displayed in the visualization area, just below the Query Menu. Your Journeys chart contains a number of components. ",
    "Event": "Each event in your Journeys chart contains a label. This label displays the number of users in this particular event at this particular step, along with the percentage of users in this particular event out of users at this particular step. To see the number of users coming from one event to the other, hover over the path linking the two events.",
    "Drop off": "This is an aggregation of the users for which the event in the prior step is the last observable event.",
    "Other": "This is an aggregation of the users who do not meet the percentage threshold selection in the query menu.",
    "Find Your Data Points": " The first time you visit the Data Filter, you will see an empty grid. To start filtering you need to: Choose your input source.You can filter data from \u2018Platforms\u2019 (iOS, Android, Web, etc.) or from incoming Feeds. Note that data for all your platforms will be included on one page in the Data Filter, so if you have used different event names or data types for different platforms, make sure you account for all of them.Add outputs.You can add an output to each column by clicking the+, or select and sort your outputs in the Choose Integrations dialog by clicking the button above, and to the right of, the grid.Choose a Data Type.Depending on the data available in your workspace, you can select from up to four data tabs: Events, Users, Screens and E-Commerce.",
    "Disable Data Points": "You can filter event data at 3 levels: Event Type, Event Name and Event Attributes. Disabling an event type will disable forwarding for all events and attributes of that type.Disabling an event name will disable forwarding for all that event\u2019s attributes. ",
    "New Data Points": "The Filter allows you to disable any current data point from being forwarded to any current output service. However, as you continue to use mParticle, update your app instrumentation, and add inputs, you will continue to generate new data points. One of the most important decisions to make in the Filter is how to handle new data points for each output service. By default, mParticle will automatically forward new data points to each output service. If you uncheck theSend new data points by defaultbox, no new data points will be forwarded to that output until you explicitly enable it in the Filter. New data points will be added to the Data Filter the first time mParticle receives them. It should only take a few minutes from mParticle receiving a data point for it to be visible in the Data Filter. ",
    "Shortcuts": "A mature mParticle project may have hundreds of events. That\u2019s a lot of filter switches. To save you time, we provide some convenience methods to help you set your filters quickly. To access the shortcuts for an output service, click the elipsis near the top of the column.  From this menu you can: Turn all filters on or off.Copy all settings for an output and apply them to another output.",
    "Create a Predictive Audience": "The first way to leverage Predictive Attributes is to them them as inclusion criteria for an audience, therefore creating aPredictive Audience. To maximize the value of your Predictive Attribute, review the information in theprediction details, and determine the likelihood range that meets your campaign goals and budget considerations. Higher likelihood rangeswill contain fewer total users with a greater likelihood of conversion. If your primary aim is to maximize the efficiency and ROI of a campaign intending to drive a specific action like driving purchases or membership upgrades, it likely makes sense to target higher likelihood percentiles.Lower likelihood rangeswill contain more users with a lower likelihood of conversion. If you have a broader objective like increasing brand awareness or driving general user acquisition, targeting users with a slightly lower conversion likelihood that still exceeds the average may be best for accomplishing your goals.",
    "Specify your likelihood range within a Predictive Audience": "Once you have determined which user likelihood range to use, you need to manually set it as a Predictive Attribute within aPredictive Audience, or update it on an existing Predictive Attribute within a Predictive Audience. To do this: Create or navigate to the Predictive Audience in which you want to apply your Predictive Attribute.Add a new Audience criterion.SelectUsersfrom the dropdown.Open theChoose User Featuredropdown and selectUser Attributes.Select your Predictive Attribute from the left-hand dropdown.Use the comparison operators to target your desired likelihood range.SelectActivateto re-calculate your Audience with your Predictive Attribute. For example, to target customers in your prediction\u2019s Most Likely Users range, you would do the following: ",
    "Forward Predictive Attributes to a partner tool": "You can forward Predictive Attributes directly to third-party tools in the same way you would with any other User Attribute. To do this, create aconnectionbetween yourCortex feed inputand the partner system of choice as an event integration.",
    "Query Predictive Attributes with the Profile API": "You can also use theProfile APIto query user profiles for Predictive Attributes, then use these attribute values to drive personalized recommendations and content suggestions throughout the entire customer journey.",
    "First step vs. individual steps": "In Funnel, you may choose a date range for only the first step or you may choose a unique date range for each step. All new queries default to \u201cFirst Step.\u201d Once a user enters the funnel, they may complete the subsequent steps at any time after the first step.  If you select \u201cIndividual Steps,\u201d you may select a different date range for each step of the funnel. If a user does not complete the event during the selected date range, then the user will not appear in count of users for that step and they will be considered \u201cnot converted.\u201d Date ranges for individual steps are selected within each row of the query builder. ",
    "Conversion limit": "A Conversion Limit is a type of time-based filter. Users must complete the steps in the funnel within a defined time limit. The time limit is applied to either the entire funnel or on a per-step basis. All new queries default to Unlimited. For a full explanation, see our article onConversion Limits. ",
    "Average conversion time": "The Average Conversion Time is displayed in the top left corner of the results field. It describes the amount of time that it took converted users to complete all of the steps of a funnel. Only users who complete all of the funnel steps are included in the average conversion time calculation. Median conversion time is not available. To increase the accuracy of your average conversion time, consider using a conversion limit to exclude outliers. ",
    "Managing Users": "Admin Users can manage the access of other users in their mParticle Account from theUser Managementtab of their User Settings page:  To add a new user, provide first and last name, an email address, and select the user\u2019s permissions. SeeRolesfor more on permisisons. ",
    "Custom Access Roles API": "mParticle provides theCustom Access Roles APIwhich mParticle account admins can use to create custom sets of permissions, or roles. You can then assign these custom roles to users of your account through the UI described above or with an API call. Learn more in theCustom Access Roles APIdeveloper documentation.",
    "Authenticating to mParticle with SSO and SAML": "mParticle uses Auth0 to authenticate logins to mParticle\u2019s web UI. This allows you to create a SAML/SSO connection to an identity provider of your choice, such as Okta. Using SAML/SSO, or federated identity management and single sign-on authentication, improves your account\u2019s overall security and the security of your customers\u2019 data. To enable a SAML/SSO connection, you must collaborate with mParticle\u2019s support team: Contact mParticle support or your account representitive, and request an ACS (Assertion Consumer Service URL) and an EntityID.Provide mParticle with an SSO URL, an optional logout URL, identity provider domain(s), and signing certificate.mParticle will configure an SSO tenant using the details you provided in step 1.Use the SSO tenant provided by mParticle to implement your existing authentication system and policies.",
    "How audiences are forwarded": "In mParticle, an audience is a set of users who match a given set of criteria. When mParticle prepares to forward an audience, it is broken down into a series of messages about audience membership. Each message contains: The name of the audienceAn identity that can be used for targeting, such as an email address, a device identity or a social media identity.Whether that identity is being added to, or removed from the audience. mParticle then translates these messages into a format that can be read by each audience output partner, and forwards them via HTTP API. Each output deals with audience information a little differently, depending on their data structure, but there are two main patterns.",
    "Direct": "Some audience output partners allow mParticle to either to directly create an audience (some call them \u2018lists\u2019, or \u2018segments\u2019) via their API, or at least to manage the membership of an existing audience. The end result will be an \u2018audience\u2019 in the partner system, containing as many identities from the original mParticle audience as the output can accept. mParticle will continue to update the membership of the audience in the partner system as users are added and removed. Email marketing and social media platforms are usually in this category.",
    "Indirect": "Not all audience output services have a concept of \u2018audiences\u2019 that mParticle can map to. Others don\u2019t allow their audiences to be directly managed via API. In these cases, mParticle usually forwards audiences as some kind of user attribute or tag. Push messaging and other mobile-oriented services often fall into this category. As an example,Braze, has it\u2019s own audience feature, called \u2018Segments\u2019, but it does not allow mParticle to create segments via API. Instead, for each Braze-supported identity in the audience, mParticle sets a tag on the user, named after the audience. You can then easily find matching users in Braze by searching for that tag. The catch here is that it is often necessary for the output service to already have a record of the users you want to target. For this reason, this type of audience integration usually works best when paired with a matching event integration.",
    "Example - Connect an audience to Mailchimp": "Just like event outputs, each audience output will follow a similar setup process, with the exact prerequisites and settings being different for each. This tutorial forwards an audience to Mailchimp as an example. You can follow the same steps with a different output, or create afree Mailchimp accountto follow along exactly.",
    "Create a Mailchimp List": "mParticle sends audiences to Mailchimp via itsList API. For this to work, You need to have already created a list in my Mailchimp account, and you need to know the List ID. You can give your Mailchimp list the same name as the mParticle audience you want to forward. . You\u2019ll also need to create a Mailchimp API Key, which you can do from theExtrastab of your Mailchimp Account Settings. ",
    "Add the Mailchimp output": "Navigate to theDirectoryin the sidebar. Locate Mailchimp and select theAudienceoption.Complete the Configuration Settings. You\u2019ll need theAPI Keyyou created in Mailchimp. All audience outputs will need different settings. This example sets theEmail Typeto \u201cHTML\u201d and disables theDouble Opt-InandDelete on Subscription Endsettings.ClickSave.",
    "Connect your Audience": "Navigate toAudiencesin the left column and open any audience page. This example uses the \u201cPotential Parisians\u201d audience, created in the previous tutorial. Click theConnecttab.ClickConnect Output.Select your Mailchimp configuration and complete theConnection Settings. Again these will be different for every output. For Mailchimp, you just need the List ID of your Mailchimp list. ClickSave.",
    "Verify: Check your list in Mailchimp": "The simplest way to check that your Connection is working is to see if your Mailchimp list is showing subscribers. For most audience outputs, mParticle begins forwarding data immediately and continues to update audiences in near real time. For some outputs, however, the design of the output partner\u2019s API requires that we queue audiences messages and upload at a regular interval. In these cases, we make a note of the upload criteria in the docs for that output. mParticle forwards to Mailchimp in realtime, and you should be start to see results in the mailchimp dashboard within ten minutes. Open theListstab in your Mailchimp dashboard. Find the list you used to set up the connection. If you see a positive subscriber count, your connection is working. ",
    "View Assisted Analysis": "To view the Assisted Analysis, select the icon highlighted below.",
    "Use Assisted Analysis": "To use the Assisted Analysis tool: 1. Run a Segmentation query. 2. Select the icon to the right of your query builder, which displays a menu on the right side of the screen. The first time you use it, you must accept the Terms of Service since this is a beta feature. 3. Choose an option: Insights: Summarizes the chart results into easily digestible insights.Suggestions: Provides suggestions for follow-up analysis or changes to consider implementing. The Assisted Analysis tool will then use AI to generate Insights or Suggestions depending on your selection. You can click Insights or Suggestions again to generate a new response. This third-party AI uses the query and resulting CSV output, not the raw event data. PII will not be sent to the third party unless included in the query or CSV output, for example, if an email address is used in the query filter.",
    "Disable Assisted Analysis": "To\u00a0 disable Assisted Analysis at the project level, click the Settings icon (gear) in the left navigation bar, then scroll down to\u00a0Privacy and Security. Under\u00a0AI Features, click Disabled.",
    "Query builder": " To begin a Journeys query, select your starting point. The first event of a Journey is always required; a user must complete this event to enter the Journeys pathway. To gain a holistic view of your product, you\u2019ll typically want to select the event that represents your customer\u2019s first interaction with your product, such as a \u201cStart Session\u201d or \u201cSplash Screen View\u201d event. It is important to note that the starting event or first step on a journey can be one or more event names. Once you have selected your starting event, the Journeys query is ready to run. The unit of measurement in Journeys is always the total amount of instances the event was performed. The more steps a Journey has; the more query performance will suffer.",
    "Analyzing as a funnel or cohort": " In order to analyze your Journeys query as a Funnel or Cohort, select the event you\u2019d like to start with, and select the Analyze as Funnel/Cohort menu item. Then, you\u2019ll be prompted to select an event in each subsequent step until you reach a singular customer journey. As you click on events the customer journey will be highlighted. When you reach the starting event, your customer journey will load into theFunneltool, or theCohorttool, depending on your selection.",
    "Reverse journeys": "The Reverse Journeys behave the same as regular Journeys. The difference is that they don\u2019t move in chronological order, they move backwards. So its starting step will be the ending step on a regular Journey. A thing to keep in mind is that these reverse journeys are slower to run than regular journeys.",
    "Dashboard settings": "To access your Dashboard Settings: Click the Saved section in the left-hand navigation menu and select a dashboard.SelectManagein the top right corner of your dashboard.SelectDashboard Settingsto open the dialog. From Dashboard Settings, you can perform the following tasks:",
    "Modify dashboard title": "In theDashboard Settingsdialog, type your desired dashboard\nname in the Title field. You can also rename your dashboard by clicking on the\ntitle in the top left corner of your dashboard.",
    "Modify dashboard description": "In theDashboard Settingsdialog, type a description in the\nDescription field. Your new description will be reflected in the top left corner of your dashboard.",
    "Change widget layout flow": "In theDashboard Settingsdialog, select a Widget Layout Flow. Free Flowlets you place your dashboard analyses wherever\nyou choose.Auto Flowautomatically compacts analyses to the top\nof your dashboard. Check out the Dashboard Visualization article for more information on organizing\nyour dashboard.",
    "Change dashboard layout mode": "In theDashboard Settingsdialog, choose how to display your\ndashboard: screen or print mode. Screen Modeis optimized for web browsers. This is the default\nlayout mode.Print Modeis optimized for printing your dashboard and\nfor scheduledreports. In print mode, you may choose to display the following: Show simulated paperShow titleShow descriptionShow date uploaded Check out the Dashboard Visualization article for more information on screen\nand print modes.",
    "Change widget color matching": "The Widget Color Matching feature within dashboard settings allows Analytics users to easily digest the widgets of their dashboards. Widgets that share the same series match visualization colors for easy comparison across the dashboard. Note that undefined values will always be black. To enable widget color matching, go toManage >Dashboard Settings, and then selectOn.  Once selected, widgets in the same series will match:  Note: Colors persist even when opening a widget from a dashboard. Although different widgets may query different events, series can still color match if they share the same breakout value. Notice that the Funnel widget and the Segmentation widget below share the same breakout values. Although both widgets measure different events, their overlapping breakout values share the same color.  Identical series are defined as the following: Segmentation: Identical series are defined by the Total Count Of/Users Who Performed selection, event name, and breakout valuesFrequency: Identical series are defined by frequency groupingsFunnel: Each step within a funnel is defaulted to a green color unless there are breakout valuesJourneys: Defined by event name",
    "Switch to full screen": "In theDashboard Settingsdialog, toggle full-screen mode settings. You may toggle the following: Show titleShow description You may also copy to your clipboard a direct link to a full-screen version of\nyour dashboard. For more information on full-screen mode, visit theOrganize Dashboardsarticle.",
    "Enable public access": "A dashboard may also be made publicly available. This means that anyone with\nthe Public Access Link can view your dashboard. To make a dashboard publicly available, from the Dialog Settings dialog, check the \u201cPublic Access\u201d checkbox. Then, simply click to copy your shareable link to your clipboard. You may\nalso generate a new public link by clickingReset Access.",
    "Filter public dashboards via API": "Analytics supports the ability to filter public dashboards programmatically via an API. You may create variants of your public dashboards by bulk adjusting each analysis\u2019 Date Ranges, Time Zones, Query Intervals, and Property Filters. The Dashboard Settings dialog provides the dashboard variant API endpoint: However, please see our Dashboard Filter API documentation to filter public dashboards via the API. This feature requires advanced instrumentation from a technical resource. Analytics uses an algorithm to balance data \u2018freshness\u2019 and efficient resource utilization when refreshing dashboards. When you apply a new filter to a dashboard or widget, please allow up to 15 minutes for changes to be reflected in the dashboard\u2019s variants. Subsequently, each request to render a dashboard or widget will display the most recently cached result before initiating an update.",
    "Dashboard privileges": "If you are on the Enterprise plan and are the Dashboard Owner, you can control\nwhether or not team members can edit a dashboard. All team members with the correct\nuser permissions can edit a dashboard by default. Dashboard privileges do not\noverride user permissions. To change the dashboard editing privileges for team members: Click theSavedsection in the left-hand navigation menu and select a dashboard.SelectManagein the top right corner of your dashboard.SelectDashboard Privilegesto open the dialog:SelectSelected Teammatesto show a list of all teammates\nwho have sufficient privileges:Click the checkbox next to each name you wish to add or remove, and then selectSave. Teammates are notified when their privileges are updated. Note the following: If you wish to change the editing privileges on a dashboard you don\u2019t own,\nyou can change the dashboard owner by makinga support request.Editing privileges don\u2019t override any other permissions or privileges in\nAnalytics. For example, if a team member has Read Only permissions, they won\u2019t appear in the list of teammates\nto select or exclude.You can duplicate a dashboard even if you can\u2019t edit it. You\u2019ll be the duplicated\ndashboard owner and can make any changes, including the dashboard privileges\nsetting.",
    "Duplicate dashboard": "To duplicate a dashboard: Click the Saved section in the left-hand navigation menu and select a dashboard.SelectManagein the top right corner of your dashboard.SelectDuplicate Dashboard. Select a name for your new dashboard, and select a folder to save your dashboard\nin.ClickDuplicateto finalize your dashboard duplication orCancelif you\u2019ve changed your mind. Note the following: You may also duplicate your dashboard by hovering in the Views dropdown.You can duplicate a dashboard even if you don\u2019t haveediting privileges.",
    "Delete dashboard": "To delete a dashboard: Click the Saved section in the left-hand navigation menu and select a dashboard.SelectManagein the top right corner of your dashboard.SelectDelete Dashboard. You will be asked to confirm if\nyou want to delete your dashboard because any analyses saved to a deleted dashboard\nwill not be recoverable. You may also delete your dashboard by hovering in the Views dropdown. ",
    "How to access the new UI": "You can toggle the new UI on or off any time, at your discretion. To turn the new UI on: Log into your mParticle account atapp.mparticle.com.Click theSettingsbutton at the bottom of the left hand nav bar.ClickGo to New Experience.",
    "How to return to the old UI": "To switch back to the original UI: Log into your mParticle account atapp.mparticle.com.Click theSettingsbutton at the bottom of the left hand nav bar.ClickGo to Classic Experience.",
    "The evolution of mParticle\u2019s UI": "At mParticle, we understand the importance of staying ahead of the curve. That\u2019s why we\u2019re proud to introduce a user interface that doesn\u2019t just update the aesthetic of the platform, but expands its functionality. Inspired by a combination of customer insights and requests, and industry trends, our team has meticulously crafted a user-centric interface that simplifies complex tasks, exposes previously hard-to-find features, and presents a comprehensive, navigable map of your data infrastructure. There are three cornerstones to the new UI: A comprehensive overview map of your entire mParticle implementationAn interactive \u201cmini-map\u201d that allows you to jump between mParticle suitesA contextual left-hand nav bar Continue reading for a detailed overview of what\u2019s changed, and what\u2019s new, in the mParticle UI.",
    "Changes to existing features": "After switching to the new UI, the first thing you\u2019ll notice is a detailed schematic of your entire mParticle implementation. Your data inputs are listed on the left, your connected outputs on the right, and all of the mParticle features and products that you use to manage your data are shown in the center. This is the new mParticle Overview Map, and its job is to give you a bird\u2019s eye view of how data flows through your particular implementation of mParticle. Think of it like an interactive transit map for your data. Every route and station is clearly labeled, and you can click on each input or feature to configure its settings. Every workspace\u2019s overview map will look a little different, depending on the exact inputs, outputs, and features that are configured. You\u2019ll also notice that the left-hand nav bar disappears when viewing the Overview Map. When viewing the Overview Map, navigate to the feature or suite you\u2019re interested in by clicking directly on the map. Once you\u2019ve navigated to a suite, you\u2019ll see the new contextual left-hand nav bar appear.",
    "The Overview Map: your guide to mParticle": "The Overview Map illustrates the direction your data travels in, from your inputs to your outputs, including the various features and tools it passes through along the way. For a detailed guide on how you can interact with your Overview Map, refer to the Overview Map user guide. For a quick summary of what the Overview Map can do, keep reading below.  Your data inputs includePlatform inputs(such as iOS, Web, or Android) andFeed inputs(such as third-party marketing tools or data warehouses).  The mParticle Data Platform includes the different tools and features you use to manage, manipulate, and leverage your data before sending it to your outputs. These tools can be separated into several product suites:  The Overview Map displays all of yourEvent outputs, third-party marketing and data warehouse tools where you can forward your event data. ",
    "The minimap": "We have also added an interactive mini-map that you can use to quickly jump between suites, no matter where you are in the platform. To access the mini-map, hover your cursor over the mParticle button in the bottom of the left-hand nav bar. Click any suite to navigate directly to that area of the platform. ",
    "The contextual navigation bar": "In addition to the mParticle Overview Map, the new UI includes a contextual left-hand navigation bar that makes it easier to access (and navigate between) specific features within each mParticle suite. While the new overview map shows you what features are related, and where they sit in your data\u2019s journey through mParticle, the updated left-hand nav bar shows you the most relevant tools and options for each specific mParticle suite. The new contextual nav bar includes a Jump To menu that shows links to other areas of the product that are the most relevant features for the particular suite you are viewing. Simply hover your cursor over Jump To in the left nav bar to view these options. When viewing the Oversight suite, the left-hand nav bar displays links to: System AlertsDSRsPrivacy The Jump To menu gives you quick access to the following relevant related tools: TrendsSetupUser ProfilesJourneys  When viewing the Data Platform suite, the left-hand nav bar displays links to: TrendsSetupLive StreamData CatalogTransformationsEvent Forwarding The Jump To menu gives you quick access to the following relevant related tools: System AlertsUser ProfilesCalculated AttributesJourneys  When viewing the Customer 360 suite, the left-hand nav bar displays links to: User ProfilesEnrichmentCalculated attributes The Jump To menu gives you quick access to the following relevant related tools: Data CatalogJourneysSetup  When viewing the Segmentation suite, the left-hand nav bar displays links to: AudiencesJourneys The Jump To menu gives you quick access to the following relevant related tools: Data CatalogSetup  When viewing the Predictions suite, the left-hand nav bar displays links to: PipelinesProjectsDataAPIsInsights  When viewing the Analytics suite, the left-hand nav bar displays links to: My Hub: hover your cursor over My Hub to view links to each Analytics toolSavedData ",
    "FAQ": "We know it takes time to get comfortable with a new interface, but here are some common questions and answers:",
    "Where is the left-hand nav bar?": "The left-hand navigation bar has been replaced with an updated nav-bar that provides contextual links: it displays the most relevant features for each particular mParticle tool suite you\u2019re currently viewing. However, when viewing the Overview Map, the nav bar is removed completely because the Overview Map provides links to each mParticle suite.",
    "Why can\u2019t I access everything?": "The overview map showcases how the entire platform works together. There might be some features that you can see in the overview map that you cannot see when you click into the feature. If you do not have access to a particular feature in the platform, you can request access from your admin.",
    "Why are some of my inputs and outputs missing?": "The overview map was created to show event flow. However, we have heard from customers that they want to see their entire data flow. We are working on an update to show additional Output categories, including Audiences.",
    "Questions": "How does interaction with the orientation screen affect onboarding flow conversion?What is the impact of the A/B test on onboarding flow conversion?",
    "Create Funnel Visualization": "To create our funnel, let\u2019s drag in the events in our new onboarding flow. Connect PetCam: Within the app, users click this button to begin the flow.Open PetCam: Once connected, users open the PetCam and start streaming.Subscribe: Users who appreciate the service will subscribe for future access. ",
    "Make a Step Optional": "The eventStart Support Chatis an optional action by the user. A user may choose to skip the orientation screen or go through the orientation, before progressing through the flow. However, our current funnel requires users to have completed the eventStart Support Chat. To also see what our numbers look like for users who chose not to Start Support Chat, we can utilize Analytics\u2019optional stepfunctionality.  Let\u2019s click the pushpin to the left ofStart Support Chatin the query builder.Next, let\u2019s execute the query to update it. Above, we can see a Funnel visualization with multiple paths. It is currently showing the onboarding flow with theStart Support Chatevent included. As we can see, the numbers are the same as in our initial funnel.  Let\u2019s click on the path aboveStart Support Chatto look at the flow that excludes this event.",
    "Create a Shared Breakout": "TheShared Breakoutfeature in Funnel allows us to segment a specific step using a property and follow those users throughout the flow.  UnderneathConnect PetCamin the query builder, select +group by. ChooseAB Groupunder Event Properties.By default, the query results will be grouped by the first events\u2019 AB test. This is because we want to measure what happens to users after seeing the different instructions for connecting the PetCam. This segments the users who didConnect PetCamby their group and follows them through the rest of the funnel. We can then measure and compare conversion rates and drop off.",
    "Compare AB Groups": "By clicking on a wedge representing either Group A or Group B, we can highlight this group. After a group has been highlighted, the percentages in the webbings will only reflect step-to-step conversion for that group.  Above we can see that 94.63% ofGroup Ausers progressed fromConnect PetCamtoOpen PetCam. ForGroup B, this number was 94.91%.",
    "How to access the Overview Map": "The Overview Map is the first page you see after logging into your account atapp.mparticle.com. You can navigate back to the Overview Map at any time by clicking the Overview button at the bottom of the left hand nav bar.  The Overview Map is unique to each workspace in your account, because each workspace may contain a different configuration of inputs, outputs, and mParticle features. If you can\u2019t find an input, output, or feature that you are expecting to see on the Overview Map, make sure you have selected the correct workspace.",
    "Overview Map settings": "The Overview Map has a few basic settings allowing you to control its appearance, and the type of information it displays.",
    "Switching views": "The Overview Map provides three different views. Each view is designed to highlight a different collection of information describing your mParticle account. You can switch between different views of the Overview Map using the View button to the left of the zoom controls.  The Connections view only displays your inputs, outputs, and the connections between them. To see which inputs and outputs are connected, hover your cursor over any input or output, and a highlighted purple line will appear showing the connections. Every output connected to a single input will be highlighted. For example, in the map below you can see that the iOS input is connected to the Amplitude and Amazon Redshift outputs, in addition to a selection of other outputs. If your workspace contains a large number of inputs or outputs, you can view them in a complete list by clicking the+ more Eventsbutton.  The Overview view provides a simplified diagram of the feature sets that you have configured in addition to your inputs and outputs.  The Data Flow view provides a complete diagram of each specific feature your data flows through between your inputs and outputs, including Rules, Data Plans, and Filters. ",
    "Navigating with the Overview Map": "The Overview Map is interactive: you can use it to navigate to different products, features, and settings within mParticle. If you see a product or feature shown on the map that you can\u2019t click on, your role doesn\u2019t have adequate permissions to view it. Contact your mParticle account administrator to request access.",
    "Inputs": "Your inputs are the sources of your customer data. You can create Platform inputs, which use the mParticle SDKs to collect and ingest data directly from different device platforms (like iOS or Web), or Feed inputs, which ingest data from third-party marketing or data warehouse tools. All of your inputs are listed on the left side of the Overview Map, with the Platform and Feed inputs separated into two groups. To add an input, click the+ Addbutton next to Platforms or Feeds under INPUTS to create an input of either type. This opens theSetup > Inputspage, where you can create a new Platform or Feed input. To view connections between an input and your outputs, hover your cursor over an input\u2019s card and a highlighted purple line will appear between the input and any connected outputs. To add a connection from an input, click the+icon next to an input card.  This opens theConnections > Connectpage for that particular platform input, where you can select one of your configured outputs to connect the input to.",
    "mParticle Suites": "The mParticle platform includes several suites, or collections, of tools that you use to monitor, manage, and leverage your customer data. There are six different suites, and they are are displayed in the center of the Overview Map:  This group of features allows you to monitor the health of your data infrastructure and manage your data privacy settings. Features here include: System AlertsObservability (currently in beta)Privacy ControlsData Subject Requests  The Data Platform features provide an overview of what data is flowing into mParticle, both in real time and historically. Features include: TrendsLivestreamData CatalogEvent Forwarding  Customer 360 is the collection of tools and features related to your customer data, including: User ProfilesGroup IdentitiesCalculated Attributes  From the Customer 360 Profile, you can gain a deeper understanding of your users\u2019 behavior, and make predictions based on their behavior to power a highly personalized, omnichannel customer experience: Predictions: discover how best to engage with your users with mParticle\u2019s machine learning and AI powered predictive analytics suiteAnalytics: gain actionable insights into how your users behave and interact with your brandSegmentation: create groups of related users and perform user journey analytics, testing, and cross-channel orchestration ",
    "Data platform management": "mParticle provides several data management tools that help you control exactly what data you ingest from your inputs, what data you send to your outputs, and to improve the quality of your data. These tools areData Plans,Rules, andFilters, and they\u2019re shown on the Overview Map at the intersections between the mParticle Suites and your inputs and outputs. ",
    "Data Plans": "Data Plans allow you to improve the quality of the data you ingest by validating inbound data against a schema. Data that doesn\u2019t fit the schema you define isn\u2019t ingested by mParticle, ensuring that you have high quality when creating Audiences, Predictions, or when forwarding data to your downstream marketing and engagement tools. Data Plans only operate on data as it is ingested from inputs into the mParticle Suites. To view your Data Plans, click thePlansbutton on the Overview Map. Learn more about Data Plans.",
    "Rules": "Rules are scripts that manipulate data that is either being ingested from an input, or forwarded to an output. They allow you to cleanse, enrich, or transform your data by changing event names, modifying event or user attributes, or adding or removing events from a batch. Rules can be used on both inbound and outbound data. To view or add Rules, click theRulesbutton on the Overview Map. Learn more about Rules.",
    "Identity Resolution": "IDSync, mParticle\u2019s identity resolution framework, allows you to manage how your users are identified wherever, and whenever, they engage with your brand. IDSync works in tandem with Customer360 to give you control over exactly what data is attributed to which user profiles. The Identity Resolution process is noted on the Overview Map between Data Platform and Customer360, but it is not currently navigable via the Overview Map. To manage your IDSync settings click theSettingsicon in the left nav bar, clickPlatform, and select the Identity Settings tab. Or, click on the identity resolution circle. Learn more about IDSync.",
    "Filter Data": "You can filter the data shown in the Live Stream in several ways: Inputs: Select an individual Platform or Feed to show only data from that input.Outputs: Select an individual output event configuration in your workspace. If you set this filter, you must also set Message Direction to either Outbound or Both In and Out.Message Direction: Select Inbound, Outbound, or Both In and Out. Inbound messages are data arriving in mParticle from instrumented apps or from partner feeds. Outbound messages are data sent by mParticle to an output service.Device: Often, during testing, you will want to monitor a specific test device. The Device drop-down allows you to choose a device to monitor from all devices that are currently sending messages to all workspaces in the account, as well as all devices that have been saved. Live Stream shows only development data, but if you filter for a specific device, the Live Stream will also show events from the Production environment. When attempting to match a device to a device ID, mParticle will look for the following per platform: iOS: IDFA (ios_advertising_idin the Events API)Android: GAID (android_advertising_id)Weband other platforms: Device Application Stamp (mp_deviceid) To save a specific device: ClickAdd/Edit Deviceto display the Device list.Click+next to the device you want to add, or clickAdd New Deviceto display the Add Device form.Enter/Modify the Device ID, Device Name, Device Type and clickAdd Device.ClickSave.",
    "Query Builder": "To begin a Funnel query, select either Conversion orConversion over Time. Conversion tracks a single customer journey funnel within a defined date range.Conversion over Time plots the funnel on a chart and compares the funnel across time intervals. The unit of measurement in Funnel is always Users who Performed. The first event of a funnel is required \u2014 a user must complete this event to enter the funnel. You may categorize your results by using theGroup Byfunction or theFilter Wherefunction. You may also combine multiple events using a For clause.  Select a different time zone from your project time zone on a per-query basis by toggling the globe icon on the top right of the query screen.  You may choose to visualize your funnel either as a multipath donut or as a stacked bar. Multipath donuts are optimized for comparing multiple funnel paths in the same visualization, whereas stacked bars are better for viewing drop-off. To switch between the two, select the visualization dropdown from the menu bar.  There are a number of available functions for each query row, including Group By, Filter, and Did [Not] Perform. While hovering the cursor over each query row, a number of options appear in the top right corner. You can create aCustom Eventfrom a query row that contains a filter where clause. Duplicate your row for speedier analysis, or minimize your row if it\u2019s taking too much room on your screen. Finally, you \u2026  After completing the first query row, you may add an unlimited number of steps to represent a full user journey. Name your steps on the right hand side of each query row, and reorder the rows by dragging and dropping. Once you have selected all of the events in your journey, select the run query button within the query visualization. ",
    "Customize your funnel": "All funnel queries can be customized further using the menu. Here, you can select thedate rangefor your query. Users in your funnel must enter the journey within the specified date range. You can also establish aConversion Limit. A conversion limit requires all users to complete the funnel within a defined time limit. The limit can be applied to the entire funnel or between each step.  In the settings dropdown, further customize your analysis by selecting aBreakout mode,Funnel DirectionorConversion Precision: Breakouts may be applied to a single step and then distributed throughout the funnel, or they can be applied to each step of the funnel independently.A Forward funnel means that a conversion rate is calculated for each step in the funnel. A Reverse funnel means that the contribution from the preceding step is calculated as the contribution rate for each step in the funnel.A \u201csequential conversion precision\u201d means that users must complete each step in the funnel in chronological order, with distinct timestamps.Finally, an \u201capproximate conversion precision\u201d means that events in the funnel may occur within the same second of the preceding step and still count as converted.  You can also create aMultipath Funnel, and make one or more steps in the funnel optional. To do this, click the pin icon next to a step in the query builder or in the visualization. The results will display multiple paths that a user may complete. You may then analyze multiple user journeys in the same visualization. In the settings dropdown is a setting for Path Exclusivity. In aninclusivemultipath funnel, users completed the steps in the selected funnel, andmay or may nothave completed the optional steps.In anexclusivemultipath funnel, users completed the steps in the selected funnel, anddid notcomplete the optional steps. ",
    "Add or Remove Teammates": "To add a new teammate, select \u201c+ New Teammate\u201d at the top right of the table, then make your selections. Select \u201cAdd Teammate\u201d to send an invitation directly to their email address. To remove a teammate, locate the user within the table, then click \u201cRemove from Project\u201d or \u201cRemove from Organization\u201d at the far right. You may also use the check box to select one or more teammates, then click \u201cRemove Selected from Project\u201d or \u201cRemove Selected from Organization\u201d to remove them.",
    "Edit Roles": "To edit the role of a teammate, locate the user within the table, then click on their corresponding Role. You may select from the available options, then confirm your selection.",
    "Teammate Status": "A teammate with \u201cPending\u201d status has received an invitation to join Analytics, but has joined the organization and / or project. The user should check their email inbox to complete their registration. A teammate with \u201cActive\u201d status has joined the organization and / or project. No further action is required.",
    "Identity": " Displays summary metrics for identity data collected about your users: % of Users: The percentage of users that have a given identity type.Overlaps: The percentage of users and count that have a combination of the two given identity types. This section is filterable by the date range, environment and input options at the top of the page.",
    "Connections": " Displays summary information representing the current workspace\u2019s usage, showing the following metrics: Inputs Active Platforms: Count of platforms configured with credentials.Active Feeds: Count of configured inbound feeds.Custom Events: Count of custom events received.User Attributes: Count of user attributes received. Outputs Active Event Integrations: Count of active event configurations.Active Audience Integrations: Count of active audience configurations.Active Real-time Audiences: Count of active real-time audiences.Standard Audiences: Count of \u2018ready\u2019 Standard Audiences.",
    "User Activity": "The User Activity view allows you to see a detailed summary of data associated with a single user. Note that only users with the Admin or Support roles can access the User Activity view. To find a user, begin entering any known ID for the user, an email address, customer ID, device ID, etc. The ID must match exactly to return a profile. If multiple profiles are returned, select the user you want from the list.",
    "User Search": "You can search for any element in your Identity Set.\nTo perform a search do the following: Navigate to theActivity>User Activityscreen. The User Search dialog displays on the screen.  Enter the search terms in the search field.You can narrow your search by clicking the Any Profile field drop-down and selecting any field that is part of your Identity Set.You can search for users that belong to agroup identityby searching for the group identifier or group attribute for the group.Click the Search button. The screen refreshes and search results are displayed in the Information screen. View individual results by clicking a row. From the individual results you can do the following: Submit a New GDPR Request. For more information about GDPR requests, seeSubject RequestsView User Events by clicking theEventstab or by scrolling to the bottom of the screen and clicking theView User EventsbuttonExpand any of the sections on the Information screen for more information about your results The information shown on this screen is explained in the following sections.",
    "Information": "The following user information will be displayed if available: mParticle IDCustomer IDEmail AddressFirst and last seen dates (across all workspaces) First and last seen dates (for the individual workspace)Data Inputs and Partner Feeds the user has appeared in.Device ID - only one Device ID will be shown, for the most recently seen device. The Advertising ID (IDFA or GAID) will be shown for preference, with the Vendor ID (IDFV or Android ID) as a fallback. List of all devices, including the date the device was last seen, device platform (iOS, Android, etc) and whetherLimit Ad Trackingis enabled for the device. List of all available user attributes. Display priority is given to reserved user attributes: AgeFirst NameLast NameGenderMobile NumberAddressCityStateZip CodeCountry All other user attributes will be displayed in alphabetical order. User attribute values may be updated by different inbound data streams. For example, a profile may be created, attributes may be added, updated, or deleted. Each change triggers a timestamp change for the altered attribute. When resolving differences in attribute values for the same profile, the value with the latest timestamp is used. A list of campaigns, showing Partner and Campaign name. A list of allAudiencesthe user is a member of.",
    "Events": "The Events tab will show a timeline of historical event data from the user,up until the previous day. Events can be filtered by Date, Input, Event Type and Device. In addition to filtering, you can also choose to highlight selected event names in the timeline. Events are grouped in batches, with the input source in bold. Click a batch heading to view common attributes for the batch, or an event name to view attributes for the event. eCommerce events show details for the transaction and also for each product, including quantity and total price per product.  Click the Link button in the details view to copy a sharable direct link to the event to your clipboard. As with theLive StreamYou can expand any event or batch to view the raw JSON data. ",
    "Edit a user profile": "The User Activity View allows you to manually edit certain properties of individual user profiles. This can be useful if you need to correct profile data and you want the updated profile data to be immediately forwarded to your downstream connections. Whenever you modify user profile data, you must select one of your configured inputs. This allows mParticle to route your changes through that input, updating the user profile, before finally forwarding the changes to any active, connected outputs. To edit a profile\u2019s user attributes: Navigate toActivity > User Activityusing the left-hand nav bar.Find the profile you want to edit by searching for one of the profile\u2019s identifiers under User Search.On the Information tab for the profile, scroll to the User Attributes section and click theEditpencil icon next to the mParticle workspace where you want to make your changes.Remember that user attributes are scoped at the workspace level, so a profile that exists in multiple workspaces may contain different attributes in each workspace.  In the Edit modal, select one of your configured and active inputs. The changes you make will be sent to every output that is connected to the input you select. If you don\u2019t want your changes sent to your downstream outputs, create or select an input that has no connected outputs.  The Edit modal displays a list of the profile\u2019s attributes and attribute values. To change an attribute value, modify or replace it using the text box to the right of the attribute\u2019s name.  To permanently delete an attribute from a profile, click thedeletetrash icon next to the attribute.  After you\u2019ve made your changes, clickUpdate. Navigate toActivity > User Activityusing the left-hand nav bar.Find the profile you want to edit by searching for one of the profile\u2019s identifiers under User Search.On the Information tab for the profile, scroll to the Consent and Compliance section and click theEditpencil icon next to the mParticle workspace where you want to make your changes.Remember that user consent and compliance values are scoped at the workspace level, so a profile that exists in multiple workspaces may contain different values in each workspace.  In the Edit modal, select one of your configured and active inputs. The changes you make will be sent to every output that is connected to the input you select. If you don\u2019t want your changes sent to your downstream outputs, create or select an input that has no connected outputs.  The Edit modal lists the profile\u2019s consent purposes (which are not editable) and the corresponding consent values. You can edit a consent value by entering eithertrueorfalsein the text box.  After you\u2019ve made your changes, clickUpdate. Navigate toActivity > User Activityusing the left-hand nav bar.Find the profile you want to edit by searching for one of the profile\u2019s identifiers under User Search.On the Information tab for the profile, look for the User Details section and click theEditpencil icon.Remember that user identities values are scoped at the organization level: a user\u2019s identities are consistent between all workspaces in your mParticle organization.  In the modal that appears, select one of your configured and active inputs. The changes you make will be sent to every output that is connected to the input you select. If you don\u2019t want your changes sent to your downstream outputs, create or select an input that has no connected outputs.  To edit one of the profile\u2019s identities, enter the new value in the text box to the right of the user identity.  To delete an identity from a profile, click thedeletetrash icon.  After you\u2019ve made your changes, clickUpdate. To add an identity to a user profile: Repeat steps 1-4 ofEdit a profile\u2019s user identities.ClickAdd Identity.  Use the left dropdown menu to select the type of identity to add, and enter the value of the new identity in the text box on the right.  ClickUpdate.",
    "System Alerts": "The System Alerts dashboard reports all errors returned when forwarding data to your connected outputs. This dashboard helps you to find any connections that are failing to forward data, and it can help you to begin debugging a connection you already know is experiencing problems by highlighting the specific errors reported.",
    "System Alerts dashboard": "To view system alerts, log into your mParticle account and navigate toActivity > System Alertsin the left nav bar.  The System Alerts dashboard lists all of your connections sorted by their alert volume, from high to low according to alerts reported during the last 12 hours. You can view alerts reported during different date ranges by using theDatedropdown menu. To view alerts reported for only the production or development environment, select the environment using theEnvironmentdropdown menu. You can also look for a specific connection with the search bar.",
    "Alert details": "To view details on the specific alerts reported for a connection, select it from the list labeledConnections.  The alerts displayed are sorted by volume and organized according to their type. To view the volume of alerts for each input, click the+icon next to the alert type. You can search for a specific alert using the search bar on the System Alerts dashboard.  The following alert types are supported:",
    "Event Forwarding": " The Event Forwarding report provides information on your app\u2019s incoming event data and the data that is forwarded to enabled output event services. This is where you can verify how much data mParticle captured per event, and how much data was forwarded.  If multiple configurations are enabled for an integration, the data sent to each configuration will be shown separately. We recommend checking this report if you notice any discrepancies in your vendor dashboards. The report displays a daily summary of events, plus counts for each message type and event name, along with the inbound and outbound counts for each enabled service.  By default a summary of data for a full day is shown, but you can also select a specific hour to display hourly data. You may notice differences between an inbound data count and an outbound data count. There are several reasons these differences may occur. Here are some good questions to start with: Did you enable/disable the service on the date in question?Are you using data filters?Is the message type not supported by the integration?Have you chosen configuration or connection settings that exclude certain message types?Are you sendingcommerce eventsthat are expanded before being forwarded?",
    "Commerce Event Types and Event Forwarding": "Four events, called commerce events, capture information about financial transactions: product commerce events, purchase or refund commerce events, promotion commerce events, and impression commerce events. Commerce events behave differently from other events: If the event is forwarded using an mParticle SDK with an embedded kit that doesn\u2019t implementlogCommerceEvent, then the event is expanded to ensure that no data is lost.If the event is forwarded server-to-server or using an mParticle SDK with an embedded kit that does supportlogCommerceEvent, then no expansion is needed, and no data is lost. In addition, the expansion behavior is different depending on the commerce event type: Product commerce events and impression commerce events expand to one event per product.Purchase or refund commerce events add an additional event with the total value.Promotion commerce events expand to one event per promotion action type such as \u201cclick\u201d or \u201cview.\u201d For more details, see theiOS SDKorAndroid SDKdocumentation.",
    "First Time vs. Recurring Behavior": "First Time:Only takes into account the first time a user performed the target behavior during the selecteddate range. Use the first time mode to model the distribution of time it took for users to first perform a target behavior. For example, if the selected time interval is daily, and a user performed the event on three different days, they will only be counted in the data for the first day on which they performed the event.Recurring:Shows the number of users who perform the target behavior over time. Users who performed the target behavior multiple times over time are considered in this analysis. For example, if the selected time interval is daily, and a user performed the event on three different days, they will be counted in the data for each of those three days. To toggle between the First Time and Recurring cohort analysis modes, click on Recurring in the Query Builder: ",
    "Complete vs. Incomplete": "In queries that use a generational breakout, such as hour, day, week, or month of the event, the setting for Only Complete/Include Incomplete controls how the cohort visualization accounts for periods with incomplete data.  If the query is set to Only Complete, only cells where all users have had a chance to complete the target behavior in the analysis will be shown. For example, consider a query showing users who first performed the event Blog View and returned to perform the event Subscribed, grouped by Week of Blog View.  With the visualization set to show Only Complete, results from this week will not be displayed. This is because users who entered the analysis by Blog View in the week have not yet had a full week to be able to complete Subscribed and show up in the completed analysis.  If the visualization is set to Include Incomplete, an extra series of data will be displayed in the cohort. These numbers will typically be lower than the completed figures, because there has not yet been enough time to capture the full extent of users who are completing the parameters of the query. By utilizing the Only Complete/Include Incomplete setting, you can customize your results so that you\u2019re viewing data as it comes in, or so that you filter incomplete data from your analysis.",
    "Cumulative vs. Non-cumulative": "The Non-Cumulative and Cumulative cohort analysis modes determine how your cohort data is analyzed over time. To toggle between the non-cumulative and cumulative cohort analysis modes, click on Non-Cumulative in the menu bar, just below the Query Builder.  Non-Cumulative Percent:Displays the percentage of users who completed the target behavior for each selectedtime intervalorbreakout.Non-Cumulative Count:Displays the count of users who completed the target behavior at each point in time defined by the selected time interval.Cumulative Percent:Displays the percentage of users who completed the target behavior for thefirst timeas a running total over time.Cumulative Count:Displays the count of users who completed the target behavior for thefirst timeas a running total over time.",
    "Export Query Results": "To download your results as a CSV file or PNG image file, simply choose the export icon located in the menu bar beneath the query builder, then select a file type. CSV exports are limited to the first 1,000 results. For detailed instructions on how to export results from each tool, see the following articles: Export Results in SegmentationExport Results in FunnelExport Results in CohortExport Results in Users",
    "Export User Data": "There are two ways to export user data from Analytics: Export user data directly from the Users tool: First, you mustcreate a query in the Users tool. Once your query is ready for export, simply select the export icon located in the menu bar beneath the query builder, and choose Download CSV. A CSV file will be emailed to the email associated with your account. There is no limit to the number of users that can be exported.  Export Event Data and Users from Segmentation, Funnel, or Cohorts: You can also download a user list from a Segmentation series or point, a specific step in your Funnel, or a Cohort cell. To download event data from a Segmentation query: Create a Segmentation QuerySelect a data point in the querySelect Download Data and Events to CSVSelect either In Entire Series or In this PointClick Proceed and you\u2019ll be emailed a download link By default, all properties are selected. The export limit is one million events. Read-only users do not have permission to run exports.  To download users: Run a query (Segmentation, Journeys, Funnel, Cohort)Select a data point in the visualizationSelect either Explore Users From Entire Series or From this PointYou\u2019ll get redirected to the Users tool, from which you can download users according to the instructions above",
    "Adjust date range": "To adjust the date range, select \u201cLast 7 Days\u201d under the left side of the query builder to open the date range settings. The start date is the first day to be included in the search. The end date is the last day.  To select fixed start and end dates, use the calendar on the left side of the dropdown. You can also enter a specific date by selecting the date at the top of the calendar and entering a value. Tick the \u201cToday\u201d checkbox to create a dynamic end date. The right side of the date range menu lists the available dynamic date ranges. Choosing a dynamic date range, for example \u201cLast 7 Days\u201d or \u201cLast Full Month\u201d, will automatically update the date range of your query, counting backwards from today. The terminology \u2018Last Full\u2019 refers to a full period of time as defined in calendar terms. For example, \u2018Last Full Year\u2019 will show data from the last year as defined January through December. For the last year of data, use \u2018Last 365 Days\u2019. If you select \u201cLast Full Week\u201d, then Analytics will analyze the most recent complete week, defined as Monday to Sunday. If you choose \u201cLast Full Month\u201d, then Analytics will analyze the most recent complete month. You can quickly navigate the calendar to select full months using the links in the lower left corner of the date range menu.",
    "Custom date ranges": "To save a new custom date range, for example \u201cLast 45 Days,\u201d simply choose \u201cAdd Custom Date Range\u201d in the lower right corner of the menu. Your previously-used custom date ranges will be saved for future queries and are viewable alongside the default dynamic date ranges. ",
    "Adjust time intervals": "In order to visualize your data, the date range will be organized into units of time called intervals. Adjust interval settings by selecting the dropdown menu to the right of the date range settings. Interval options are dependent on the date range selected. For example, if the selected date range is \u201cLast 7 Days\u201d, then the available time intervals will be Hourly, Daily, and Full Range. A Weekly interval setting will not be available because there is only one week in a seven-day date range. If the date range is changed to \u201cLast 90 Days\u201d, then the interval dropdown will include Weekly and Monthly intervals, but will no longer allow Hourly analysis due to visualization constraints. To calculate the total event or user count across an entire date range, select \u201cFull Range\u201d. For information on how to tailor date and time settings for use with a specific tool, see our documentation regarding: Date Range and Time Settings in SegmentationDate Range and Time Settings in FunnelDate Range and Time Settings in CohortJourneys: Date RangeTime Settings in UsersTime and Interval Settings in Dashboards",
    "Period of Day": "ThePeriod of Dayfeature allows you to see your events broken out into five timestamp intervals: Overnight:12:00am - 4:59amMorning:5:00am - 9:59amDaytime:10:00am - 5:59pmEvening:6:00pm - 8:59pmNight:9:00pm - 11:59pm To break out an event byPeriod of Day: Select thegroup byclause under the event.Setgroup bytoPeriod of Day. ",
    "Get some more data": "Up until this point, you\u2019ve been testing your account with a single development build of your app. This works well to establish basic data throughput. The Audiences feature allows you to target segments of your users based on their activity or attributes. So to effectively use Audiences, even at the testing stage, your app needs multiple users! If you\u2019re not ready to enable the mParticle SDKs in your Production app yet, you can either spin up multiple development environments, or try using theEvents APIto supply some test data in bulk.",
    "Create your Audience": "The mPTravel app lets users watch video content about travel destinations. This tutorial creates an audience to allow mPTravel to target users who view content about a paticular destination with deals for that destination.",
    "Create Criteria": "To define an audience, you need to specify some selection criteria. ClickAdd Criteria.Choose the type of criteria you want to create. Except for theUserstype, which is covered below, these criteria all correspond to mParticle event types. ClickEventsto target custom events.There are three distinct aspects of an event criteria that you can define:Event name- mParticle populates a dropdown list based on all event names received for the workspace. This means that you can only select events that have already been captured by mParticle. This example targets the \u201cPlay Video\u201d event name.Attributes- you can refine your criteria further by setting attribute conditions. This example targets only instances of the Play Video event where the \u201ccategory\u201d attribute has a value of \u201cDestination Intro\u201d and the \u201cdestination\u201d attribute has a value of \u201cParis\u201d.Note that this example creates anExact Matchcondition, but there are other types of condition to explore. For example, if you set \u201cdestination\u201dContains\u201cFrance\u201d, then you could match events with a \u201cdestination\u201d of both \u201cParis, France\u201d and \u201cCannes, France\u201d.The types of condition available depend on what kind of data an attribute holds. For example, an attribute that records a number value will haveGreater ThanandLess Thanconditions. mParticle automatically detects what type of data an attribute holds. However, you can manually set the data type by clicking the type symbol.Don\u2019t change the data type unless you really know what you\u2019re doing. If you force the data type to beNumber, and all your attribute values are strings, your condition will always fail! As long as you\u2019re sending the same type of data consistently for each attribute, you shouldn\u2019t have to worry about it.Recency / Frequency- Sets how many times the user needs to trigger a matching event, and in what time period, in order to meet the condition. If you don\u2019t specify anything here, the default forRecency / Frequencyis \u201cGreater than 0 events in the last 30 days\u201d.When you\u2019re happy with your criteria, clickDone.",
    "Add Multiple Criteria": "You could save this audience right now and target all users who have watched mPTravel\u2019s Paris content in the past three days. But, what if you have some extra special limited deals that you want to save for your premium members? You can\u2019t just tell everyone! You need to add a second criteria. Whenever you have multiple criteria, you need to decide how to evaluate them together. There are three options: And- both conditions have to be true for a user to be added to the audienceOr- a user will be added to the audience if either condition is trueExclude- a user will be added only if the first condition is true, but the second is false. Exclude is great for use cases like abandoned cart targeting. You can select users who triggered an Add to Cart event, then exclude users who triggered a Purchase event. To target users who watched Paris content, AND are premium members, chooseAnd.  This is a good opportunity to look at theUsercriteria type, as it\u2019s a little different. Where the other criteria match users who have triggered a particular event, theUsercriteria looks at all other information you might know about your users: the type of device they use, where they live, their custom user attributes, etc. This example targets users with a user attribute of \u201cstatus\u201d, with a value of \u201cPremium\u201d. When you\u2019ve added as many criteria as you need, clickSave as Draftto come back to your definition later, orActivateto start calculating.  When you activate the audience, you\u2019ll be asked if you want to set up an A/B Test. SelectNofor now, to go to the Connections Screen.",
    "Check that size is greater than zero": "After you finish defining your audience you will be taken straight to the Audience Connection screen. Connecting an audience will be covered in the next section. First, check that your audience definition is working as expected. Start by selectingAudiencesfrom the left column to go to the main Audiences page. Audiences take time to calculate, so if you\u2019ve only just activated it, you\u2019ll probably see aSizeof 0 for your audience. Mouseover the pie chart to see how far along the calculation process is.  After a while, as long as you have users that match your criteria, you should start to see the value of theSizecolumn increase.  If the audience is 100% calculated, and your size is still zero, there may be an issue with your conditions.",
    "Download to verify individual memberships": "In some cases, it might be enough just to know that your audience is matching users. However, if you know specific identities of users who should match your criteria, you can check that they matched by downloading your entire audience in CSV form. Follow the instructionshereto download your audience.",
    "Segmentation Group By Clauses": "You may apply a Group By function to each row in your segmentation query. This\nwill categorize results by each Breakout. Breakouts can be hidden from your visualization by selecting them by name in\nthe field below the visualization area. You may view a single Breakout by double-clicking\non its name in the same field.",
    "Group By Numerical Properties": "InSegmentation, query rows whose events have numeric string properties can be grouped by the sum of or the average of event properties. For example, you can find the sum of all purchase prices of Paid Purchases within the last 7 days. ",
    "Multiple Group By Clauses": "Any query row may be broken out using multiple properties. For example, if Open App is broken out by the properties Browser Name and Marketing Channel, the resulting Breakout will include all possible combinations of these values. In this example, the combinations would include: Chrome | EmailChrome | DirectFirefox | EmailFirefox | Direct And so on.",
    "Funnel Group By Clauses": "You may apply a Group By function to categorize users via event property, user property, or user segment.  To isolate each group\u2019s conversion rate, select the corresponding slice of the donut in the visualization area. The Total Conversion Rate and Average Conversion Time displayed in the top left corner, and the conversion rate in the path between each step, will change to reflect only users that are in the selected breakout. ",
    "Cohort Group By Clauses": "Cohorts are defined by a shared generation or a shared property. A generation is a unit of time, such as a month. A monthly cohort would include all users who entered the cohort during that month. A property is a characteristic or attribute, such as device type. Cohorts defined by device type would include all users with an iPhone, all users with an Android, etc. ",
    "Breakout Display Options": "Queries with a breakout are limited to display a maximum of 50 breakout results. Use the selector in the query builder to show the top or bottom 5-50 results. If there are less than five breakout results, the menu will not appear.  To view more than 50 breakouts, export the data from the tool as a CSV file: Exporting Results in SegmentationExporting Results in FunnelExporting Results from Cohort",
    "account": "Each customer has at least one mParticle account, which contains one or more workspaces. All accounts for the same customer are contained within an organization (org). These three logical containers control different types of scope. For example, The Profile API is set to workspace scope, while the Platform API is set to account scope.",
    "act-as feed": "A feed you can configure as if it comes from an iOS, Android, or Web platform (Act as Platformoption in the Feed Configuration). Data from the feed can be forwarded to any output that supports the specified platform type.",
    "alias": "Aliasing is a feature that allows clients to associate anonymous customer events to post-sign up events.  Functionally, aliasing performs a copy operation from a source MPID to target MPID. The source MPID is unchanged and still accessible in the system. This feature is supported by the Profile Link and Profile Conversion identity strategies.",
    "ARN": "Amazon Resource Name. A complete ARN is required for partner lambda integrations and some Amazon Redshift integrations.",
    "attribute": "A key-value pair that provides additional information about an event, user, or product. For example, a custom event Play Video might have the attribute ofcategorywith a value ofdocumentary.",
    "audience": "A set of users connected to an integration for the purpose of engaging those users. Audiences may bereal-timeorstandard: real-timeaudiences are populated based on recently received data.standardaudiences are populated from historical data. If the type of audience isn\u2019t specified, then the reference is likely toreal-timeaudiences unless stated otherwise.",
    "audience real-time lookback window": "A date range for how far back you can look to create real-time (not standard) audience segments, apply event enrichment of profiles, and to keep calculated attribute values up to date after initiatilization. Most lookback windows are 30, 60, or 90 days. Lookback windows are defined in the service agreement and are sometimes referred to as \u201chot storage.\u201d Contrast withdata retention.",
    "AWS": "Amazon Web Services. mParticle accounts are assigned to an AWS region that provides optimal performance.",
    "batch": "The basic processing unit for all mParticle data. A batch contains data about a single user of your app, on a maximum of one device. And includes an array of events along with information about the user and device. You can inspect raw batches in JSON format in the Live Stream and User Activity View.",
    "Beta release": "An early release of mParticle products or features. Seereleasesfor more details.",
    "calculated attribute": "A read-only user attribute with a value that is automatically calculated as new event data is received. Examples of calculated attributes include a total count of events, aggregation of events, the discrete occurrence of events, or lists of unique event attributes.",
    "certified partner": "A company that isa certified solutions partner or technology partnerwith mParticle.",
    "channel": "The type of input by which a batch reached mParticle. Not to be confused with platform. For example, a batch for the Android platform can arrive via three different channels: the SDK, the server-to-server Event API, or an \u2018act as\u2019 partner feed.",
    "client-side": "Data forwarded directly from a device or web browser to an integration partner. Client-side integrations often require a kit to be included with the mParticle SDK. Some client-side kits have a configuration option to work in tandem with a server-side integration. Contrast withserver-side.",
    "cold storage": "Seedata retention.",
    "commerce events": "A special mParticle event type that tracks actions related to products and promotions. Examples of commerce events are Add to Cart, Purchase and Refund.",
    "configuration settings and connection settings": "Settings for event and audience integrations are split into two sections: configuration settings and connection settings. Configuration settings define an output and are reused for each connection.Connection settings are specific to the input (platform, feed, or audience) being connected.",
    "connection": "A configuration that defines how data flows into mParticle (input) or is forwarded out of it (output).",
    "consent": "mParticle lets you track a user\u2019s consent for their data to be captured. Consent is tracked according to a predefined consent framework. mParticle supports the GDPR and CCPA frameworks.",
    "Cortex": "*For existing customers, the appointment of Deel, Inc. is effective as of August 30, 2024. For new customers, the appointment of this subprocessor is effective immediately. **For existing customers, the appointment of Pivoting Owl, Inc. (Thena) is effective as of August 4, 2024. For new customers, the appointment of this subprocessor is effective immediately.",
    "credentials": "A key and secret used to access the mParticle Events API.",
    "custom event": "An event type that can capture any type of user activity in your app. A basic custom event contains a name, a custom event type, and a free-form map of attributes. See alsoatribute.",
    "custom feed": "A feed from any data source including the mParticle Events API. Contrast withact-as feed,unbound feed, orplatform input.",
    "custom mapping": "The relationship between a custom event, screen view, or commerce event and the corresponding event in the integration partner.",
    "data map": "A definition of how one data model equates to another data model in either the same or a different data store.",
    "data model": "A definition of how data objects are structured. In Warehouse Sync, a data model may include a data map.",
    "data plan": "A codified set of expectations about the extent and shape of your data collected with mParticle. Data plans contain data points and metadata: a plan name, plan ID, version, and description.",
    "data point": "An event, user attribute, or user identity that is unique within an mParticle workspace, defined for each type of data received from an input.",
    "data privacy controls": "A set of mParticle features for working with consent and data subject requests.",
    "data retention": "The maximum period of time that mParticle stores profile and event data. The duration of the time period is governed by your long-term data retention policy, which is defined in your contract. Contrast withaudience real-time lookback window.",
    "data type": "The type of data contained in an attribute value. mParticle supports the following data types: string, number, boolean, and date.",
    "data warehouse": "A type of integration partner, such as Snowflake, Google BigQuery, and Amazon Redshift.",
    "development (DEV)": "Seeenvironment.",
    "device application stamp": "A unique identifier generated for each unique device the first time it is seen on a given platform in an mParticle workspace. Some event outputs use the Device Application Stamp (DAS) as part of a fallback strategy when other identities are not available.",
    "device standard": "Term for the device used to access your app or website. Examples of devices include an iPhone, an Android phone, a web browser, or an XBox.",
    "DSR": "From the GDPR specification, a data subject request.",
    "Early Access (EA) release": "An early release of mParticle products or features. Seereleasesfor more details.",
    "environment": "Each event batch is associated with an environment: eitherdevelopment(DEV) orproduction(PROD). All development data can be inspected in the Live Stream to enable debugging. You can also create separate event outputs to handle development and production data.",
    "event": "An event is a tracked user action. Examples of events are a user loading a page, clicking a button, or opening an email. Every analysis in Analytics starts with at least one event to analyze behavioral patterns. You can think of events as the \u201cwhat\u201d that a user has done.",
    "feed": "A stream of data into mParticle from either your own data source or a partner. Seeact-as feed,custom feed,platform input, andunbound feed.",
    "field transformation": "Specifically for Warehouse Sync, a field transformation is a data map between an external data source and mParticle\u2019s JSON schema. Field transformations define which individual key:value pairs of data in an external data source correlate to which key:value pairs of data in mParticle.",
    "filter": "A definition that blocks a data point from being forwarded to a particular output.",
    "forward": "Send data from an input to an output.",
    "Generally Available (GA) release": "The release of mParticle products or features that are typically available to all customers. Seereleasesfor more details.",
    "GDPR": "The General Data Protection Regulation is a set of regulations passed by the European Union. mParticle provides two features to help clients manage their obligations under the GDPR: Consent Management, and Data Subject Request processing.",
    "hot storage": "Seeaudience real-time lookback window.",
    "IAM": "AWS Identity and Access Management. Using a custom AWS Lambda functionARNto apply rules in mParticle requires the configuration of an IAM User and IAM Role.",
    "identity priorities": "The order of precedence for matching user profiles. See alsoidentity strategyandIDSync.",
    "identity strategy": "The strategy that determines which user profile to add data to when the current user (known user) can be identified, and what to do when the current user can\u2019t be identified (anonymous user). You are assigned an identity strategy when your org is created. See alsoidentity prioritiesandIDSync.",
    "IDFA": "Identifier for advertisers on iPhones. An Apple IDFA is similar to an advertising cookie, in that it enables an advertiser to understand that a user of a particular phone has taken an action like a click or an app install.",
    "IDSync": "A set of mParticle features for managing how you identify your users across devices:identity strategy,identity priorities, and the Identity API.",
    "input": "The configuration that defines how a partner sends data to an output. Inputs may be one of several types: Platform inputs capture data sent by mParticle partners from an operating-system-specific device or the web. For example, \u2018iOS\u2019, \u2018Android\u2019, or \u2018web.\u2019Feeds capture data sent by mParticle partners using feed integrations. There are several types of feeds:act-as feed,custom feed, andunbound feed.",
    "install": "A data point tracked by many mParticle partners, representing the action of a user installing the app on their device. In mParticle, an install corresponds to an Application State Transition event, of typeApplication Launch, where the attributeis_first_runistrue.",
    "integration": "The flow of data from one of mParticle\u2019s partners to another. Types of integration include: event, audience, data warehouse, feed, data subject request, and cookie sync. Also referred to asintegration partnerorintegration service.",
    "kit": "A component you add to an mParticle SDK that communicates directly with an integration partner from the app client. Usually the kit includes some or all of the partner\u2019s own client-side SDK. Kits are not the same as SDKs. Also referred to as embedded kits. Kits are typically not needed for server-side integrations.",
    "mapping": "Each of mParticle\u2019s integration partners uses a slightly different data structure, with different names for key data points. Mapping is the process of transforming mParticle data into a format that can be used by a partner, and vice versa. For some integration, mapping is customizable. For example, if a partner only collects one user ID, you may need to decide which mParticle identity type to map to the partner\u2019s user ID. See alsocustom mapping.",
    "Metered Integration": "An integration type in which mParticle runs a secondary processing service for mapping data. These integrations consume credits and are enabled by the mParticle account team.",
    "MPID": "A unique identifier (64 bit signed integer) that each user is assigned in mParticle to aid in processing identity and profile data.",
    "MAU*": "Monthly active users.",
    "MTU": "Monthly tracked user, a measurement used in mParticle billing. An MTU is any profile stored in mParticle that has been updated or has generated at least one tracked event within a calendar month. Contrast withVBP.",
    "organization (org)": "Each customer of mParticle is assigned an org, which contains one or more accounts. An account contains one or more workspaces. Different features of mParticle are scoped to org, account, and workspace.",
    "output": "The configuration that defines how a service receives data from an input via either mParticle servers or directly from the client.",
    "partner": "Apps and services that can receive data from, or forward data to, mParticle via an integration. Downstream partners are connected by an output configuration to mParticle, and upstream partners are connected by an input configuration. Also referred to as \u201cintegration partner.\u201dA company that isa certified solutions partner or technology partnerwith mParticle.",
    "pipeline": "Generally speaking, a pipeline is a data definition describing the data that flows continuously from a source to a destination. Cortex machine-learning pipelines transform raw data into machine learning predictions. These pipelines are used in the CDP to create user predictions.Warehouse Sync pipelines ingest predefined selections of data into mParticle from databases in external warehouses. Warehouse Sync pipelines can be configured to run automatically according to a schedule, or they can be configured and run manually.",
    "platform input": "An operating system such as iOS, Android, Roku, or the web that serves as an input. Contrast withact-as feedorcustom feedorunbound feed.",
    "product": "mParticle representation of a physical or virtual product or service that your users can buy. Products are referenced in Commerce events.",
    "production (PROD)": "Seeenvironment.",
    "premium feature": "A feature of mParticle that requires an additional license. Submit a request tomParticle Supportto request a premium feature.",
    "profile": "A complete record of what you\u2019ve learned about a given user over time, across all channels, continuously updated and maintained in real time as new data is captured.",
    "purchase": "A type of commerce event captured when a user of your app buys one or more products.",
    "real-time audience": "A set of users connected to an integration for the purpose of engaging those users. Real-time audiences are populated based on recently received data. Contrast withstandard audience.",
    "releases": "mParticle has two types of releases: BetaAn initial release of products or features thatmParticle expects to make generally available.mParticle typically offers Beta release functionality free of charge to customers who want to test and provide feedback on future functionality.General Availability (GA)A release of products or features that have been fully tested and validated for scalability, quality, and usability. Any product or feature not labeled Beta or Early Access (EA) in documentation is a GA release. GA features are rolled out to customers over a period of time.GA release functionality is available for purchase to all customers.",
    "rule": "Rules allow you to cleanse, enrich and transform your incoming data before it is forwarded.",
    "screen event": "An event type used for tracking navigation within an app.",
    "SDK": "A code library created and maintained by mParticle to track data in your native and web apps. Note that the preferred terminology varies between platforms. This includes native SDKs for iOS and Android, a JavaScript snippet on Web and various libraries, modules, and plugins used for mobile development frameworks like Xamarin and React Native.",
    "server-side": "Data forwarded from mParticle servers to an integration partner, rather than directly from a client (such as a mobile device). Server-side integrations typically do not require that a kit be added to the mParticle SDK.",
    "server-to-server": "A channel for incoming data such as the Event API.",
    "standard audience": "A premium feature that enables you to define and build audiences based on long-term historical data. Contrast withreal-time audience.",
    "UAV": "SeeUser Activity View.",
    "unbound feed": "A feed that can\u2019t be configured to behave as if it came from a specified platform (there is noAct as Platformoption in the Feed Configuration). Contrast withact-as feed.",
    "user": "The person or system who caused an event to occur. Users may be anonymous or known.Someone who has access to the mParticle system is an mParticle user.",
    "User attribute change (UAC)": "A user attribute change event: anevent_type : attribute_change_event. An SDK uploads an event whenever a user attribute changes to denote new attributes, changing attributes, and removed attributes. This allows for calculation of the current user attribute state for each event within an mParticle upload.",
    "User Activity View (UAV)": "The page in the Activity section of mParticle that allows you to view a detailed summary of data associated with a single user.",
    "user profile": "Seeprofile.",
    "VBP": "Value-based pricing (VBP), an alternative toMTUmParticle billing. VBP is aligned with a customer\u2019s usage and scales as the customer\u2019s needs grow.",
    "workspace": "A workspace is the basic container for data in an mParticle account. An account has one workspace already created; more can be created at any time. These logical containers control different types of scope. For example, the Profile API is set to workspace scope, while the Platform API is set to account scope.",
    "event property": "Event properties describe the context of an event. For example, event properties for the event \u201cButton Click\u201d could include the device type used to perform the event, the time zone the event was performed in, or the web browser through which the event was performed.",
    "user property": "Analytics isn\u2019t just about tracking events, it\u2019s also about tracking user. User Properties are the properties associated with the user performing an event, such as demographic factors, an email address, or the marketing channel through which the user was originally acquired. While event properties can differ from event to event, user properties are associated with every event performed by a given user.",
    "query builder": "Every analysis in Analytics is built in thequery builder. Here, you may combine events, event properties, and user properties to create and visualize an analysis.",
    "query row": "Aquery rowis a section within a query, and contains events, event properties, and/or user properties that will determine the analysis. A query may be composed of one or more query rows.",
    "value": "When an event property or user property is broken apart into its components, these components are referred to as values. For example, when looking at the property \u201cPlatform\u201d, \u201ciPhone\u201d and \u201cAndroid\u201d are potential values.",
    "numeric value": "A value that contains only numbers. Numeric values may be used in calculations. For example, values under the property \u201cPurchase Type\u201d containing specific price information are numeric values.",
    "string value": "A value that contains letters, numbers, or other characters. String values are not used in calculations. For example, a User ID consisting only of numbers is a string value.",
    "widget": "Awidgetis a module on adashboardwithin Analytics. Widgets provide both access to and results from saved analyses. Any analysis built inSegmentation, andCohortmay be added to a dashboard as a widget.",
    "Goal": "Understand whether users have tried out the PetCam.",
    "Create a Target Segment": "Create auser segmentfor all users who opened the PetBox app but have not opened PetCam since\nit was launched in Segmentation. ChangeTotal count oftoUsers who performed. Select theOpen Appevent from the data dropdown\nand selectOpen PetCamfrom theDid [not] Performdropdown.Selecting an event fromDid [not] Performinvokes the\u201cFor\u201d clause.\nWe use this clause to help us understand whether a user performed an action\nbefore/after the target event. We are also able to examine this action based\non a specific date range. In this case, we selecteddid not do,betweenand used03/01/xxxx to Todayfor our \u201cFor\u201d clause date range as March\n1st is when the PetCam was launched.Since we are saving a user segment out of this query, we also want to open\nup the date range to03/01/xxxx to Todayso we can capture\nall of the users who have opened the app since the feature was released.\nThis is your final query:Note: The date ranges used have a static start date\nof03/01/xxxxand a relative end date ofToday. Using a dynamic date range enables us to\nexamine only those who have not used the filters feature even at\na later date but remember to save it as auser segment with a daily update cadence.Below is theCreate a User Segmentmodal. Name the segment\nand provide a description for future ease of use.Select a category or create a new one by typing the name in so you\ncan easily find your user segment in the future.Toggle toDaily.",
    "Analyze or Activate with this User Segment": "Congratulations on creating the user segment. Let\u2019s put it to good use.",
    "Saving to the Dashboard to Track Trends": "Analyze the ratio of users who have opened PetCam against those who have\nnot opened PetCam using thecalculator tool.\nFirst, create two queries: one filters for events performed by users within\nthis segment and one filters for events performed by users who are not in\nthis segment.Now, we can use the calculator tool to calculate our ratio.After we run the query, we can save the line chart to adashboardas one widget and the average as a separate metric widget. Now you can track\nthis ratio on your dashboard.",
    "Export your users to send targeted notifications": "You can export your users to engage with them outside of Analytics in one of\ntwo ways: Download aCSVand upload it to your marketing tools.Connect to ourSegments API(available for Pro and Enterprise users) to make API calls to your automation\ntools. This tutorial shows just a few of the ways you can use Analytics to analyze your\ndata and achieve actionable insights from it. If you have any questions or comments,\nplease reach out tosupport@mparticle.com.",
    "Identity resolution": "Every piece of data collected is attributed to a user. This data attribution is stored in a user profile. Identity resolution is the process of determining which user profile incoming data should be added to. This process also includes: Determining if the current user has an existing user profile (or if the user is known).Deciding what to do with the collected data if the user does not have an existing user profile (if the user is anonymous).",
    "How mParticle identifies users": "At a high level, there are three steps in the mParticle identity resolution process:\nAn identification request is made via one of the mParticle platform SDKs or the HTTP API. The identification request includes all available user identities, or identifiers, such as a customer ID, email address, or phone number. mParticle iterates through your account\u2019s identity priority in ascending order, comparing the identifiers included in the request with each identifier in your identity priority. Remember, your identity priority is a list of identifiers organized according to their ability to confidently find the right profile for the user in question. For example, a customer ID or email address is more likely to be unique to a single user than a device ID, because a device ID could be shared by multiple users. Depending on the identity strategy configured for your account, mParticle returns the user profile matching the identifiers provided in the request. If mParticle finds an existing user profile for the user you are trying to identify, then this profile is returned to the SDK or API and the event and user data collected will be attached to this profile. If no existing profiles match the supplied identifiers (according to your identity priority), then mParticle will either create a new user profile to use, or it will do nothing. Whether or not a new profile is created (and how data is attached to that profile) is determined by your identity strategy.",
    "Example: tracking a new user through a signup flow": "Let\u2019s look at an example using the profile conversion identity strategy. Remember that the profile conversion strategy is designed to create a complete record of a user\u2019s journey through a common signup funnel. The following example is broken down into 5 basic stages, beginning with a new user navigating to an ecommerce app and ending the creation of an account while purchasing a product.",
    "1. A new user opens your app": "Imagine that a new user navigates to your ecommerce app and begins to browse different products. Let\u2019s say that the user does not have an account, nor do they create one.",
    "2. mParticle attempts to identify the user": "As soon as the user opens your app, an identity request is automatically made to mParticle to look for a matching user profile. The identity priority for this example is: Customer IDEmail addressUsernamePhone number Since the user hasn\u2019t logged in or created an account yet, no customer ID, email address, or username is provided with the identification request.",
    "3. mParticle creates an anonymous profile for the user": "mParticle iterates through the identity priority, but since no identifiers are provided that match any existing profiles, mParticle creates a new anonymous user profile based on the user\u2019s device ID. Like all user profiles, this new anonymous profile is assigned a unique MPID (mParticle ID). mParticle continues to collect data about the user\u2019s behavior and stores it in the new anonymous profile.",
    "4. The user signs up for an account": "Imagine that the user picks out a product they want, they add it to their cart, and then they begin the purchase flow. Before they complete their purchase, they create an account.",
    "5. mParticle adds the login ID to the anonymous profile, making it a known profile": "At this point, mParticle has finally received a login ID (the username or email address provided when the user signs up). Since the profile conversion strategy is designed to track users through the entire signup funnel (starting with anonymous browsing and ending with account creation), mParticle does not create a new user profile based on the newly supplied login ID. Instead, mParticle adds the provided login ID to the existing anonymous profile created in step 3 that was used to store the user\u2019s browsing activity leading to the final purchase and account creation. This guarantees the creation of a complete record of the customer\u2019s journey, providing more valuable insights into your customer\u2019s behavior and your app\u2019s performance.",
    "Manage Events": "To access events, open the Events and Properties data manager.",
    "Hide/Unhide Events": "Select the Archive icon to hide events. Once archived, events will not appear in any of the Analytics tools. They will also not appear in the user\u2019s event history. To unarchive an event, go to the filter in the upper right-hand corner of the data manager and choose the Archived Events icon. This will display events which have been archived. Click the three vertical dots on the right-hand side of the event\u2019s column, then clickActivateto unarchive the event.",
    "Rename, Define, and Categorize Events": "Under theDisplay Namecolumn, you may rename an event or event property. Events or event properties without a label will default to their original name.Under theDescriptioncolumn, you may add a description to an event. Definitions are useful for new users, and/or when your project contains similarly named events.Under theCategorycolumn, you may create categories and assign events and event properties to them. Creating categories helps keep your data panel organized.",
    "Joining Events": "Events & Properties settings allow you to join multiple events into one event, a \u201cjoined event\u201d. In a query, a joined event composed of Event A and Event B will represent auser who performed Event A or Event B. To create a joined event: Select the checkbox next to Event Type on the left side of the screen for each event you would like to join together. (If you don\u2019t see these boxes, you may need to request permissions from your project administrator).When the boxes for two or more events are selected, the Join Selected option will be available in the pop-up menu.Name the joined event and choose Save.",
    "Organize Widgets": "Free Flowlets you place your dashboard analyses wherever on your dashboard.Auto Flowautomatically compacts analyses to the top of your dashboard. For more information on organizing your dashboard, check out theDashboard Visualizationarticle.",
    "Dashboard Refresh Intervals": "Your dashboard analyses will refresh at the interval designated in the dashboard menu bar. For more information on dashboard refresh intervals, check out theTime and Interval Settings in Dashboardsarticle.",
    "Open Query": "To open the underlying query behind an analysis, select the analysis\u2019 title. Then, you may edit your query and save. You may also open the underlying query behind an analysis by selecting on the three dots in the top right of your analysis, and selecting \u201cOpen Query\u201d.",
    "Settings": "To edit your analysis\u2019 settings, select the three dots in the top right of your analysis, and select \u201cSettings\u201d.",
    "Title": "Type your desired analysis name in the \u201cTitle\u201d field. You may also rename your analysis by clicking into the analysis itself and renaming the query from the query tool.",
    "Display Options": "In your analysis settings, check the corresponding checkboxes to display the following within your dashboard analysis: Show TitleShow DateShow Legend",
    "Embed Widget": "This feature is available only for users on our Enterprise pricing plan. Add a live widget to your website, shared workspace, or other HTML environment to promote data access and establish a single source of truth. To get started, open your favorite Analytics dashboard, then locate the analysis you\u2019d like to embed. Next, open analysis settings (click the three dots that appear upon hover in the top right corner of the analysis), select your preferred display size, and click on the iframe code to copy it to your clipboard. You can now paste your widget anywhere iframe elements are enabled. Analytics supports the ability to filter embedded analyses programmatically via an API. You may create variants of your embedded analyses by adjusting Date Ranges, Time Zones, Query Intervals, and Property Filters. To configure this, please see ourDashboard Filter API documentation. Please note that this feature requires advanced instrumentation from a technical resource. When refreshing dashboards, Analytics uses an algorithm to balance data \u2018freshness\u2019 and efficient resource utilization. When first applying a new filter to a dashboard or analysis, please allow up to 15 minutes for changes to be reflected in the dashboard\u2019s variants. Subsequently, each request to render a dashboard or analysis will display the most recently cached result before initiating an update.",
    "Move": "To move an analysis, select the three-dot menu in the top right corner. Then, select \u201cMove\u201d. Select a dashboard in which to move your analysis, and confirm by choosing \u201cMove\u201d, or click \u201cCancel\u201d if you changed your mind.",
    "Duplicate": "To duplicate an analysis, select the three-dot menu in the top right corner. Then, select \u201cDuplicate\u201d. Create a name for your new analysis, and select a dashboard in which to save it. Select \u201cDuplicate\u201d to finalize your dashboard duplication, or choose \u201cCancel\u201d if you\u2019ve changed your mind.",
    "Delete": "To delete a dashboard, select the three-dot menu in the top right corner of your analysis. Then, choose \u201cDelete\u201d. You will be asked to confirm if you want to delete your analysis. Please note that your analysis will not be recoverable.",
    "Optimize Dashboard Performance": "Here are a few ways to optimize your dashboard for improved query performance: Minimize date ranges:Unless the date range of an analysis is updated, it will continue displaying more data as time goes on. The more historical data the system has to refresh, the greater the impact on performance. To avoid this, restrict the date ranges for analyses to only what is required for data accuracy and gathering insights.Minimize widgets in a dashboard:The fewer widgets in a dashboard, the faster the dashboard will be refreshed when filters are applied.Select User Attributes Efficiently in your Analyses:Minimize querying/updating user attributes in an analysis.Streamline Segmentation Queries:Simplify segmentation queries, especially those with multiple \u2018for\u2019 clauses.Optimize Text Field Queries:Avoid \u2018Contains\u2019 clauses with a large number of values in text fields. Note:With the exception of \u201cMinimize widgets in a dashboard,\u201d all optimizations above are valid best practices at the analysis level as well.",
    "Query menu": " All Journeys queries may be customized further using the query menu. Moving left to right\u2026",
    "Repeat events": " Use the Repeat Events component to merge sequentially repeated events into one single event. For example, if you track a Page View event that fires with each page view on your website, your most common Journey is likely to be Page View to Page View to Page View to\u2026you guessed it, Page View! If you track meta-events such as the example above, you\u2019ll want to Collapse your repeated events into one single event. If you don\u2019t track meta-events such as the example above, you\u2019re fine to leave your Repeated Events as Expanded. This feature can impact query performance. For example:  Path from A (ignoring repeated): A \u2192 B \u2192 C \u2192 D \u2192 END Paths from B (ignoring repeated): B \u2192 C \u2192 D \u2192 END (3x) Paths from C (ignoring repeated): C \u2192 D \u2192 END (2x)",
    "Events with breakouts": "This menu bar item will display any events that are broken out by an event property. Here, you may delete any such events. To break out events by an event property, see theEvent Menusection.",
    "Excluded events": "This menu bar item will display any events that have been excluded from the visualization. Here, you may delete the excluded event to have it appear in the visualization again. To exclude events, see theEvent Menusection.",
    "Percentage threshold": "On the right of the query menu, we have two additional options: Percentage Threshold and Number of Steps. Both of these settings impact the height and depth of your Journeys visualization. The percentage threshold governs the height of your visualization. Each Journey label contains a percentage number. This percentage is the percentage of your total users who arrive at the selected event, at the selected step. In the Percentage Threshold menu item, you may select your percentage threshold. A lower percentage threshold means more possible Journeys. A maximum of 10 Journeys can be displayed in each step. At each step, we only keep the events that account for at least X% of the total paths at that step. This is another report feature that reduces noise by allowing you to control what defines a significant percentage of users performing the next step.  Please note that query time may increase as you lower the percentage threshold.",
    "Number of steps": "The number of steps governs the depth of your visualization. By default, Journeys displays 5 steps. You may add up to 15 total Journeys steps. Please note that query time may increase as you add more and more steps.",
    "Opting in / out": "To opt in to Journeys 2.0, navigate to eitherAudiences > JourneysorAudiences > Real-time, and hover over the text \u201cLEGACY EXPERIENCE\u201d next to the top breadcrumb navigation. Then, click \u201cTry the New Experience\u201d in the modal that appears upon hover:  Once opted in to Journeys 2.0, you can revert to the legacy Audiences experience at any time by hovering over \u201cNEW AUDIENCES EXPERIENCES\u201d in the top navigation and clicking \u201cRevert to Legacy Experience\u201d: ",
    "Journeys landing page": "After opting in, theAudiences > Journeyssection of the UI will display a table that includes every Audience and Journey you have created, sorted byLast Updatedby default.  The Journeys 2.0 landing page represents two main types of customer segments: Single-step JourneysRows with aTotal Audiencesvalue of 1 represent both single-step Journeys that you have created in the past, as well as legacy Real-time Audiences that have been migrated here as single-step Journeys.Multi-step JourneysRows with aTotal Audiencesvalue greater than 1 are multi-step Journeys, which are represented in this view as nested Audiences. Click on the \u201d+\u201d icon next to the name of a multi-step Journey to expand it to expose the individual milestones (or Audiences) the Journey contains. Clicking any of the nested Audiences will bring you to the Journeys canvas, where you can update this or any Audience contained within the Journey.",
    "Audiences vs. Milestones": "Functionally, a Milestone and an audience are the same. Both represent a targeted segment of customers that you can forward to integration partners, run A/B splits on, and otherwise leverage in campaigns. Their only difference is how you create them in mParticle: A Milestoneis an individual step within a journey that you have defined to capture a meaningful moment in the customer lifecycle (i.e. a free trial signup, an email interaction, or an ad click). Each Milestone creates an audience of customers who fit the criteria you have set for it.An audienceis a customer segment that you have created.",
    "Update an existing Journey / Audience": "To update a journey (either single- or multi-step), expand it by clicking the \u201d+\u201d icon to the left of its name, then click on one of the milestones nested beneath it. This will display the Milestone editor within the Journey canvas, where you can update the inclusion criteria for this particular Audience:  Closing the Milestone editor will display the full Journey canvas, which you can use to add additional Journeys features likenew paths,additional milestones, andA/B tests.  At any time after a Journey has been created, you can edit that Journey\u2019s data inputs. First, click on the top node in the Journey which has the heading \u201cJourney Inputs\u201d:  Next, add or remove inputs as you see fit, then clickUpdate. ",
    "Create a new Journey": "The Journeys canvas provides a flexible and powerful tool for creating targeted customer segments. Let\u2019s look at two of the main ways you can use Journeys to create audiences:",
    "Single-Milestone Audiences": "As mentioned above, creating a Milestone within a Journey is functionally equivalent to a creating a Real-time Audience. By using the selection criteria and logical operators within a Milestone, you can create highly targeted customer segments for your campaigns. Let\u2019s explore how to do this. At the Journeys landing page (Audiences > Journeys), click theNew Journeybutton in the top right-hand corner of the screen.  At theCreate Journeymodal, name your Journey, select your inputs, and clickCreate. In the Journey canvas, click the \u201d+\u201d icon followed by theMilestoneoption to create your first (and only) Milestone in the Journey.  For each audience you create, you can select the environment(s) from which you want users in that audience to come from (Production, Development, or both).  Test data in Development may overwrite Production data in partner systems.If you send dummy or anonymized data to your Development environment, you should select the Production environment for any audiences you forward to partners to prevent unwanted overwriting. For example, you send test customer profiles to Development environment with real customer IDs but anonymized emails for privacy purposes, the anonymized emails can overwrite real ones in any system you forward them to.Parent and child audience environment should align.If a parent audience pulls its members from Development and its child pulls its members from Production, the child audience will be empty (and vice versa). In the example below,Wait list Subscribers(child) andInterested and Engaged(parent) both use the Development environment, which is why the child audience has been populated with users. The other child audiences (Lapsed SubscribersandAbandoned Cart Subscribers) both use the Production environment which conflicts with their parent\u2019s environment, and therefore have no users. As you define your audience criteria, a list of suggested matching values will appear based on what you\u2019ve entered. This feature works both when building new audiences and fine-tuning existing ones, helping you save time, reduce manual effort, and improves accuracy. To use this feature, you must have one of the following standard Roles:User,Admin,Audiences-only,Support, orAdmin+Compliance. Alternatively, you can create a Custom Role with any of the following tasks:audiences:draft,audiences:edit,catalog, oraudiences. In the Milestone editor, you can begin adding criteria to target the users you want in your audience. Once you have added your first criterion, you can use the Boolean operatorsAnd,Or, andExcludeto create logical relationships with subsequent criteria.   Continue adding and combining operators to hone in on the precise user segment that matters to your business goals. The Milestone below, for example, uses three criteria and two Boolean operators to create a segment of customers who are on iOS 10.0, have started at least three sessions within the last seven days, who arenotlocated in New York City. ",
    "Nested audiences from multiple Milestones": "Now let\u2019s see another approach to using  Journeys and Milestones to create targeted customer segments, which is is equivalent to creating nested audiences in the legacy Real-time Audiences experience. To explore how this works, let\u2019s first look at a nested audience created with the legacy Real-time Audience builder as an example:  This customer segment uses membership in three separate Audiences\u2013\u2013App Downloaders, Recent Users (Last 30 Days), and Engaged with Ad\u2013\u2013as its membership criteria (which is what makes it a \u201cnested\u201d audience). Now, let\u2019s see how we can recreate this segment as a Journey. First, create a new Journey, then click the \u201d+\u201d icon under the top (and only) node in your Journey to create a new Milestone. For this Milestone, set membership in the first audience from your nested audience (App Downloaders) as the sole inclusion criteria:  Now, create two more Milestones for the remaining two audiences in the original nested audience:  This Journey is now effectively the same customer segment as the nested audience created with the legacy Real-time Audience builder, with the added advantages of being able to add A/B Tests, new paths, and partner integrations at each individual Milestone (or nested audience), and the ability to visualize each Milestone that comprises the segment in one place.",
    "Audience Insights": "TheInsightstab in the audience builder gives you quick visibility into the customers included in your audience. This helps you better understand who you are targeting and ensure your audience meets your business goals. TheInsightstab displays four metrics: Compare your audience size over different time periods to track growth trends and fluctuations. The size metric shows: Daily average audience size per weekCustom date range comparisons (e.g., Last 30 days)Percentage changes between periodsPartial week data clearly marked Analyze the distribution of users across different attributes to understand your audience demographics and characteristics: View user distribution by various attributes (e.g., Location)See percentage breakdowns for each attribute valueCompare relative sizes of different user segmentsFilter and sort distribution data as needed Assess the potential reach of your campaign across different user identifiers: Track identified users per identity type (e.g., Email Address, Customer ID)View coverage percentages for each identifierCompare reach across multiple identification methodsMonitor identity match rates and coverage gaps Find audiences that share users with your current audience to optimize targeting and prevent duplicate messaging: See other active audiences containing the same usersView overlap percentages between audiencesCompare audience sizes and shared user countsIdentify potential audience consolidation opportunities Note:Numbers shown in the Insights tab are sampled by default. For precise figures, use the\u201cRe-run analysis\u201doption to perform a full audience analysis. Each section of theInsightstab helps you make informed decisions about your audience targeting strategy and campaign optimization. Use these metrics to refine your audiences and ensure they align with your marketing objectives.",
    "Activate a Milestone in a campaign": "Once you have created a Milestone that you want to forward to an external tool for use in a campaign, click theConnect Outputbutton in the Audience tile under the Milestone in your Journey, then follow the steps to connect that audience to any of your connected outputs. ",
    "Selective User Attribute forwarding": "You can optionally include additional User Attributes, beyond identities, when forwarding to each Audience output. This enables you to use richer data in your activation platform, such as LTV, lead score or propensity to convert. In the last step of the process to connect an output, select which account and workspace level attributes you would like to forward to that particular tool: ",
    "Organize journeys / audiences with Tags": "As you continue to use create new audiences, you can use tags to keep them organized and allow team members to easily see the purpose of each audience at a glance. For example, you can use tags to group your audiences by campaign type, giving them names likeretargeting,lead gen, andproduct launch. Navigate to the Journeys homepage.Click the+icon next to the name of a Journey to expose its milestones.Click theTagsicon in the row for that milestone.Enter the name of your new tag in the text bar.Note: Tags have an 18 character limit.  To apply your tag, click the name of the new tag below the text bar. To add additional tags, select name of the tag(s) that has already been applied, open the dropdown menu, and click the additional tags you would like to add. Tag names can be used as search queries to return all audiences that have that particular tag applied.",
    "Share Audiences within Journeys between accounts": "You can share Audiences within Journeys between your organization\u2019s accounts, with detailed control over what data is shared. Share data broadly or restrict it to only what\u2019s needed for a campaign. This feature does not affect data shared with third-party tools.",
    "Adjust sharing settings": "At the Journey\u2019s homepage, open theActionsmenu for the Audience you wish to share.SelectShareto open the sharing modal.  In the modal, view accounts the audience is shared with and their sharing permissions. There are four sharing permission levels: Permission Levels| Level | Access Details |\n| --------- | -------- |\n|Owner| Full access to the audience, including editing, audience updates, and connecting outputs. Admins can set permissions. |\n|Private| The audience is invisible to the receiving account. |\n|View only| Visible to the receiving account but cannot connect to outputs. |\n|Usable| Visible to the receiving account and can connect to outputs. Audience definition cannot be edited. |",
    "Share audience with new account": "In the sharing modal, select the+icon.Choose the account and the desired permission settings.",
    "View shared audiences": "To view audiences that have been shared with an account, navigate toSegmentation, then selectShared Audiencesin the left-hand navigation.",
    "Open the Funnel Tool": "Select Create New Analysis, and then find Conversion and Drop Off in the dropdown menu. Selecting Conversion and Drop Off will set up a pre-configured query to identify points of friction between events. ",
    "Break out by Marketing Channel": "Tobreak outour results, let\u2019s add a User or Event Property. In this case, we\u2019ll use the event propertyMarketing Channel. Breaking out our funnel by this property will group users based on the marketing channel they were acquired through. To add these breakouts, re-expand the query builder and select Group By underneath the first funnel step, then chooseMarketing Channelunder the event properties tab. ",
    "Select from First/Last/All": "We\u2019re interested in understanding the marketing channels these users were acquired from.  Select First next to the event propertyMarketing Channelin the query builder. First will only display the channel that first led users to the homepage within the specified date range and will exclude marketing channels users were re-acquired through later on.",
    "Run the Query": "  The table below the chart is broken out by events along the X axis, and by marketing channel along the Y axis. For example, the first row tells us that of users who were acquired through Paid search: 43,484 completedSite Visit.10,309 completedBlog View, a 23.71% conversion fromSite Visit.5,551 completedSubscribe, a 53.85% conversion fromBlog View.The total conversion rate across all steps was 12.77%",
    "Compare Conversion Rates": "Let\u2019s click the row corresponding to the largest count in user signup, which is Email.  Clicking a row in the table expands the selection. All conversion rates within the webbings and top line update to reflect the selected campaign source. The total conversion rate is 10.26%, while the conversion rate for users whose acquisition marketing channel was Email is 8.34%. Now, let\u2019s compare that to the row with the highest (non-direct) conversion rate, which is Paid (12.77% conversion).",
    "View all trace activity": "The Trace Activity page displays a list of all recent traces for your development data, and any traces you have configured for your production data. All traces are available for up to 14 days. To view the details for a specific trace, click the purple ID under the Trace ID column.",
    "Trace status": "You can access a trace from Observability as soon as mParticle begins receiving and processing data, but it\u2019s important to note that a trace can\u2019t provide complete information about a data flow until all data in the trace has been fully processed. This typically occurs within 30 minutes. Traces that are ready to be used for troubleshooting will display a \u201cComplete\u201dTrace Statuson the Trace Details Page. Traces for data flows that are still being processed have a Trace Status of \u201cIn Progress\u201d.",
    "Sort and filter trace activity": "You can filter your results by time frame by clicking the button labeled \u201cLast hour\u201d and selecting one of the predefined date ranges or entering a custom range.  To further sort and filter your results, clickSort and Filtersto view the following options: Use theOrderdropdown menu to sort your traces from most recent to oldest, or oldest to most recent. UnderFilters, select any of the following criteria to limit the traces displayed: Trace Type:Event: displays only traces for the Events APIIdentity: displays only traces for the IDSync APIResult:Success: displays only traces where all data was processed without any issues.Insight: includes traces that experienced an interruption in data flow resulting from a configuration setting (such as a Rule or Filter).Needs Attention: displays only traces that include an error message.Warning: includes traces where an issue was encountered during data processing that could be resolved with a retry.Environment:Production: displays only traces for data in your Production environmentDevelopment: displays only traces for data in your Development environmentmPID: filters results based on the MPID associated with a call to the IDSync APITrace Configuration ID: displays only traces created for the given trace configuration IDInputs: filters results based on one of your configured data inputsOutputs: filters results based on one of your configured data outputs After selecting your desired sorting and filter options, clickApply. This refreshes the Trace Activity page to display only filters matching your selected criteria.",
    "Customize trace activity page": "You can configure the columns that are displayed on the trace activity page by clicking theView Columnsbutton.  To remove a column from the trace activity page, click the toggle switch. Some columns, like \u201cTrace ID\u201d, cannot be removed. To change the order of the columns, click and drag the handle next to the column name. ",
    "Trace details": "After opening the details page for a specific trace, you will see the following information:",
    "Trace summary": " Trace ID: the unique ID for the trace.Trace Configuration ID: the ID of the trace configuration that the trace belongs to.mParticle ID: any MPID that is associated with the trace.Trace Status: either \u201cIn Progress\u201d or \u201cComplete\u201d. The trace status indicates whether a trace contains enough information for it to be useful when troubleshooting your data flow. Traces that are \u201cIn Progress\u201d indicate that the data being traced is still being processed. For more information, seeTrace status.Trace Start Time: the date and time (in UTC) when the data was initially received by the mParticle platform.Duration: the total time elapsed during the trace.",
    "Setup details": " Inputs: the name of the input configuration where the data originated.Output Configurations: the configured outputs for the data.Data plans: any active data plans that were used during processing.Environment: indicates whether the data flowed through your development or production environment.",
    "Trace result": "For each trace, you will see eitherSuccess,Insight,Needs Attention, orWarningdisplayed under \u201cResult\u201d along with any applicable messages under \u201cAdditional information\u201d. You can use this information to determine whether an issue encountered during the trace was intentional or accidental, and what steps you may need to take to resolve any issues.  In some cases, you might see additional information with instructions to contact mParticle Support or your mParticle Account Representative, who can help you determine the root cause of an error or issue.",
    "Related traces": "It\u2019s possible for one process to trigger other related processes in the mParticle platform. Any related processes that are traced will be listed here. For example, when mParticle ingests a batch of event data or a request to make a bulk update to your data, each subsequent data flow will have its own unique trace, which you can find and access here.",
    "Timeline view": " The Timeline View provides a visual picture of how your data flows through the mParticle platform, broken into different spans, with each span representing a different stage of data processing. Hover your cursor over the span in the timeline to see its exact start and end times. To view details for a specific span, click on the span within the timeline and review the information panel at the bottom of the UI. Not all spans will be presented sequentially, and some will appear to occur at the same time. This is because mParticle executes different processes in parallel to reduce the amount of time it takes to process your data. Most gaps between spans on your timeline are likely due to networking delays or internal processes that are not represented on the trace timeline.",
    "Span details": " The Span Details view provides more granular information about a particular span. The details shown will vary depending on the span category. To learn more about each span category, view theSpan Glossary.",
    "About trace IDs": "Each individual trace is uniquely identified by a 36 character Trace ID resembling66e0c0cd9bb8998a579595e42bae7077. Trace IDs are essential for pinpointing specific data processing traces. You can find them in two primary ways:",
    "1) Find a trace ID in an API response": "Trace IDs are included with all responses to calls to the Events and IDSync APIs. To find a trace ID in an API response: Search your API response for the header titledX-MP-Trace-Id. The value of this header is the trace ID for the corresponding API call. ",
    "2) Find a trace ID in Live Stream": "Log into your mParticle account and navigate toActivity > Live Stream.Select any event row.In the right hand sidebar, you will find the trace ID associated with that event.",
    "Search for a trace using a trace ID": "To search for a specific trace on the Trace Activity page: Find and copy the trace ID from an API response or Live Stream as described inAbout trace IDs.Log into your mParticle account and navigate toOversight > Observability.From the Trace Activity page, enter your trace ID in the search bar and clickSearch. ",
    "Open a trace from Live Stream": "You can also view trace details for a data flow directly from the mParticle Live Stream. Log into your mParticle account and navigate toActivity > Live Stream.Select the event row you want to view the trace for.In the right hand side bar, you will find the trace ID associated with that event.Click on the Trace ID to open up the Trace Details page in the Observability suite. ",
    "Trace configurations": "To view your trace configurations, navigate toObservability > Trace Configurationsin the left hand navigation.  You define what data you want to trace using a trace configuration. A trace configuration initiates an individual trace with a unique trace ID for each request to the Events or Identity API according to the trace configuration settings you specify. The trace configurations page displays a list of all configurations, sorted from newest to oldest. You can see the start and end date and times for each configuration, the trace configuration ID, the percentage of data that will be traced, and one of the following trace configuration statuses: Active: Active trace configurations will generate a trace for any data ingested from the configured input(s).Pending: Pending trace configurations will become active once the selected start date and time is reached.Completed: Trace configurations are marked Completed when the scheduled duration for the trace has elapsed.Canceled: If you cancel the trace configuration, its status is changed to Canceled. ",
    "Create a trace configuration": "To create a new trace configuration: Navigate toObservability > Trace Configurations, and clickCreate Trace Configuration.This opens the Add Tracing Configuration window:UnderInputs, select the connected data input you want to trace.You can select one of your configured Warehouse Sync pipelines as the input. Warehouse Sync pipelines will be listed under the Feeds input category.Use the date and time picker to select theStart Date & TimeandDurationfor your trace. Traces will only be generated after the start time and for the duration you specify.If you select a Warehouse Sync pipeline as your input to trace, you will see a unique trace generated for each event batch within the timeframe you specify.Select the percentage of your data you would like to be traced using the Sample Size drop down menu. You can select either 1%, 3%, or 5% for small sample sizes, or you can select 10% through 100% in increments of 10 for larger sample sizes.You should uselargesample size for short-term tracing. For example, when first launching a new mParticle configuration, creating a short-term trace configuration with a large sample size can help you detect problems early on.You should use asmallsample size for long-term tracing. For example, once you have a stable configuration but still want to monitor your data, you can create a long-term trace configuration with a small sample size to keep your tracing cost low.Use the date and time picker to select theStart Time (UTC)andDurationfor your trace. Traces will only be generated after the start time and for the duration you specify.ClickSubmit. After clicking Submit, you will see your new tracing configuration listed on the Trace Configurations page.",
    "User continuity": "A common scenario for a media or ecommerce app goes something like this: User downloads an app but has not yet registered an account.User browses around in the app. Visits screens A, B and C. Data collection begins, but since the user has not yet registered, this activity is stored against an anonymous user profile identified only by an anonymous ID (such as a device ID).User decides to register for an account, creating a new logged-in user profile, and continues to use the app while signed in. Visits screens D and E, and buys product F. How should the data from this interaction be organized? There are two basic approaches: Link the new logged-in user profile with the original anonymous one. This approach yields a continuous view of the user journey.At the moment of user registration, create a new user profile and keep the post-signup activity completely separate from the pre-signup activity. There are compelling business and legal arguments for and against each approach. By choosing the first approach, you have a chance to preserve a complete history of a user\u2019s experience with your app. This might be invaluable for improving your funnel. However, you also introduce the possibility of mingling data from several users into a single profile. For example, on a shared device, multiple users might access the app in a pre-signup state. The second approach sacrifices the possibility of collecting a user\u2019s entire history under a single continuous view. However, you can be sure that the data from your logged-in users is never mixed up with data from a different user. Quarantining anonymous data from known user data may also be required by law. IDSync is designed to let you make smart decisions about user continuity that fit the needs of your app and to give you transparency into how user profiles are created and updated.",
    "Cross-device tracking": "Users often interact with an app ecosystem through more than one device. For example, users might interact with an eCommerce app through both a native app and a web browser, or view media content on a web browser, a native app, or a Roku channel. Many apps will want to track events and lifetime value for a user across all platforms, but others will prefer to keep data for each platform separate. IDSync allows mParticle to support both use cases, and to harness 3rd party data to decisively link data generated from your apps with data from other sources, like CRM Feeds.",
    "Cross-app tracking": "Your product ecosystem may be spread not just across multiple platforms, but also multiple apps. Needs for tracking users across multiple apps will vary depending on your business model. For example, a gaming organization might publish dozens of individual games and want to track their user\u2019s LTV across all their apps. By creating workspaces for each app group under the same mParticle account, you can allow them to share a pool of users, and create only one profile per known user, no matter how many of your apps they use. Alternatively, you might wish to define different groups of users for different apps within the same ecosystem. For example, you might have one app for vendors and another for buyers, with a completely different set of metrics for each group. IDSync allows mParticle to support either use case.",
    "Customer experience personalization": "Personalization of customer experience (CX) is a top priority for marketers. Personalization reduces friction and increases conversions by presenting relevant in-context content that increases customer awareness, engagement, and satisfaction. The Immutable Identity Setting enables marketers to use the mParticle Profile API to get the most up-to-date real-time user identities, device identities, user attributes, and audience memberships. The Profile API uses either an identifier with Immutable Identity set or the mParticle Identifier to match a user profile. Additionally, IDSync Search allows marketers to query User Profiles by any known identifier, such as email, mobile phone, or device identity, and return all matched user identity values including the mParticle ID. The mParticle ID can then be used with Profile API to get the values necessary to personalize the customer experience.",
    "Privacy compliance": "The ability to provide evidence that demonstrates that your organization is in regulatory compliance is important to every Chief Privacy Officer and corporate information security executive. GDPR and CCPA data privacy controls and traceability are core to mParticle\u2019s user profile data policies. In addition, the IDSync Search capability can verify that a matching User Profile exists. It can also be used after a GDPR or CCPA User Profile Delete Request has been processed, to validate that the process has completed successfully and thereby validate compliance.",
    "Mutable identities": "Different user identifiers have different lifespans and degrees of specificity. A Customer ID or a social media ID permanently identify a single user, while an IP Address or Session ID may not be sufficient to identify a single user and can change at any time. Other identifiers fall somewhere in between. Email addresses, for example, do identify a single user, but a user may change their email address over time. IDSync gives you the tools to update identifiers for a User Profile without losing that user\u2019s history.",
    "Identity translation": "With mParticle managing all available identities for a user, you\u2019re freed up to focus on your data. One messaging service requires an email address while another needs Push Tokens? Don\u2019t worry about it. Build your messaging audiences in mParticle based on any criteria you need and mParticle will forward the correct identities for each service, as long as they are available.",
    "Access Usage & Billing": "To access the Usage & Billing page: Log into your mParticle account.Ensure you have an Admin & Complianceuser role.In the left navigation, click theSettingsicon and selectUsage & Billing.  The Usage & Billings page presents a list of your invoices for the current calendar year, sorted from most recent to oldest, with the currentmonth-to-date invoiceat the top. ",
    "View invoices from a different year": "To view invoices from a different year, click the button showing the year in the upper left, and select a different year. ",
    "Download an invoice": "To view an invoice for a particular month, clickDownloadwithin that month\u2019s row. Invoices are provided as Microsoft Excel .xlsx files and must be downloaded to be viewed. Both \u201cIn progress\u201d and \u201cCompleted\u201d invoices are provided as .xlsx files. ",
    "How invoices are formatted": "Each invoice file is organized into four tabs: The Definitions & Calculation Logic tab of your invoice lists the name of each product you used during the billing period and the product family it belongs to. The \u201cDefinition\u201d column defines the specific units used to meter the product and the \u201cHow It\u2019s Calculated\u201d column explains how those units are metered. The Overall Summary tab contains your Usage Summary, Credit Summary, and Credit Ledger: TheUsage Summarylists each product you used, the volume of your usage (in millions), the price in mParticle credits, and the total cost.TheCredit Summaryshows your current Credit Grant, or your credit balance.TheCredit Ledgerlists individual credit transactions, such as when credit grants are created or credits are spent. The Overall Summary tab is a report of your usage across all of your accounts and workspace. The Usage by Workspace tab breaks down your usage by each of your workspaces, denoted by the workspace ID under the \u201cWorkspace\u201d column. The Usage by Account tab breaks down your usage by each of your accounts, denoted by the account ID under the \u201cAccount\u201d tab.",
    "Invoice status": "Invoices will always display one of two status: In progressCompleted",
    "In progress invoices": "An invoice with theIn progressstatus provides a real-time report of your usage from the beginning of the billing period to the current date. These invoices always include\u201cmonth-to-date\u201dat the end of their name to help differentiate them from Completed invoices. In-progress invoices are intended to provide an estimate of your usage and credit consumption in real-time for the current billing period that hasn\u2019t completed yet. When the billing period ends, the invoice is markedCompleted.",
    "Completed invoices": "Invoices with theCompletedstatus provide a finalized report of your usage and how you are charged for eachbillable item. Invoices are not markedCompleteduntil the 2nd day of the following month. For example, your January invoice will not be finalized and markedCompleteduntil February 2.",
    "Calculations": "The following table lists all supported calculations. *Setting the date range toWithin the Lastcauses all calculations to be asynchronous. Be aware of the following before creating your calculation attributes: Calculated attributes require server-side forwarding. Therefore, this feature isn\u2019t available for kit-only integrations that solely support client-side forwarding.All timestamp values are in ISO 8601 format in the UTC timezone.Several calculations produce results with types that depend on the type of the event attribute selected; for example,FirstValue` returns a string if the selected event attribute is a string. All attribute values in our platform are stored as strings, including calculated attributes.Calculation speeds listed are after the values have been initialized.For unique lists, up to 100 values are calculated. The values are selected based roughly on the order in which mParticle received the data, though the ordering is not guaranteed. When viewing and using unique lists values (in User activity view, Profile API etc.), values are returned in alphabetical order.When using aliasing to transition from an anonymous to a known user profile, mParticle doesn\u2019t copy the calculated attribute or trigger a recalculation on the resulting profile.For aggregation CAs:More than one attribute may occur the same number of times, creating a tie. To break the tie, mParticle sorts the attribute name alphabetically and chooses the first attribute.After the first 100 values are collected for aMost FrequentorUnique ListCA, no more values are collected. ForMost Frequent, the frequency of the first 100 are continuously evaluated, but no new values are added. ForUnique List, mParticle keeps only the first 100 seen values. To trigger a re-collection of values for either calculation type, edit the CA definition or create a new one.",
    "Type conversions": "Some calculated attributes, likesum, require numeric event attributes to function. If you select an attribute that is not detected as the correct type, the platform will warn you about using those fields in the calculated attribute definition. Youcan still usethe calculated attribute despite the warning, and it will attempt to convert the string values into numerics. For example, if you pass the attributeamountin as a numeric string like\"34.32\", asumcalculation will still work correctly: the string\"34.32\"will be converted to the decimal value34.32. ",
    "Conditions": "When you define a calculated attribute, you can add conditional logic. For example, if you wanted to count the total number of times a promotion was clicked, but only for certain currencies, you could add the condition \u201cwhere the currency code is only AUD or EUR.\u201d The following conditions are available for all four categories of calculated attributes (Count, Aggregation, Occurrence, and List): ContainsDoes not ContainExact MatchDoes not MatchPatternExistsNot ExistsIs EmptyIs In List",
    "Commerce quantity fields": "The following behaviors affect commerce event attributes (commerce_events) in some calculated attributes:",
    "Product events": "The product action (product_action) and product impression (product_impression) attributes can be used in the quantity field for calculations. Note the following behaviors: Average uses the quantity field in a product array when averaging the value of the price field.First value picks the first product in an array if the product attribute is selected.Last value picks the last product in an array if the product attribute is selected.First timestamp is the batch timestamp.Last timestamp is the batch timestamp.Unique list can pick any field in an events or products array.Most frequent uses quantity for calculating the most frequent for non-numeric fields such as brand, category, or coupon code.",
    "Promotion events": "The promotion action (promotion_action) attribute can be used in the quantity field for calculations. Note the following behaviors: First value picks the first promotion in a promotions array.Last value picks the last promotion in a promotions array.First timestamp is the batch timestamp.Last timestamp is the batch timestamp.Unique list can pick any field in an events or promotions array.Most frequent can pick any field in an events or promotions array.",
    "Adding Queries to a Dashboard": " In Segmentation, Funnel, and Cohort, you can add a query to your customer journey dashboard by navigating to the Save to Dashboard button in the top right-hand corner. In the Funnel tool, you will be given a few options for how your analysis will display in the dashboard. Select as many as you would like displayed in your dashboard, and each will become a separate widget. ",
    "Rename and Delete Widgets": "First, find the three-dot menu to the upper right of the analysis in a dashboard. Choose Settings to rename the dashboard. Select on the name of the widget to edit the individual analysis.  ",
    "Setting Up Scheduled Reports": "Once you have built your dashboard to your liking, now is when we suggest you schedule reports to be sent on a cadence of your choosing. Users can schedule their dashboard results to be sent to other Analytics users, executives, or external partners and stakeholders. Once a report is created, your dashboard will refresh at the selected date and time, and a PDF will be sent to the selected individuals. In addition to the PDF snapshot, Analytics users may access your dashboard to view results in real time. Any teammate with access to your Analytics project can create a Scheduled Report for an existing dashboard. Now, let\u2019s dive into how to create a Scheduled Report. To create a scheduled report, you must first create a dashboard to send it from. If you already have a dashboard that you\u2019d like to use, navigate to that dashboard using the View dropdown in the top navigation menu. In this case, we will use a previously built dashboard called \u201cTest Dashboard\u201d for the purposes of this demonstration. Once you have selected your desired dashboard and changed your dashboard layout mode to Print Mode, click to open the Reports dropdown in Dashboards settings in the top right of the dashboard. Here, you may view your existing Scheduled Reports or create a new Scheduled Report as shown below:  You are now ready to build a dashboard and share it with your team, congratulations!",
    "Requests": "Since the best match strategy does not support login IDs, there are nologinorlogoutrequests, onlyidentify, for identifying a user based on your configured identity priority and the information in the request, andmodifyfor altering an identity for a given MPID.",
    "Breakout mode": "To understand which types of users are converting, you may apply property breakouts to your Funnel analysis. In Funnel, there are two types of property breakout modes:",
    "Per step": "An event property or user property breakout will be applied to each step individually for the entire group of users in each step. For example, a user may complete steps in a conversion funnel on a variety of devices. To explore which type of device the user used at each individual step, use Per Step. ",
    "Shared": "An event property or user property will be applied to a single step and then shared across the entire funnel. For example, some of your users may enter the top of the funnel through their mobile device, whereas others may enter through their desktop computer. To compare conversion rates through the entire funnel via mobile vs. desktop, regardless of the device used in subsequent steps, use Shared and apply your breakout to the first step in the funnel. ",
    "Attribution": "When applying a Group By clause to a funnel analysis, you must select whether the steps are broken out by thefirst,lastorallproperty attribution. The default selection is first.  First:Use the first observed property value to attribute users to a breakoutLast:Use the last observed property value to attribute users to a breakoutAll:Use all observed property values to attribute users to a breakout All of the above options are subject to the selected date range for your funnel query.",
    "Group by first or last": "When the funnel query is set to group byfirstorlastproperty values, the results are grouped by the first or last time the event was observed for each user. For example, when the query is set to \u201cgroup by last\u201d,  if a particular user performs the eventBlog Viewthree times during the selected date range, the third time they performed the event is represented in the groupings in all of the donuts (because the Funnel breakout mode is Shared). With the default (Shared) breakout settings, thisBlog Viewgrouping is applied across all of the subsequent steps in the funnel. If you wish to see the users\u2019 last action for each step, switch the breakout mode to Each Step in Settings.",
    "Group by all": "When the funnel query is set to group byallproperty values, additional comma separated breakout values are created that represent all possible combinations of behaviors that users might take. To illustrate how grouping by all users\u2019 events works, consider three users who performed the eventDownloadduring a given time frame, and are grouped by browser: User A performedDownloadfrom a Mac.User B performedDownloadfrom a PC.User C performedDownloadfrom a PC, and then from an Android device. When grouped by all values, the resulting funnel donut for these users would show three breakouts: one for Mac, one for PC, and one for PC | Android.",
    "Define your Cohort": "To begin a cohort query, determine an initiating event (called the Cohort event). The first event of a cohort is required; a user must complete the initiating event or Cohort event and then return to perform a second event which is explained in the section \u201cTarget Behavior\u201d. Custom Events and Merged Events can be used. As with other tools, you may apply aFilter Where.  You may select a different time zone from your project time zone on a per query basis by locating the globe icon on the top right of the query screen. You can chain multiple events in a sequence using an \u201cand then performed\u201d clause to define your cohort of users. In the following example, PetBox wants to measure users who download their app, and then start the app.",
    "Generations and Breakouts": "In addition to the first event, cohorts can be defined by a shared generation or a shared property. A generation is a unit of time, such as a month. A monthly cohort would include all users who entered the cohort during that month. A property is a characteristic or attribute, such as device type. Cohorts defined by device type would include all users with an iPhone, all users with an Android, etc. A user will only appearoncein the results of a cohort analysis. For generation cohorts, users will be put into the property breakout in which they first appear during the time interval. ",
    "Target Behavior": "After selecting an initiating event, you must select a Target Behavior event. This second event of a cohort is also required; Custom Events and Merged Events can be used. As with the initiating event, you are also able to apply a Filter Where. Event:Often this is an event that is repeated multiple times, such as a purchase. This is the event that represents the subsequent user behavior that you wish to analyze.Revenue:Target behavior can also be represented asrevenue. Using revenue as the target behavior will analyze the revenue generated over time by each cohort.",
    "Date and Time Range Settings": "Every query requires you to select a date range. In Cohort analysis, the date range refers to the time period during which a user completes all steps of the cohort query, defined in Row A. All new queries default to Last 30 Days. To open the date range selector dropdown, click on Last 30 Days. The start date is the first day to be included in the search. The end date is the last day. As mentioned in Cohort Basics, you canset the time zonefor your Cohort query. ",
    "Generation": "Every cohort is defined by a first event, a breakout, and a second event. To appear in the results, a user must complete the first event and also have a defined value in the breakout. Breakouts may be an Event Property, a User Property, or a User Segment; or they may be a Generation; or they may be a combination. A generation is a time-based grouping that describes the cohort. It describes when the first event occurred. You may select Hour, Day, Week, or Month. For example, to create monthly cohorts of users using the date they signed for a newsletter, the first event would be Newsletter Signup and the generation would be Monthly. This will produce a list of all users who signed up for the newsletter, broken out by month: January signups, February signups, March signups, etc. ",
    "Recurring vs First-Time": "You may adjust your cohort query to observe the frequency of recurring target behavior or to measure the first time that the target behavior occurred after completing the first event. If a query is set to Recurring, then a user will appear multiple times within a row if the user repeated the target Behavior multiple times. If a query is set to First-Time, then a user will appear only once in a row, describing when they completed the target behavior.  To exemplify the difference between recurring and first-time cohort queries, consider a box subscription company that wants to measure customer behavior among their newsletter subscribers. The first event could be \u201cNewsletter Sign Up\u201d and the target behavior could be \u201cPurchase\u201d. User A signed up for the newsletter in January, and then completed purchases in March, April, and May. In a recurring cohort query, the user will appear in the January cohort row, and they will appear in the Month 3, Month 4, and Month 5 columns. In a first-time cohort query, the user will appear only in the Month 3 column. Thus, exclusivity of cohorts as they appear in the interval counts only occurs in a first-time cohort. Recurring cohorts can have a user appear in multiple intervals, though the total number of users will be exclusive.",
    "Cumulative vs Non-Cumulative": "All new queries default to the Non-Cumulative setting. Non-cumulative cohort queries show the count or percentage of users in the cohort who performed the target behavior within the interval. Cumulative cohort queries show the count or percentage of users in the cohort who performed the target behavior as a running total over time. Cumulative counts are only available for queries measuring first-time behavior. For a full explanation of cumulative and non-cumulative, seeCumulative vs. Non-Cumulative Analysis in Cohort.  Non-Cumulative Percent:Displays the percentage of users who completed the target behavior for each selectedtime intervalorbreakout.Non-Cumulative Count:Displays the count of users who completed the target behavior at each point in time defined by the selected time interval.Cumulative Percent:Displays the percentage of users who completed the target behavior for thefirst timeas a running total over time.Cumulative Count:Displays the count of users who completed the target behavior for thefirst timeas a running total over time.",
    "Visualization Options": "Cohort analyses have four different visualization options: Circle Heatmap, Heatmap, Line Chart and Area Chart. You can toggle between these options in the visualization dropdowns. You can also download a cohort analysis as a CSV file. ",
    "Annotations": "Cohort annotations act as general notes about the cohort analysis over the entire designateddate range. To add an annotation to Cohort, click on the Annotation flag icon in the query builder window and click Add an Annotation. To access existing annotations, click the Annotation icon in the Data Panel. For more information about annotations, visitthis article.",
    "Integration Requirements": "Before starting the integration process, any customers who are self-hosting Rudderstack\u2019s Data Plane should confirm their current versions by navigating to their Rudderstack Data Plane URL. This URL can be found where they initialize their SDKherein the value for<DATA_PLANE_URL>. Once you have the URL, you can locateDATA_PLANE_URL/versionsto get the transform and server versions. Ensure those are the latest, and if not, update them. Self-hosted Rudderstack users should ensure they are running the following versions or later: rudderlabs/rudder-server: 05102021.075534rudderlabs/rudder-transformer: 05102021.062759 SeeRudderstack Software Releasesfor the latest software versions.",
    "Begin an analysis": "First, you must select whether you would like to analyze users who performed an event, or users who are in a user segment. The default selection is performed an event. To toggle this, click on the dropdown at the top of the query builder.  Once you have selected your analysis type, begin inputting events in the query builder. If you have selected performed an event, you may input many events in the query builder. In this case, each user must have completed the specified events chronologically in the order that they were inputted.  You may select a different time zone from your project time zone on a per query basis by clicking on the globe icon on the top right of the query screen.  Then, select the date range in which users must complete the specified events. Users must have completed all events in the query builder within the specified date range.  Then, click the play button to run the query. The Users tool will return a list of users who have either performed the sequence of events, or users that are in the specified user segment, along with all of their user properties.",
    "Row order": "To be included in a User Insights list, users must have completed the events in the query builder in the specified order. To re-order the event sequence, click and drag a row in the query builder into the desired position in the sequence. ",
    "Searching results": "Search for users and their user properties by any matching value by using the search box in the upper right corner of the chart window. ",
    "User first lookup": "You can navigate directly to a user\u2019s profile by inputting their user ID, either authenticated or unauthenticated.  This feature drastically reduces the time associated with viewing a particular user\u2019s properties, event history, or event timeline.",
    "Access user insights from Segments, Funnels, and Cohorts": "Cross-Tool Compatibility allows you to recreate your analysis from one tool in another tool within the Analytics suite. User Insights may be used to view analysis results from Segmentation, Funnel, and Cohort and drill down into the behavior of individual users. To learn more about how User Insights complement each Analytics tool, view the articles below: Cross-tool compatibility in SegmentationCross-tool compatibility in FunnelsCross-tool compatibility in Cohort",
    "Rows in the Query Builder": " Query Rows represent user actions. A row may contain any combination of events, event properties, and user properties. Rows in the query builder are numbered for easy identification in analysis results.",
    "Create a Query Row": " Every row begins with an initial event. To add an event to a query builder, you can click on+ Select an Event Once an event is added, additional clauses and filters may be added: Understand Breakouts with By ClausesUnderstand Filters With Where ClausesModify Filters With And/Or Clauses",
    "Label a Query Row": " Each query row may have a custom annotation added to it. This allows you to organize your analysis, improve readability, and ensure that data points in analysis results are appropriately labeled. These labels remain a part of the query when saved, added to a dashboard, or shared via URL.",
    "Collapse the Query Builder": " To collapse the query builder, choose the collapse icon. It is located on the right hand side of the query builder at the bottom of the query rows. Collapsing the query row does not affect the analysis, it only reduces the size of the query builder. To expand the query builder again, select the expand icon.",
    "Save a Query Row as a Custom Event": " Some query rows may be saved as custom events. Query rows may be saved as custom events if they consist of a single event and/or contain one or more Filter Where clauses. Use custom events to save query rows that you create frequently. To be saved as a custom event, a row: Must contain 1 event, and no For clausesMay contain an unlimited number of Where clausesMust not contain By clauses Once saved, the new event will appear in the Data Panel.",
    "Duplicate a Query Row": " When creating variations of an existing row, Duplicate in the query row menu enables you to easily clone rows for modification.",
    "Deactivate a Query Row": " Hiding a query row excludes it from analysis results, but does not delete it. This can be useful for viewing analysis components individually, or tentatively excluding rows as you progress through an analysis. To deactivate a row, go to the right of the event and select the deactivate button. You can reactivate the row by selecting the Activate button.",
    "Delete a Query Row": " Deleting a query row will permanently remove it from the query. To delete a row, navigate to the right side of an event and choose the delete button.",
    "Create an IAM Role for the Lambda": "Your AWS Lambda needs to have an Execution Role that allows it to use the Kinesis Stream and CloudWatch. (For more information on setting up IAM Roles, please seethe official AWS tutorial.) Go toIAM Managementin the Console and chooseRolesfrom the sidebar.ClickCreate role.For the type of trusted entity selectAWS Serviceand for the service that will use this role chooseLambda. ClickNext: Permissionsat the bottom of the screen.Now you need to choose a permission policy for the role. The Lambda needs to have read access to Kinesis and write access to CloudWatch logs - for that we will chooseAWSLambdaKinesisExecutionRole. Search forAWSLambdaKinesisExecutionRolein the search and mark the checkbox as shown below.ClickNext: Reviewat the bottom of the screen.On the next screen provide a name for the newly created role underRole Name, then clickCreate roleto finish the process.",
    "Create the Lambda Function": "The Lambda function can be created either directly through AWS Console or through other tools like the AWS CLI. For this integration, the recommended memory setting is 256 MB and because the JVM has to cold start when the function is called for the first time on a new instance, you should set a high timeout value; 90 seconds should be safe. As with the IAM Role, we will be using the AWS Console to get our Lambda function up and running.Make sure you are in the same region as where your Kinesis streams are defined. On the Console navigate to theLambdasection and clickCreate a function(runtime should beJava 8).Write a name for your function inName. In theRoledropdown pickChoose an existing role; then in the dropdown below choose the name of the role you created in the previous step. ClickCreate function.The Lambda has been created, although it does not do anything yet. We need to provide the code and configure the function:a. Take a look at theFunction codebox. In theHandlertextbox paste:com.snowplowanalytics.indicative.LambdaHandler::recordHandlerb. From theCode entry typedropdown pickUpload a file from Amazon S3. A textbox labeled S3 Link URL will appear. We are hosting the code through ourhosted assets. You will need to choose the S3 bucket in the same region as your AWS Lambda function: for example if your Lambda isus-east-1region, paste the following URL:s3://snowplow-hosted-assets-us-east-1/relays/indicative/indicative-relay-0.4.0.jarin the textbox. Take a look atthis tableto pick the right bucket name for your region. Make sureRuntimeisJava 8.Get theAPI Keyfrom step 4 from the Analytics UI.BelowFunction codesettings you will find a section calledEnvironment variables.a. In the first row, first column (the key), typeINDICATIVE_API_KEY. In the second column (the value), paste your API Key.b. The relay lets you configure the following filters:- UNUSED_EVENTS: events that will not be relayed to Analytics;\n  - UNUSED_ATOMIC_FIELDS: fields of the [canonical](https://github.com/snowplow/snowplow/wiki/canonical-event-model) Snowplow event that will not be relayed to Analytics;\n  - UNUSED_CONTEXTS: contexts whose fields will not be relayed to Analytics. Out of the box, the relay is configured to use the following defaults: To change the defaults, you can pass in your own lists of events, atomic fields or contexts to be filtered out. For example: Similarly to setting up the API key, the first column (key) needs to be set to the specified environment variable name in ALLCAPS. The second column (value) is your own list as a comma-separated string with no spaces. If you only specify the environment variable name but do not provide a list of values, then nothing will be filtered out. If you do not set any of the environment variables, the defaults will be used. Scroll down a bit and take a look at theBasic settingsbox. There you can set memory and timeout limits for the Lambda. As mentioned earlier, we recommend setting 256 MB of memory or higher (on AWS Lambda the CPU performance scales linearly with the amount of memory) and a high timeout time of 1 minute 30 seconds. kinesis As a final step, add your Snowplow enriched Kinesis stream as an event source for the Lambda function. You can followthe official AWS tutorialif you are using AWS CLI or do it directly from the AWS Console using the following instructions. Scroll to the top of the page and from the list of triggers in theDesignerconfiguration up top, chooseKinesis. Take a look at theConfiguretriggers section which just appeared below. Choose your Kinesis stream that contains Snowplow enriched events. Set the batch size to your liking - 100 is a reasonable setting. Note that this is amaximumbatch size, the function can be triggered with fewer records. For the starting position we recommendTrim horizon, which starts processing the stream from an observable start (Alternatively, you can selectAt timestampto start sending data from a particular date). Click theAddbutton to finish the trigger configuration. Make sureEnable triggeris selected.  Save the changes by clicking theSavebutton in the top-right part of the page.",
    "Validate Your Data": "Go to yourIndicative projectto check if you are receiving data. You can also go to thedebug consoleto troubleshoot the relay in real time.",
    "Export Users from a Point": "To download a list of users from a single point, click on any data point or table cell within your results. Within the dropdown, select Download Users in this Point to CSV. A list containing all users associated with this data point will be emailed to the address connected to your Analytics account. There is no limit on the number of users that can be exported.",
    "Export Users from a Series": "To download a list of users from an entire series, click on any data point or table cell within your results. Within the dropdown, select Explore Users. If your query contains a series, you may choose to view users from a single data point or from the entire series. From this Point creates a list of users from only one interval whereas From Entire Series creates a list of users from all of the intervals within the date range. Once your Users query results have loaded, simply click on the export icon located in the menu bar beneath the query builder, then select Download CSV. Users analysis is not available for metric visualizations.",
    "Direct link to specific user activity timeline": "Analytics users may directly access a specific user\u2019s Activity Timeline by appending the User ID to the end of this url in the following fashion: app.indicative.com/#/users/detail/USER_ID_HERE An example link would be app.indicative.com/#/users/detail/6834fa-dfa12-4389. Using this direct link would lead to that user\u2019s specific User Activity Timeline: ",
    "Pin event properties": "You can pin additional fields to the expanded session view: See important event properties at a glance without having to expand each eventPersist these properties across sessionsEach person can choose their own items to pin To pin an event property in the user activity timeline: Navigate to the Users page by clicking the green plus sign and selectUser Lookup by Event:Select an event and run the analysis:Analytics displays all the users associated with that event:Click on a user to display event properties:Click on the Events (in this example, 80 Events), and for the event whose property you wish to pin, hover to the left of the label until you see the pin icon:Click the pin to pin that property.Now the property you pinned is displayed for every event: To unpin a property, click the pin icon next to any of the pinned events to deselect it. Note the following about event property pins: Pins are set at the user level and are persistent across sessionsPins are set at the project level and do not persist between projectsPins are limited to 5 per user per projectAll Analytics users across all plans should have access to pin, as long as they have access to the Users toolAnalytics doesn\u2019t support event-specific pinning. A pin applies to all qualified events.",
    "Manage your event tiers": "There are two ways to modify your event tiers: Individually through the mParticle UIIn bulk by modifying a CSV formatted Event Volume Report",
    "Modify an event tier in the UI": "To change the event tier using the UI: Navigate toData Master > Catalogand select the event type such asCustom EventsorScreen View.Next to the event, click theEvent Tierdrop-down and select a new tier.ClickApply. ",
    "Modify event tiers in bulk": "You can make bulk modifications to your event tiers by downloading a copy of your Event Volume Report, entering your new tiers in the New Event Tier column in the downloaded file, and then re-uploading your new report to commit your changes in mParticle. To make bulk changes to your event tiers: Navigate toData Master > Catalogand hover your cursor over theEvent Tiertooltip.ClickManage Event Tiers.  SelectDownload Event Volume Reportand clickNext.  Under Select Event Volume Data Range, use the date picker to select one of the date range presets or enter a specific date range to include in your Event Volume Report.Regardless of the date range you select for your report, it will represent every event type that mParticle has ingested for your account. The date range selection simply allows you to view the relative volume of events ingested for each type within a specific timeframe.   After entering the date range of the event data you want included in your report, clickDownload Report.Open the Event Volume Report downloaded to your computer. Find the row for each event type you want to change the tier for, and enter the tier in theNew Event Tiercolumn. Make sure to save your changes.Remember, the three event tiers areConnect,PreserveandPersonalize.  To upload your modified report, repeat steps 1-2, selectBulk Update Event Tiers, and clickNext.  In the Bulk Update Event Tiers modal, clickUploadand select your modified Event Volume Report.ClickUpdate Event Tiersto upload your modified Event Volume Report. When the upload is complete, a new Event Volume Report is automatically generated and downloaded to your computer so you can confirm that your changes were made correctly.",
    "Change the default event tier": "By default, mParticle assigns all event types to the Personalize tier. However, you can change the default assignment to Preserve or Connect if your account usesvalue-based pricing. Changing the default tier is helpful. For example, if you create a data plan with many new event types and most of them won\u2019t be needed for real-time evaluations, you could assign them to the Preserve or Connect tier by default. To change the default event tier for a workspace: In the left-hand navigation, click theSettingsgear cog to open the settings menu.  ClickWorkspaces.From the list of workspaces, click a workspace name to open the workspace settings dialog.ExpandDefaultsand use the drop-down in the Event Tiers section to choose a new default tier.ClickSaveto save your changes. After you set a default tier, all new events of all types seen in mParticle are assigned the new default tier. You can override the default tier assignment of an event type inData Master > Catalog. For example, you could change the default tier to Connect in the workspace setting and also set the tier for event type MyEvent to Personalize. Then, all new and existing events of type MyEvent are assigned the Personalize tier, while all other new events are given the new default, Connect.",
    "Event Volume Report": "The event volume report, accessible fromData Master > Catalog, is a CSV file that lists all the events used in calculated attributes and audiences. It also shows data volumes for the time range you specify.  The Event Volume Report displays an approximate count of each event type ingested from your inputs within a certain date range. You can use these metrics to identify event types to change to a different event tier, or event types to exclude from calculated attributes and audience definitions.",
    "Event Volume Report and value-based pricing": "There are differences between the Event Volume Report and your Monthly Credit Usage Report, which is a record of thebillable itemsyou are charged for. To learn how billable items are calculated, seeUsage metering for value-based pricing (VBP). The two reports present different views of your data ingestion: TheMonthly Credit Usage Reportprovides a retrospective view of the services your account is charged for according to the billing logic documented inUsage metering for value-based pricing.TheEvent Volume Reportprovides a view of how data is being ingested at the time the report is generated, and should be used to help you select your event tier assignments.",
    "Differences between the Event Volume Report and Monthly Credit Usage Report": "Following is a list of differences between the Event Volume Report and Monthly Credit Usage Report: The Event Volume Report doesn\u2019t include events added or dropped as a result of Rules, but those events are included in the Credit Usage Report.The Event Volume Report displays event tier assignments at the time the report is generated, but the Credit Usage Report displays event tier assignments for events at the time they were ingested and processed. For example, imagine the following scenario:Day 1 of your billing period: you ingest one million events with theScreen Viewevent type in theConnectevent tier.Day 5 of your billing period: you change the event tier forScreen ViewfromConnecttoPreserveand you ingest another one million events.Day 6 of your billing period: you download your Event Volume Report and see two millionScreen Viewevents in thePreserveevent tier. However, your Credit Usage Report will show one million events in theConnecttier and one million events in thePreservetier.The Event Volume Report includes automatically generated events like thebatch.SystemNotifications, which could include events likeConsentGrantedandConsentDenied. However, automatically generated events are not included in the Monthly Credit Report, because they are not billable items.If you download an Event Volume Report before a batch of events is processed, they will not be reflected in the Event Volume Report, but they will be reflected in the Credit Usage Report.Events timestamped 72 hours prior to when they are ingested are not be reflected in the Event Volume Report.For example: Imagine that on February 24, 2022 at 11:00 UTC, you ingest an event that is timestamped atFebruary 21, 2022 at 10:00 UTC. An Event Volume Report report created on February 21st will not reflect that event. However, it will be reflected in your Monthly Credit Report.",
    "Set a project-level sample size": "By default, Query-time Sampling is set toDisabledat the project level. This means that 100% of users will be queried unless a user toggles on sampling at the query level. Project admins can set queries to sample 10% of users as the default behavior. To do this: Navigate to theGeneraltab underProject Settings.Set Query-time Sampling (underFormats and Defaults) toEnabled. ",
    "Toggle sampling at the user level": "Individual users can toggle Query-time Sampling on and off when executing queries by clicking the lightning icon at the top-right of the query. A filled-in icon means sampling is enabled:  When sampling is enabled or disabled at the user level, this setting will persist within a session, but will be reset when changing projects.",
    "Applicable analysis types": "Query-time sampling is available inSegmentation,Funnel, andCohortanalyses. Samples can also be applied in the User tool in Event Lookup mode, but but not in User ID or User Segment lookup modes.",
    "Data Forwarding and Connections": "Data about an event, including individual attributes, may be forwarded from the input to an output in a variety of ways:  Server-side: Data is forwarded from the web or client app to mParticle servers, stored there, and from there is forwarded on to an output or destination such as Braze. For these types of forwarding, no client setup is needed, because the client communicates directly with mParticle.Client-side: Data is forwarded from the web or client app directly to the output, unseen by mParticle servers. For this type of forwarding, you must set up your client, usually by adding a kit to the platform SDK of the client app. Some integrations allow you to choose either server-side or client-side when you configure a connection.",
    "Connection Workflow": "No matter which data flow your integration uses, a connection is required. A connection is the combination of an input, an output, and the configuration information required to make the connection work. Most of the configuration information is specified in the mParticle UI, but some values may need to be fetched from the output, and you may need to add a kit to an SDK for some client-side integrations. The Connections screen controls how event data from your inputs (iOS, Android, Web, Feeds, etc) is forwarded to your output platforms. You must set up a separate connection for each input-output-configuration combination. For each connection, you have several opportunities to cleanse and filter your data, to ensure that each output receives the data you want it to receive, in the correct format.  Each output has its own requirements, so the process for setting up each connection is a little different. However, all connections require these basic steps: Create a connection.If needed,add a kit to the SDK for your input platform.Activate the connection.Verify that data is being forwarded.",
    "Create a Connection": "Data flows once an input and output are connected and the connection is active. For an overview before you create your first connection, view the following video:  1. Select an input Navigate toConnections > Connectin the mParticle UI.From the list displayed, select the input you want to configure. If the list is empty, go toSetup > Inputsto create an input. 2. Apply \u2018All Outputs\u2019 transformations Once you have an input selected, you can set up transformations that are applied to all outputs connected to that input. ClickAll Outputsto see options. There are two transformations that can be applied here: RulesUser Splits 3. Select an Output Once you have selected an input, you will see a list of available outputs that can receive data from your selected input. If this list is empty, go toSetup > Outputsto create some outputs. The following video shows how to create an output:  The mParticle UI may indicate that you need or may need to add a reference to a kit in your platform dependency configuration. You can do this after you create the connection but before you change theConnection Statusto active. SeeAdd a Kitfor more information. 4. Complete Connection Settings Complete any settings that apply to the connection. These will be different for every output but can include: Credentials or Account/Workspace identifiersWhat user identifiers and attributes should be sent. You must choose aUser Identification(identity type) or data may not flow.Encoding to be used for identifying dataHow custom attributes should be mappedHow to handle attributes specific to the OutputThe minimum platform version of your input that the connection will forward data from.If you set theMin Platform Versionconnection setting, then mParticle will only forward data from inputs with anapplication_versionthat is equal to or greater than the version you set. This allows you to create a separate connection for different versions of your app.The version number you set for the Min Platform Version connection setting must only contain numeric characters (0through9) and decimals (.) Using any non-numeric characters when setting your Min Platform Version will cause the connection to fail. 5. Apply \u2018Specific Output\u2019 transformations The second set of transformations apply only to your selected Output. ClickSpecific Outputto see options. Specific output transformations include: Event Filter- note, this is not part of Connections Screen but should be configured before the next step if needed.Specific Outputs RulesForwarding RulesCustom MappingsUser Sampling",
    "Add a Kit": "When you configured your output in step 3, the mParticle UI may have indicated that you do need or may need a kit added to the SDK for your app or web pages: If so, check the integration documentation for your output. If a kit is required, follow the instructions for adding the kit to your input platform dependency configuration.",
    "Activate the Connection": "After you have completed the required settings, set up any transformations, and added a kit (if needed), you are ready to activate the connection: Navigate toConnections.Select the input for your connection.Click the output you are ready to activate.Click the Connection Settings gear icon.Click theConnection Statusslider so it displaysActive.ClickSave. Very large data volumes may take up to 48 hours to process. To reduce processing time, reduce the number of sessions your account sends to fewer than 200,000 per day.",
    "Verify Your Connection": "Verify that data is flowing. Check in the mParticle UI and in your downstream app or system (output). Wait for the time indicated in the mparticle UI to ensure your connection has been activated. Additionally, some outputs such as Google Analytics have their own processing delays. Check the \u201cData Processing\u201d section of [the integration documentation(/integrations) for your output.OpenData Master > Live Streamand select the following values:The input fromInputsThe output fromOutputsInMessage Direction, selectBoth In and Outto check whether events are being forwarded.InDevice, leave the default valueAll Dev Dataunless you are verifying the flow to a device in production. In that case, choose the relevant device.Check that you have chosen aUser Identification(identity type).Events should be listed as they occur:  If you don\u2019t see events being forwarded,troubleshoot your connection. To verify that data is arriving n the downstream system: In mParticle, look inData Master > Live Streamand select an event.Search for that event in your downstream system. If you don\u2019t see events being forwarded,troubleshoot your connection.",
    "Troubleshooting Connections": "Follow these steps to troubleshoot an event connection: Make sure you have waited for the time period specified in the mParticle UI before troubleshooting further.CheckActivity > System Alertsfor any fatal errors or warnings and resolve them.If your events are not appearing in the output although the mParticle Live Stream suggests that the connection is active, follow the steps inVerify your connectionto ensure your connection is working. Although Live Stream has indicated that events are forwarded downstream, there might be issues downstream that prevent successful forwarding. The next steps will help you find this type of problem.Does your connection depend on a kit? Does your connection use a kit to forward data downstream? Has the kit been included in your application? If yes, check your application for HTTP requests directed to the partner. Have they succeeded or are they reporting errors?Does your connection use batch forwarding? Some outputs use batch forwarding. You might have to wait longer for events to arrive in these systems (approximately 10 minutes or after several event batches have been collected). Still not sure what\u2019s wrong? ContactmParticle Support.",
    "All Outputs Transformations": "SeeRulesfor more information on all-output rules.",
    "Specific Output Transformations": "mParticle lets you customize the data that you send to each output. There are many reasons to do this, including: Filtering out personally identifiable information (PII);Filtering out data containing company insights you don\u2019t want to share with a particular service;Filtering out events that you don\u2019t need to track in a particular service;Filtering out information from places or customer types you don\u2019t want to track in a particular service;Enriching the data you send to a service with extra user info from an external source;Reformatting your data to match what a particular service accepts.",
    "The Data Filter": "Unlike other transformations, the data filter exists on its own page, separate from the Connections screen. A data filter allows you to decide which events/attributes you want to send to each output. By default, all event attributes are enabled when you first activate a connection. From the event filter you can: Decide whether new events and attributes should be forwarded by default.Turn forwarding on/off for each event, by event name.Turn forwarding on/off for attributes of each event, by attribute name. SeeThe Data Filterfor more information.",
    "Forwarding Rules": "Like the event filter, forwarding rules let you filter out events from being sent to an Output. But where the event filter is based on event and attribute names, forwarding rules look at values, which lets you build some more complex conditions. There are several types of forwarding rules. Attribute:Attribute rules take an event attribute name and a value. You can choose to either not forward events that match the rule, or to only forward events that match the rule, excluding all others. Greater than / less than comparisons are not possible. Matching is case sensitive and exact.  If an attribute is criteria for the forwarding rule, but is omitted from the source payload, it is treated as if the attribute key exists and the value doesn\u2019t match.Attribution:Attribution rules filter events according to Publisher information. You can choose to exclude events attributed to a specific publisher, or forward only events attributed to that publisher.Consent:Data privacy controlsallow you to filter events based on whether a user has given consent to a particular data collection purpose.ID Sync: ID Sync rules allow you to only forward data from logged-in users. A logged in user is one with at least oneLogin ID, as defined by your Identity Strategy.",
    "User Sampling": "User Sampling is applied to a single output and sends only a subset, or sample of your data to an output. The main reason to do this is to control costs on services that charge by volume of data. Data is sampled on a user level, not an event level - if you select a 50% sample, mParticle forwards all data received from half of your users, not half of each user\u2019s data. ",
    "Specific Output Rules": "SeeRulesfor more information about specific-output rules.",
    "Custom Mappings": "Some services allow your incoming events to be translated into events specific to the service. For example, if you have a custom event named \u201cNextLevel\u201d, typically this event would be forwarded as a custom event to a service. With custom mappings, you can specify that this event be forwarded to a service using their specific event name. For example: For partners that support custom mappings, the output\u2019s events are listed on the left side of theCustom Mappingstab. For each event, you can then select an mParticle event and associated attributes to map to the partner\u2019s event.  The following integrations support custom mappings: AgilOneAlgoliaAmazon Mobile AnalyticsAppLovinAppsFlyerCriteoFacebookFiksuGoogle Ads Enhanced ConversionsGoogle Analytics for FirebaseGoogle Analytics 4 (GA4)IterableNCR AlohaOptimizelySimpleReachSnapchatTikTok If an event has a Custom Mapping for a particular connection, it will be displayed with an icon in the Event Filter  If you turn off forwarding of an event with a Custom Mapping, the mapping information will be deleted.",
    "mParticle Forwarder Module": "The final and most crucial transformation step is the mParticle Forwarder Module itself. After all your other transformations have been completed, the forwarder module turns your data into messages to the output in its preferred format. Each integration has its own forwarding module. Settings for the forwarder are derived from three places: Some data is handled based on hard-coded settings. For example, any device information (such as device model or operating system type) the output service accepts is usually forwarded without the user needing to set anything up.Some data is handled according to the connection settings. For example, in the Mixpanel settings, you can choose whether you want to forward session events, and decide what they should be called.Anycustom mappingsyou have created. Based on these settings, mParticle transforms your data into a format accepted by the output. This can involve extensively reformatting the data. For example, Mixpanel\u2019s API accepts events, with attributes given as a flat set of key-value pairs. To fit this structure, a single mParticle eCommerce event with four products will be transformed into four Mixpanel Events - one for each product - with common attributes, such as user and device info, repeated for each event. The documentation for each integration will tell you what you need to know about how data is transformed to be accepted by the Output service.",
    "Best Practice for Transformations": "mParticle provides many opportunities to transform and enrich your data. It is often possible to perform the same transformation in more than one place. For example, if you wanted to drop all Application State Transition events for a given output, you could use the event filter, or you could write a condition in an output rule. There are advantages to each choice. The event filter can be used by anyone with the appropriate access to your organization in the mParticle Dashboard, so it is easy to update and maintain. Writing a rule gives you much finer control over your data, but rules may be difficult for non-developers to understand or alter. Make the necessary transformations to your data in as few steps as possible. The fewer times you alter your data, the easier your integration will be to troubleshoot and maintain.",
    "More Help": "To find out more about event integrations in mParticle, view the following video: ",
    "Calculate Using Rows": "To create a calculation using two rows, simply select the row name you\u2019d like to use, then drag and drop the row name into the first (top) or second (bottom) drop zone. You may switch drop zones by selecting the up and down arrows to the left of the calculator field. Remove a row from the calculator by selecting the red \u201cX\u201d icon before the row name. The calculator supports the four basic mathematical operations- addition, subtraction, multiplication, and division- and can also automatically calculate a percentage. Percentage and Division are based on the same calculation, with the difference being that Percentage does an additional calculation of multiplying the resulting value by 100. To change the operation for your calculation, simply select the relevant icon. Once you\u2019ve completed each field and selected an operation, choose Calculate to create a new query row containing your calculation. Selecting a calculation row will re-open the tool for editing. Calculations are visualized in the chart area alongside the underlying queries.",
    "Calculate Using Numbers": "To add a number to your calculation, simply select the first (top) or second (bottom) drop zone and enter the value. Numerical values can be used independently or in combination with a query row.",
    "Complex Calculations": "Only two rows may be used in a single calculation. If you\u2019d like to create a calculation with more than two rows, you\u2019ll need to use a calculation query row as a component of another calculation. For example, to calculate the sum of A+B+C, you may calculate A+B and then use this new row to calculate (A+B)+C. Changes in underlying query rows are calculated before calculation query rows, so your calculation will update automatically if its underlying data changes. You may hide the component queries by hovering the cursor over the row you\u2019d like to hide, then selecting Deactivate Row. This will not delete the row, but the hidden row will not appear in your results. Deactivated rows are grayed out in the query builder.",
    "How It\u2019s Done": "Take a look at the following example query: To decide which blog view events will be counted, Analytics will look at the timestamp on each blog view and check to see if there is a create profile event that falls in the 7 subsequent calendar days. If this is the case, the blog view will be counted in the analysis. Note: Did [not] Perform\u00a0clause events could have occurred outside of the main date range in the query builder. For example,\u00a0Subscribe\u00a0events in the chart window could have occurred within the Last 7 Days of the current date, as set in the date range menu at the base of the query builder. However, the\u00a0Email Clicked\u00a0and\u00a0Blog View\u00a0events will have occurred within the seven days prior to the\u00a0Subscribe\u00a0event. If\u00a0a\u00a0Subscribe\u00a0event occurred six days ago, and\u00a0Email Clicked\u00a0occurred six days prior to that, the\u00a0Subscribe\u00a0event will be counted even though\u00a0Email Clicked\u00a0occurred 12 days prior to today.",
    "Access the Annotation Manager": "There are three ways to access the annotation manager. Choose Manage Data in the main menu bar on the left of your screen, then select Annotations on the top menu bar. Annotations settings allows you to view, edit, archive, delete, or subscribe to your existing annotations, add a new annotation and manage categories.From any tool, select the flag dropdown in the top right corner of the chart area. This opens the in-widget annotation manager.From any dashboard, select +New on the top menu bar, then Annotation Widget. This creates a standalone Annotation Timeline widget, displaying annotations throughout the history of your project. ",
    "Add an Annotation": "To add an annotation from the Annotations settings, select +Add Annotation at the top of the table to the left of the search bar. To add an annotation within the in-widget annotation manager, select +New in the top right corner of the manager or click anywhere along the timeline at the bottom of the manager. The Add an Annotation screen contains seven fields. Category and Annotation Name are required fields. Description, Select Events, Select Subscribers, Start Date, and End Date are optional fields. The optional fields will automatically populate with default contents which can be customized at your discretion. ",
    "Manage Annotations": "Use categories to organize your annotations by theme, by team, by product, or by the events that are associated with the annotation. Assign a different color and label for each category. To edit an annotation, select the flag icon in the annotation manager, then choose the pencil icon to open the Edit Annotation screen.",
    "Annotation Timeline Widget": "The Annotation Timeline widget displays all of the annotations throughout the history of your project. It is a standalone widget displayed alongside the other widgets on a dashboard. To add an Annotation Timeline widget, select +New in the dashboard options located at the top right corner of the dashboard, then choose Annotation Widget. The Annotation Timeline widget will be created at the bottom of your dashboard. You may relocate or resize the widget just like any other dashboard widget. You may also interact with the widget to view, edit, delete, or subscribe to your existing annotations, add a new annotation and manage categories. Please note that the widget displays only annotations attached to events that are present in queries on the dashboard.",
    "Creating a reverse funnel": "There are two ways to create a reverse funnel. The first, and the most common, is to create a forward funnel in the query builder, and then reverse it in the Settings dropdown within the menu bar. Once you change the funnel direction to reverse, notice that the change is reflected in the query builder. Each step after step A now begins with \u201cand preceded by.\u201d While in reverse funnel mode, date ranges will still apply to Row A, or the left-most donut in the Funnel visualization. In the following example, PetBox is trying to measure the influence of their email marketing campaign on who eventually purchased their product sales.  A forward funnel measures how many people who clicked their email and then went on to purchase the product. However, the more relevant journey here is a reverse funnel. Once PetBox changes the funnel direction to reverse, they can now calculate the amount of users that came through their email campaign as a percentage of the total users who purchased their product. ",
    "Rule execution": "Each of your Inputs, such as for your web, mobile, or server-to-server data, has an individually configured data pipeline, and each Input\u2019s pipeline contains the same key stages. Rules are therefore applied for a specific Input\u2019s pipeline, and it\u2019s up to you choose where in that Input\u2019s pipeline each Rule is executed. A single Input pipeline may contain multiple Rules each stage.",
    "mParticle pipeline stages": "Stage 1 - Input Data is received by mParticle for a specific Input (such as Web, iOS, or a custom server feed). Stage 2 - Storage and Processing The Input\u2019s data is stored and processed by mParticle, including: mParticle\u2019s profile system, which stores user data and enriches the Input\u2019s data based on the existing profile of that user.mParticle Data Master tool Stage 3 - Output The Input\u2019s data is sent individually to the mParticle Audience system and 300+ partner integrations. In this stage the pipeline actually branches out with a single Input potentially being connected to many Outputs.",
    "Rule application": "Rules are applied to a specific Input\u2019s pipeline. There are two places in the pipeline where rules can be applied: In between Stage 1 and Stage 2 Rules executed between Stage 1 and Stage 2 affect the data sent to both Stage 2 and then Stage 3, including the mParticle profile store, audience store, and all outputs. These are labeled \u201cAll Output\u201d Rules in your mParticle dashboard. In between Stage 2 and Stage 3 You can also apply a rule right before it\u2019s sent to aspecificOutput. This lets you mutate data to handle specific requirements or mappings that need to occur for a given partner integration.",
    "Rule requirements": "All rules accept an mParticle Events API batch object and can return a modified or null batch object.There are some differences in error handling and available fields depending on pipeline location. SeeRules Developer Guidefor details.A 200ms timeout applies to all rules. You can choose if a batch should be dropped or continue unprocessed by the rules in the case of a timeout.Rules are executed on the server and only act on data forwarded downstream server-to-server. A warning is shown in the dashboard if you set up one of the following rules:A rule for integrations that forwards data client-side via a kit.A rule for hybrid integrations that support forwarding via client-side and server-to-server.If you are using a rule to modify user identities or user attributes, you must include a \u201cUser Identity Change Event\u201d (user_identity_change) or a \u201cUser Attribute Change Event\u201d (user_attribute_change). SeeRules Developer Guidefor an example ofuser_attribute_changein a rule.",
    "Create a function in AWS": "mParticle rules are hosted in your AWS account as Lambda functions. To do this, you need to be able to provide an Amazon Resource Number (ARN) for your rule. See theAWS Lambda documentationfor help creating a function.  The Lambda functions used for rules must be hosted in the same AWS region as yourmParticle account. The name of the function must begin with \u201cmpr\u201dYour development rule must have an alias of \u201c$LATEST\u201dYour production rule must have an alias of \u201cPROD\u201d Your ARNs should look something like this: arn:aws:lambda:us-east-1:999999999999:function:mprmylambdafunction:PROD arn:aws:lambda:us-east-1:999999999999:function:mprmylambdafunction:$LATEST",
    "IAM user": "To connect to your AWS Lambda function, you must provide the AWS Access Key ID and Secret Access Key for anIAM user. In the IAM dashboard, add the following permissions policy for the user:",
    "IAM role": "You will also need to create a role in IAM. Assign this role the same policy document created above.  Assign this role to each Lambda function you plan to deploy as an mParticle rule. ",
    "Creating a rule in the dashboard": "Create a rule by navigating toData Master > RulesClickNew Rule.  Enter your Development and Production ARNs and clickTest. ",
    "Error handling": "When you first test a rule, you must select aFailure Action. This determines what happens if your rule throws an unhandled exception. There is no default action, you must select one of the following: If you chooseDiscard, an unhandled exception causes your rule returnnull, effectively dropping the batch.If you chooseProceed, an unhandled exception causes your rule to return the unaltered batch object, proceeding as if the rule had not been applied. Regardless of which option you choose, it\u2019s best practice to handle all exceptions in your code, rather than falling back on the above defaults. This is especially true if your rule deals with events, where an unhandled exception from just one event could lead to all events in the batch being dropped.",
    "Javascript syntax": "Your code must be a validLambda function. batchis the complete incoming batch object.contextis a required argument for Lambda functions, but is effectivelynullfor mParticle rules.",
    "Testing rules": "The first time you test a rule, you are asked to provide a name, description and failure action. After naming a rule, you can test it by using one of the sample templates provided in the Test rule dialog. You can also copy and paste batch JSON from your Live Stream. If you do this, be sure to pick a full batch to copy, not a single event.  ClickTestto run. Optionally, check a box to save your JSON template in local storage for future testing. You must enter validbatchJSON in the code editor. If there are any syntactical errors in your code, warning or error icons will display next to the line number with details of the problem so you can correct. After clickingTest, you can examine the JSON output from your function to see that the input has been modified as expected. After a successful test you can clickSaveto save the Rule. Due to recent updates in AWS Lambda, it may be necessary to wait one minute after a successful test in order to save the Rule. If your test fails, try examining thelogsfor any console output.",
    "Versioning": "When you first create a rule, by default it will only be applied toDEVdata. As well as testing a rule with sample JSON you should test the rule in your dev environment to make sure data reaching your output services is as expected. When you are ready to apply a rule to your production data, clickPromote to Prodon the rule page. This will create a \u201cv1\u201d production rule. Note that before a rule can be promoted to Prod, you must remove allconsole.log()statements. If you need to make changes, choose$LATESTfrom theVersiondropdown. All other versions are read only. Test your changes with your dev environment and, when you are ready, clickPromote to Prodto create \u201cv2\u201d of your production rule. Note that you can have a maximum of 50 versions per rule. If you have too many versions, select a version and click the trash can icon to the right of the version number to delete it.",
    "Logs": "To help you with troubleshooting rules, mParticle maintains logs for each rule where you can view all console output. From an individual rule page, select theLogstab. You can filter messages by date range or search for keywords. ",
    "Deleting rules": "From the rules listing, select theDeleteaction to delete the rule. If the rule is applied to any connections, it will be removed from those connections.",
    "Goal/Business Question": "Are cat food purchasers also purchasing cat toys?",
    "Create a Segment of Cat Food Purchasers": "We first need to create auser segmentfor cat food purchasers in Segmentation. You can do so by selecting thePurchase Productevent and forFilter Where, selectProduct Category. In the text box to\nthe right ofis equal to, you should type \u201dFood\u201c.If the start of the query row readsTotal count ofinstead\nofUsers who performed, you should make that change as well.\nYour final results should look like this:Use04/01/xxxx to Todayin your date range selector and\nrun the query. Click on any point and hover overCreate User Segment. Be sure to then click onFrom entire seriesto get the users from the full date range.Below is theCreate a User Segmentmodal. Name the segment\nand provide a description so we can remember what we saved in the future.\nSelect a category or create a new one by typing the name in so you can easily\nfind your user segment in the future. Note theOne-time/Dailytoggle. In this case, we want the user segment\nto update with the most recent results (refreshed daily), so we will selectDaily.",
    "Examine and Extract for Cross-Selling": "All of the prep work is done. You have set up a user segment of cat food purchasers\nand are ready to do your analysis.",
    "Back to our question: Is this group purchasing cat toys?": "Let\u2019s build out our query. We want to see the number ofUsers who performedPurchase Product, usingFilter WhereProduct Categoryis \u201dToys\u201d\nthis time, instead of \u201cFood\u201d.Remember, we want to see if cat food purchasers are buying cat toys so the\nonly thing missing from this query are the cat food purchasers. Click on\nthe newFilter Whereselector and find yourCat Food Purchaserssegment.Now run the query and let\u2019s take a look at the results.",
    "Export your users to send a targeted campaign": "You can export your users to engage with them outside of Analytics in one of\ntwo ways: Download aCSVand upload it to your marketing tools.Connect to ourSegments API(available for Pro and Enterprise users) to make API calls to your automation\ntools. We hope this tutorial gives you ideas on how you can use Analytics to analyze\nyour data and achieve actionable insights from it. If you have any questions\nor comments, please reach out tosupport@mparticle.com.",
    "Track Events": "We offer two options for submitting events to our REST API: submitting one event at a time, or submitting batches of events.",
    "Track a Single Event": "Single events can be tracked either through the POST method or the GET method Our REST API\u2019s single event endpoint is https://api.indicative.com/service/eventorhttps://api.indicative.com/service/event/{Your API KEY} To send us an event, you\u2019ll need to make a POST request to that URL with a Content-Type of \u2018application/json\u2019. In the POST body, include a JSON object with the following fields: As an example, your JSON Object would resemble the following: Our servers will respond with one of a few status codes indicating the outcome of your request: Analytics supports GET requests via its HTTP API. To utilize, send the same arguments included in the JSON of a normal POST request encoded as query parameters, with the exception of the properties dictionary. To send properties, include multiple query parameters named \u201cpropertyNames\u201d and \u201cpropertyValues\u201d corresponding to the keys and values in the properties dictionary. Note that the number of propertyNames and propertyValues must be the same. Our REST API\u2019s single event endpoint is: https://api.indicative.com/service/event To send us an event, you\u2019ll need to make a GET request to that URL with a Content-Type of \u2018application/json\u2019. In the GET body, include a JSON object with the following fields: As an example, your GET request would resemble the following: Our servers will respond with one of a few status codes indicating the outcome of your request:",
    "Track a Batch of Events": "To send multiple events at once, you\u2019ll want to use our batch endpoint at: https://api.indicative.com/service/event/batch It works similarly to the single event endpoint \u2014 it requires a POST with Content-Type: \u2018application/json\u2019 and a POST body containing the following: We recommend that your implementation include up to 100 events per batch. All events sent in a batch default to the current time. To override this, you can set the \u201ceventTime\u201d key for each event to a specific Unix timestamp value. As an example, a batch of two events that would be assigned with the current time would resemble the following: The status codes returned by the batch endpoint are the same as the single event endpoint specified above. When sending\u00a0either single or batched\u00a0events, there is a rate\u00a0limit of 250 events per second. If you need a higher rate limit, please contactsupport@mparticle.com.",
    "Identify Users": "Analytics\u2019 Identify endpoint allows customers to update user properties for a specified unique key. User properties that are specified through this endpoint persist until the property value has been overwritten by another call to the Identify or Track Events APIs. Our REST API\u2019s Identify endpoint is: https://api.indicative.com/service/identifyorhttps://api.indicative.com/service/identify/{Your API KEY} To call this method, make a POST request to the above URL with a Content-Type of \u2018application/json\u2019. In the POST body, include a JSON object with the following fields:",
    "Alias Users": "Our REST API\u2019s alias endpoint is: https://api.indicative.com/service/aliasorhttps://api.indicative.com/service/alias/{Your API KEY} For more details on how aliasing works, see more detailed documentationhere. To send us an alias, you\u2019ll need to make a POST request to that URL with a Content-Type of \u2018application/json\u2019. In the POST body, include a JSON object with the following fields:",
    "Suppress User Data": "Analytics\u2019 Suppress endpoint allows customers to suppress future data from being processed for a specified unique key. The REST API\u2019s Suppress endpoint is: https://api.indicative.com/service/suppressorhttps://api.indicative.com/service/suppress/{Your API KEY} To call this method, make a POST request to the above URL with a Content-Type of \u2018application/json\u2019. In the POST body, include a JSON object with the following fields:",
    "Delete User Data": "Analytics\u2019 Delete endpoint allows customers to submit a request to irrevocably delete data for a specified unique key from Analytics. In addition, a call to this endpoint suppresses future data from being processed for a specified unique key. The REST API\u2019s Suppress endpoint is: https://api.indicative.com/service/deleteorhttps://api.indicative.com/service/delete/{Your API KEY} To call this method, make a POST request to the above URL with a Content-Type of \u2018application/json\u2019. In the POST body, include a JSON object with the following fields:",
    "Creating a \u201cDid [Not] Perform\u201d Clause": "To use a Did [not] Perform clause, select an event from the Did [not] Perform data dropdown. This article contains information about modifications that will provide you with more ways to understand your users.",
    "Did vs Did Not Do": "After selecting an event in the Did [not] Perform dropdown, decide whether you want to understand the data of users whodidthis event ordid not dothis event. For example: Show the total count of events where users who did\u00a0Subscribe\u00a0alsodidEmail Clicked\u00a0at least once within the prior 7 days.Show the total count of events where users who did\u00a0Subscribedid not doEmail Clicked\u00a0within the prior 7 days. Note: Switch from \u201cTotal count of\u201d to \u201cUsers who performed\u201d to examine the count of users rather than the count of events, but keep in mind that this is the dropdown that determines whether the query returns an event count or user count. When the query is set to \u201cTotal count of\u201d, users may be counted in the results even though they explicitly meet the did not perform clause criteria. This is because, in \u201cTotal count of\u201d mode, the query may count a user multiple times if they satisfy the query criteria. To illustrate this, consider a query measuring \u201cTotal count of Subscribe for users who did not do Email Clicked within the prior 1 hour\u201d. A user who logs on for the first time at 12pm and performs Subscribe will have their action counted because they did not perform Email Clicked within the prior hour. If the same user then performs Email Clicked at 1:30pm, and then performs Subscribe again at 2pm, their action will not be counted twice because this instance does not meet the criteria, as they did do Email Clicked within the hour prior.",
    "Adjusting Event Count": "After adding an event using the Did [not] Perform clause, you can adjust for the number of times users performed this event, such as greater than or equal to one time, less than five times, or greater than three times. For example, you can view the event count for users who did\u00a0Subscribe, and\u00a0also did\u00a0Email Clicked\u00a0greater than three times within the seven days prior to subscribing.",
    "Adjusting Date Range": "After adjusting the event count, customize the date range. The first menu allows you to select whether the Did [not] Perform\u00a0clause event should have occurred prior to the initial event, subsequent to the initial event, or between two selected calendar dates. When using between, the second menu displays a calendar, allowing you to select a specific date range. When using within the prior/subsequent, the second menu allows you to select a preset or custom count based on minutes, hours, days, weeks, or months.",
    "Multiple \u201cFor\u201d Clauses": "Multiple Did [not] Perform\u00a0clauses may be added by selecting additional events from the Did [not] Perform dropdown. As events are added, you can adjust the event count and date range of each additional\u00a0Did [not] Perform\u00a0clause. When using a relative date range, the date range is always relative to the date of the base event in the query andnotthe other\u00a0Did [not] Perform\u00a0clauses. For example, you could view the event count for users who did\u00a0Subscribe\u00a0for users who also did\u00a0Email Clicked\u00a0two times or more within the prior seven days,andalso did\u00a0Blog View\u00a0three times or more within the prior seven days. This will show users who clicked email links and viewed content within seven days before subscribing.",
    "Difference between Funnel and a Did [not] Perform Clause": "The most common method of analyzing a user journey is to use theFunnel tool. The Funnel tool is specifically designed to measure user journeys, and is highly customizable through features such asConversion Precision,Optional Steps, andTracking Properties. However, user journeys combined with aConversion Limitcan be recreated in aSegmentationquery combined with a did [not] perform clause. However, these two methods can yield different results. Consider the following queries:",
    "Did [not] Perform:": "These two queries are measuring the same user journey - a user who does\u00a0Email Clicked, and then within 24 hours also does\u00a0Subscribe. However, the funnel query shows a considerably lower amount of converted users. This is due to the following: A\u00a0Funnel\u00a0query only counts a user once. Therefore, the funnel tool will only consider thefirstinstance of an event.A\u00a0Segmentation\u00a0query will count users more than once. Therefore, the segmentation tool considersallinstances of an event. Consider the following example: User performs\u00a0Email Clicked\u00a0at 10:00 AM on 2/27User performs\u00a0Email Clicked\u00a0at 13:00 PM on 2/27User performs\u00a0Subscribe\u00a0at 11:00 AM on 2/28 The funnel tool will look at the first instance of\u00a0Email Clicked\u00a0(at 10AM), and see that the user did not complete Subscribe within 24 hours of that instance of\u00a0Email Clicked.\u00a0This user is therefore not counted in the funnel analysis. The segmentation tool will look at all instances of\u00a0Email Clicked, and therefore see that the user performed\u00a0Email Clicked\u00a0at 13PM, and\u00a0Subscribe\u00a0within 24 hours of that event. This user is therefore counted in the segmentation analysis. So, when comparing user journeys in Funnel to user journeys in Segmentation, the user count will always be lower in Funnel due to the fact that the Funnel tool only counts each user once, and looks at the first instance of the initial event.",
    "Open the NBA workflow": "To create a new NBA, navigate toEnrichment > Predictive Attributes, selectNew Predictive Attribute, then chooseBest Offer for Each Customer. This takes you to the NBA canvas, where you will carry out the three main steps that will generate a Next Best Action for your users.",
    "Define the business outcome you want to achieve.": "Give your predictive attribute a name in thePredictive Attribute Namefield. The name you enter will generate the name for this attribute that is added to your user profiles.Click the dropdown in theBusiness Goalsection. For this use case, we will select the optionDrive subscription upgrades.",
    "Identify the users you want to target.": "ClickAdd criteriain theTarget Userssection.In the query builder, select among Event and User Attributes, define their values, and string them together with logical operators to define a targeted user segment.",
    "Define potential offers for your users": "In thePotential Offerssection, give your first potential offer a name.Note: The name of each offer will appear as the predictive attribute value added to the user profile.Using the query builder next to the offer name, define the event that indicates a user has successfully engaged with this offer.SelectAdd offerto continue adding additional offers.You can define up to 10 offers. Here are what the three offers for our subscription upgrade campaign would look like:",
    "Create your Next Best Action": "Once you have completed all three steps, clickCreateto generate your prediction. Predictions can take up to 24 hours to calculate. Once calculated, each user\u2019s optimal offer will be added to their user profile. Predictions are updated weekly. Once NBAs are generated, they function asPredictive Attributesthat are added to theUser Profile.",
    "Step 1. Create a user segment in Analytics": "Click on an individual step within any funnel you have created in Analytics (i.e. a single bar in a bar graph, an individual data point in a line graph, etc.).Select eitherCreate User SegmentorSync User Segmentfrom the context menu:  Enter information to define your new user segment:To create the audience in mParticle, setProfile SynctoOn:  Select the mParticle workspace(s) in which you would like this segment to be available for activation. (Note: The checkbox below the workspace selector is automatically selected. If you unselect it, you will need to create the audience as described in themParticle documentation.)ClickSync & Go to Connect Audience. (If you have pop-ups blocked, allow them for this page.)",
    "Step 2. Activate your segment in mParticle": "Completing the previous step will bring you toAudiences > Real-timein mParticle, where theConnecttab will be displayed:  Click a listed output, or clickConnect Outputto select an output and configuration.",
    "Step 3. Verify your data flow": "Use the method described inVerify Your Audience Connectionto ensure that data is flowing as expected.",
    "Saving to a Dashboard": "When you select Save into Dashboard, a pop-up window will appear. Use the dropdown menu to select the dashboard in which to save your Segmentation query. If you want to save your query to a new dashboard, you can create a dashboard from the save window.",
    "Export users": "To download a list of users from a funnel, click on any data point or table cell within your results. Within the dropdown, select \u201cExplore Users.\u201d If your query contains a series, you may choose to view users from a single data point or from the entire series. \u201cFrom this Point\u201d creates a list of users from only one interval whereas \u201cFrom Entire Series\u201d creates a list of users from all of the intervals within the date range. Once your Users query results have loaded, simply click on the export icon located in the menu bar beneath the query builder, then select \u201cDownload CSV.\u201d Users interoperability is not available for metric visualizations.  To download a list of users from a single point, click on any data point or table cell within your results. Within the dropdown, select \u201cDownload Users in this Point to CSV.\u201d A list containing all users associated with this data point will be emailed to the address connected to your Indicative account. There is no limit on the number of users that can be exported. ",
    "All users who entered the funnel": "To access a list of all users who entered the funnel during the date range, click on or in the circle representing Step A in the results field. Within the dropdown, select \u201cDownload Users in this Point to CSV.\u201d  Alternatively, you may click on Column A in the table at the bottom of the results field. Be sure to select the row labeled \u201cOverall\u201d to view all users who entered the funnel.  If your funnel contains a breakout, you may choose to view users from a single breakout or from the entire step. \u201cFrom Breakout\u201d creates a list of users from only one breakout whereas \u201cFrom Entire Step\u201d creates a list of users from all of the breakouts. ",
    "Users from subsequent steps": "To access a list of all users who completed a step other than Step A, click on or in the circle representing the relevant step in the results field. Then, click to select the relevant path from the table within the Path Insights dropdown. Within the dropdown, select \u201cDownload Users to CSV.\u201d If your funnel contains a breakout, you may choose to view users from a single breakout within the step whereas \u201cFrom Entire Step\u201d creates a list of users from all of the breakouts within the step.  Alternatively, you may click on the relevant column in the table at the bottom of the results field. To access a list of users from only one breakout within the step, select the relevant cell, taking into account which step (represented by a column) and which breakout (represented by a row). To access a list of users from all of the breakouts within the step, select the row labeled \u201cOverall.\u201d ",
    "Users from a single path in a multistep funnel": "To access a list of all users who completed a single path within a multipath funnel, click on or in the circle representing the last step in the results field. Then, click to select the relevant path from the table within the Path Insights dropdown. Within the dropdown, select \u201cDownload Users to CSV.\u201d The relevant path will be highlighted in the results field upon hover.  It is not possible to select a single path within a multipath funnel from within the table at the bottom of the results field.",
    "Export results": "To download your results as a CSV file, simply click on the export icon located in the menu bar beneath the query builder. The download will contain a CSV version of the table at the bottom of the results field.  To download your results as a CSV file in a conversion over time funnel, simply click on the export icon located in the menu bar beneath the query builder, then select a file type. You may choose to export your analysis results as a CSV file or as a PNG image file. CSV exports are limited to the first 1,000 results. ",
    "List view": "You can use the Catalog\u2019s List view to: View a centralized listing of all the data points.Spot data points which are duplicated, inconsistently named, etc.Identify and eliminate unnecessary or redundant data points The list view displays six main categories: Custom EventsScreen ViewsCommerceUser InformationApplication LifecycleConsent ",
    "Date Range, Search, and Filter": "You can filter the list view to display specific data points: Date Range: show data points that have been seen within a selected date range.Search: show data points with a matching name or description.Filter icon: show data points that match the criteria you specify:Input/App Version: show data points that have been seen for the selected inputs/app versions.Environment: show data points that have been seen in thedevorprodenvironments.Channel: show data points that have been seen for the selected channel. Channel is distinct from input and describes how a data point arrived at mParticle. For example, a data point may arrive from the client side, server side, or from a partner feed. Valid channels include:SDKFeedServer to ServerPixel Combine date ranges and filters with your search terms to quickly browse and explore data points. Setting a filter will also clear any current category selection.",
    "Details view": "The details view gives you detailed information on an individual data point, including environments the event has been captured for, and when an event was last seen for each platform. Users withadminaccess can annotate data points in the following ways: Tags: a freeform list of labels you\u2019d like to associate to the data point.External Link: a link to your wiki or any other resource containing documentation about the data pointDescription: a custom text field where you can describe the data point, expected attributes, how it\u2019s used, and any other relevant information.Additional Names: a list of alternate names the data point is known by. For example, legacy names or names from a partner feed.",
    "Data Point Attributes": "Your event data points may include attributes, and the details view shows every attribute name that has ever been seen within the given data point. You can see the total volume received in the last 30 days, when the attribute was last seen, and thedetecteddata type. The supported detection types are: StringString-listBooleanNumberDate-time ",
    "Stats view": "For data points, the stats view shows two important groups of statistics for a selected date: Inputstats show how many instances of the event have been received, by platform and channel.Outputstats show the volume sent to each output, as well as the delta between the number of events received and outgoing messages sent. This delta can be useful for troubleshooting, but note that the difference between volume sent and received usually doesn\u2019t indicate a problem. Expansion of eCommerce events can cause multiple messages to be sent to an output for a single event. Likewise, filtering or an output partners minimum requirements can cause mParticle not to forward every event we receive. ",
    "Audience sharing permissions": "Permissions allow the account which owns an audience to define what data is shared with any receiving accounts. Permissions can be set per-audience for each account in your organization.",
    "Create default audience-level permissions": "From your Settings page, navigate to theAudience Settingstab. From here you can view and edit current default audience-level sharing settings or add new defaults.  Note that any changes to your defaults willonlybe applied to any new audience created. Changing your defaults won\u2019t update existing sharing settings for current audiences.",
    "See which audiences are shared": "You can access all audiences that are shared with you via theShared with metab of theAudiencespage. TheAccesscolumn shows the level of access your account as been granted. ",
    "Create or update a shared audience": "To share an audience or to update the access settings for a shared audience, start by navigating to the mainAudiencesview.Select the audience you would like to change.Click the details icon to view theAudience Details.Click the gear cog next toAccess Levelsto view the access sharing settings.Select an account within your organization to share the audience with by clicking the+button. You can change thepermission levelfor each account via the dropdown menu underShared Level.ClickSaveto save any changes.",
    "Respond to an access request": "Users of an account with \u201cview only\u201d access are able to submit a request for additional access. A notification email is sent to the creator of the audience. If you choose to grant the request you can update the audience permissionsas above. You can also seek clarification from the requester by replying to the email.",
    "Request \u2018usable\u2019 access for a \u2018view only\u2019 audience": "If you have \u201cview only\u201d access for an audience, you can request a different level of access.  To make a request you must provide: The sharing level you are requesting.The date range of the campaign you are proposing.Details of the proposed campaign.Activation details, including the proposed campaign platforms.  The owner of the audience will be automatically notified of your request by email and may request further details by email reply.",
    "Identity-level permissions": "In addition to setting access permission for each audience, you can choose whether to make each identity type available to each account when you share audiences. For example, you can choose to make Google Advertising ID and Apple IDFA available to a particular account, but email unavailable. These settings are at the account level and apply to all audiences shared from the account. These identity filters applyonlywhen the user hasn\u2019t already been seen in the receiving account. Only users with Admin access can manage identity-level permissions. Navigate to yourSettingspage.Navigate to theIdentity Settingstab and scroll to theIdentity Sharingheading. From here you can see how many accounts have been granted permission to receive each identity type, and add new permissions.For each identity type, you can view a list of current permissions and add or update permissions by account.",
    "Table View": "Selecting Table View allows ytable-viewou to view the underlying data of your analysis. Data is broken out along the X-axis by intervals of the selected time interval. Along the Y-axis, data is broken out byquery rowspresent in the query builder, ordered by row number.",
    "Aggregate": "In Table View, you can choose the \u201cAGG\u201d button to view aggregate numbers. This will add an extra column at the far right end of the chart showing aggregate numbers for each row. Count shows you underlying data that is visualized the amount of users for each time period. These are the same values that you see when hovering over data points on the graph.",
    "Average": "In Table View, you can choose the \u201cAVG\u201d button to view averages. This will add an extra column at the far right end of the chart showing the average of the numbers in each row.",
    "Count, Statistics, and Breakout Modes": "There are three modes that can be explored in table view. Count\u00a0mode displays counts of users, broken out by the selected time interval. These are the same values that you see when hovering over data points on the graph.Statistics\u00a0mode shows statistical summaries that are automatically calculated from the raw data. This gives you access to information that can help identify important trends in your data and build out analysis.Breakout\u00a0mode allows you to use the event and user property breakouts as column headers. To use Breakout mode, each query row must have the same event or user property breakout applied. X- and Y-axes are switchable via the double arrow button on the upper left corner of the table.",
    "Line Chart": "Selecting Line Chart will display analysis results as a graphical display of information that changes continuously over time.",
    "Bar Chart": "Selecting Bar Chart will display analysis results as separate sets of vertical bars above each period in the selected time interval.",
    "Stacked Bar Chart": "The Stacked Bar Chart is a visualization option available when an event has beenbroken outby its component event properties or user properties. The option to use a Stacked Bar Chart appears after aGroup By clausehas been added to one or more query rows.",
    "Pie Chart": "The Pie Chart is a visualization option available whenbreaking outa single event by an event property or user property with Full Range as thetime interval.",
    "Combination Line/Bar Chart": "The Pie Chart is the default visualization when performing acalculation. After performing a calculation, selecting either the Line Chart or Bar Chart options will present the same visualization. The bars represent the component events of the calculation, broken out by time interval, and the line represents the results of the calculation across time intervals.",
    "United States Map": "This chart type is used to break out a single event using location-based data; for example, if you wanted to see email opens broken out by state. Currently, this chart type only supports locations within the United States.",
    "Tracing": "The foundation of Observability istracing. As data flows through mParticle, it is ingested, processed, and routed to various services or forwarders in discrete steps. Atracein mParticle\u2019s Observability is a detailed record that connects all of these steps on through a single timeline. Think of a trace as a trip report for your data: it shows where it came from, where it went, and any stops it made along the way. Tracing is provided by default for all data flowing in your development environment. For production data tracing, Observability lets you configure what data is traced through customizableTrace Configurations. When Observability is used in conjunction with tools like System Alerts and Live Stream, it can help you diagnose and troubleshoot issues with your data pipelines.",
    "How does tracing work?": "All data sent through your mParticle development environment is traced by default, and can be viewed on the Trace Activity page. If you want to trace production data, you can do so by creating a customtrace configurationto gain specific insights. If a data flow has an active trace configuration, a call made to one of thesupported mParticle servicesinitiates a unique trace. Each trace is identified by aTrace ID, which you can use to find and view specific details about the data\u2019s journey in the Observability tool. Traces record information such as which input the data originated from, which outputs the data will be sent to after processing, any rules or filters applied to the data, any user MPID\u2019s related to the trace, the types of events included in the data, error codes, and any data plans that will be used to process the data. In addition to these details, the trace details page presents a graphic display of the following stages, known as \u201cspans\u201d, that your data happen to flow through: Data ingest: the initial step of receiving data from one of the mParticle SDKs, a feed, or a warehouse sync pipeline.Processing: the stage in which event batches are received and routed to the next stage.Transformations: the process of applying rules to your data. This stage is executed both when data is ingested into mParticle and when it is forwarded to an output.Identity Resolution: the process of identifying or creating user profiles to attribute user data with.Payload Delivery: the stage in which outbound data is collected and forwarded to one of your connected outputs.",
    "What data can be traced?": "Observability provides end-to-end tracing for all event data, starting from it\u2019s first ingested through via an mParticle platform SDK or server-to-server inputs, to when data is forwarded to any of the real-time event destinations. Specifically, tracing is available for data processed by the following APIs: Events APIIDSync API(Identity Resolution)You can also use Observability to trace data ingested fromWarehouse Syncpipelines. By default, data in your development environment is traced automatically. You don\u2019t need to configure anything, and you can immediately begin reviewing trace details for your development data from Live Stream or the Trace Activity page in Observability. To trace your production data, you must firstcreate a trace configurationthat specifies which data you want traced.",
    "Tracing limitations": "Observability does not currently does not currently support tracing for data sent to mParticle\u2019s bulk forwarding event outputs or audience pipelines.",
    "Observability fair use limitations": "You can trace up to 10% of your annual batch volume sent through your production environment at no extra charge. If you exceed this limit: Your mParticle account representative may contact you to discuss your needs and options.You will not be able to create new trace configurations.New traces will not be generated for any data sent through your existing trace configurations, but any existing traces will remain available until 14 days after they were created.Your development data will still be traced automatically.",
    "How long are traces available?": "Traces for development and production data are accessible for up to 14 days.",
    "How do I configure traces for my production data?": "To learn more about the information provided in a trace and how to create a trace configuration for your production data, continue reading theObservability User Guide.",
    "Tracing best practices": "Observability provides detailed information about how data is ingested, processed, and forwarded through mParticle\u2019s CDP. To make the best use of Observability while keeping your cost low, we recommend the following strategies: 100% of your development data is traced automatically, at no extra cost to you. This makes Observability a powerful testing and troubleshooting tool that you should use when setting up a new mParticle configuration. By sending test data through your development environment, you can gain insights into how your platform or feed inputs are functioning; whether your data plans, rules, and filters are behaving as expected; and whether the data you expect to see in your output is successfully being forwarded. Even after deploying a well tested mParticle configuration to process your production data, it\u2019s still possible to encounter issues. mParticle recommends creating a tracing configuration to monitor new configurations with a high sample rate(up to 100%)but for a short period of time(up to one week). This allows you to detect issues quickly. Remember that you can trace up to 10% of your annual batch volume at no extra charge, but if you reach this limit you will be unable to create new trace configurations. If you do reach this limit, you can contact your mParticle account representative to discuss the unique needs of your business and what additional tracing options may be available. Once a new mParticle configuration has been successfully processing production data for a period of about a week, you can transition to a long-term monitoring approach. mParticle recommends creating long-term tracing configurations(more than one week)with a low sample rate(under 10%). This allows you to still detect and troubleshoot anomalies that may occur in your existing pipelines without causing you to reach yourtracing usage limit. If you do begin to approach your limit, your mParticle account representative will contact you.",
    "Common uses of data privacy controls": "Data privacy controls are flexible and customizable, allowing you to build any data flow or consent-based logic you need. Use mParticle\u2019s data privacy controls to help comply with CCPA\u2019s \u201cdo not sell my data\u201d requirement by collecting users who opt-out and blocking those users\u2019 data from flowing to any \u2018data sale\u2019 output by: Recording a CCPA data sale opt-out as a user consent (more information below)Identify which outputs count as \u2018data sale\u2019 and apply the below forwarding rule to themApplying a forwarding rule of: Do not forward if CCPA Data sale opt out is present GDPR defines consent as one method of lawful data processing. One common setup is to: Define a processing purpose of \u2018marketing\u2019Prompt users for affirmative consent for \u2018marketing\u2019Identify which outputs would perform \u2018analytics\u2019 processingApply a forwarding rule of: Only forward user data if GDPR Consent for \u2018marketing\u2019 is true",
    "Data privacy and the mParticle platform": "Once enabled and configured, data privacy work with the mParticle platform to ingest and pass on consent state: Define categories of data collection called consent purposes.Store the consent state in a user\u2019s profile.Control data flow based on stored consent.Send user consent state to your integrations (outputs). ",
    "Enabling data privacy controls": "Data privacy controls save user consent decisions and applies them to data flows. Enable GDPR and/or CCPA compliance features on your workspace fromWorkspace Settings>Workspace>Regulation.  For GDPR, create a set of purposes fromPrivacy > Privacy Settingsin the dashboard. For CCPA, once it is enabled in your workspace, the purposedata_sale_opt_outis automatically created. The SDKs and mParticle UIs facilitate using this purpose, so you don\u2019t need to hardcode it anywhere. ",
    "Consent properties": "The mParticle format for a single record of a user decision on a privacy prompt,.consent, is ourconsent_stateobject. This is used for both GDPR-style opt-in consent and for CCPA-style opt-out. For each user or workspace, consent state can be stored for each possible combination of regulation and purpose. For each purpose, the following fields are supported. All fields are optional, exceptconsented,timestamp_unixtime_ms,regulationandpurpose. Theregulationandpurposefields are built into the structure. Be sure to include your privacy and compliance experts when deciding how to implement optional fields.",
    "Example consent state": "Consent state can be logged via the HTTP API simply by including a consent state object in a batch, mirroring the structure of the user profile (above):",
    "Collecting consent state": "For detailed definitions of how to report consent state, review the sections of our API and SDK references that cover data privacy controls: Web SDKiOS SDKAndroid SDKAMP SDKHTTP API Additionally, ourintegration with OneTrustallows you to ingest customer consent states into mParticle.",
    "Audiences": "  ",
    "Connections and forwarding rules": "Consent state can be used to create forwarding rules that selectively filter data based on a users consent state, in real time and per-person. For example, you can choose to only forward data from users who have given consent for a particular purpose. For CCPA, you may want a forwarding rule to apply a data sale opt-out. In this example, users\u2019 who have a consent state oftruefor the CCPA purpose ofdata_sale_opt_outwill NOT have their data forwarded (if the consent state is missing or false for that purpose, data will flow): For GDPR, you may want a forwarding rule to only send data when a single purpose is consented: If you set up a Forwarding Rule for an embedded kit integration, the iOS and Android SDKs will check consent status for the user on initialization. If the rule condition fails, the kit will not be initialized. Note that kits are only initialized when a session begins or on user change, so if consent status changes in the course of a session, while mParticle will immediately stop forwarding data to the kit, it is possible that an embedded kit may remain active and independently forwarding data to a partner from the client until the session ends.",
    "Forwarding consent state to partners": "When the consent state of a profile changes, that change can be communicated to mParticle event integrations. If theconsent_stateobject on an incoming event batch contains changes from the existing profile, mParticle adds a \u2018system notification\u2019 to the batch for each consent state change before the batch is sent to incoming forwarders. This notification contains the full old and new consent state objects: There are currently two ways that consent state changes are forwarded to mParticle event integrations: Some partners accept raw event batch data from mParticle, mostly for data storage or custom analytics use cases. For these partners, mParticle will forward the \u2018system_notifications\u2019 object with each relevant event batch. Forwarding of system notifications can be turned off with a user setting. Integrations that can currently receive the system notifications object include:Amazon KinesisAmazon S3Amazon SNSAmazon SQSGoogle Pub/SubGoogle Cloud StorageMicrosoft Azure Event HubsSlackWebhookmParticle is working with other partners to support forwarding consent state changes as a Custom Event. These events contain the new consent state information as custom attributes, a custom event type of\"Consent\", and an event name of\"Consent Given\"or\"Consent Rejected\". These consent events are forwarded to supporting partners as standard custom events.{\"data\":{\"event_name\":\"Consent Given\",\"custom_event_type\":\"Consent\",\"custom_attributes\":{\"consented\":\"true\",\"document\":\"location_collection_agreement_v4\",\"hardware_id\":\"IDFA:a5d934n0-232f-4afc-2e9a-3832d95zc702\",\"purpose\":\"location_collection\",\"location\":\"17 Cherry Tree Lane\",\"regulation\":\"GDPR\",\"timestamp_unixtime_ms\":1523039002083},\"event_type\":\"custom_event\"}}Partners that currently accept these custom consent state events include:AmplitudeSnowplowMixpanelGoogle AnalyticsSalesforce DMP \u201cGDPR Consent Change\u201d is  listed as a data type in theIntegrations directoryand we will update this list as more partners add support. Please reach out to your success manager if you would like to distribute consent to an additional partner.",
    "Data subject requests": "mParticle helps you respond todata subject requestsas mandated by the GDPR and CCPA regulations. You can search for integrations that support data subject requests in theIntegrationspage. Search on categoryData Subject Request.",
    "Ingest GPC signals": "The California Consumer Protection Act (CCPA) and the upcoming CPRA (California Privacy Rights Act) require that users can signal their privacy choices. In support of that requirement, you can ingestGlobal Privacy Control (GPC) signalswith mParticle. Browsers append the GPC signal to HTTP requests and make it queryable via the DOM. This signal is designed to convey a person\u2019s request to websites and services to not sell or share their personal information with third parties, perthe Global Privacy Control specification. This opt-out is at the browser level, allowing users to turn on the GPC signal for all or specific websites. The workflow for ingesting and forwarding GPC signals via SDK or Events API: ",
    "Sample code for GPC": "This sample code show two options: mapping to a GDPR purpose and mapping to a user attribute.",
    "mParticle to Analytics mapping": "",
    "Permissions and access": "Organization admins automatically have access to all portfolios created within an account. However, user access to portfolios is not automatically shared with users who have access to the underlying projects. These users must be invited by another user with access to the portfolio to join.",
    "Impact to Analytics Features": "All existing project-level features and analysis types are available within portfolios with the following exceptions: Events and Properties metadata editing:You cannot edit event or property metadata (descriptions, categories, etc.) within theAnalytics Data Managerin a portfolio. This applies to both event and user properties. You can still view all events, properties, mappings, and autocomplete values, however.User Segments:While you can create new user segments within a portfolio, you do not have access to user segments created in underlying projects.Global Project Filters:Global Filtersset within an underlying project will not be shared with the portfolio.Annotations:Annotationsset within an underlying project will not be editable.",
    "Start using Portfolio Analytics": "Reach out to your customer service representative to enable Portfolio Analytics for your account.",
    "Export results from a users query": "First, you mustcreate a query in the Users tool.Once your query is ready for export, simply click on the export icon located in the menu bar beneath the query builder, and select Download CSV. A CSV file will be emailed to the email associated with your account. There is no limit to the number of users that can be exported. ",
    "Export users from segmentation, funnel, or cohort": "You can also download a user list from a Segmentation series or point, a specific step in your Funnel, or a Cohort cell. To download a user list from Segmentation, you must firstcreate a query.Once your query is ready for export, click into the point or cell from which you wish to download data. A menu will appear. Then, select \u201cDownload Users in this Point to CSV\u201d. A CSV file will then be sent to the email associated with your account. If you wish to download users from an entire series, you must select \u201cExplore Users\u201d -> \u201cFrom Entire Series\u201d from the aforementioned menu. You will then be redirected to our Users tool, from which you can download users according to the instructions above.  To download a user list from Funnel, you must firstcreate a query.Once you have done so, simply click into the funnel step from which you wish to download data. A menu will appear. Then, select \u201cDownload Users to CSV\u201d. A CSV file will then be sent to the email associated with your account.  To download a user list from Cohort, you must firstcreate a query.Once you have done so, simply click into the point or cell from which you wish to download data. A menu will appear. Then, select \u201cDownload Users to CSV\u201d. A CSV file will then be sent to the email associated with your account. ",
    "Inputs, outputs, and connections": "  ",
    "Additional topics": "      You can also visitthe mParticle YouTube channelto view a wide variety of mParticle topics.",
    "Simple": "Use Simple Track By mode when each tracking property applies to each event in the funnel. For example, PetBox may want to track the user journey from Site Visit -> Open App -> Purchase Product tracked by Session ID. You may select up to 3 tracking properties in simple mode. Add a tracking property to your funnel analysis by hovering over the tracking line at the bottom of thequery builder, and selecting an event property from the \u201c+Select a Property\u201d dropdown. Please note that only properties that apply for all events selected in the funnel will appear in this dropdown. Once you have selected a tracking property, run the query for the changes to be applied.  A maximum of three tracking properties may be added to a funnel analysis. By using tracking properties, PetBox is able to ensure that each user journey in this funnel occurs whilst holding User IDandSession ID constant.  PetBox has now ensured that all events in the user journey are completed by the same user, and in the same session. Thus, this funnel analysis is counting user-property pairs, and not just users.",
    "Total count vs. users who performed": "Once you have added a tracking property to your Funnel analysis, you may select whether to return users or counts. This can be toggled to the left of the first event in your query builder.  Total Count ofreturns unique user-property pairs. Thus, a user may be represented in the Funnel more than once, provided that said user completes the funnel with more than one tracking property value.Users who performedreturns unique users. Thus, a user may only be represented in the funnel once. The user will be represented by thefirsttime they perform the events selected in the funnel."
}